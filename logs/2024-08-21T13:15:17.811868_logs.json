[
  {
    "Request": "Given the fields `paper_text`, `guidance`, produce the fields `task_candidates`.\n\n---\n\nFollow the following format.\n\nPaper Text: Full text from research paper PDF\nGuidance: Guidance for extracting task candidates\nTask Candidates: JSON list of task candidate dicts with keys 'name', 'description', and 'relevant_paper_text'. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCandidate\": {\"description\": \"Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\\n\\nA good task candidate:\\n- describes a task that an agent can implement\\n- typically a programming task, e.g. modifying a 'baseline' implementation\\n- which reflects or mirrors actual tasks undertaken to produce the research paper\\n- and requires research and/or engineering expertise to implement correctly\\n- for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\\n- ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\\n- the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\", \"properties\": {\"name\": {\"description\": \"Name of the task\", \"title\": \"Name\", \"type\": \"string\"}, \"description\": {\"description\": \"Description of the task\", \"title\": \"Description\", \"type\": \"string\"}, \"relevant_paper_text\": {\"description\": \"Text from the paper that is relevant to the task\", \"title\": \"Relevant Paper Text\", \"type\": \"string\"}, \"scoring_feasibility\": {\"description\": \"\\nOn a scale of 1-10, how feasible is it to write a python function to objectively score an \\nimplementation of this task, verifying that key functionality is implemented as specified?\\n\", \"title\": \"Scoring Feasibility\", \"type\": \"integer\"}, \"llm_tractability\": {\"description\": \"Probability in (0, 1) that a frontier LLM can generate a correct implementation of this task\", \"title\": \"Llm Tractability\", \"type\": \"number\"}, \"expert_tractability\": {\"description\": \"Probability in (0, 1) that a human subject matter expert can generate a correct implementation of this task\", \"title\": \"Expert Tractability\", \"type\": \"number\"}, \"layman_tractability\": {\"description\": \"Probability in (0, 1) that a layman can generate a correct implementation of this task\", \"title\": \"Layman Tractability\", \"type\": \"number\"}}, \"required\": [\"name\", \"description\", \"relevant_paper_text\", \"scoring_feasibility\", \"llm_tractability\", \"expert_tractability\", \"layman_tractability\"], \"title\": \"TaskCandidate\", \"type\": \"object\"}}, \"properties\": {\"tasks\": {\"description\": \"Tasks extracted from the paper\", \"items\": {\"$ref\": \"#/$defs/TaskCandidate\"}, \"title\": \"Tasks\", \"type\": \"array\"}}, \"required\": [\"tasks\"], \"title\": \"TaskCandidates\", \"type\": \"object\"}\n\n---\n\nPaper Text:\n94% on CIFAR-10 in 3.29 Seconds on a Single GPU\nKeller Jordan\nkjordan4077@gmail.com\nAbstract\nCIFAR-10 is among the most widely used datasets in machine learning, facilitating\nthousands of research projects per year. To accelerate research and reduce the cost\nof experiments, we introduce training methods for CIFAR-10 which reach 94%\naccuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when\nrun on a single NVIDIA A100 GPU. As one factor contributing to these training\nspeeds, we propose a derandomized variant of horizontal flipping augmentation,\nwhich we show improves over the standard method in every case where flipping is\nbeneficial over no flipping at all. Our code is released at https://github.com/\nKellerJordan/cifar10-airbench .\n1 Introduction\nCIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-\ning thousands of research projects per year1. Research can be accelerated and the cost of experiments\nreduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In\nthis paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single\nNVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,\n2023). To support scenarios where higher performance is needed, we additionally develop methods\ntargeting 95% and 96% accuracy. We release the following methods in total.\n1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).\n2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).\n3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).\n4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).\nAll runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-\nmatically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the\nfirst uses torch.compile to improve GPU utilization. It is intended for experiments where many\nnetworks are trained at once in order to amortize the one-time compilation cost. The non-compiled\nairbench94 variant can be easily installed and run using the following command.\n1pip install airbench\n2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"\nOne motivation for the development of these training methods is that they can accelerate the experi-\nmental iteration time of researchers working on compatible projects involving CIFAR-10. Another\nmotivation is that they can decrease the cost of projects involving a massive number of trained\nnetworks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used\n3 million trained networks to demonstrate that the outputs of a trained neural network on a given test\ninput follow an approximately linear function of the vector of binary choices of which examples the\nmodel was trained on. Another example is Jordan (2023), a study on training variance which used\n180 thousand trained networks to show that standard trainings have little variance in performance on\n1https://paperswithcode.com/datasets\nPreprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024\nFigure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal\nflipping augmentation, which flips each image with 50% probability per epoch. This results in some images\nbeing redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a\ndeterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\ntheir test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds\nand 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make\nit possible to replicate these studies, or conduct similar ones, with fewer computational resources.\nFast training also enables the rapid accumulation of statistical significance for subtle hyperparameter\ncomparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10\naccuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation\nbetween runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the\nimprovement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training\nthis will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.\nOur work builds on prior training speed projects. We utilize a modified version of the network,\ninitialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen\npatch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over\nprior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-\nure 1, Section 3.6, Section 5.2).\n2 Background\nOur objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set\nin the shortest possible amount of time. Timing begins when the method is first given access to\ntraining data, and ends when it produces test-set predictions. The method is considered valid if its\nmean accuracy over repeated runs is at least 94%.\nWe chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the\n2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more\nrecent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10\nseconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page\n(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is\ntysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,\n94% is the level of human accuracy reported by Karpathy (2011).\nWe note the following consequences of how the method is timed. First, it is permitted for the program\nto begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins\nwhen the training data is first accessed. This is helpful because otherwise the first run of training\nis typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA\nimproves the performance of a trained network by running it on multiple augmented views of each\ntest input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping\nTTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods\nattain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\nThe CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.\nIt is divided into a training set of 50,000 images and a validation set of 10,000 images. As a\nmatter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was\n2\n80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than\nairbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94\ncan be attributed entirely to algorithmic progress rather than compute scaling.\n3 Methods\n3.1 Network architecture and baseline training\nWe train a convolutional network with a total of 1.97 million parameters, following tysam-code\n(2023) with a few small changes. It contains seven convolutions with the latter six being divided into\nthree blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this\nsection we offer some comments on the main design choices.\nThe network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed\nentirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,\n2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with\nno padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather\nthan the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff\nbetween throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\nFollowing Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,\nand disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is\nscaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture\ndiffers only in that we decrease the number of output channels in the third block from 512 to 256,\nand we add learnable biases to the first convolution.\nAs our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of\n0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the\nmaximum at 20% of the way through training, and then decreases to zero. For data augmentation we\nuse random horizontal flipping alongside 2-pixel random translation. For translation we use reflection\npadding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note\nthat what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a\nrandom 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the\nnetwork is run on both a given test image and its mirror and inferences are made based on the average\nof the two outputs. With optimized choices of learning rate, momentum, and weight decay, this\nbaseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.\n3.2 Frozen patch-whitening initialization\nFollowing Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-\nwhitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code\n(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches\nacross the training distribution, so that their outputs have identity covariance matrix. The second 12\nFigure 2: The first layer\u2019s weights after whitening\ninitialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,\nso that input information is preserved through the\nactivation which follows. Figure 2 shows the result.\nWe do not update this layer\u2019s weights during training.\nDeparting from tysam-code (2023), we add learn-\nable biases to this layer, yielding a small performance\nboost. The biases are trained for 3 epochs, after which\nwe disable their gradient to increase backward-pass\nthroughput, which improves training speed without\nreducing accuracy. We also obtain a slight perfor-\nmance boost relative to tysam-code (2023) by re-\nducing the constant added to the eigenvalues during\ncalculation of the patch-whitening initialization for\nthe purpose of preventing numerical issues in the case\nof a singular patch-covariance matrix.\nPatch-whitening initialization is the single most impactful feature. Adding it to the baseline more\nthan doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.\n3\nRandom reshuffling Alternating flip Mean accuracy\nNo No 93.40%\nNo Yes 93.48%\nYes No 93.92%\nYes Yes 94.01%\nTable 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating\nflip (which we propose) reduce training data redundancy and improve performance.\n3.3 Identity initialization\ndirac : We initialize all convolutions after the first as partial identity transforms. That is, for a\nconvolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an\nidentity transform of the input, and leave the remaining N\u2212Mto their default initialization. In\nPyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight\nwof each convolutional layer. This method partially follows tysam-code (2023), which used a more\ncomplicated scheme where the identity weights are mixed in with the original initialization, which\nwe did not find to be more performant. With this feature added, training attains 94% accuracy in 18\nepochs taking 6.8 A100-seconds.\n3.4 Optimization tricks\nscalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor\nof64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%\nin 13.5 epochs taking 5.1 A100-seconds.\nlookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.\nWe note that Lookahead has also been found effective in prior work on training speed for ResNet-\n18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6\nA100-seconds.\n3.5 Multi-crop evaluation\nmulticrop : To generate predictions, we run the trained network on six augmented views of each\ntest image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a\nversion which is translated down-and-to-the-right by one pixel, and the mirrored versions of all\nthree. Predictions are made using a weighted average of all six outputs, where the two views of the\nuntranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125\neach. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.\nWe note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-\nmonyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of\nevaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using\nmore crops does improve performance, but the increase to inference time outweighs the potential\ntraining speedup.\n3.6 Alternating flip\nTo speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,\nwhich we motivate as follows. When training neural networks, it is standard practice to organize\ntraining into a set of epochs during which every training example is seen exactly once. This\ndiffers from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),\nwhich calls for data to be repeatedly sampled with-replacement from the training set, resulting in\nexamples being potentially seen multiple redundant times within a short window of training. The\nuse of randomly ordered epochs of data for training has a different name, being called the random\nreshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our\ntraining dataset consists of Nunique examples, then sampling data with replacement causes every\n\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N\nunique examples on average. On the other hand, random reshuffling leads to all Nunique examples\nbeing seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason\nthat it is beneficial to maximize the number of unique inputs seen per window of training time.\n4\nWe extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We\nfirst note that standard random horizontal flipping augmentation can be defined as follows.\n1import torch\n2def random_flip ( inputs ):\n3 # Applies random flipping to a batch of images\n4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)\n5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]\nListing 1: Random flip\nIf horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2\nwhich may be seen during training. Potentially, every pair of consecutive epochs could contain every\nunique input. But our main observation is that with standard random horizontal flipping, half of the\nimages will be redundantly flipped the same way during both epochs, so that on average only 1.5N\nunique inputs will be seen.\naltflip : To address this, we propose to modify standard random horizontal flipping augmentation as\nfollows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},\nwe flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we\nflip only those inputs which were flipped in the first epoch. We provide the following implementation\nwhich avoids the need for extra memory by using a pseudorandom function to decide the flips.\n1import torch\n2import hashlib\n3def hash_fn (n, seed =42) :\n4 k = n * seed\n5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,\n16)\n6def alternating_flip (inputs , indices , epoch ):\n7 # Applies alternating flipping to a batch of images\n8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist\n() ])\n9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)\n10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\nListing 2: Alternating flip\nThe result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in\nFigure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.\nAdding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final\ntraining method airbench94.py , the entire contents of which can be found in Section E. It reaches\n94% accuracy in 3.83 seconds on an NVIDIA A100.\n3.7 Compilation\nThe final step we take to speed up training is a non-algorithmic one: we compile our training\nmethod using torch.compile in order to more efficiently utilize the GPU. This results in a training\nscript which is mathematically equivalent (up to small differences in floating point arithmetic) to\nthe non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29\nA100-seconds. The downside is that the one-time compilation process takes up to several minutes to\ncomplete before training runs can begin, so that it is only beneficial when we plan to execute many\nruns of training at once. We release this version as airbench94_compiled.py .\n4 95% and 96% targets\nTo address scenarios where somewhat higher performance is desired, we additionally develop methods\ntargeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .\nTo attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel\ncount of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce\nthe learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%\naccuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.\n2Assuming none of the training inputs are already mirrors of each other.\n5\nFigure 4: Training speedups accumulate additively. Removing individual features from airbench94\nincreases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces\nthe epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-\n94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.\nFigure 3: FLOPs vs. error rate tradeoff. Our\nthree training methods apparently follow a linear\nlog-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-\nVries & Taylor, 2017) augmentation and raise the\ntraining epochs to 40. We add a third convolution to\neach block, and scale the first block to 128 channels\nand the second two to 512. We also add a residual\nconnection across the later two convolutions of each\nblock, which we find is still beneficial despite the fact\nthat we are already using identity initialization (Sec-\ntion 3.3) to ease gradient flow. Finally, we reduce\nthe learning rate by a factor of 0.78. These changes\nyield airbench96 which attains 96.05% accuracy in\n46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.\nFigure 3 shows the FLOPs and error rate of each of\nour three training methods.\n5 Experiments\n5.1 Interaction between features\nTo gain a better sense of the impact of each feature on training speed, we compare two quantities.\nFirst, we measure the number of epochs that can be saved by adding the feature to the whitened\nbaseline (Section 3.2). Second, we measure the number of epochs that must be added when the\nfeature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-\ntion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it\nfrom the final airbench94 increases epochs-to-94% from 9.9 to 12.8.\nFigure 4 shows both quantities for each feature. Surprisingly, we find that for all features except\nmulti-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even\nthough the whitened baseline requires more than twice as many epochs as the final configuration.\nThis indicates that the interaction between most features is additive rather than multiplicative.\n5.2 Does alternating flip generalize?\nIn this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of\ntraining configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all\ncases except those where neither alternating nor random flip improve over using no flipping at all.\nFor CIFAR-10 we consider the performance boost given by alternating flip across the follow-\ning 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and\nairbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For\n6\nFigure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,\nthe use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%\ntraining speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.\n95% confidence intervals are shown around each point.\neach configuration we compare the performance of alternating and random flip in terms of their mean\naccuracy across n= 400 runs of training.\nFigure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip\nimproves performance in every setting. To get a sense for how big the improvement is, we estimate\nthe effective speedup for each case, i.e., the fraction of epochs that could be saved by switching\nfrom random to alternating flip while maintaining the level of accuracy of random flip. We begin\nby fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of\neach random flip-based training configuration. We use these curves to calculate the effective speedup\nafforded by switching from random to alternating flip. For example, airbench94 with random flip\nand without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The\nsame configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law\nfit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.\nNote that using a power-law yields a more conservative estimate relative to using linear interpolation\nbetween the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.\nTable 2 shows the result. We observe the following patterns. First, the addition of extra augmentation\n(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that\nthe main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the\nsame way for many epochs in a row; we speculate that adding extra augmentation reduces the negative\nimpact of these cases because it increases data diversity. Next, TTA reduces the gap between random\nand alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),\nindicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training\nfor longer consistently increases the effective speedup given by alternating flip.\nWe next study ImageNet trainings with the following experiment. We train a ResNet-18 with a\nvariety of train and test crops, comparing three flipping options: alternating flip, random flip, and no\nflipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192\ncenter crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)\nto denote the latter. We also consider two training crops: 192x192 inception-style random resized\ncrop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area\nranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the\nshorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write\n7\nBaseline Cutout Epochs Speedup Speedup (w/ TTA)\nairbench94 No 10 15.0% 5.30%\nairbench94 No 20 27.1% 21.3%\nairbench94 No 40 38.3% 36.4%\nairbench94 No 80 102% 31.8%\nairbench94 Yes 10 3.84% 1.13%\nairbench94 Yes 20 7.42% 2.00%\nairbench94 Yes 40 18.6% 9.28%\nairbench94 Yes 80 29.2% 14.25%\nairbench96 Yes 10 4.94% 1.11%\nairbench96 Yes 20 8.99% 3.58%\nairbench96 Yes 40 17.2% 6.48%\nairbench96 Yes 80 18.8% Not measured\nTable 2: Effective speedups given by switching from random flip to alternating flip. The two configurations\nmost closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw\naccuracy values of the airbench94 experiments.\nFlipping augmentation option\nTrain crop Test crop Epochs TTA None Random Alternating\nHeavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28\nHeavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28\nLight RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26\nLight RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26\nHeavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28\nHeavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28\nLight RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26\nLight RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26\nLight RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16\nHeavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4\nTable 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for\nthose trainings where random flip improves significantly over not flipping at all. The single best flipping option\nin each row is bolded when the difference is statistically significant.\nHeavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided\nin Section C.\nTable 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC\nis better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly\nbetter when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test\nresolution discrepancy (Touvron et al., 2019).\nFor trainings which use Light RRC, we find that switching from random flip to alternating flip\nprovides a substantial boost to performance, amounting to a training speedup of more than 25%.\nIn Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to\nalternating flip improves performance by more than increasing the training duration from 16 to 20\nepochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our\nresults on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit\nfrom alternating flip. Indeed, even turning flipping off completely does not significantly reduce the\nperformance of these trainings. We conclude that alternating flip improves over random flip for every\ntraining scenario where the latter improves over no flipping at all.\n8\nEpochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE\n1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312\n2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233\n1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269\n1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533\n2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433\n1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444\nTable 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).\n5.3 Variance and class-wise calibration\nPrevious sections have focused on understanding what factors affect the first moment of accuracy\n(the mean). In this section we investigate the second moment, finding that TTA reduces variance at\nthe cost of calibration.\nOur experiment is to execute 10,000 runs of airbench94 training with several hyperparameter\nsettings. For each setting we report both the variance in test-set accuracy as well as an estimate of the\ndistribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.\nTable 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set\nvariance, replicating the main finding of Jordan (2023). This is a surprising result because these\ntrainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7\nas much distribution-wise variance when run for a similar duration, and reached a low variance only\nwhen run for 64 epochs. We conclude from this comparison that distribution-wise variance is more\nstrongly connected to the rate of convergence of a training rather than its duration as such. We also\nnote that the low distribution-wise variance of airbench94 indicates it has high training stability.\nUsing TTA significantly reduces the test-set variance, such that all three settings with TTA have lower\ntest-set variance than any setting without TTA. However, test-set variance is implied by the class-wise\ncalibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this\nreduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,\nwe compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which\nmeasures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA\nhas a higher CACE than every setting without TTA, confirming the hypothesis.\n6 Discussion\nIn this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7\nfaster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the\nairbench Python package.\nWe developed airbench solely with the goal of maximizing training speed on CIFAR-10. In\nSection B we find that it also generalizes well to other tasks. For example, without any extra tuning,\nairbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.\nOne factor contributing to the training speed of airbench was our finding that training can be accel-\nerated by partially derandomizing the standard random horizontal flipping augmentation, resulting in\nthe variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating\nflip improves the performance of every training we considered (Section 5.2), with the exception of\nthose trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to\nus, the standard ImageNet trainings that we considered do not significantly benefit from horizontal\nflipping. Future work might investigate whether it is possible to obtain derandomized improvements\nto other augmentations besides horizontal flip.\nThe methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,\nwith fixed performance and hardware constraints. These constraints mean that we cannot improve\nperformance by simply scaling up the amount of computational resources used; instead we are forced\nto develop new methods like the alternating flip. We look forward to seeing what other new methods\nfuture work discovers to push training speed further.\n9\nReferences\nDimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.\nDan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-\nperformance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,\n2011.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\nmark and competition. 2017.\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet\nor cifar-10. arXiv preprint arXiv:1810.03505 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\npp. 248\u2013255. IEEE, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552 , 2017.\nMert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic\ngradient descent. Mathematical Programming , 186:49\u201384, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 , 2016.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.\npmlr, 2015.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\nsgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.\nKeller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and\ninevitable. arXiv preprint arXiv:2304.01910 , 2023.\nAndrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.\nio/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for\nadvanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT\nLicense.\nGuillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander\nM \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,\nBenjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.\nBenchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural\nInformation Processing Systems , 35:25404\u201325421, 2022.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.\nDavid Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/\nhow-to-train-your-resnet-8-bag-of-tricks/ .\n10\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics , 22(3):400\u2013407, 1951.\nSantiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:\n//github.com/apple/ml-cifar-10-faster , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.\ntysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/\nhlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. Advances in neural information processing systems , 32, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,\n2016.\nMichael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps\nforward, 1 step back. Advances in neural information processing systems , 32, 2019.\n11\nA Network architecture\n1from torch import nn\n2\n3class Flatten (nn. Module ):\n4 def forward (self , x):\n5 return x. view (x. size (0) , -1)\n6\n7class Mul (nn. Module ):\n8 def __init__ (self , scale ):\n9 super (). __init__ ()\n10 self . scale = scale\n11 def forward (self , x):\n12 return x * self . scale\n13\n14def conv (ch_in , ch_out ):\n15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,\n16 padding =\u2019same \u2019, bias = False )\n17\n18def make_net ():\n19 act = lambda : nn. GELU ()\n20 bn = lambda ch: nn. BatchNorm2d (ch)\n21 return nn. Sequential (\n22 nn. Sequential (\n23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),\n24 act () ,\n25 ),\n26 nn. Sequential (\n27 conv (24 , 64) ,\n28 nn. MaxPool2d (2) ,\n29 bn (64) , act () ,\n30 conv (64 , 64) ,\n31 bn (64) , act () ,\n32 ),\n33 nn. Sequential (\n34 conv (64 , 256) ,\n35 nn. MaxPool2d (2) ,\n36 bn (256) , act () ,\n37 conv (256 , 256) ,\n38 bn (256) , act () ,\n39 ),\n40 nn. Sequential (\n41 conv (256 , 256) ,\n42 nn. MaxPool2d (2) ,\n43 bn (256) , act () ,\n44 conv (256 , 256) ,\n45 bn (256) , act () ,\n46 ),\n47 nn. MaxPool2d (3) ,\n48 Flatten () ,\n49 nn. Linear (256 , 10, bias = False ),\n50 Mul (1/9) ,\n51 )\nListing 3: Minimal PyTorch code for the network architecture used by airbench94 .\nWe note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock\ntime. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel\nsize of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this\ndoes not improve the wallclock training time on an A100. Reducing the batch size is another easy\nway to save FLOPs but not wallclock time.\n12\nDataset Flipping? Cutout? ResNet-18 airbench96\nCIFAR-10 Yes No 95.55% 95.61%\nCIFAR-10 Yes Yes 96.01% 96.05%\nCIFAR-100 Yes No 77.54% 79.27%\nCIFAR-100 Yes Yes 78.04% 79.76%\nCINIC-10 Yes No 87.58% 87.78%\nCINIC-10 Yes Yes not measured 88.22%\nSVHN No No 97.35% 97.38%\nSVHN No Yes not measured 97.64%\nTable 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We\ndirectly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off\nflipping for SVHN).\nB Extra dataset experiments\nWe developed airbench with the singular goal of maximizing training speed on CIFAR-10. To\nfind out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its\nperformance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-\n10 (Darlow et al., 2018).\nOn CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both\nthe case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not\n(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than\nResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say\nthat it generalizes.\nWe compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.\nWe do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal\non CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better\nperformance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where\nairbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is\nused and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows\nstrong generalization to other tasks.\nWe note that this comparison between airbench96 and ResNet-18 training is fair in the sense that\nit does demonstrate that the former has good generalization, but unfair in the sense that it does not\nindicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time\naugmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training\nwould outperform airbench96 if it were run using test-time augmentation. However, it also takes\n5-10 times longer to complete. The decision of which to use may be situational.\nThe accuracy values we report for ResNet-18 training are from the following sources. We tried\nto select the highest values we could find for each setting. Moreau et al. (2022) reports attaining\n95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports\nattaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on\nCIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.\nC ImageNet training details\nOur ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/\nlibffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and\nmomentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution\n160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of\ntraining. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.\n13\nHyperparameters Flipping augmentation option\nEpochs Cutout TTA None Random Alternating\n10 No No 92.3053 93.0988 93.2798\n20 No No 92.8166 93.7446 93.8652\n40 No No 93.0143 94.0133 94.0729\n80 No No 93.0612 94.1169 94.1628\n10 No Yes 93.4071 93.9488 94.0186\n20 No Yes 93.8528 94.5565 94.6530\n40 No Yes 94.0381 94.7803 94.8203\n80 No Yes 94.0638 94.8506 94.8676\n10 Yes No 91.8487 92.0402 92.1374\n20 Yes No 92.8474 93.3825 93.4876\n40 Yes No 93.2675 94.1014 94.1952\n80 Yes No 93.4193 94.4311 94.5204\n10 Yes Yes 92.6455 92.7780 92.8103\n20 Yes Yes 93.7862 94.1306 94.1670\n40 Yes Yes 94.3090 94.8511 94.8960\n80 Yes Yes 94.5253 95.1839 95.2362\nTable 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a\nmean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise\ndifference in means is statistically significant.\nD Extra tables & figures\nFigure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in\nSection 5.3.\nE Complete training code\n1\"\"\"\n2airbench94 .py\n33.83 s runtime on an A100 ; 0.36 PFLOPs .\n4Evidence for validity : 94.01 average accuracy in n =1000 runs .\n5\n6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB\nwith the following nvidia -smi:\n7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :\n11.7\n14\n8torch . __version__ == \u20192.1.2+ cu118 \u2019\n9\"\"\"\n10\n11# ############################################\n12# Setup / Hyperparameters #\n13# ############################################\n14\n15import os\n16import sys\n17import uuid\n18from math import ceil\n19\n20import torch\n21from torch import nn\n22import torch .nn. functional as F\n23import torchvision\n24import torchvision . transforms as T\n25\n26torch . backends . cudnn . benchmark = True\n27\n28\"\"\"\n29We express the main training hyperparameters ( batch size , learning\nrate , momentum , and weight decay ) in decoupled form , so that each\none can be tuned independently . This accomplishes the following :\n30* Assuming time - constant gradients , the average step size is decoupled\nfrom everything but the lr.\n31* The size of the weight decay update is decoupled from everything but\nthe wd.\n32In constrast , normally when we increase the ( Nesterov ) momentum , this\nalso scales up the step size proportionally to 1 + 1 / (1 -\nmomentum ), meaning we cannot change momentum without having to re -\ntune the learning rate . Similarly , normally when we increase the\nlearning rate this also increases the size of the weight decay ,\nrequiring a proportional decrease in the wd to maintain the same\ndecay strength .\n33\n34The practical impact is that hyperparameter tuning is faster , since\nthis parametrization allows each one to be tuned independently .\nSee https :// myrtle .ai/ learn /how -to -train -your - resnet -5-\nhyperparameters /.\n35\"\"\"\n36\n37hyp = {\n38 \u2019opt \u2019: {\n39 \u2019 train_epochs \u2019: 9.9 ,\n40 \u2019batch_size \u2019: 1024 ,\n41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples\n42 \u2019momentum \u2019: 0.85 ,\n43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (\ndecoupled from learning rate )\n44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not\nweight decay ) for BatchNorm biases\n45 \u2019 label_smoothing \u2019: 0.2 ,\n46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the\nwhitening layer bias before freezing\n47 },\n48 \u2019aug \u2019: {\n49 \u2019flip \u2019: True ,\n50 \u2019translate \u2019: 2,\n51 },\n52 \u2019net \u2019: {\n53 \u2019widths \u2019: {\n54 \u2019block1 \u2019: 64,\n55 \u2019block2 \u2019: 256 ,\n56 \u2019block3 \u2019: 256 ,\n15\n57 },\n58 \u2019 batchnorm_momentum \u2019: 0.6 ,\n59 \u2019 scaling_factor \u2019: 1/9 ,\n60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :\n0= none , 1= mirror , 2= mirror + translate\n61 },\n62}\n63\n64# ############################################\n65# DataLoader #\n66# ############################################\n67\n68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )\n69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )\n70\n71def batch_flip_lr ( inputs ):\n72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .\nview (-1, 1, 1, 1)\n73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\n74\n75def batch_crop (images , crop_size ):\n76 r = ( images . size ( -1) - crop_size )//2\n77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =\nimages . device )\n78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),\ndevice = images .device , dtype = images . dtype )\n79 # The two cropping methods in this if - else produce equivalent\nresults , but the second is faster for r > 2.\n80 if r <= 2:\n81 for sy in range (-r, r +1):\n82 for sx in range (-r, r +1):\n83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)\n84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size\n, r+sx:r+sx+ crop_size ]\n85 else :\n86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size\n+2* r), device = images .device , dtype = images . dtype )\n87 for s in range (-r, r +1):\n88 mask = ( shifts [:, 0] == s)\n89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]\n90 for s in range (-r, r +1):\n91 mask = ( shifts [:, 1] == s)\n92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+\ncrop_size ]\n93 return images_out\n94\n95class CifarLoader :\n96 \"\"\"\n97 GPU - accelerated dataloader for CIFAR -10 which implements\nalternating flip augmentation .\n98 \"\"\"\n99\n100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,\ndrop_last =None , shuffle =None , gpu =0):\n101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .\npt \u2019)\n102 if not os. path . exists ( data_path ):\n103 dset = torchvision . datasets . CIFAR10 (path , download =True ,\ntrain = train )\n104 images = torch . tensor ( dset . data )\n105 labels = torch . tensor ( dset . targets )\n106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:\ndset . classes }, data_path )\n107\n108 data = torch . load ( data_path , map_location = torch . device (gpu))\n16\n109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [\n\u2019labels \u2019], data [\u2019classes \u2019]\n110 # It \u2019s faster to load + process uint8 data than to load\npreprocessed fp16 data\n111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).\nto( memory_format = torch . channels_last )\n112\n113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )\n114 self . proc_images = {} # Saved results of image processing to\nbe done on the first epoch\n115 self . epoch = 0\n116\n117 self . aug = aug or {}\n118 for k in self .aug . keys ():\n119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019\n% k\n120\n121 self . batch_size = batch_size\n122 self . drop_last = train if drop_last is None else drop_last\n123 self . shuffle = train if shuffle is None else shuffle\n124\n125 def __len__ ( self ):\n126 return len( self . images )// self . batch_size if self . drop_last\nelse ceil (len( self . images )/ self . batch_size )\n127\n128 def __iter__ ( self ):\n129\n130 if self . epoch == 0:\n131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .\nimages )\n132 # Randomly flip all images on the first epoch as according\nto definition of alternating flip\n133 if self .aug.get(\u2019flip \u2019, False ):\n134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (\nimages )\n135 # Pre -pad images to save time when doing random\ntranslation\n136 pad = self .aug.get (\u2019translate \u2019, 0)\n137 if pad > 0:\n138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019\nreflect \u2019)\n139\n140 if self .aug.get(\u2019translate \u2019, 0) > 0:\n141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .\nshape [ -2])\n142 elif self .aug.get (\u2019flip \u2019, False ):\n143 images = self . proc_images [\u2019flip \u2019]\n144 else :\n145 images = self . proc_images [\u2019norm \u2019]\n146 if self .aug.get(\u2019flip \u2019, False ):\n147 if self . epoch % 2 == 1:\n148 images = images . flip ( -1)\n149\n150 self . epoch += 1\n151\n152 indices = ( torch . randperm if self . shuffle else torch . arange )(\nlen ( images ), device = images . device )\n153 for i in range (len ( self )):\n154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]\n155 yield ( images [ idxs ], self . labels [ idxs ])\n156\n157# ############################################\n158# Network Components #\n159# ############################################\n160\n17\n161class Flatten (nn. Module ):\n162 def forward (self , x):\n163 return x. view (x. size (0) , -1)\n164\n165class Mul (nn. Module ):\n166 def __init__ (self , scale ):\n167 super (). __init__ ()\n168 self . scale = scale\n169 def forward (self , x):\n170 return x * self . scale\n171\n172class BatchNorm (nn. BatchNorm2d ):\n173 def __init__ (self , num_features , momentum , eps =1e -12 ,\n174 weight =False , bias = True ):\n175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )\n176 self . weight . requires_grad = weight\n177 self . bias . requires_grad = bias\n178 # Note that PyTorch already initializes the weights to one and\nbiases to zero\n179\n180class Conv (nn. Conv2d ):\n181 def __init__ (self , in_channels , out_channels , kernel_size =3,\npadding =\u2019same \u2019, bias = False ):\n182 super (). __init__ ( in_channels , out_channels , kernel_size =\nkernel_size , padding = padding , bias = bias )\n183\n184 def reset_parameters ( self ):\n185 super (). reset_parameters ()\n186 if self . bias is not None :\n187 self . bias . data . zero_ ()\n188 w = self . weight . data\n189 torch .nn. init . dirac_ (w[:w. size (1) ])\n190\n191class ConvGroup (nn. Module ):\n192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):\n193 super (). __init__ ()\n194 self . conv1 = Conv ( channels_in , channels_out )\n195 self . pool = nn. MaxPool2d (2)\n196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )\n197 self . conv2 = Conv ( channels_out , channels_out )\n198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )\n199 self . activ = nn. GELU ()\n200\n201 def forward (self , x):\n202 x = self . conv1 (x)\n203 x = self . pool (x)\n204 x = self . norm1 (x)\n205 x = self . activ (x)\n206 x = self . conv2 (x)\n207 x = self . norm2 (x)\n208 x = self . activ (x)\n209 return x\n210\n211# ############################################\n212# Network Definition #\n213# ############################################\n214\n215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019\n][ \u2019 batchnorm_momentum \u2019]):\n216 whiten_kernel_size = 2\n217 whiten_width = 2 * 3 * whiten_kernel_size **2\n218 net = nn. Sequential (\n219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True\n),\n220 nn. GELU () ,\n18\n221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],\nbatchnorm_momentum ),\n222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],\nbatchnorm_momentum ),\n223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],\nbatchnorm_momentum ),\n224 nn. MaxPool2d (3) ,\n225 Flatten () ,\n226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),\n227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,\n228 )\n229 net [0]. weight . requires_grad = False\n230 net = net. half (). cuda ()\n231 net = net.to( memory_format = torch . channels_last )\n232 for mod in net. modules ():\n233 if isinstance (mod , BatchNorm ):\n234 mod . float ()\n235 return net\n236\n237# ############################################\n238# Whitening Conv Initialization #\n239# ############################################\n240\n241def get_patches (x, patch_shape ):\n242 c, (h, w) = x. shape [1] , patch_shape\n243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h\n,w). float ()\n244\n245def get_whitening_parameters ( patches ):\n246 n,c,h,w = patches . shape\n247 patches_flat = patches . view (n, -1)\n248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n\n249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance\n, UPLO =\u2019U\u2019)\n250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.\nreshape (c*h*w,c,h,w). flip (0)\n251\n252def init_whitening_conv (layer , train_set , eps =5e -4):\n253 patches = get_patches ( train_set , patch_shape = layer . weight . data .\nshape [2:])\n254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )\n255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)\n256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -\neigenvectors_scaled ))\n257\n258# ###########################################\n259# Lookahead #\n260# ###########################################\n261\n262class LookaheadState :\n263 def __init__ (self , net):\n264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().\nitems ()}\n265\n266 def update (self , net , decay ):\n267 for ema_param , net_param in zip( self . net_ema . values () , net.\nstate_dict (). values ()):\n268 if net_param . dtype in ( torch .half , torch . float ):\n269 ema_param . lerp_ ( net_param , 1- decay )\n270 net_param . copy_ ( ema_param )\n271\n272# ###########################################\n273# Logging #\n274# ###########################################\n275\n19\n276def print_columns ( columns_list , is_head =False , is_final_entry = False ):\n277 print_string = \u2019\u2019\n278 for col in columns_list :\n279 print_string += \u2019| %s \u2019 % col\n280 print_string += \u2019|\u2019\n281 if is_head :\n282 print (\u2019-\u2019*len( print_string ))\n283 print ( print_string )\n284 if is_head or is_final_entry :\n285 print (\u2019-\u2019*len( print_string ))\n286\n287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,\n\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]\n288def print_training_details ( variables , is_final_entry ):\n289 formatted = []\n290 for col in logging_columns_list :\n291 var = variables .get (col. strip () , None )\n292 if type (var) in (int , str):\n293 res = str(var)\n294 elif type (var) is float :\n295 res = \u2019{:0.4 f}\u2019. format ( var)\n296 else :\n297 assert var is None\n298 res = \u2019\u2019\n299 formatted . append (res. rjust (len (col)))\n300 print_columns ( formatted , is_final_entry = is_final_entry )\n301\n302# ###########################################\n303# Evaluation #\n304# ###########################################\n305\n306def infer (model , loader , tta_level =0) :\n307 \"\"\"\n308 Test - time augmentation strategy (for tta_level =2):\n309 1. Flip / mirror the image left -to - right (50% of the time ).\n310 2. Translate the image by one pixel either up -and - left or down -and\n- right (50% of the time , i.e. both happen 25% of the time ).\n311\n312 This creates 6 views per image ( left / right times the two\ntranslations and no - translation ), which we evaluate and then\nweight according to the given probabilities .\n313 \"\"\"\n314\n315 def infer_basic (inputs , net):\n316 return net( inputs ). clone ()\n317\n318 def infer_mirror (inputs , net):\n319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))\n320\n321 def infer_mirror_translate ( inputs , net ):\n322 logits = infer_mirror (inputs , net )\n323 pad = 1\n324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)\n325 inputs_translate_list = [\n326 padded_inputs [:, :, 0:32 , 0:32] ,\n327 padded_inputs [:, :, 2:34 , 2:34] ,\n328 ]\n329 logits_translate_list = [ infer_mirror ( inputs_translate , net)\n330 for inputs_translate in\ninputs_translate_list ]\n331 logits_translate = torch . stack ( logits_translate_list ). mean (0)\n332 return 0.5 * logits + 0.5 * logits_translate\n333\n334 model . eval ()\n335 test_images = loader . normalize ( loader . images )\n20\n336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][\ntta_level ]\n337 with torch . no_grad ():\n338 return torch .cat ([ infer_fn (inputs , model ) for inputs in\ntest_images . split (2000) ])\n339\n340def evaluate (model , loader , tta_level =0) :\n341 logits = infer (model , loader , tta_level )\n342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()\n343\n344# ###########################################\n345# Training #\n346# ###########################################\n347\n348def main (run):\n349\n350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]\n351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]\n352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]\n353 # Assuming gradients are constant in time , for Nesterov momentum ,\nthe below ratio is how much larger the default steps will be than\nthe underlying per - example gradients . We divide the learning rate\nby this ratio in order to ensure steps are the same scale as\ngradients , regardless of the choice of momentum .\n354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))\n355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning\nrate for PyTorch SGD\n356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale\n357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]\n358\n359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019\nlabel_smoothing \u2019], reduction =\u2019none \u2019)\n360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)\n361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =\nbatch_size , aug=hyp[\u2019aug \u2019])\n362 if run == \u2019warmup \u2019:\n363 # The only purpose of the first run is to warmup , so we can\nuse dummy data\n364 train_loader . labels = torch . randint (0, 10, size =( len (\ntrain_loader . labels ) ,), device = train_loader . labels . device )\n365 total_train_steps = ceil (len( train_loader ) * epochs )\n366\n367 model = make_net ()\n368 current_steps = 0\n369\n370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in\nk and p. requires_grad ]\n371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019\nnot in k and p. requires_grad ]\n372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,\nweight_decay =wd/ lr_biases ),\n373 dict ( params = other_params , lr=lr , weight_decay =wd/\nlr)]\n374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,\nnesterov = True )\n375\n376 def triangle (steps , start =0, end =0, peak =0.5) :\n377 xp = torch . tensor ([0 , int( peak * steps ), steps ])\n378 fp = torch . tensor ([ start , 1, end ])\n379 x = torch . arange (1+ steps )\n380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])\n381 b = fp [: -1] - (m * xp [: -1])\n382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1\n383 indices = torch . clamp ( indices , 0, len(m) - 1)\n384 return m[ indices ] * x + b[ indices ]\n21\n385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,\npeak =0.23)\n386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:\nlr_schedule [i])\n387\n388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /\ntotal_train_steps )**3\n389 lookahead_state = LookaheadState ( model )\n390\n391 # For accurately timing GPU code\n392 starter = torch . cuda . Event ( enable_timing = True )\n393 ender = torch . cuda . Event ( enable_timing = True )\n394 total_time_seconds = 0.0\n395\n396 # Initialize the first layer using statistics of training images\n397 starter . record ()\n398 train_images = train_loader . normalize ( train_loader . images [:5000])\n399 init_whitening_conv ( model [0] , train_images )\n400 ender . record ()\n401 torch . cuda . synchronize ()\n402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n403\n404 for epoch in range ( ceil ( epochs )):\n405\n406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019\nwhiten_bias_epochs \u2019])\n407\n408 # ###################\n409 # Training #\n410 # ###################\n411\n412 starter . record ()\n413\n414 model . train ()\n415 for inputs , labels in train_loader :\n416\n417 outputs = model ( inputs )\n418 loss = loss_fn ( outputs , labels ).sum ()\n419 optimizer . zero_grad ( set_to_none = True )\n420 loss . backward ()\n421 optimizer . step ()\n422 scheduler . step ()\n423\n424 current_steps += 1\n425\n426 if current_steps % 5 == 0:\n427 lookahead_state . update (model , decay = alpha_schedule [\ncurrent_steps ]. item ())\n428\n429 if current_steps >= total_train_steps :\n430 if lookahead_state is not None :\n431 lookahead_state . update (model , decay =1.0)\n432 break\n433\n434 ender . record ()\n435 torch . cuda . synchronize ()\n436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n437\n438 # ###################\n439 # Evaluation #\n440 # ###################\n441\n442 # Print the accuracy and loss from the last training batch of\nthe epoch\n22\n443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().\nmean (). item ()\n444 train_loss = loss . item () / batch_size\n445 val_acc = evaluate (model , test_loader , tta_level =0)\n446 print_training_details ( locals () , is_final_entry = False )\n447 run = None # Only print the run number once\n448\n449 # ###################\n450 # TTA Evaluation #\n451 # ###################\n452\n453 starter . record ()\n454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019\ntta_level \u2019])\n455 ender . record ()\n456 torch . cuda . synchronize ()\n457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n458\n459 epoch = \u2019eval \u2019\n460 print_training_details ( locals () , is_final_entry = True )\n461\n462 return tta_val_acc\n463\n464if __name__ == \" __main__ \":\n465 with open (sys. argv [0]) as f:\n466 code = f. read ()\n467\n468 print_columns ( logging_columns_list , is_head = True )\n469 main (\u2019warmup \u2019)\n470 accs = torch . tensor ([ main (run) for run in range (25) ])\n471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))\n472\n473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }\n474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))\n475 os. makedirs ( log_dir , exist_ok = True )\n476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)\n477 print (os. path . abspath ( log_path ))\n478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))\nListing 4: airbench94.py\n23\nGuidance:"
  },
  {
    "Response": "Here are some potential task candidates extracted from the paper:\n\n{\n  \"tasks\": [\n    {\n      \"name\": \"Implement alternating flip data augmentation\",\n      \"description\": \"Implement the proposed alternating flip data augmentation method, which deterministically flips images in alternating epochs after the first epoch to avoid redundancy and speed up training.\",\n      \"relevant_paper_text\": \"To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\",\n      \"scoring_feasibility\": 9,\n      \"llm_tractability\": 0.8,\n      \"expert_tractability\": 0.95,\n      \"layman_tractability\": 0.3\n    },\n    {\n      \"name\": \"Implement patch-whitening initialization\",\n      \"description\": \"Implement the patch-whitening initialization method for the first convolutional layer, which initializes the filters based on the eigenvectors of the covariance matrix of patches from the training data.\",\n      \"relevant_paper_text\": \"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\",\n      \"scoring_feasibility\": 8,\n      \"llm_tractability\": 0.6,\n      \"expert_tractability\": 0.9,\n      \"layman_tractability\": 0.1\n    },\n    {\n      \"name\": \"Implement multi-crop test-time augmentation\",\n      \"description\": \"Implement the proposed multi-crop test-time augmentation method, which evaluates the model on 6 augmented views of each test image and averages the outputs with specified weights.\",\n      \"relevant_paper_text\": \"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\",\n      \"scoring_feasibility\": 9,\n      \"llm_tractability\": 0.7,\n      \"expert_tractability\": 0.9,\n      \"layman_tractability\": 0.4\n    }\n  ]\n}"
  },
  {
    "Request": "Make a very succinct json object that validates with the following schema\n\n---\n\nFollow the following format.\n\nJson Schema: ${json_schema}\nJson Object: ${json_object}\n\n---\n\nJson Schema: {\"$defs\": {\"TaskCandidate\": {\"description\": \"Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\\n\\nA good task candidate:\\n- describes a task that an agent can implement\\n- typically a programming task, e.g. modifying a 'baseline' implementation\\n- which reflects or mirrors actual tasks undertaken to produce the research paper\\n- and requires research and/or engineering expertise to implement correctly\\n- for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\\n- ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\\n- the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\", \"properties\": {\"name\": {\"description\": \"Name of the task\", \"title\": \"Name\", \"type\": \"string\"}, \"description\": {\"description\": \"Description of the task\", \"title\": \"Description\", \"type\": \"string\"}, \"relevant_paper_text\": {\"description\": \"Text from the paper that is relevant to the task\", \"title\": \"Relevant Paper Text\", \"type\": \"string\"}, \"scoring_feasibility\": {\"description\": \"\\nOn a scale of 1-10, how feasible is it to write a python function to objectively score an \\nimplementation of this task, verifying that key functionality is implemented as specified?\\n\", \"title\": \"Scoring Feasibility\", \"type\": \"integer\"}, \"llm_tractability\": {\"description\": \"Probability in (0, 1) that a frontier LLM can generate a correct implementation of this task\", \"title\": \"Llm Tractability\", \"type\": \"number\"}, \"expert_tractability\": {\"description\": \"Probability in (0, 1) that a human subject matter expert can generate a correct implementation of this task\", \"title\": \"Expert Tractability\", \"type\": \"number\"}, \"layman_tractability\": {\"description\": \"Probability in (0, 1) that a layman can generate a correct implementation of this task\", \"title\": \"Layman Tractability\", \"type\": \"number\"}}, \"required\": [\"name\", \"description\", \"relevant_paper_text\", \"scoring_feasibility\", \"llm_tractability\", \"expert_tractability\", \"layman_tractability\"], \"title\": \"TaskCandidate\", \"type\": \"object\"}}, \"properties\": {\"tasks\": {\"description\": \"Tasks extracted from the paper\", \"items\": {\"$ref\": \"#/$defs/TaskCandidate\"}, \"title\": \"Tasks\", \"type\": \"array\"}}, \"required\": [\"tasks\"], \"title\": \"TaskCandidates\", \"type\": \"object\"}\nJson Object:"
  },
  {
    "Response": "Json Object: {\n  \"tasks\": [\n    {\n      \"name\": \"Implement Gradient Descent\",\n      \"description\": \"Implement a basic gradient descent algorithm for linear regression\",\n      \"relevant_paper_text\": \"We used gradient descent to optimize the model parameters\",\n      \"scoring_feasibility\": 8,\n      \"llm_tractability\": 0.9,\n      \"expert_tractability\": 0.95,\n      \"layman_tractability\": 0.2\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `paper_text`, `guidance`, produce the fields `task_candidates`.\n\n---\n\nFollow the following format.\n\nPaper Text: Full text from research paper PDF\n\nGuidance: Guidance for extracting task candidates\n\nPast Error in Task Candidates: An error to avoid in the future\n\nTask Candidates: JSON list of task candidate dicts with keys 'name', 'description', and 'relevant_paper_text'. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCandidate\": {\"description\": \"Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\\n\\nA good task candidate:\\n- describes a task that an agent can implement\\n- typically a programming task, e.g. modifying a 'baseline' implementation\\n- which reflects or mirrors actual tasks undertaken to produce the research paper\\n- and requires research and/or engineering expertise to implement correctly\\n- for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\\n- ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\\n- the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\", \"properties\": {\"name\": {\"description\": \"Name of the task\", \"title\": \"Name\", \"type\": \"string\"}, \"description\": {\"description\": \"Description of the task\", \"title\": \"Description\", \"type\": \"string\"}, \"relevant_paper_text\": {\"description\": \"Text from the paper that is relevant to the task\", \"title\": \"Relevant Paper Text\", \"type\": \"string\"}, \"scoring_feasibility\": {\"description\": \"\\nOn a scale of 1-10, how feasible is it to write a python function to objectively score an \\nimplementation of this task, verifying that key functionality is implemented as specified?\\n\", \"title\": \"Scoring Feasibility\", \"type\": \"integer\"}, \"llm_tractability\": {\"description\": \"Probability in (0, 1) that a frontier LLM can generate a correct implementation of this task\", \"title\": \"Llm Tractability\", \"type\": \"number\"}, \"expert_tractability\": {\"description\": \"Probability in (0, 1) that a human subject matter expert can generate a correct implementation of this task\", \"title\": \"Expert Tractability\", \"type\": \"number\"}, \"layman_tractability\": {\"description\": \"Probability in (0, 1) that a layman can generate a correct implementation of this task\", \"title\": \"Layman Tractability\", \"type\": \"number\"}}, \"required\": [\"name\", \"description\", \"relevant_paper_text\", \"scoring_feasibility\", \"llm_tractability\", \"expert_tractability\", \"layman_tractability\"], \"title\": \"TaskCandidate\", \"type\": \"object\"}}, \"properties\": {\"tasks\": {\"description\": \"Tasks extracted from the paper\", \"items\": {\"$ref\": \"#/$defs/TaskCandidate\"}, \"title\": \"Tasks\", \"type\": \"array\"}}, \"required\": [\"tasks\"], \"title\": \"TaskCandidates\", \"type\": \"object\"}\n\n---\n\nPaper Text:\n94% on CIFAR-10 in 3.29 Seconds on a Single GPU\nKeller Jordan\nkjordan4077@gmail.com\nAbstract\nCIFAR-10 is among the most widely used datasets in machine learning, facilitating\nthousands of research projects per year. To accelerate research and reduce the cost\nof experiments, we introduce training methods for CIFAR-10 which reach 94%\naccuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when\nrun on a single NVIDIA A100 GPU. As one factor contributing to these training\nspeeds, we propose a derandomized variant of horizontal flipping augmentation,\nwhich we show improves over the standard method in every case where flipping is\nbeneficial over no flipping at all. Our code is released at https://github.com/\nKellerJordan/cifar10-airbench .\n1 Introduction\nCIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-\ning thousands of research projects per year1. Research can be accelerated and the cost of experiments\nreduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In\nthis paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single\nNVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,\n2023). To support scenarios where higher performance is needed, we additionally develop methods\ntargeting 95% and 96% accuracy. We release the following methods in total.\n1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).\n2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).\n3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).\n4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).\nAll runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-\nmatically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the\nfirst uses torch.compile to improve GPU utilization. It is intended for experiments where many\nnetworks are trained at once in order to amortize the one-time compilation cost. The non-compiled\nairbench94 variant can be easily installed and run using the following command.\n1pip install airbench\n2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"\nOne motivation for the development of these training methods is that they can accelerate the experi-\nmental iteration time of researchers working on compatible projects involving CIFAR-10. Another\nmotivation is that they can decrease the cost of projects involving a massive number of trained\nnetworks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used\n3 million trained networks to demonstrate that the outputs of a trained neural network on a given test\ninput follow an approximately linear function of the vector of binary choices of which examples the\nmodel was trained on. Another example is Jordan (2023), a study on training variance which used\n180 thousand trained networks to show that standard trainings have little variance in performance on\n1https://paperswithcode.com/datasets\nPreprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024\nFigure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal\nflipping augmentation, which flips each image with 50% probability per epoch. This results in some images\nbeing redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a\ndeterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\ntheir test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds\nand 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make\nit possible to replicate these studies, or conduct similar ones, with fewer computational resources.\nFast training also enables the rapid accumulation of statistical significance for subtle hyperparameter\ncomparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10\naccuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation\nbetween runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the\nimprovement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training\nthis will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.\nOur work builds on prior training speed projects. We utilize a modified version of the network,\ninitialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen\npatch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over\nprior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-\nure 1, Section 3.6, Section 5.2).\n2 Background\nOur objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set\nin the shortest possible amount of time. Timing begins when the method is first given access to\ntraining data, and ends when it produces test-set predictions. The method is considered valid if its\nmean accuracy over repeated runs is at least 94%.\nWe chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the\n2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more\nrecent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10\nseconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page\n(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is\ntysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,\n94% is the level of human accuracy reported by Karpathy (2011).\nWe note the following consequences of how the method is timed. First, it is permitted for the program\nto begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins\nwhen the training data is first accessed. This is helpful because otherwise the first run of training\nis typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA\nimproves the performance of a trained network by running it on multiple augmented views of each\ntest input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping\nTTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods\nattain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\nThe CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.\nIt is divided into a training set of 50,000 images and a validation set of 10,000 images. As a\nmatter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was\n2\n80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than\nairbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94\ncan be attributed entirely to algorithmic progress rather than compute scaling.\n3 Methods\n3.1 Network architecture and baseline training\nWe train a convolutional network with a total of 1.97 million parameters, following tysam-code\n(2023) with a few small changes. It contains seven convolutions with the latter six being divided into\nthree blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this\nsection we offer some comments on the main design choices.\nThe network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed\nentirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,\n2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with\nno padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather\nthan the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff\nbetween throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\nFollowing Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,\nand disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is\nscaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture\ndiffers only in that we decrease the number of output channels in the third block from 512 to 256,\nand we add learnable biases to the first convolution.\nAs our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of\n0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the\nmaximum at 20% of the way through training, and then decreases to zero. For data augmentation we\nuse random horizontal flipping alongside 2-pixel random translation. For translation we use reflection\npadding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note\nthat what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a\nrandom 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the\nnetwork is run on both a given test image and its mirror and inferences are made based on the average\nof the two outputs. With optimized choices of learning rate, momentum, and weight decay, this\nbaseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.\n3.2 Frozen patch-whitening initialization\nFollowing Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-\nwhitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code\n(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches\nacross the training distribution, so that their outputs have identity covariance matrix. The second 12\nFigure 2: The first layer\u2019s weights after whitening\ninitialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,\nso that input information is preserved through the\nactivation which follows. Figure 2 shows the result.\nWe do not update this layer\u2019s weights during training.\nDeparting from tysam-code (2023), we add learn-\nable biases to this layer, yielding a small performance\nboost. The biases are trained for 3 epochs, after which\nwe disable their gradient to increase backward-pass\nthroughput, which improves training speed without\nreducing accuracy. We also obtain a slight perfor-\nmance boost relative to tysam-code (2023) by re-\nducing the constant added to the eigenvalues during\ncalculation of the patch-whitening initialization for\nthe purpose of preventing numerical issues in the case\nof a singular patch-covariance matrix.\nPatch-whitening initialization is the single most impactful feature. Adding it to the baseline more\nthan doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.\n3\nRandom reshuffling Alternating flip Mean accuracy\nNo No 93.40%\nNo Yes 93.48%\nYes No 93.92%\nYes Yes 94.01%\nTable 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating\nflip (which we propose) reduce training data redundancy and improve performance.\n3.3 Identity initialization\ndirac : We initialize all convolutions after the first as partial identity transforms. That is, for a\nconvolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an\nidentity transform of the input, and leave the remaining N\u2212Mto their default initialization. In\nPyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight\nwof each convolutional layer. This method partially follows tysam-code (2023), which used a more\ncomplicated scheme where the identity weights are mixed in with the original initialization, which\nwe did not find to be more performant. With this feature added, training attains 94% accuracy in 18\nepochs taking 6.8 A100-seconds.\n3.4 Optimization tricks\nscalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor\nof64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%\nin 13.5 epochs taking 5.1 A100-seconds.\nlookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.\nWe note that Lookahead has also been found effective in prior work on training speed for ResNet-\n18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6\nA100-seconds.\n3.5 Multi-crop evaluation\nmulticrop : To generate predictions, we run the trained network on six augmented views of each\ntest image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a\nversion which is translated down-and-to-the-right by one pixel, and the mirrored versions of all\nthree. Predictions are made using a weighted average of all six outputs, where the two views of the\nuntranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125\neach. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.\nWe note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-\nmonyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of\nevaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using\nmore crops does improve performance, but the increase to inference time outweighs the potential\ntraining speedup.\n3.6 Alternating flip\nTo speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,\nwhich we motivate as follows. When training neural networks, it is standard practice to organize\ntraining into a set of epochs during which every training example is seen exactly once. This\ndiffers from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),\nwhich calls for data to be repeatedly sampled with-replacement from the training set, resulting in\nexamples being potentially seen multiple redundant times within a short window of training. The\nuse of randomly ordered epochs of data for training has a different name, being called the random\nreshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our\ntraining dataset consists of Nunique examples, then sampling data with replacement causes every\n\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N\nunique examples on average. On the other hand, random reshuffling leads to all Nunique examples\nbeing seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason\nthat it is beneficial to maximize the number of unique inputs seen per window of training time.\n4\nWe extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We\nfirst note that standard random horizontal flipping augmentation can be defined as follows.\n1import torch\n2def random_flip ( inputs ):\n3 # Applies random flipping to a batch of images\n4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)\n5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]\nListing 1: Random flip\nIf horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2\nwhich may be seen during training. Potentially, every pair of consecutive epochs could contain every\nunique input. But our main observation is that with standard random horizontal flipping, half of the\nimages will be redundantly flipped the same way during both epochs, so that on average only 1.5N\nunique inputs will be seen.\naltflip : To address this, we propose to modify standard random horizontal flipping augmentation as\nfollows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},\nwe flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we\nflip only those inputs which were flipped in the first epoch. We provide the following implementation\nwhich avoids the need for extra memory by using a pseudorandom function to decide the flips.\n1import torch\n2import hashlib\n3def hash_fn (n, seed =42) :\n4 k = n * seed\n5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,\n16)\n6def alternating_flip (inputs , indices , epoch ):\n7 # Applies alternating flipping to a batch of images\n8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist\n() ])\n9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)\n10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\nListing 2: Alternating flip\nThe result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in\nFigure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.\nAdding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final\ntraining method airbench94.py , the entire contents of which can be found in Section E. It reaches\n94% accuracy in 3.83 seconds on an NVIDIA A100.\n3.7 Compilation\nThe final step we take to speed up training is a non-algorithmic one: we compile our training\nmethod using torch.compile in order to more efficiently utilize the GPU. This results in a training\nscript which is mathematically equivalent (up to small differences in floating point arithmetic) to\nthe non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29\nA100-seconds. The downside is that the one-time compilation process takes up to several minutes to\ncomplete before training runs can begin, so that it is only beneficial when we plan to execute many\nruns of training at once. We release this version as airbench94_compiled.py .\n4 95% and 96% targets\nTo address scenarios where somewhat higher performance is desired, we additionally develop methods\ntargeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .\nTo attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel\ncount of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce\nthe learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%\naccuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.\n2Assuming none of the training inputs are already mirrors of each other.\n5\nFigure 4: Training speedups accumulate additively. Removing individual features from airbench94\nincreases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces\nthe epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-\n94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.\nFigure 3: FLOPs vs. error rate tradeoff. Our\nthree training methods apparently follow a linear\nlog-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-\nVries & Taylor, 2017) augmentation and raise the\ntraining epochs to 40. We add a third convolution to\neach block, and scale the first block to 128 channels\nand the second two to 512. We also add a residual\nconnection across the later two convolutions of each\nblock, which we find is still beneficial despite the fact\nthat we are already using identity initialization (Sec-\ntion 3.3) to ease gradient flow. Finally, we reduce\nthe learning rate by a factor of 0.78. These changes\nyield airbench96 which attains 96.05% accuracy in\n46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.\nFigure 3 shows the FLOPs and error rate of each of\nour three training methods.\n5 Experiments\n5.1 Interaction between features\nTo gain a better sense of the impact of each feature on training speed, we compare two quantities.\nFirst, we measure the number of epochs that can be saved by adding the feature to the whitened\nbaseline (Section 3.2). Second, we measure the number of epochs that must be added when the\nfeature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-\ntion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it\nfrom the final airbench94 increases epochs-to-94% from 9.9 to 12.8.\nFigure 4 shows both quantities for each feature. Surprisingly, we find that for all features except\nmulti-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even\nthough the whitened baseline requires more than twice as many epochs as the final configuration.\nThis indicates that the interaction between most features is additive rather than multiplicative.\n5.2 Does alternating flip generalize?\nIn this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of\ntraining configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all\ncases except those where neither alternating nor random flip improve over using no flipping at all.\nFor CIFAR-10 we consider the performance boost given by alternating flip across the follow-\ning 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and\nairbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For\n6\nFigure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,\nthe use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%\ntraining speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.\n95% confidence intervals are shown around each point.\neach configuration we compare the performance of alternating and random flip in terms of their mean\naccuracy across n= 400 runs of training.\nFigure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip\nimproves performance in every setting. To get a sense for how big the improvement is, we estimate\nthe effective speedup for each case, i.e., the fraction of epochs that could be saved by switching\nfrom random to alternating flip while maintaining the level of accuracy of random flip. We begin\nby fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of\neach random flip-based training configuration. We use these curves to calculate the effective speedup\nafforded by switching from random to alternating flip. For example, airbench94 with random flip\nand without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The\nsame configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law\nfit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.\nNote that using a power-law yields a more conservative estimate relative to using linear interpolation\nbetween the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.\nTable 2 shows the result. We observe the following patterns. First, the addition of extra augmentation\n(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that\nthe main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the\nsame way for many epochs in a row; we speculate that adding extra augmentation reduces the negative\nimpact of these cases because it increases data diversity. Next, TTA reduces the gap between random\nand alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),\nindicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training\nfor longer consistently increases the effective speedup given by alternating flip.\nWe next study ImageNet trainings with the following experiment. We train a ResNet-18 with a\nvariety of train and test crops, comparing three flipping options: alternating flip, random flip, and no\nflipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192\ncenter crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)\nto denote the latter. We also consider two training crops: 192x192 inception-style random resized\ncrop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area\nranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the\nshorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write\n7\nBaseline Cutout Epochs Speedup Speedup (w/ TTA)\nairbench94 No 10 15.0% 5.30%\nairbench94 No 20 27.1% 21.3%\nairbench94 No 40 38.3% 36.4%\nairbench94 No 80 102% 31.8%\nairbench94 Yes 10 3.84% 1.13%\nairbench94 Yes 20 7.42% 2.00%\nairbench94 Yes 40 18.6% 9.28%\nairbench94 Yes 80 29.2% 14.25%\nairbench96 Yes 10 4.94% 1.11%\nairbench96 Yes 20 8.99% 3.58%\nairbench96 Yes 40 17.2% 6.48%\nairbench96 Yes 80 18.8% Not measured\nTable 2: Effective speedups given by switching from random flip to alternating flip. The two configurations\nmost closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw\naccuracy values of the airbench94 experiments.\nFlipping augmentation option\nTrain crop Test crop Epochs TTA None Random Alternating\nHeavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28\nHeavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28\nLight RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26\nLight RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26\nHeavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28\nHeavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28\nLight RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26\nLight RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26\nLight RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16\nHeavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4\nTable 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for\nthose trainings where random flip improves significantly over not flipping at all. The single best flipping option\nin each row is bolded when the difference is statistically significant.\nHeavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided\nin Section C.\nTable 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC\nis better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly\nbetter when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test\nresolution discrepancy (Touvron et al., 2019).\nFor trainings which use Light RRC, we find that switching from random flip to alternating flip\nprovides a substantial boost to performance, amounting to a training speedup of more than 25%.\nIn Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to\nalternating flip improves performance by more than increasing the training duration from 16 to 20\nepochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our\nresults on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit\nfrom alternating flip. Indeed, even turning flipping off completely does not significantly reduce the\nperformance of these trainings. We conclude that alternating flip improves over random flip for every\ntraining scenario where the latter improves over no flipping at all.\n8\nEpochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE\n1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312\n2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233\n1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269\n1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533\n2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433\n1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444\nTable 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).\n5.3 Variance and class-wise calibration\nPrevious sections have focused on understanding what factors affect the first moment of accuracy\n(the mean). In this section we investigate the second moment, finding that TTA reduces variance at\nthe cost of calibration.\nOur experiment is to execute 10,000 runs of airbench94 training with several hyperparameter\nsettings. For each setting we report both the variance in test-set accuracy as well as an estimate of the\ndistribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.\nTable 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set\nvariance, replicating the main finding of Jordan (2023). This is a surprising result because these\ntrainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7\nas much distribution-wise variance when run for a similar duration, and reached a low variance only\nwhen run for 64 epochs. We conclude from this comparison that distribution-wise variance is more\nstrongly connected to the rate of convergence of a training rather than its duration as such. We also\nnote that the low distribution-wise variance of airbench94 indicates it has high training stability.\nUsing TTA significantly reduces the test-set variance, such that all three settings with TTA have lower\ntest-set variance than any setting without TTA. However, test-set variance is implied by the class-wise\ncalibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this\nreduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,\nwe compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which\nmeasures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA\nhas a higher CACE than every setting without TTA, confirming the hypothesis.\n6 Discussion\nIn this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7\nfaster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the\nairbench Python package.\nWe developed airbench solely with the goal of maximizing training speed on CIFAR-10. In\nSection B we find that it also generalizes well to other tasks. For example, without any extra tuning,\nairbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.\nOne factor contributing to the training speed of airbench was our finding that training can be accel-\nerated by partially derandomizing the standard random horizontal flipping augmentation, resulting in\nthe variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating\nflip improves the performance of every training we considered (Section 5.2), with the exception of\nthose trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to\nus, the standard ImageNet trainings that we considered do not significantly benefit from horizontal\nflipping. Future work might investigate whether it is possible to obtain derandomized improvements\nto other augmentations besides horizontal flip.\nThe methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,\nwith fixed performance and hardware constraints. These constraints mean that we cannot improve\nperformance by simply scaling up the amount of computational resources used; instead we are forced\nto develop new methods like the alternating flip. We look forward to seeing what other new methods\nfuture work discovers to push training speed further.\n9\nReferences\nDimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.\nDan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-\nperformance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,\n2011.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\nmark and competition. 2017.\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet\nor cifar-10. arXiv preprint arXiv:1810.03505 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\npp. 248\u2013255. IEEE, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552 , 2017.\nMert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic\ngradient descent. Mathematical Programming , 186:49\u201384, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 , 2016.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.\npmlr, 2015.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\nsgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.\nKeller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and\ninevitable. arXiv preprint arXiv:2304.01910 , 2023.\nAndrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.\nio/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for\nadvanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT\nLicense.\nGuillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander\nM \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,\nBenjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.\nBenchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural\nInformation Processing Systems , 35:25404\u201325421, 2022.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.\nDavid Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/\nhow-to-train-your-resnet-8-bag-of-tricks/ .\n10\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics , 22(3):400\u2013407, 1951.\nSantiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:\n//github.com/apple/ml-cifar-10-faster , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.\ntysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/\nhlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. Advances in neural information processing systems , 32, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,\n2016.\nMichael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps\nforward, 1 step back. Advances in neural information processing systems , 32, 2019.\n11\nA Network architecture\n1from torch import nn\n2\n3class Flatten (nn. Module ):\n4 def forward (self , x):\n5 return x. view (x. size (0) , -1)\n6\n7class Mul (nn. Module ):\n8 def __init__ (self , scale ):\n9 super (). __init__ ()\n10 self . scale = scale\n11 def forward (self , x):\n12 return x * self . scale\n13\n14def conv (ch_in , ch_out ):\n15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,\n16 padding =\u2019same \u2019, bias = False )\n17\n18def make_net ():\n19 act = lambda : nn. GELU ()\n20 bn = lambda ch: nn. BatchNorm2d (ch)\n21 return nn. Sequential (\n22 nn. Sequential (\n23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),\n24 act () ,\n25 ),\n26 nn. Sequential (\n27 conv (24 , 64) ,\n28 nn. MaxPool2d (2) ,\n29 bn (64) , act () ,\n30 conv (64 , 64) ,\n31 bn (64) , act () ,\n32 ),\n33 nn. Sequential (\n34 conv (64 , 256) ,\n35 nn. MaxPool2d (2) ,\n36 bn (256) , act () ,\n37 conv (256 , 256) ,\n38 bn (256) , act () ,\n39 ),\n40 nn. Sequential (\n41 conv (256 , 256) ,\n42 nn. MaxPool2d (2) ,\n43 bn (256) , act () ,\n44 conv (256 , 256) ,\n45 bn (256) , act () ,\n46 ),\n47 nn. MaxPool2d (3) ,\n48 Flatten () ,\n49 nn. Linear (256 , 10, bias = False ),\n50 Mul (1/9) ,\n51 )\nListing 3: Minimal PyTorch code for the network architecture used by airbench94 .\nWe note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock\ntime. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel\nsize of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this\ndoes not improve the wallclock training time on an A100. Reducing the batch size is another easy\nway to save FLOPs but not wallclock time.\n12\nDataset Flipping? Cutout? ResNet-18 airbench96\nCIFAR-10 Yes No 95.55% 95.61%\nCIFAR-10 Yes Yes 96.01% 96.05%\nCIFAR-100 Yes No 77.54% 79.27%\nCIFAR-100 Yes Yes 78.04% 79.76%\nCINIC-10 Yes No 87.58% 87.78%\nCINIC-10 Yes Yes not measured 88.22%\nSVHN No No 97.35% 97.38%\nSVHN No Yes not measured 97.64%\nTable 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We\ndirectly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off\nflipping for SVHN).\nB Extra dataset experiments\nWe developed airbench with the singular goal of maximizing training speed on CIFAR-10. To\nfind out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its\nperformance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-\n10 (Darlow et al., 2018).\nOn CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both\nthe case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not\n(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than\nResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say\nthat it generalizes.\nWe compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.\nWe do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal\non CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better\nperformance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where\nairbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is\nused and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows\nstrong generalization to other tasks.\nWe note that this comparison between airbench96 and ResNet-18 training is fair in the sense that\nit does demonstrate that the former has good generalization, but unfair in the sense that it does not\nindicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time\naugmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training\nwould outperform airbench96 if it were run using test-time augmentation. However, it also takes\n5-10 times longer to complete. The decision of which to use may be situational.\nThe accuracy values we report for ResNet-18 training are from the following sources. We tried\nto select the highest values we could find for each setting. Moreau et al. (2022) reports attaining\n95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports\nattaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on\nCIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.\nC ImageNet training details\nOur ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/\nlibffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and\nmomentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution\n160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of\ntraining. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.\n13\nHyperparameters Flipping augmentation option\nEpochs Cutout TTA None Random Alternating\n10 No No 92.3053 93.0988 93.2798\n20 No No 92.8166 93.7446 93.8652\n40 No No 93.0143 94.0133 94.0729\n80 No No 93.0612 94.1169 94.1628\n10 No Yes 93.4071 93.9488 94.0186\n20 No Yes 93.8528 94.5565 94.6530\n40 No Yes 94.0381 94.7803 94.8203\n80 No Yes 94.0638 94.8506 94.8676\n10 Yes No 91.8487 92.0402 92.1374\n20 Yes No 92.8474 93.3825 93.4876\n40 Yes No 93.2675 94.1014 94.1952\n80 Yes No 93.4193 94.4311 94.5204\n10 Yes Yes 92.6455 92.7780 92.8103\n20 Yes Yes 93.7862 94.1306 94.1670\n40 Yes Yes 94.3090 94.8511 94.8960\n80 Yes Yes 94.5253 95.1839 95.2362\nTable 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a\nmean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise\ndifference in means is statistically significant.\nD Extra tables & figures\nFigure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in\nSection 5.3.\nE Complete training code\n1\"\"\"\n2airbench94 .py\n33.83 s runtime on an A100 ; 0.36 PFLOPs .\n4Evidence for validity : 94.01 average accuracy in n =1000 runs .\n5\n6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB\nwith the following nvidia -smi:\n7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :\n11.7\n14\n8torch . __version__ == \u20192.1.2+ cu118 \u2019\n9\"\"\"\n10\n11# ############################################\n12# Setup / Hyperparameters #\n13# ############################################\n14\n15import os\n16import sys\n17import uuid\n18from math import ceil\n19\n20import torch\n21from torch import nn\n22import torch .nn. functional as F\n23import torchvision\n24import torchvision . transforms as T\n25\n26torch . backends . cudnn . benchmark = True\n27\n28\"\"\"\n29We express the main training hyperparameters ( batch size , learning\nrate , momentum , and weight decay ) in decoupled form , so that each\none can be tuned independently . This accomplishes the following :\n30* Assuming time - constant gradients , the average step size is decoupled\nfrom everything but the lr.\n31* The size of the weight decay update is decoupled from everything but\nthe wd.\n32In constrast , normally when we increase the ( Nesterov ) momentum , this\nalso scales up the step size proportionally to 1 + 1 / (1 -\nmomentum ), meaning we cannot change momentum without having to re -\ntune the learning rate . Similarly , normally when we increase the\nlearning rate this also increases the size of the weight decay ,\nrequiring a proportional decrease in the wd to maintain the same\ndecay strength .\n33\n34The practical impact is that hyperparameter tuning is faster , since\nthis parametrization allows each one to be tuned independently .\nSee https :// myrtle .ai/ learn /how -to -train -your - resnet -5-\nhyperparameters /.\n35\"\"\"\n36\n37hyp = {\n38 \u2019opt \u2019: {\n39 \u2019 train_epochs \u2019: 9.9 ,\n40 \u2019batch_size \u2019: 1024 ,\n41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples\n42 \u2019momentum \u2019: 0.85 ,\n43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (\ndecoupled from learning rate )\n44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not\nweight decay ) for BatchNorm biases\n45 \u2019 label_smoothing \u2019: 0.2 ,\n46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the\nwhitening layer bias before freezing\n47 },\n48 \u2019aug \u2019: {\n49 \u2019flip \u2019: True ,\n50 \u2019translate \u2019: 2,\n51 },\n52 \u2019net \u2019: {\n53 \u2019widths \u2019: {\n54 \u2019block1 \u2019: 64,\n55 \u2019block2 \u2019: 256 ,\n56 \u2019block3 \u2019: 256 ,\n15\n57 },\n58 \u2019 batchnorm_momentum \u2019: 0.6 ,\n59 \u2019 scaling_factor \u2019: 1/9 ,\n60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :\n0= none , 1= mirror , 2= mirror + translate\n61 },\n62}\n63\n64# ############################################\n65# DataLoader #\n66# ############################################\n67\n68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )\n69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )\n70\n71def batch_flip_lr ( inputs ):\n72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .\nview (-1, 1, 1, 1)\n73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\n74\n75def batch_crop (images , crop_size ):\n76 r = ( images . size ( -1) - crop_size )//2\n77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =\nimages . device )\n78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),\ndevice = images .device , dtype = images . dtype )\n79 # The two cropping methods in this if - else produce equivalent\nresults , but the second is faster for r > 2.\n80 if r <= 2:\n81 for sy in range (-r, r +1):\n82 for sx in range (-r, r +1):\n83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)\n84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size\n, r+sx:r+sx+ crop_size ]\n85 else :\n86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size\n+2* r), device = images .device , dtype = images . dtype )\n87 for s in range (-r, r +1):\n88 mask = ( shifts [:, 0] == s)\n89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]\n90 for s in range (-r, r +1):\n91 mask = ( shifts [:, 1] == s)\n92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+\ncrop_size ]\n93 return images_out\n94\n95class CifarLoader :\n96 \"\"\"\n97 GPU - accelerated dataloader for CIFAR -10 which implements\nalternating flip augmentation .\n98 \"\"\"\n99\n100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,\ndrop_last =None , shuffle =None , gpu =0):\n101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .\npt \u2019)\n102 if not os. path . exists ( data_path ):\n103 dset = torchvision . datasets . CIFAR10 (path , download =True ,\ntrain = train )\n104 images = torch . tensor ( dset . data )\n105 labels = torch . tensor ( dset . targets )\n106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:\ndset . classes }, data_path )\n107\n108 data = torch . load ( data_path , map_location = torch . device (gpu))\n16\n109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [\n\u2019labels \u2019], data [\u2019classes \u2019]\n110 # It \u2019s faster to load + process uint8 data than to load\npreprocessed fp16 data\n111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).\nto( memory_format = torch . channels_last )\n112\n113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )\n114 self . proc_images = {} # Saved results of image processing to\nbe done on the first epoch\n115 self . epoch = 0\n116\n117 self . aug = aug or {}\n118 for k in self .aug . keys ():\n119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019\n% k\n120\n121 self . batch_size = batch_size\n122 self . drop_last = train if drop_last is None else drop_last\n123 self . shuffle = train if shuffle is None else shuffle\n124\n125 def __len__ ( self ):\n126 return len( self . images )// self . batch_size if self . drop_last\nelse ceil (len( self . images )/ self . batch_size )\n127\n128 def __iter__ ( self ):\n129\n130 if self . epoch == 0:\n131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .\nimages )\n132 # Randomly flip all images on the first epoch as according\nto definition of alternating flip\n133 if self .aug.get(\u2019flip \u2019, False ):\n134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (\nimages )\n135 # Pre -pad images to save time when doing random\ntranslation\n136 pad = self .aug.get (\u2019translate \u2019, 0)\n137 if pad > 0:\n138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019\nreflect \u2019)\n139\n140 if self .aug.get(\u2019translate \u2019, 0) > 0:\n141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .\nshape [ -2])\n142 elif self .aug.get (\u2019flip \u2019, False ):\n143 images = self . proc_images [\u2019flip \u2019]\n144 else :\n145 images = self . proc_images [\u2019norm \u2019]\n146 if self .aug.get(\u2019flip \u2019, False ):\n147 if self . epoch % 2 == 1:\n148 images = images . flip ( -1)\n149\n150 self . epoch += 1\n151\n152 indices = ( torch . randperm if self . shuffle else torch . arange )(\nlen ( images ), device = images . device )\n153 for i in range (len ( self )):\n154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]\n155 yield ( images [ idxs ], self . labels [ idxs ])\n156\n157# ############################################\n158# Network Components #\n159# ############################################\n160\n17\n161class Flatten (nn. Module ):\n162 def forward (self , x):\n163 return x. view (x. size (0) , -1)\n164\n165class Mul (nn. Module ):\n166 def __init__ (self , scale ):\n167 super (). __init__ ()\n168 self . scale = scale\n169 def forward (self , x):\n170 return x * self . scale\n171\n172class BatchNorm (nn. BatchNorm2d ):\n173 def __init__ (self , num_features , momentum , eps =1e -12 ,\n174 weight =False , bias = True ):\n175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )\n176 self . weight . requires_grad = weight\n177 self . bias . requires_grad = bias\n178 # Note that PyTorch already initializes the weights to one and\nbiases to zero\n179\n180class Conv (nn. Conv2d ):\n181 def __init__ (self , in_channels , out_channels , kernel_size =3,\npadding =\u2019same \u2019, bias = False ):\n182 super (). __init__ ( in_channels , out_channels , kernel_size =\nkernel_size , padding = padding , bias = bias )\n183\n184 def reset_parameters ( self ):\n185 super (). reset_parameters ()\n186 if self . bias is not None :\n187 self . bias . data . zero_ ()\n188 w = self . weight . data\n189 torch .nn. init . dirac_ (w[:w. size (1) ])\n190\n191class ConvGroup (nn. Module ):\n192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):\n193 super (). __init__ ()\n194 self . conv1 = Conv ( channels_in , channels_out )\n195 self . pool = nn. MaxPool2d (2)\n196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )\n197 self . conv2 = Conv ( channels_out , channels_out )\n198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )\n199 self . activ = nn. GELU ()\n200\n201 def forward (self , x):\n202 x = self . conv1 (x)\n203 x = self . pool (x)\n204 x = self . norm1 (x)\n205 x = self . activ (x)\n206 x = self . conv2 (x)\n207 x = self . norm2 (x)\n208 x = self . activ (x)\n209 return x\n210\n211# ############################################\n212# Network Definition #\n213# ############################################\n214\n215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019\n][ \u2019 batchnorm_momentum \u2019]):\n216 whiten_kernel_size = 2\n217 whiten_width = 2 * 3 * whiten_kernel_size **2\n218 net = nn. Sequential (\n219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True\n),\n220 nn. GELU () ,\n18\n221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],\nbatchnorm_momentum ),\n222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],\nbatchnorm_momentum ),\n223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],\nbatchnorm_momentum ),\n224 nn. MaxPool2d (3) ,\n225 Flatten () ,\n226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),\n227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,\n228 )\n229 net [0]. weight . requires_grad = False\n230 net = net. half (). cuda ()\n231 net = net.to( memory_format = torch . channels_last )\n232 for mod in net. modules ():\n233 if isinstance (mod , BatchNorm ):\n234 mod . float ()\n235 return net\n236\n237# ############################################\n238# Whitening Conv Initialization #\n239# ############################################\n240\n241def get_patches (x, patch_shape ):\n242 c, (h, w) = x. shape [1] , patch_shape\n243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h\n,w). float ()\n244\n245def get_whitening_parameters ( patches ):\n246 n,c,h,w = patches . shape\n247 patches_flat = patches . view (n, -1)\n248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n\n249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance\n, UPLO =\u2019U\u2019)\n250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.\nreshape (c*h*w,c,h,w). flip (0)\n251\n252def init_whitening_conv (layer , train_set , eps =5e -4):\n253 patches = get_patches ( train_set , patch_shape = layer . weight . data .\nshape [2:])\n254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )\n255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)\n256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -\neigenvectors_scaled ))\n257\n258# ###########################################\n259# Lookahead #\n260# ###########################################\n261\n262class LookaheadState :\n263 def __init__ (self , net):\n264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().\nitems ()}\n265\n266 def update (self , net , decay ):\n267 for ema_param , net_param in zip( self . net_ema . values () , net.\nstate_dict (). values ()):\n268 if net_param . dtype in ( torch .half , torch . float ):\n269 ema_param . lerp_ ( net_param , 1- decay )\n270 net_param . copy_ ( ema_param )\n271\n272# ###########################################\n273# Logging #\n274# ###########################################\n275\n19\n276def print_columns ( columns_list , is_head =False , is_final_entry = False ):\n277 print_string = \u2019\u2019\n278 for col in columns_list :\n279 print_string += \u2019| %s \u2019 % col\n280 print_string += \u2019|\u2019\n281 if is_head :\n282 print (\u2019-\u2019*len( print_string ))\n283 print ( print_string )\n284 if is_head or is_final_entry :\n285 print (\u2019-\u2019*len( print_string ))\n286\n287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,\n\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]\n288def print_training_details ( variables , is_final_entry ):\n289 formatted = []\n290 for col in logging_columns_list :\n291 var = variables .get (col. strip () , None )\n292 if type (var) in (int , str):\n293 res = str(var)\n294 elif type (var) is float :\n295 res = \u2019{:0.4 f}\u2019. format ( var)\n296 else :\n297 assert var is None\n298 res = \u2019\u2019\n299 formatted . append (res. rjust (len (col)))\n300 print_columns ( formatted , is_final_entry = is_final_entry )\n301\n302# ###########################################\n303# Evaluation #\n304# ###########################################\n305\n306def infer (model , loader , tta_level =0) :\n307 \"\"\"\n308 Test - time augmentation strategy (for tta_level =2):\n309 1. Flip / mirror the image left -to - right (50% of the time ).\n310 2. Translate the image by one pixel either up -and - left or down -and\n- right (50% of the time , i.e. both happen 25% of the time ).\n311\n312 This creates 6 views per image ( left / right times the two\ntranslations and no - translation ), which we evaluate and then\nweight according to the given probabilities .\n313 \"\"\"\n314\n315 def infer_basic (inputs , net):\n316 return net( inputs ). clone ()\n317\n318 def infer_mirror (inputs , net):\n319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))\n320\n321 def infer_mirror_translate ( inputs , net ):\n322 logits = infer_mirror (inputs , net )\n323 pad = 1\n324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)\n325 inputs_translate_list = [\n326 padded_inputs [:, :, 0:32 , 0:32] ,\n327 padded_inputs [:, :, 2:34 , 2:34] ,\n328 ]\n329 logits_translate_list = [ infer_mirror ( inputs_translate , net)\n330 for inputs_translate in\ninputs_translate_list ]\n331 logits_translate = torch . stack ( logits_translate_list ). mean (0)\n332 return 0.5 * logits + 0.5 * logits_translate\n333\n334 model . eval ()\n335 test_images = loader . normalize ( loader . images )\n20\n336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][\ntta_level ]\n337 with torch . no_grad ():\n338 return torch .cat ([ infer_fn (inputs , model ) for inputs in\ntest_images . split (2000) ])\n339\n340def evaluate (model , loader , tta_level =0) :\n341 logits = infer (model , loader , tta_level )\n342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()\n343\n344# ###########################################\n345# Training #\n346# ###########################################\n347\n348def main (run):\n349\n350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]\n351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]\n352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]\n353 # Assuming gradients are constant in time , for Nesterov momentum ,\nthe below ratio is how much larger the default steps will be than\nthe underlying per - example gradients . We divide the learning rate\nby this ratio in order to ensure steps are the same scale as\ngradients , regardless of the choice of momentum .\n354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))\n355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning\nrate for PyTorch SGD\n356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale\n357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]\n358\n359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019\nlabel_smoothing \u2019], reduction =\u2019none \u2019)\n360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)\n361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =\nbatch_size , aug=hyp[\u2019aug \u2019])\n362 if run == \u2019warmup \u2019:\n363 # The only purpose of the first run is to warmup , so we can\nuse dummy data\n364 train_loader . labels = torch . randint (0, 10, size =( len (\ntrain_loader . labels ) ,), device = train_loader . labels . device )\n365 total_train_steps = ceil (len( train_loader ) * epochs )\n366\n367 model = make_net ()\n368 current_steps = 0\n369\n370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in\nk and p. requires_grad ]\n371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019\nnot in k and p. requires_grad ]\n372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,\nweight_decay =wd/ lr_biases ),\n373 dict ( params = other_params , lr=lr , weight_decay =wd/\nlr)]\n374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,\nnesterov = True )\n375\n376 def triangle (steps , start =0, end =0, peak =0.5) :\n377 xp = torch . tensor ([0 , int( peak * steps ), steps ])\n378 fp = torch . tensor ([ start , 1, end ])\n379 x = torch . arange (1+ steps )\n380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])\n381 b = fp [: -1] - (m * xp [: -1])\n382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1\n383 indices = torch . clamp ( indices , 0, len(m) - 1)\n384 return m[ indices ] * x + b[ indices ]\n21\n385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,\npeak =0.23)\n386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:\nlr_schedule [i])\n387\n388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /\ntotal_train_steps )**3\n389 lookahead_state = LookaheadState ( model )\n390\n391 # For accurately timing GPU code\n392 starter = torch . cuda . Event ( enable_timing = True )\n393 ender = torch . cuda . Event ( enable_timing = True )\n394 total_time_seconds = 0.0\n395\n396 # Initialize the first layer using statistics of training images\n397 starter . record ()\n398 train_images = train_loader . normalize ( train_loader . images [:5000])\n399 init_whitening_conv ( model [0] , train_images )\n400 ender . record ()\n401 torch . cuda . synchronize ()\n402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n403\n404 for epoch in range ( ceil ( epochs )):\n405\n406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019\nwhiten_bias_epochs \u2019])\n407\n408 # ###################\n409 # Training #\n410 # ###################\n411\n412 starter . record ()\n413\n414 model . train ()\n415 for inputs , labels in train_loader :\n416\n417 outputs = model ( inputs )\n418 loss = loss_fn ( outputs , labels ).sum ()\n419 optimizer . zero_grad ( set_to_none = True )\n420 loss . backward ()\n421 optimizer . step ()\n422 scheduler . step ()\n423\n424 current_steps += 1\n425\n426 if current_steps % 5 == 0:\n427 lookahead_state . update (model , decay = alpha_schedule [\ncurrent_steps ]. item ())\n428\n429 if current_steps >= total_train_steps :\n430 if lookahead_state is not None :\n431 lookahead_state . update (model , decay =1.0)\n432 break\n433\n434 ender . record ()\n435 torch . cuda . synchronize ()\n436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n437\n438 # ###################\n439 # Evaluation #\n440 # ###################\n441\n442 # Print the accuracy and loss from the last training batch of\nthe epoch\n22\n443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().\nmean (). item ()\n444 train_loss = loss . item () / batch_size\n445 val_acc = evaluate (model , test_loader , tta_level =0)\n446 print_training_details ( locals () , is_final_entry = False )\n447 run = None # Only print the run number once\n448\n449 # ###################\n450 # TTA Evaluation #\n451 # ###################\n452\n453 starter . record ()\n454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019\ntta_level \u2019])\n455 ender . record ()\n456 torch . cuda . synchronize ()\n457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n458\n459 epoch = \u2019eval \u2019\n460 print_training_details ( locals () , is_final_entry = True )\n461\n462 return tta_val_acc\n463\n464if __name__ == \" __main__ \":\n465 with open (sys. argv [0]) as f:\n466 code = f. read ()\n467\n468 print_columns ( logging_columns_list , is_head = True )\n469 main (\u2019warmup \u2019)\n470 accs = torch . tensor ([ main (run) for run in range (25) ])\n471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))\n472\n473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }\n474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))\n475 os. makedirs ( log_dir , exist_ok = True )\n476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)\n477 print (os. path . abspath ( log_path ))\n478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))\nListing 4: airbench94.py\n23\n\nGuidance: \n\nPast Error in Task Candidates: ValueError('json output should start and end with { and }')\n\nTask Candidates:"
  },
  {
    "Response": "{\n  \"tasks\": [\n    {\n      \"name\": \"Implement alternating flip data augmentation\",\n      \"description\": \"Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in an alternating pattern across epochs rather than randomly, to reduce redundancy.\",\n      \"relevant_paper_text\": \"To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we flip only those inputs which were flipped in the first epoch.\",\n      \"scoring_feasibility\": 9,\n      \"llm_tractability\": 0.8,\n      \"expert_tractability\": 0.95,\n      \"layman_tractability\": 0.3\n    },\n    {\n      \"name\": \"Implement decoupled hyperparameters\",\n      \"description\": \"Implement the decoupled hyperparameter scheme described in the paper, where batch size, learning rate, momentum, and weight decay are expressed in a form that allows each to be tuned independently.\",\n      \"relevant_paper_text\": \"We express the main training hyperparameters (batch size, learning rate, momentum, and weight decay) in decoupled form, so that each one can be tuned independently. This accomplishes the following:\\n* Assuming time-constant gradients, the average step size is decoupled from everything but the lr.\\n* The size of the weight decay update is decoupled from everything but the wd.\",\n      \"scoring_feasibility\": 8,\n      \"llm_tractability\": 0.7,\n      \"expert_tractability\": 0.9,\n      \"layman_tractability\": 0.2\n    },\n    {\n      \"name\": \"Implement patch-whitening initialization\",\n      \"description\": \"Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper. This involves initializing the filters based on the eigenvectors of the covariance matrix of patches from the training data.\",\n      \"relevant_paper_text\": \"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\",\n      \"scoring_feasibility\": 7,\n      \"llm_tractability\": 0.6,\n      \"expert_tractability\": 0.85,\n      \"layman_tractability\": 0.1\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `candidate_input`, `candidate_output`, `requirements`, `attempt_num`, `previous_critiques`, produce the fields `self_critique`, `should_retry`.\n\n---\n\nFollow the following format.\n\nCandidate Input: ${candidate_input}\n\nCandidate Output: ${candidate_output}\n\nRequirements: ${requirements}\n\nAttempt Num: ${attempt_num}\n\nPrevious Critiques: ${previous_critiques}\n\nSelf Critique: Self-critique of the task candidate\n\nShould Retry: Should the task candidate be retried? This may be expensive. True/False (Respond with true or false)\n\n---\n\nCandidate Input:\n94% on CIFAR-10 in 3.29 Seconds on a Single GPU\nKeller Jordan\nkjordan4077@gmail.com\nAbstract\nCIFAR-10 is among the most widely used datasets in machine learning, facilitating\nthousands of research projects per year. To accelerate research and reduce the cost\nof experiments, we introduce training methods for CIFAR-10 which reach 94%\naccuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when\nrun on a single NVIDIA A100 GPU. As one factor contributing to these training\nspeeds, we propose a derandomized variant of horizontal flipping augmentation,\nwhich we show improves over the standard method in every case where flipping is\nbeneficial over no flipping at all. Our code is released at https://github.com/\nKellerJordan/cifar10-airbench .\n1 Introduction\nCIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-\ning thousands of research projects per year1. Research can be accelerated and the cost of experiments\nreduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In\nthis paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single\nNVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,\n2023). To support scenarios where higher performance is needed, we additionally develop methods\ntargeting 95% and 96% accuracy. We release the following methods in total.\n1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).\n2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).\n3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).\n4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).\nAll runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-\nmatically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the\nfirst uses torch.compile to improve GPU utilization. It is intended for experiments where many\nnetworks are trained at once in order to amortize the one-time compilation cost. The non-compiled\nairbench94 variant can be easily installed and run using the following command.\n1pip install airbench\n2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"\nOne motivation for the development of these training methods is that they can accelerate the experi-\nmental iteration time of researchers working on compatible projects involving CIFAR-10. Another\nmotivation is that they can decrease the cost of projects involving a massive number of trained\nnetworks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used\n3 million trained networks to demonstrate that the outputs of a trained neural network on a given test\ninput follow an approximately linear function of the vector of binary choices of which examples the\nmodel was trained on. Another example is Jordan (2023), a study on training variance which used\n180 thousand trained networks to show that standard trainings have little variance in performance on\n1https://paperswithcode.com/datasets\nPreprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024\nFigure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal\nflipping augmentation, which flips each image with 50% probability per epoch. This results in some images\nbeing redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a\ndeterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\ntheir test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds\nand 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make\nit possible to replicate these studies, or conduct similar ones, with fewer computational resources.\nFast training also enables the rapid accumulation of statistical significance for subtle hyperparameter\ncomparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10\naccuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation\nbetween runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the\nimprovement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training\nthis will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.\nOur work builds on prior training speed projects. We utilize a modified version of the network,\ninitialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen\npatch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over\nprior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-\nure 1, Section 3.6, Section 5.2).\n2 Background\nOur objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set\nin the shortest possible amount of time. Timing begins when the method is first given access to\ntraining data, and ends when it produces test-set predictions. The method is considered valid if its\nmean accuracy over repeated runs is at least 94%.\nWe chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the\n2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more\nrecent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10\nseconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page\n(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is\ntysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,\n94% is the level of human accuracy reported by Karpathy (2011).\nWe note the following consequences of how the method is timed. First, it is permitted for the program\nto begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins\nwhen the training data is first accessed. This is helpful because otherwise the first run of training\nis typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA\nimproves the performance of a trained network by running it on multiple augmented views of each\ntest input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping\nTTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods\nattain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\nThe CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.\nIt is divided into a training set of 50,000 images and a validation set of 10,000 images. As a\nmatter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was\n2\n80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than\nairbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94\ncan be attributed entirely to algorithmic progress rather than compute scaling.\n3 Methods\n3.1 Network architecture and baseline training\nWe train a convolutional network with a total of 1.97 million parameters, following tysam-code\n(2023) with a few small changes. It contains seven convolutions with the latter six being divided into\nthree blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this\nsection we offer some comments on the main design choices.\nThe network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed\nentirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,\n2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with\nno padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather\nthan the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff\nbetween throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\nFollowing Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,\nand disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is\nscaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture\ndiffers only in that we decrease the number of output channels in the third block from 512 to 256,\nand we add learnable biases to the first convolution.\nAs our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of\n0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the\nmaximum at 20% of the way through training, and then decreases to zero. For data augmentation we\nuse random horizontal flipping alongside 2-pixel random translation. For translation we use reflection\npadding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note\nthat what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a\nrandom 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the\nnetwork is run on both a given test image and its mirror and inferences are made based on the average\nof the two outputs. With optimized choices of learning rate, momentum, and weight decay, this\nbaseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.\n3.2 Frozen patch-whitening initialization\nFollowing Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-\nwhitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code\n(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches\nacross the training distribution, so that their outputs have identity covariance matrix. The second 12\nFigure 2: The first layer\u2019s weights after whitening\ninitialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,\nso that input information is preserved through the\nactivation which follows. Figure 2 shows the result.\nWe do not update this layer\u2019s weights during training.\nDeparting from tysam-code (2023), we add learn-\nable biases to this layer, yielding a small performance\nboost. The biases are trained for 3 epochs, after which\nwe disable their gradient to increase backward-pass\nthroughput, which improves training speed without\nreducing accuracy. We also obtain a slight perfor-\nmance boost relative to tysam-code (2023) by re-\nducing the constant added to the eigenvalues during\ncalculation of the patch-whitening initialization for\nthe purpose of preventing numerical issues in the case\nof a singular patch-covariance matrix.\nPatch-whitening initialization is the single most impactful feature. Adding it to the baseline more\nthan doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.\n3\nRandom reshuffling Alternating flip Mean accuracy\nNo No 93.40%\nNo Yes 93.48%\nYes No 93.92%\nYes Yes 94.01%\nTable 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating\nflip (which we propose) reduce training data redundancy and improve performance.\n3.3 Identity initialization\ndirac : We initialize all convolutions after the first as partial identity transforms. That is, for a\nconvolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an\nidentity transform of the input, and leave the remaining N\u2212Mto their default initialization. In\nPyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight\nwof each convolutional layer. This method partially follows tysam-code (2023), which used a more\ncomplicated scheme where the identity weights are mixed in with the original initialization, which\nwe did not find to be more performant. With this feature added, training attains 94% accuracy in 18\nepochs taking 6.8 A100-seconds.\n3.4 Optimization tricks\nscalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor\nof64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%\nin 13.5 epochs taking 5.1 A100-seconds.\nlookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.\nWe note that Lookahead has also been found effective in prior work on training speed for ResNet-\n18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6\nA100-seconds.\n3.5 Multi-crop evaluation\nmulticrop : To generate predictions, we run the trained network on six augmented views of each\ntest image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a\nversion which is translated down-and-to-the-right by one pixel, and the mirrored versions of all\nthree. Predictions are made using a weighted average of all six outputs, where the two views of the\nuntranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125\neach. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.\nWe note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-\nmonyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of\nevaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using\nmore crops does improve performance, but the increase to inference time outweighs the potential\ntraining speedup.\n3.6 Alternating flip\nTo speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,\nwhich we motivate as follows. When training neural networks, it is standard practice to organize\ntraining into a set of epochs during which every training example is seen exactly once. This\ndiffers from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),\nwhich calls for data to be repeatedly sampled with-replacement from the training set, resulting in\nexamples being potentially seen multiple redundant times within a short window of training. The\nuse of randomly ordered epochs of data for training has a different name, being called the random\nreshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our\ntraining dataset consists of Nunique examples, then sampling data with replacement causes every\n\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N\nunique examples on average. On the other hand, random reshuffling leads to all Nunique examples\nbeing seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason\nthat it is beneficial to maximize the number of unique inputs seen per window of training time.\n4\nWe extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We\nfirst note that standard random horizontal flipping augmentation can be defined as follows.\n1import torch\n2def random_flip ( inputs ):\n3 # Applies random flipping to a batch of images\n4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)\n5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]\nListing 1: Random flip\nIf horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2\nwhich may be seen during training. Potentially, every pair of consecutive epochs could contain every\nunique input. But our main observation is that with standard random horizontal flipping, half of the\nimages will be redundantly flipped the same way during both epochs, so that on average only 1.5N\nunique inputs will be seen.\naltflip : To address this, we propose to modify standard random horizontal flipping augmentation as\nfollows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},\nwe flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we\nflip only those inputs which were flipped in the first epoch. We provide the following implementation\nwhich avoids the need for extra memory by using a pseudorandom function to decide the flips.\n1import torch\n2import hashlib\n3def hash_fn (n, seed =42) :\n4 k = n * seed\n5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,\n16)\n6def alternating_flip (inputs , indices , epoch ):\n7 # Applies alternating flipping to a batch of images\n8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist\n() ])\n9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)\n10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\nListing 2: Alternating flip\nThe result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in\nFigure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.\nAdding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final\ntraining method airbench94.py , the entire contents of which can be found in Section E. It reaches\n94% accuracy in 3.83 seconds on an NVIDIA A100.\n3.7 Compilation\nThe final step we take to speed up training is a non-algorithmic one: we compile our training\nmethod using torch.compile in order to more efficiently utilize the GPU. This results in a training\nscript which is mathematically equivalent (up to small differences in floating point arithmetic) to\nthe non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29\nA100-seconds. The downside is that the one-time compilation process takes up to several minutes to\ncomplete before training runs can begin, so that it is only beneficial when we plan to execute many\nruns of training at once. We release this version as airbench94_compiled.py .\n4 95% and 96% targets\nTo address scenarios where somewhat higher performance is desired, we additionally develop methods\ntargeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .\nTo attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel\ncount of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce\nthe learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%\naccuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.\n2Assuming none of the training inputs are already mirrors of each other.\n5\nFigure 4: Training speedups accumulate additively. Removing individual features from airbench94\nincreases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces\nthe epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-\n94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.\nFigure 3: FLOPs vs. error rate tradeoff. Our\nthree training methods apparently follow a linear\nlog-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-\nVries & Taylor, 2017) augmentation and raise the\ntraining epochs to 40. We add a third convolution to\neach block, and scale the first block to 128 channels\nand the second two to 512. We also add a residual\nconnection across the later two convolutions of each\nblock, which we find is still beneficial despite the fact\nthat we are already using identity initialization (Sec-\ntion 3.3) to ease gradient flow. Finally, we reduce\nthe learning rate by a factor of 0.78. These changes\nyield airbench96 which attains 96.05% accuracy in\n46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.\nFigure 3 shows the FLOPs and error rate of each of\nour three training methods.\n5 Experiments\n5.1 Interaction between features\nTo gain a better sense of the impact of each feature on training speed, we compare two quantities.\nFirst, we measure the number of epochs that can be saved by adding the feature to the whitened\nbaseline (Section 3.2). Second, we measure the number of epochs that must be added when the\nfeature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-\ntion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it\nfrom the final airbench94 increases epochs-to-94% from 9.9 to 12.8.\nFigure 4 shows both quantities for each feature. Surprisingly, we find that for all features except\nmulti-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even\nthough the whitened baseline requires more than twice as many epochs as the final configuration.\nThis indicates that the interaction between most features is additive rather than multiplicative.\n5.2 Does alternating flip generalize?\nIn this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of\ntraining configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all\ncases except those where neither alternating nor random flip improve over using no flipping at all.\nFor CIFAR-10 we consider the performance boost given by alternating flip across the follow-\ning 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and\nairbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For\n6\nFigure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,\nthe use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%\ntraining speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.\n95% confidence intervals are shown around each point.\neach configuration we compare the performance of alternating and random flip in terms of their mean\naccuracy across n= 400 runs of training.\nFigure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip\nimproves performance in every setting. To get a sense for how big the improvement is, we estimate\nthe effective speedup for each case, i.e., the fraction of epochs that could be saved by switching\nfrom random to alternating flip while maintaining the level of accuracy of random flip. We begin\nby fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of\neach random flip-based training configuration. We use these curves to calculate the effective speedup\nafforded by switching from random to alternating flip. For example, airbench94 with random flip\nand without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The\nsame configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law\nfit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.\nNote that using a power-law yields a more conservative estimate relative to using linear interpolation\nbetween the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.\nTable 2 shows the result. We observe the following patterns. First, the addition of extra augmentation\n(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that\nthe main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the\nsame way for many epochs in a row; we speculate that adding extra augmentation reduces the negative\nimpact of these cases because it increases data diversity. Next, TTA reduces the gap between random\nand alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),\nindicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training\nfor longer consistently increases the effective speedup given by alternating flip.\nWe next study ImageNet trainings with the following experiment. We train a ResNet-18 with a\nvariety of train and test crops, comparing three flipping options: alternating flip, random flip, and no\nflipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192\ncenter crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)\nto denote the latter. We also consider two training crops: 192x192 inception-style random resized\ncrop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area\nranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the\nshorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write\n7\nBaseline Cutout Epochs Speedup Speedup (w/ TTA)\nairbench94 No 10 15.0% 5.30%\nairbench94 No 20 27.1% 21.3%\nairbench94 No 40 38.3% 36.4%\nairbench94 No 80 102% 31.8%\nairbench94 Yes 10 3.84% 1.13%\nairbench94 Yes 20 7.42% 2.00%\nairbench94 Yes 40 18.6% 9.28%\nairbench94 Yes 80 29.2% 14.25%\nairbench96 Yes 10 4.94% 1.11%\nairbench96 Yes 20 8.99% 3.58%\nairbench96 Yes 40 17.2% 6.48%\nairbench96 Yes 80 18.8% Not measured\nTable 2: Effective speedups given by switching from random flip to alternating flip. The two configurations\nmost closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw\naccuracy values of the airbench94 experiments.\nFlipping augmentation option\nTrain crop Test crop Epochs TTA None Random Alternating\nHeavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28\nHeavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28\nLight RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26\nLight RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26\nHeavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28\nHeavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28\nLight RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26\nLight RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26\nLight RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16\nHeavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4\nTable 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for\nthose trainings where random flip improves significantly over not flipping at all. The single best flipping option\nin each row is bolded when the difference is statistically significant.\nHeavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided\nin Section C.\nTable 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC\nis better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly\nbetter when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test\nresolution discrepancy (Touvron et al., 2019).\nFor trainings which use Light RRC, we find that switching from random flip to alternating flip\nprovides a substantial boost to performance, amounting to a training speedup of more than 25%.\nIn Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to\nalternating flip improves performance by more than increasing the training duration from 16 to 20\nepochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our\nresults on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit\nfrom alternating flip. Indeed, even turning flipping off completely does not significantly reduce the\nperformance of these trainings. We conclude that alternating flip improves over random flip for every\ntraining scenario where the latter improves over no flipping at all.\n8\nEpochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE\n1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312\n2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233\n1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269\n1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533\n2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433\n1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444\nTable 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).\n5.3 Variance and class-wise calibration\nPrevious sections have focused on understanding what factors affect the first moment of accuracy\n(the mean). In this section we investigate the second moment, finding that TTA reduces variance at\nthe cost of calibration.\nOur experiment is to execute 10,000 runs of airbench94 training with several hyperparameter\nsettings. For each setting we report both the variance in test-set accuracy as well as an estimate of the\ndistribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.\nTable 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set\nvariance, replicating the main finding of Jordan (2023). This is a surprising result because these\ntrainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7\nas much distribution-wise variance when run for a similar duration, and reached a low variance only\nwhen run for 64 epochs. We conclude from this comparison that distribution-wise variance is more\nstrongly connected to the rate of convergence of a training rather than its duration as such. We also\nnote that the low distribution-wise variance of airbench94 indicates it has high training stability.\nUsing TTA significantly reduces the test-set variance, such that all three settings with TTA have lower\ntest-set variance than any setting without TTA. However, test-set variance is implied by the class-wise\ncalibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this\nreduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,\nwe compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which\nmeasures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA\nhas a higher CACE than every setting without TTA, confirming the hypothesis.\n6 Discussion\nIn this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7\nfaster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the\nairbench Python package.\nWe developed airbench solely with the goal of maximizing training speed on CIFAR-10. In\nSection B we find that it also generalizes well to other tasks. For example, without any extra tuning,\nairbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.\nOne factor contributing to the training speed of airbench was our finding that training can be accel-\nerated by partially derandomizing the standard random horizontal flipping augmentation, resulting in\nthe variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating\nflip improves the performance of every training we considered (Section 5.2), with the exception of\nthose trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to\nus, the standard ImageNet trainings that we considered do not significantly benefit from horizontal\nflipping. Future work might investigate whether it is possible to obtain derandomized improvements\nto other augmentations besides horizontal flip.\nThe methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,\nwith fixed performance and hardware constraints. These constraints mean that we cannot improve\nperformance by simply scaling up the amount of computational resources used; instead we are forced\nto develop new methods like the alternating flip. We look forward to seeing what other new methods\nfuture work discovers to push training speed further.\n9\nReferences\nDimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.\nDan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-\nperformance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,\n2011.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\nmark and competition. 2017.\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet\nor cifar-10. arXiv preprint arXiv:1810.03505 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\npp. 248\u2013255. IEEE, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552 , 2017.\nMert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic\ngradient descent. Mathematical Programming , 186:49\u201384, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 , 2016.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.\npmlr, 2015.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\nsgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.\nKeller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and\ninevitable. arXiv preprint arXiv:2304.01910 , 2023.\nAndrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.\nio/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for\nadvanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT\nLicense.\nGuillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander\nM \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,\nBenjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.\nBenchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural\nInformation Processing Systems , 35:25404\u201325421, 2022.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.\nDavid Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/\nhow-to-train-your-resnet-8-bag-of-tricks/ .\n10\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics , 22(3):400\u2013407, 1951.\nSantiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:\n//github.com/apple/ml-cifar-10-faster , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.\ntysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/\nhlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. Advances in neural information processing systems , 32, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,\n2016.\nMichael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps\nforward, 1 step back. Advances in neural information processing systems , 32, 2019.\n11\nA Network architecture\n1from torch import nn\n2\n3class Flatten (nn. Module ):\n4 def forward (self , x):\n5 return x. view (x. size (0) , -1)\n6\n7class Mul (nn. Module ):\n8 def __init__ (self , scale ):\n9 super (). __init__ ()\n10 self . scale = scale\n11 def forward (self , x):\n12 return x * self . scale\n13\n14def conv (ch_in , ch_out ):\n15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,\n16 padding =\u2019same \u2019, bias = False )\n17\n18def make_net ():\n19 act = lambda : nn. GELU ()\n20 bn = lambda ch: nn. BatchNorm2d (ch)\n21 return nn. Sequential (\n22 nn. Sequential (\n23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),\n24 act () ,\n25 ),\n26 nn. Sequential (\n27 conv (24 , 64) ,\n28 nn. MaxPool2d (2) ,\n29 bn (64) , act () ,\n30 conv (64 , 64) ,\n31 bn (64) , act () ,\n32 ),\n33 nn. Sequential (\n34 conv (64 , 256) ,\n35 nn. MaxPool2d (2) ,\n36 bn (256) , act () ,\n37 conv (256 , 256) ,\n38 bn (256) , act () ,\n39 ),\n40 nn. Sequential (\n41 conv (256 , 256) ,\n42 nn. MaxPool2d (2) ,\n43 bn (256) , act () ,\n44 conv (256 , 256) ,\n45 bn (256) , act () ,\n46 ),\n47 nn. MaxPool2d (3) ,\n48 Flatten () ,\n49 nn. Linear (256 , 10, bias = False ),\n50 Mul (1/9) ,\n51 )\nListing 3: Minimal PyTorch code for the network architecture used by airbench94 .\nWe note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock\ntime. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel\nsize of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this\ndoes not improve the wallclock training time on an A100. Reducing the batch size is another easy\nway to save FLOPs but not wallclock time.\n12\nDataset Flipping? Cutout? ResNet-18 airbench96\nCIFAR-10 Yes No 95.55% 95.61%\nCIFAR-10 Yes Yes 96.01% 96.05%\nCIFAR-100 Yes No 77.54% 79.27%\nCIFAR-100 Yes Yes 78.04% 79.76%\nCINIC-10 Yes No 87.58% 87.78%\nCINIC-10 Yes Yes not measured 88.22%\nSVHN No No 97.35% 97.38%\nSVHN No Yes not measured 97.64%\nTable 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We\ndirectly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off\nflipping for SVHN).\nB Extra dataset experiments\nWe developed airbench with the singular goal of maximizing training speed on CIFAR-10. To\nfind out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its\nperformance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-\n10 (Darlow et al., 2018).\nOn CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both\nthe case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not\n(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than\nResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say\nthat it generalizes.\nWe compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.\nWe do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal\non CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better\nperformance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where\nairbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is\nused and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows\nstrong generalization to other tasks.\nWe note that this comparison between airbench96 and ResNet-18 training is fair in the sense that\nit does demonstrate that the former has good generalization, but unfair in the sense that it does not\nindicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time\naugmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training\nwould outperform airbench96 if it were run using test-time augmentation. However, it also takes\n5-10 times longer to complete. The decision of which to use may be situational.\nThe accuracy values we report for ResNet-18 training are from the following sources. We tried\nto select the highest values we could find for each setting. Moreau et al. (2022) reports attaining\n95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports\nattaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on\nCIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.\nC ImageNet training details\nOur ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/\nlibffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and\nmomentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution\n160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of\ntraining. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.\n13\nHyperparameters Flipping augmentation option\nEpochs Cutout TTA None Random Alternating\n10 No No 92.3053 93.0988 93.2798\n20 No No 92.8166 93.7446 93.8652\n40 No No 93.0143 94.0133 94.0729\n80 No No 93.0612 94.1169 94.1628\n10 No Yes 93.4071 93.9488 94.0186\n20 No Yes 93.8528 94.5565 94.6530\n40 No Yes 94.0381 94.7803 94.8203\n80 No Yes 94.0638 94.8506 94.8676\n10 Yes No 91.8487 92.0402 92.1374\n20 Yes No 92.8474 93.3825 93.4876\n40 Yes No 93.2675 94.1014 94.1952\n80 Yes No 93.4193 94.4311 94.5204\n10 Yes Yes 92.6455 92.7780 92.8103\n20 Yes Yes 93.7862 94.1306 94.1670\n40 Yes Yes 94.3090 94.8511 94.8960\n80 Yes Yes 94.5253 95.1839 95.2362\nTable 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a\nmean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise\ndifference in means is statistically significant.\nD Extra tables & figures\nFigure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in\nSection 5.3.\nE Complete training code\n1\"\"\"\n2airbench94 .py\n33.83 s runtime on an A100 ; 0.36 PFLOPs .\n4Evidence for validity : 94.01 average accuracy in n =1000 runs .\n5\n6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB\nwith the following nvidia -smi:\n7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :\n11.7\n14\n8torch . __version__ == \u20192.1.2+ cu118 \u2019\n9\"\"\"\n10\n11# ############################################\n12# Setup / Hyperparameters #\n13# ############################################\n14\n15import os\n16import sys\n17import uuid\n18from math import ceil\n19\n20import torch\n21from torch import nn\n22import torch .nn. functional as F\n23import torchvision\n24import torchvision . transforms as T\n25\n26torch . backends . cudnn . benchmark = True\n27\n28\"\"\"\n29We express the main training hyperparameters ( batch size , learning\nrate , momentum , and weight decay ) in decoupled form , so that each\none can be tuned independently . This accomplishes the following :\n30* Assuming time - constant gradients , the average step size is decoupled\nfrom everything but the lr.\n31* The size of the weight decay update is decoupled from everything but\nthe wd.\n32In constrast , normally when we increase the ( Nesterov ) momentum , this\nalso scales up the step size proportionally to 1 + 1 / (1 -\nmomentum ), meaning we cannot change momentum without having to re -\ntune the learning rate . Similarly , normally when we increase the\nlearning rate this also increases the size of the weight decay ,\nrequiring a proportional decrease in the wd to maintain the same\ndecay strength .\n33\n34The practical impact is that hyperparameter tuning is faster , since\nthis parametrization allows each one to be tuned independently .\nSee https :// myrtle .ai/ learn /how -to -train -your - resnet -5-\nhyperparameters /.\n35\"\"\"\n36\n37hyp = {\n38 \u2019opt \u2019: {\n39 \u2019 train_epochs \u2019: 9.9 ,\n40 \u2019batch_size \u2019: 1024 ,\n41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples\n42 \u2019momentum \u2019: 0.85 ,\n43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (\ndecoupled from learning rate )\n44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not\nweight decay ) for BatchNorm biases\n45 \u2019 label_smoothing \u2019: 0.2 ,\n46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the\nwhitening layer bias before freezing\n47 },\n48 \u2019aug \u2019: {\n49 \u2019flip \u2019: True ,\n50 \u2019translate \u2019: 2,\n51 },\n52 \u2019net \u2019: {\n53 \u2019widths \u2019: {\n54 \u2019block1 \u2019: 64,\n55 \u2019block2 \u2019: 256 ,\n56 \u2019block3 \u2019: 256 ,\n15\n57 },\n58 \u2019 batchnorm_momentum \u2019: 0.6 ,\n59 \u2019 scaling_factor \u2019: 1/9 ,\n60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :\n0= none , 1= mirror , 2= mirror + translate\n61 },\n62}\n63\n64# ############################################\n65# DataLoader #\n66# ############################################\n67\n68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )\n69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )\n70\n71def batch_flip_lr ( inputs ):\n72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .\nview (-1, 1, 1, 1)\n73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\n74\n75def batch_crop (images , crop_size ):\n76 r = ( images . size ( -1) - crop_size )//2\n77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =\nimages . device )\n78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),\ndevice = images .device , dtype = images . dtype )\n79 # The two cropping methods in this if - else produce equivalent\nresults , but the second is faster for r > 2.\n80 if r <= 2:\n81 for sy in range (-r, r +1):\n82 for sx in range (-r, r +1):\n83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)\n84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size\n, r+sx:r+sx+ crop_size ]\n85 else :\n86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size\n+2* r), device = images .device , dtype = images . dtype )\n87 for s in range (-r, r +1):\n88 mask = ( shifts [:, 0] == s)\n89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]\n90 for s in range (-r, r +1):\n91 mask = ( shifts [:, 1] == s)\n92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+\ncrop_size ]\n93 return images_out\n94\n95class CifarLoader :\n96 \"\"\"\n97 GPU - accelerated dataloader for CIFAR -10 which implements\nalternating flip augmentation .\n98 \"\"\"\n99\n100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,\ndrop_last =None , shuffle =None , gpu =0):\n101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .\npt \u2019)\n102 if not os. path . exists ( data_path ):\n103 dset = torchvision . datasets . CIFAR10 (path , download =True ,\ntrain = train )\n104 images = torch . tensor ( dset . data )\n105 labels = torch . tensor ( dset . targets )\n106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:\ndset . classes }, data_path )\n107\n108 data = torch . load ( data_path , map_location = torch . device (gpu))\n16\n109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [\n\u2019labels \u2019], data [\u2019classes \u2019]\n110 # It \u2019s faster to load + process uint8 data than to load\npreprocessed fp16 data\n111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).\nto( memory_format = torch . channels_last )\n112\n113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )\n114 self . proc_images = {} # Saved results of image processing to\nbe done on the first epoch\n115 self . epoch = 0\n116\n117 self . aug = aug or {}\n118 for k in self .aug . keys ():\n119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019\n% k\n120\n121 self . batch_size = batch_size\n122 self . drop_last = train if drop_last is None else drop_last\n123 self . shuffle = train if shuffle is None else shuffle\n124\n125 def __len__ ( self ):\n126 return len( self . images )// self . batch_size if self . drop_last\nelse ceil (len( self . images )/ self . batch_size )\n127\n128 def __iter__ ( self ):\n129\n130 if self . epoch == 0:\n131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .\nimages )\n132 # Randomly flip all images on the first epoch as according\nto definition of alternating flip\n133 if self .aug.get(\u2019flip \u2019, False ):\n134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (\nimages )\n135 # Pre -pad images to save time when doing random\ntranslation\n136 pad = self .aug.get (\u2019translate \u2019, 0)\n137 if pad > 0:\n138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019\nreflect \u2019)\n139\n140 if self .aug.get(\u2019translate \u2019, 0) > 0:\n141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .\nshape [ -2])\n142 elif self .aug.get (\u2019flip \u2019, False ):\n143 images = self . proc_images [\u2019flip \u2019]\n144 else :\n145 images = self . proc_images [\u2019norm \u2019]\n146 if self .aug.get(\u2019flip \u2019, False ):\n147 if self . epoch % 2 == 1:\n148 images = images . flip ( -1)\n149\n150 self . epoch += 1\n151\n152 indices = ( torch . randperm if self . shuffle else torch . arange )(\nlen ( images ), device = images . device )\n153 for i in range (len ( self )):\n154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]\n155 yield ( images [ idxs ], self . labels [ idxs ])\n156\n157# ############################################\n158# Network Components #\n159# ############################################\n160\n17\n161class Flatten (nn. Module ):\n162 def forward (self , x):\n163 return x. view (x. size (0) , -1)\n164\n165class Mul (nn. Module ):\n166 def __init__ (self , scale ):\n167 super (). __init__ ()\n168 self . scale = scale\n169 def forward (self , x):\n170 return x * self . scale\n171\n172class BatchNorm (nn. BatchNorm2d ):\n173 def __init__ (self , num_features , momentum , eps =1e -12 ,\n174 weight =False , bias = True ):\n175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )\n176 self . weight . requires_grad = weight\n177 self . bias . requires_grad = bias\n178 # Note that PyTorch already initializes the weights to one and\nbiases to zero\n179\n180class Conv (nn. Conv2d ):\n181 def __init__ (self , in_channels , out_channels , kernel_size =3,\npadding =\u2019same \u2019, bias = False ):\n182 super (). __init__ ( in_channels , out_channels , kernel_size =\nkernel_size , padding = padding , bias = bias )\n183\n184 def reset_parameters ( self ):\n185 super (). reset_parameters ()\n186 if self . bias is not None :\n187 self . bias . data . zero_ ()\n188 w = self . weight . data\n189 torch .nn. init . dirac_ (w[:w. size (1) ])\n190\n191class ConvGroup (nn. Module ):\n192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):\n193 super (). __init__ ()\n194 self . conv1 = Conv ( channels_in , channels_out )\n195 self . pool = nn. MaxPool2d (2)\n196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )\n197 self . conv2 = Conv ( channels_out , channels_out )\n198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )\n199 self . activ = nn. GELU ()\n200\n201 def forward (self , x):\n202 x = self . conv1 (x)\n203 x = self . pool (x)\n204 x = self . norm1 (x)\n205 x = self . activ (x)\n206 x = self . conv2 (x)\n207 x = self . norm2 (x)\n208 x = self . activ (x)\n209 return x\n210\n211# ############################################\n212# Network Definition #\n213# ############################################\n214\n215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019\n][ \u2019 batchnorm_momentum \u2019]):\n216 whiten_kernel_size = 2\n217 whiten_width = 2 * 3 * whiten_kernel_size **2\n218 net = nn. Sequential (\n219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True\n),\n220 nn. GELU () ,\n18\n221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],\nbatchnorm_momentum ),\n222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],\nbatchnorm_momentum ),\n223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],\nbatchnorm_momentum ),\n224 nn. MaxPool2d (3) ,\n225 Flatten () ,\n226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),\n227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,\n228 )\n229 net [0]. weight . requires_grad = False\n230 net = net. half (). cuda ()\n231 net = net.to( memory_format = torch . channels_last )\n232 for mod in net. modules ():\n233 if isinstance (mod , BatchNorm ):\n234 mod . float ()\n235 return net\n236\n237# ############################################\n238# Whitening Conv Initialization #\n239# ############################################\n240\n241def get_patches (x, patch_shape ):\n242 c, (h, w) = x. shape [1] , patch_shape\n243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h\n,w). float ()\n244\n245def get_whitening_parameters ( patches ):\n246 n,c,h,w = patches . shape\n247 patches_flat = patches . view (n, -1)\n248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n\n249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance\n, UPLO =\u2019U\u2019)\n250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.\nreshape (c*h*w,c,h,w). flip (0)\n251\n252def init_whitening_conv (layer , train_set , eps =5e -4):\n253 patches = get_patches ( train_set , patch_shape = layer . weight . data .\nshape [2:])\n254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )\n255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)\n256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -\neigenvectors_scaled ))\n257\n258# ###########################################\n259# Lookahead #\n260# ###########################################\n261\n262class LookaheadState :\n263 def __init__ (self , net):\n264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().\nitems ()}\n265\n266 def update (self , net , decay ):\n267 for ema_param , net_param in zip( self . net_ema . values () , net.\nstate_dict (). values ()):\n268 if net_param . dtype in ( torch .half , torch . float ):\n269 ema_param . lerp_ ( net_param , 1- decay )\n270 net_param . copy_ ( ema_param )\n271\n272# ###########################################\n273# Logging #\n274# ###########################################\n275\n19\n276def print_columns ( columns_list , is_head =False , is_final_entry = False ):\n277 print_string = \u2019\u2019\n278 for col in columns_list :\n279 print_string += \u2019| %s \u2019 % col\n280 print_string += \u2019|\u2019\n281 if is_head :\n282 print (\u2019-\u2019*len( print_string ))\n283 print ( print_string )\n284 if is_head or is_final_entry :\n285 print (\u2019-\u2019*len( print_string ))\n286\n287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,\n\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]\n288def print_training_details ( variables , is_final_entry ):\n289 formatted = []\n290 for col in logging_columns_list :\n291 var = variables .get (col. strip () , None )\n292 if type (var) in (int , str):\n293 res = str(var)\n294 elif type (var) is float :\n295 res = \u2019{:0.4 f}\u2019. format ( var)\n296 else :\n297 assert var is None\n298 res = \u2019\u2019\n299 formatted . append (res. rjust (len (col)))\n300 print_columns ( formatted , is_final_entry = is_final_entry )\n301\n302# ###########################################\n303# Evaluation #\n304# ###########################################\n305\n306def infer (model , loader , tta_level =0) :\n307 \"\"\"\n308 Test - time augmentation strategy (for tta_level =2):\n309 1. Flip / mirror the image left -to - right (50% of the time ).\n310 2. Translate the image by one pixel either up -and - left or down -and\n- right (50% of the time , i.e. both happen 25% of the time ).\n311\n312 This creates 6 views per image ( left / right times the two\ntranslations and no - translation ), which we evaluate and then\nweight according to the given probabilities .\n313 \"\"\"\n314\n315 def infer_basic (inputs , net):\n316 return net( inputs ). clone ()\n317\n318 def infer_mirror (inputs , net):\n319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))\n320\n321 def infer_mirror_translate ( inputs , net ):\n322 logits = infer_mirror (inputs , net )\n323 pad = 1\n324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)\n325 inputs_translate_list = [\n326 padded_inputs [:, :, 0:32 , 0:32] ,\n327 padded_inputs [:, :, 2:34 , 2:34] ,\n328 ]\n329 logits_translate_list = [ infer_mirror ( inputs_translate , net)\n330 for inputs_translate in\ninputs_translate_list ]\n331 logits_translate = torch . stack ( logits_translate_list ). mean (0)\n332 return 0.5 * logits + 0.5 * logits_translate\n333\n334 model . eval ()\n335 test_images = loader . normalize ( loader . images )\n20\n336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][\ntta_level ]\n337 with torch . no_grad ():\n338 return torch .cat ([ infer_fn (inputs , model ) for inputs in\ntest_images . split (2000) ])\n339\n340def evaluate (model , loader , tta_level =0) :\n341 logits = infer (model , loader , tta_level )\n342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()\n343\n344# ###########################################\n345# Training #\n346# ###########################################\n347\n348def main (run):\n349\n350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]\n351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]\n352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]\n353 # Assuming gradients are constant in time , for Nesterov momentum ,\nthe below ratio is how much larger the default steps will be than\nthe underlying per - example gradients . We divide the learning rate\nby this ratio in order to ensure steps are the same scale as\ngradients , regardless of the choice of momentum .\n354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))\n355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning\nrate for PyTorch SGD\n356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale\n357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]\n358\n359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019\nlabel_smoothing \u2019], reduction =\u2019none \u2019)\n360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)\n361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =\nbatch_size , aug=hyp[\u2019aug \u2019])\n362 if run == \u2019warmup \u2019:\n363 # The only purpose of the first run is to warmup , so we can\nuse dummy data\n364 train_loader . labels = torch . randint (0, 10, size =( len (\ntrain_loader . labels ) ,), device = train_loader . labels . device )\n365 total_train_steps = ceil (len( train_loader ) * epochs )\n366\n367 model = make_net ()\n368 current_steps = 0\n369\n370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in\nk and p. requires_grad ]\n371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019\nnot in k and p. requires_grad ]\n372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,\nweight_decay =wd/ lr_biases ),\n373 dict ( params = other_params , lr=lr , weight_decay =wd/\nlr)]\n374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,\nnesterov = True )\n375\n376 def triangle (steps , start =0, end =0, peak =0.5) :\n377 xp = torch . tensor ([0 , int( peak * steps ), steps ])\n378 fp = torch . tensor ([ start , 1, end ])\n379 x = torch . arange (1+ steps )\n380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])\n381 b = fp [: -1] - (m * xp [: -1])\n382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1\n383 indices = torch . clamp ( indices , 0, len(m) - 1)\n384 return m[ indices ] * x + b[ indices ]\n21\n385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,\npeak =0.23)\n386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:\nlr_schedule [i])\n387\n388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /\ntotal_train_steps )**3\n389 lookahead_state = LookaheadState ( model )\n390\n391 # For accurately timing GPU code\n392 starter = torch . cuda . Event ( enable_timing = True )\n393 ender = torch . cuda . Event ( enable_timing = True )\n394 total_time_seconds = 0.0\n395\n396 # Initialize the first layer using statistics of training images\n397 starter . record ()\n398 train_images = train_loader . normalize ( train_loader . images [:5000])\n399 init_whitening_conv ( model [0] , train_images )\n400 ender . record ()\n401 torch . cuda . synchronize ()\n402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n403\n404 for epoch in range ( ceil ( epochs )):\n405\n406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019\nwhiten_bias_epochs \u2019])\n407\n408 # ###################\n409 # Training #\n410 # ###################\n411\n412 starter . record ()\n413\n414 model . train ()\n415 for inputs , labels in train_loader :\n416\n417 outputs = model ( inputs )\n418 loss = loss_fn ( outputs , labels ).sum ()\n419 optimizer . zero_grad ( set_to_none = True )\n420 loss . backward ()\n421 optimizer . step ()\n422 scheduler . step ()\n423\n424 current_steps += 1\n425\n426 if current_steps % 5 == 0:\n427 lookahead_state . update (model , decay = alpha_schedule [\ncurrent_steps ]. item ())\n428\n429 if current_steps >= total_train_steps :\n430 if lookahead_state is not None :\n431 lookahead_state . update (model , decay =1.0)\n432 break\n433\n434 ender . record ()\n435 torch . cuda . synchronize ()\n436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n437\n438 # ###################\n439 # Evaluation #\n440 # ###################\n441\n442 # Print the accuracy and loss from the last training batch of\nthe epoch\n22\n443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().\nmean (). item ()\n444 train_loss = loss . item () / batch_size\n445 val_acc = evaluate (model , test_loader , tta_level =0)\n446 print_training_details ( locals () , is_final_entry = False )\n447 run = None # Only print the run number once\n448\n449 # ###################\n450 # TTA Evaluation #\n451 # ###################\n452\n453 starter . record ()\n454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019\ntta_level \u2019])\n455 ender . record ()\n456 torch . cuda . synchronize ()\n457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n458\n459 epoch = \u2019eval \u2019\n460 print_training_details ( locals () , is_final_entry = True )\n461\n462 return tta_val_acc\n463\n464if __name__ == \" __main__ \":\n465 with open (sys. argv [0]) as f:\n466 code = f. read ()\n467\n468 print_columns ( logging_columns_list , is_head = True )\n469 main (\u2019warmup \u2019)\n470 accs = torch . tensor ([ main (run) for run in range (25) ])\n471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))\n472\n473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }\n474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))\n475 os. makedirs ( log_dir , exist_ok = True )\n476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)\n477 print (os. path . abspath ( log_path ))\n478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))\nListing 4: airbench94.py\n23\n\nCandidate Output: tasks=[TaskCandidate(name='Implement alternating flip data augmentation', description='Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in an alternating pattern across epochs rather than randomly, to reduce redundancy.', relevant_paper_text='To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we flip only those inputs which were flipped in the first epoch.', scoring_feasibility=9, llm_tractability=0.8, expert_tractability=0.95, layman_tractability=0.3), TaskCandidate(name='Implement decoupled hyperparameters', description='Implement the decoupled hyperparameter scheme described in the paper, where batch size, learning rate, momentum, and weight decay are expressed in a form that allows each to be tuned independently.', relevant_paper_text='We express the main training hyperparameters (batch size, learning rate, momentum, and weight decay) in decoupled form, so that each one can be tuned independently. This accomplishes the following:\\n* Assuming time-constant gradients, the average step size is decoupled from everything but the lr.\\n* The size of the weight decay update is decoupled from everything but the wd.', scoring_feasibility=8, llm_tractability=0.7, expert_tractability=0.9, layman_tractability=0.2), TaskCandidate(name='Implement patch-whitening initialization', description='Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper. This involves initializing the filters based on the eigenvectors of the covariance matrix of patches from the training data.', relevant_paper_text='Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.', scoring_feasibility=7, llm_tractability=0.6, expert_tractability=0.85, layman_tractability=0.1)]\n\nRequirements:\nExtract all promising task candidates from paper. Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\n\n    A good task candidate:\n    - describes a task that an agent can implement\n    - typically a programming task, e.g. modifying a 'baseline' implementation\n    - which reflects or mirrors actual tasks undertaken to produce the research paper\n    - and requires research and/or engineering expertise to implement correctly\n    - for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\n    - ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\n    - the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\n    \n\nAttempt Num: 0\n\nPrevious Critiques:"
  },
  {
    "Response": "Self Critique:\n\nI'll evaluate the task candidates I extracted based on the given requirements:\n\n1. Implement alternating flip data augmentation:\nStrengths:\n- Describes a concrete programming task to modify a baseline implementation\n- Reflects an actual task undertaken in the research\n- Requires engineering expertise to implement correctly\n- Relatively straightforward to write an objective scoring function (e.g. check if images are flipped in the correct pattern across epochs)\n- Scoring function would be much easier to produce than implementing the task itself\n\nWeaknesses:\n- May not require significant research capabilities, mostly engineering\n\n2. Implement decoupled hyperparameters:\nStrengths:\n- Describes a programming task to modify hyperparameter handling\n- Reflects work done in the research\n- Requires some engineering expertise\n\nWeaknesses:\n- May be challenging to write an objective scoring function beyond checking if the hyperparameters are decoupled\n- Implementing the task may not be significantly harder than writing the scoring function\n- Does not require much research capability\n\n3. Implement patch-whitening initialization:\nStrengths:\n- Describes a concrete programming task\n- Reflects actual work from the research\n- Requires both research understanding and engineering skill\n- Possible to write an objective scoring function (e.g. check if filters are initialized correctly)\n- Scoring function would be easier than implementation\n\nWeaknesses:\n- May be very challenging for an agent without strong math/ML background\n\nOverall, the alternating flip and patch-whitening tasks seem to best meet the criteria. The decoupled hyperparameters task is less suitable. I could improve by extracting more task candidates from other parts of the paper, especially those requiring more research capabilities.\n\nShould Retry: true"
  },
  {
    "Request": "Given the fields `paper_text`, `guidance`, produce the fields `task_candidates`.\n\n---\n\nFollow the following format.\n\nPaper Text: Full text from research paper PDF\nGuidance: Guidance for extracting task candidates\nTask Candidates: JSON list of task candidate dicts with keys 'name', 'description', and 'relevant_paper_text'. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCandidate\": {\"description\": \"Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\\n\\nA good task candidate:\\n- describes a task that an agent can implement\\n- typically a programming task, e.g. modifying a 'baseline' implementation\\n- which reflects or mirrors actual tasks undertaken to produce the research paper\\n- and requires research and/or engineering expertise to implement correctly\\n- for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\\n- ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\\n- the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\", \"properties\": {\"name\": {\"description\": \"Name of the task\", \"title\": \"Name\", \"type\": \"string\"}, \"description\": {\"description\": \"Description of the task\", \"title\": \"Description\", \"type\": \"string\"}, \"relevant_paper_text\": {\"description\": \"Text from the paper that is relevant to the task\", \"title\": \"Relevant Paper Text\", \"type\": \"string\"}, \"scoring_feasibility\": {\"description\": \"\\nOn a scale of 1-10, how feasible is it to write a python function to objectively score an \\nimplementation of this task, verifying that key functionality is implemented as specified?\\n\", \"title\": \"Scoring Feasibility\", \"type\": \"integer\"}, \"llm_tractability\": {\"description\": \"Probability in (0, 1) that a frontier LLM can generate a correct implementation of this task\", \"title\": \"Llm Tractability\", \"type\": \"number\"}, \"expert_tractability\": {\"description\": \"Probability in (0, 1) that a human subject matter expert can generate a correct implementation of this task\", \"title\": \"Expert Tractability\", \"type\": \"number\"}, \"layman_tractability\": {\"description\": \"Probability in (0, 1) that a layman can generate a correct implementation of this task\", \"title\": \"Layman Tractability\", \"type\": \"number\"}}, \"required\": [\"name\", \"description\", \"relevant_paper_text\", \"scoring_feasibility\", \"llm_tractability\", \"expert_tractability\", \"layman_tractability\"], \"title\": \"TaskCandidate\", \"type\": \"object\"}}, \"properties\": {\"tasks\": {\"description\": \"Tasks extracted from the paper\", \"items\": {\"$ref\": \"#/$defs/TaskCandidate\"}, \"title\": \"Tasks\", \"type\": \"array\"}}, \"required\": [\"tasks\"], \"title\": \"TaskCandidates\", \"type\": \"object\"}\n\n---\n\nPaper Text:\n94% on CIFAR-10 in 3.29 Seconds on a Single GPU\nKeller Jordan\nkjordan4077@gmail.com\nAbstract\nCIFAR-10 is among the most widely used datasets in machine learning, facilitating\nthousands of research projects per year. To accelerate research and reduce the cost\nof experiments, we introduce training methods for CIFAR-10 which reach 94%\naccuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when\nrun on a single NVIDIA A100 GPU. As one factor contributing to these training\nspeeds, we propose a derandomized variant of horizontal flipping augmentation,\nwhich we show improves over the standard method in every case where flipping is\nbeneficial over no flipping at all. Our code is released at https://github.com/\nKellerJordan/cifar10-airbench .\n1 Introduction\nCIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-\ning thousands of research projects per year1. Research can be accelerated and the cost of experiments\nreduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In\nthis paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single\nNVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,\n2023). To support scenarios where higher performance is needed, we additionally develop methods\ntargeting 95% and 96% accuracy. We release the following methods in total.\n1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).\n2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).\n3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).\n4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).\nAll runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-\nmatically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the\nfirst uses torch.compile to improve GPU utilization. It is intended for experiments where many\nnetworks are trained at once in order to amortize the one-time compilation cost. The non-compiled\nairbench94 variant can be easily installed and run using the following command.\n1pip install airbench\n2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"\nOne motivation for the development of these training methods is that they can accelerate the experi-\nmental iteration time of researchers working on compatible projects involving CIFAR-10. Another\nmotivation is that they can decrease the cost of projects involving a massive number of trained\nnetworks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used\n3 million trained networks to demonstrate that the outputs of a trained neural network on a given test\ninput follow an approximately linear function of the vector of binary choices of which examples the\nmodel was trained on. Another example is Jordan (2023), a study on training variance which used\n180 thousand trained networks to show that standard trainings have little variance in performance on\n1https://paperswithcode.com/datasets\nPreprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024\nFigure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal\nflipping augmentation, which flips each image with 50% probability per epoch. This results in some images\nbeing redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a\ndeterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\ntheir test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds\nand 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make\nit possible to replicate these studies, or conduct similar ones, with fewer computational resources.\nFast training also enables the rapid accumulation of statistical significance for subtle hyperparameter\ncomparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10\naccuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation\nbetween runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the\nimprovement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training\nthis will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.\nOur work builds on prior training speed projects. We utilize a modified version of the network,\ninitialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen\npatch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over\nprior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-\nure 1, Section 3.6, Section 5.2).\n2 Background\nOur objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set\nin the shortest possible amount of time. Timing begins when the method is first given access to\ntraining data, and ends when it produces test-set predictions. The method is considered valid if its\nmean accuracy over repeated runs is at least 94%.\nWe chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the\n2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more\nrecent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10\nseconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page\n(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is\ntysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,\n94% is the level of human accuracy reported by Karpathy (2011).\nWe note the following consequences of how the method is timed. First, it is permitted for the program\nto begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins\nwhen the training data is first accessed. This is helpful because otherwise the first run of training\nis typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA\nimproves the performance of a trained network by running it on multiple augmented views of each\ntest input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping\nTTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods\nattain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\nThe CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.\nIt is divided into a training set of 50,000 images and a validation set of 10,000 images. As a\nmatter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was\n2\n80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than\nairbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94\ncan be attributed entirely to algorithmic progress rather than compute scaling.\n3 Methods\n3.1 Network architecture and baseline training\nWe train a convolutional network with a total of 1.97 million parameters, following tysam-code\n(2023) with a few small changes. It contains seven convolutions with the latter six being divided into\nthree blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this\nsection we offer some comments on the main design choices.\nThe network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed\nentirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,\n2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with\nno padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather\nthan the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff\nbetween throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\nFollowing Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,\nand disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is\nscaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture\ndiffers only in that we decrease the number of output channels in the third block from 512 to 256,\nand we add learnable biases to the first convolution.\nAs our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of\n0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the\nmaximum at 20% of the way through training, and then decreases to zero. For data augmentation we\nuse random horizontal flipping alongside 2-pixel random translation. For translation we use reflection\npadding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note\nthat what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a\nrandom 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the\nnetwork is run on both a given test image and its mirror and inferences are made based on the average\nof the two outputs. With optimized choices of learning rate, momentum, and weight decay, this\nbaseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.\n3.2 Frozen patch-whitening initialization\nFollowing Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-\nwhitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code\n(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches\nacross the training distribution, so that their outputs have identity covariance matrix. The second 12\nFigure 2: The first layer\u2019s weights after whitening\ninitialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,\nso that input information is preserved through the\nactivation which follows. Figure 2 shows the result.\nWe do not update this layer\u2019s weights during training.\nDeparting from tysam-code (2023), we add learn-\nable biases to this layer, yielding a small performance\nboost. The biases are trained for 3 epochs, after which\nwe disable their gradient to increase backward-pass\nthroughput, which improves training speed without\nreducing accuracy. We also obtain a slight perfor-\nmance boost relative to tysam-code (2023) by re-\nducing the constant added to the eigenvalues during\ncalculation of the patch-whitening initialization for\nthe purpose of preventing numerical issues in the case\nof a singular patch-covariance matrix.\nPatch-whitening initialization is the single most impactful feature. Adding it to the baseline more\nthan doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.\n3\nRandom reshuffling Alternating flip Mean accuracy\nNo No 93.40%\nNo Yes 93.48%\nYes No 93.92%\nYes Yes 94.01%\nTable 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating\nflip (which we propose) reduce training data redundancy and improve performance.\n3.3 Identity initialization\ndirac : We initialize all convolutions after the first as partial identity transforms. That is, for a\nconvolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an\nidentity transform of the input, and leave the remaining N\u2212Mto their default initialization. In\nPyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight\nwof each convolutional layer. This method partially follows tysam-code (2023), which used a more\ncomplicated scheme where the identity weights are mixed in with the original initialization, which\nwe did not find to be more performant. With this feature added, training attains 94% accuracy in 18\nepochs taking 6.8 A100-seconds.\n3.4 Optimization tricks\nscalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor\nof64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%\nin 13.5 epochs taking 5.1 A100-seconds.\nlookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.\nWe note that Lookahead has also been found effective in prior work on training speed for ResNet-\n18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6\nA100-seconds.\n3.5 Multi-crop evaluation\nmulticrop : To generate predictions, we run the trained network on six augmented views of each\ntest image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a\nversion which is translated down-and-to-the-right by one pixel, and the mirrored versions of all\nthree. Predictions are made using a weighted average of all six outputs, where the two views of the\nuntranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125\neach. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.\nWe note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-\nmonyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of\nevaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using\nmore crops does improve performance, but the increase to inference time outweighs the potential\ntraining speedup.\n3.6 Alternating flip\nTo speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,\nwhich we motivate as follows. When training neural networks, it is standard practice to organize\ntraining into a set of epochs during which every training example is seen exactly once. This\ndiffers from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),\nwhich calls for data to be repeatedly sampled with-replacement from the training set, resulting in\nexamples being potentially seen multiple redundant times within a short window of training. The\nuse of randomly ordered epochs of data for training has a different name, being called the random\nreshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our\ntraining dataset consists of Nunique examples, then sampling data with replacement causes every\n\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N\nunique examples on average. On the other hand, random reshuffling leads to all Nunique examples\nbeing seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason\nthat it is beneficial to maximize the number of unique inputs seen per window of training time.\n4\nWe extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We\nfirst note that standard random horizontal flipping augmentation can be defined as follows.\n1import torch\n2def random_flip ( inputs ):\n3 # Applies random flipping to a batch of images\n4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)\n5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]\nListing 1: Random flip\nIf horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2\nwhich may be seen during training. Potentially, every pair of consecutive epochs could contain every\nunique input. But our main observation is that with standard random horizontal flipping, half of the\nimages will be redundantly flipped the same way during both epochs, so that on average only 1.5N\nunique inputs will be seen.\naltflip : To address this, we propose to modify standard random horizontal flipping augmentation as\nfollows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},\nwe flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we\nflip only those inputs which were flipped in the first epoch. We provide the following implementation\nwhich avoids the need for extra memory by using a pseudorandom function to decide the flips.\n1import torch\n2import hashlib\n3def hash_fn (n, seed =42) :\n4 k = n * seed\n5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,\n16)\n6def alternating_flip (inputs , indices , epoch ):\n7 # Applies alternating flipping to a batch of images\n8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist\n() ])\n9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)\n10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\nListing 2: Alternating flip\nThe result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in\nFigure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.\nAdding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final\ntraining method airbench94.py , the entire contents of which can be found in Section E. It reaches\n94% accuracy in 3.83 seconds on an NVIDIA A100.\n3.7 Compilation\nThe final step we take to speed up training is a non-algorithmic one: we compile our training\nmethod using torch.compile in order to more efficiently utilize the GPU. This results in a training\nscript which is mathematically equivalent (up to small differences in floating point arithmetic) to\nthe non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29\nA100-seconds. The downside is that the one-time compilation process takes up to several minutes to\ncomplete before training runs can begin, so that it is only beneficial when we plan to execute many\nruns of training at once. We release this version as airbench94_compiled.py .\n4 95% and 96% targets\nTo address scenarios where somewhat higher performance is desired, we additionally develop methods\ntargeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .\nTo attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel\ncount of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce\nthe learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%\naccuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.\n2Assuming none of the training inputs are already mirrors of each other.\n5\nFigure 4: Training speedups accumulate additively. Removing individual features from airbench94\nincreases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces\nthe epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-\n94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.\nFigure 3: FLOPs vs. error rate tradeoff. Our\nthree training methods apparently follow a linear\nlog-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-\nVries & Taylor, 2017) augmentation and raise the\ntraining epochs to 40. We add a third convolution to\neach block, and scale the first block to 128 channels\nand the second two to 512. We also add a residual\nconnection across the later two convolutions of each\nblock, which we find is still beneficial despite the fact\nthat we are already using identity initialization (Sec-\ntion 3.3) to ease gradient flow. Finally, we reduce\nthe learning rate by a factor of 0.78. These changes\nyield airbench96 which attains 96.05% accuracy in\n46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.\nFigure 3 shows the FLOPs and error rate of each of\nour three training methods.\n5 Experiments\n5.1 Interaction between features\nTo gain a better sense of the impact of each feature on training speed, we compare two quantities.\nFirst, we measure the number of epochs that can be saved by adding the feature to the whitened\nbaseline (Section 3.2). Second, we measure the number of epochs that must be added when the\nfeature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-\ntion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it\nfrom the final airbench94 increases epochs-to-94% from 9.9 to 12.8.\nFigure 4 shows both quantities for each feature. Surprisingly, we find that for all features except\nmulti-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even\nthough the whitened baseline requires more than twice as many epochs as the final configuration.\nThis indicates that the interaction between most features is additive rather than multiplicative.\n5.2 Does alternating flip generalize?\nIn this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of\ntraining configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all\ncases except those where neither alternating nor random flip improve over using no flipping at all.\nFor CIFAR-10 we consider the performance boost given by alternating flip across the follow-\ning 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and\nairbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For\n6\nFigure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,\nthe use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%\ntraining speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.\n95% confidence intervals are shown around each point.\neach configuration we compare the performance of alternating and random flip in terms of their mean\naccuracy across n= 400 runs of training.\nFigure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip\nimproves performance in every setting. To get a sense for how big the improvement is, we estimate\nthe effective speedup for each case, i.e., the fraction of epochs that could be saved by switching\nfrom random to alternating flip while maintaining the level of accuracy of random flip. We begin\nby fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of\neach random flip-based training configuration. We use these curves to calculate the effective speedup\nafforded by switching from random to alternating flip. For example, airbench94 with random flip\nand without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The\nsame configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law\nfit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.\nNote that using a power-law yields a more conservative estimate relative to using linear interpolation\nbetween the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.\nTable 2 shows the result. We observe the following patterns. First, the addition of extra augmentation\n(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that\nthe main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the\nsame way for many epochs in a row; we speculate that adding extra augmentation reduces the negative\nimpact of these cases because it increases data diversity. Next, TTA reduces the gap between random\nand alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),\nindicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training\nfor longer consistently increases the effective speedup given by alternating flip.\nWe next study ImageNet trainings with the following experiment. We train a ResNet-18 with a\nvariety of train and test crops, comparing three flipping options: alternating flip, random flip, and no\nflipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192\ncenter crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)\nto denote the latter. We also consider two training crops: 192x192 inception-style random resized\ncrop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area\nranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the\nshorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write\n7\nBaseline Cutout Epochs Speedup Speedup (w/ TTA)\nairbench94 No 10 15.0% 5.30%\nairbench94 No 20 27.1% 21.3%\nairbench94 No 40 38.3% 36.4%\nairbench94 No 80 102% 31.8%\nairbench94 Yes 10 3.84% 1.13%\nairbench94 Yes 20 7.42% 2.00%\nairbench94 Yes 40 18.6% 9.28%\nairbench94 Yes 80 29.2% 14.25%\nairbench96 Yes 10 4.94% 1.11%\nairbench96 Yes 20 8.99% 3.58%\nairbench96 Yes 40 17.2% 6.48%\nairbench96 Yes 80 18.8% Not measured\nTable 2: Effective speedups given by switching from random flip to alternating flip. The two configurations\nmost closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw\naccuracy values of the airbench94 experiments.\nFlipping augmentation option\nTrain crop Test crop Epochs TTA None Random Alternating\nHeavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28\nHeavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28\nLight RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26\nLight RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26\nHeavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28\nHeavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28\nLight RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26\nLight RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26\nLight RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16\nHeavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4\nTable 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for\nthose trainings where random flip improves significantly over not flipping at all. The single best flipping option\nin each row is bolded when the difference is statistically significant.\nHeavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided\nin Section C.\nTable 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC\nis better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly\nbetter when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test\nresolution discrepancy (Touvron et al., 2019).\nFor trainings which use Light RRC, we find that switching from random flip to alternating flip\nprovides a substantial boost to performance, amounting to a training speedup of more than 25%.\nIn Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to\nalternating flip improves performance by more than increasing the training duration from 16 to 20\nepochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our\nresults on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit\nfrom alternating flip. Indeed, even turning flipping off completely does not significantly reduce the\nperformance of these trainings. We conclude that alternating flip improves over random flip for every\ntraining scenario where the latter improves over no flipping at all.\n8\nEpochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE\n1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312\n2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233\n1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269\n1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533\n2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433\n1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444\nTable 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).\n5.3 Variance and class-wise calibration\nPrevious sections have focused on understanding what factors affect the first moment of accuracy\n(the mean). In this section we investigate the second moment, finding that TTA reduces variance at\nthe cost of calibration.\nOur experiment is to execute 10,000 runs of airbench94 training with several hyperparameter\nsettings. For each setting we report both the variance in test-set accuracy as well as an estimate of the\ndistribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.\nTable 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set\nvariance, replicating the main finding of Jordan (2023). This is a surprising result because these\ntrainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7\nas much distribution-wise variance when run for a similar duration, and reached a low variance only\nwhen run for 64 epochs. We conclude from this comparison that distribution-wise variance is more\nstrongly connected to the rate of convergence of a training rather than its duration as such. We also\nnote that the low distribution-wise variance of airbench94 indicates it has high training stability.\nUsing TTA significantly reduces the test-set variance, such that all three settings with TTA have lower\ntest-set variance than any setting without TTA. However, test-set variance is implied by the class-wise\ncalibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this\nreduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,\nwe compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which\nmeasures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA\nhas a higher CACE than every setting without TTA, confirming the hypothesis.\n6 Discussion\nIn this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7\nfaster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the\nairbench Python package.\nWe developed airbench solely with the goal of maximizing training speed on CIFAR-10. In\nSection B we find that it also generalizes well to other tasks. For example, without any extra tuning,\nairbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.\nOne factor contributing to the training speed of airbench was our finding that training can be accel-\nerated by partially derandomizing the standard random horizontal flipping augmentation, resulting in\nthe variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating\nflip improves the performance of every training we considered (Section 5.2), with the exception of\nthose trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to\nus, the standard ImageNet trainings that we considered do not significantly benefit from horizontal\nflipping. Future work might investigate whether it is possible to obtain derandomized improvements\nto other augmentations besides horizontal flip.\nThe methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,\nwith fixed performance and hardware constraints. These constraints mean that we cannot improve\nperformance by simply scaling up the amount of computational resources used; instead we are forced\nto develop new methods like the alternating flip. We look forward to seeing what other new methods\nfuture work discovers to push training speed further.\n9\nReferences\nDimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.\nDan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-\nperformance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,\n2011.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\nmark and competition. 2017.\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet\nor cifar-10. arXiv preprint arXiv:1810.03505 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\npp. 248\u2013255. IEEE, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552 , 2017.\nMert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic\ngradient descent. Mathematical Programming , 186:49\u201384, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 , 2016.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.\npmlr, 2015.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\nsgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.\nKeller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and\ninevitable. arXiv preprint arXiv:2304.01910 , 2023.\nAndrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.\nio/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for\nadvanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT\nLicense.\nGuillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander\nM \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,\nBenjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.\nBenchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural\nInformation Processing Systems , 35:25404\u201325421, 2022.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.\nDavid Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/\nhow-to-train-your-resnet-8-bag-of-tricks/ .\n10\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics , 22(3):400\u2013407, 1951.\nSantiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:\n//github.com/apple/ml-cifar-10-faster , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.\ntysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/\nhlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. Advances in neural information processing systems , 32, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,\n2016.\nMichael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps\nforward, 1 step back. Advances in neural information processing systems , 32, 2019.\n11\nA Network architecture\n1from torch import nn\n2\n3class Flatten (nn. Module ):\n4 def forward (self , x):\n5 return x. view (x. size (0) , -1)\n6\n7class Mul (nn. Module ):\n8 def __init__ (self , scale ):\n9 super (). __init__ ()\n10 self . scale = scale\n11 def forward (self , x):\n12 return x * self . scale\n13\n14def conv (ch_in , ch_out ):\n15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,\n16 padding =\u2019same \u2019, bias = False )\n17\n18def make_net ():\n19 act = lambda : nn. GELU ()\n20 bn = lambda ch: nn. BatchNorm2d (ch)\n21 return nn. Sequential (\n22 nn. Sequential (\n23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),\n24 act () ,\n25 ),\n26 nn. Sequential (\n27 conv (24 , 64) ,\n28 nn. MaxPool2d (2) ,\n29 bn (64) , act () ,\n30 conv (64 , 64) ,\n31 bn (64) , act () ,\n32 ),\n33 nn. Sequential (\n34 conv (64 , 256) ,\n35 nn. MaxPool2d (2) ,\n36 bn (256) , act () ,\n37 conv (256 , 256) ,\n38 bn (256) , act () ,\n39 ),\n40 nn. Sequential (\n41 conv (256 , 256) ,\n42 nn. MaxPool2d (2) ,\n43 bn (256) , act () ,\n44 conv (256 , 256) ,\n45 bn (256) , act () ,\n46 ),\n47 nn. MaxPool2d (3) ,\n48 Flatten () ,\n49 nn. Linear (256 , 10, bias = False ),\n50 Mul (1/9) ,\n51 )\nListing 3: Minimal PyTorch code for the network architecture used by airbench94 .\nWe note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock\ntime. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel\nsize of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this\ndoes not improve the wallclock training time on an A100. Reducing the batch size is another easy\nway to save FLOPs but not wallclock time.\n12\nDataset Flipping? Cutout? ResNet-18 airbench96\nCIFAR-10 Yes No 95.55% 95.61%\nCIFAR-10 Yes Yes 96.01% 96.05%\nCIFAR-100 Yes No 77.54% 79.27%\nCIFAR-100 Yes Yes 78.04% 79.76%\nCINIC-10 Yes No 87.58% 87.78%\nCINIC-10 Yes Yes not measured 88.22%\nSVHN No No 97.35% 97.38%\nSVHN No Yes not measured 97.64%\nTable 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We\ndirectly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off\nflipping for SVHN).\nB Extra dataset experiments\nWe developed airbench with the singular goal of maximizing training speed on CIFAR-10. To\nfind out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its\nperformance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-\n10 (Darlow et al., 2018).\nOn CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both\nthe case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not\n(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than\nResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say\nthat it generalizes.\nWe compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.\nWe do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal\non CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better\nperformance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where\nairbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is\nused and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows\nstrong generalization to other tasks.\nWe note that this comparison between airbench96 and ResNet-18 training is fair in the sense that\nit does demonstrate that the former has good generalization, but unfair in the sense that it does not\nindicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time\naugmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training\nwould outperform airbench96 if it were run using test-time augmentation. However, it also takes\n5-10 times longer to complete. The decision of which to use may be situational.\nThe accuracy values we report for ResNet-18 training are from the following sources. We tried\nto select the highest values we could find for each setting. Moreau et al. (2022) reports attaining\n95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports\nattaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on\nCIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.\nC ImageNet training details\nOur ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/\nlibffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and\nmomentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution\n160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of\ntraining. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.\n13\nHyperparameters Flipping augmentation option\nEpochs Cutout TTA None Random Alternating\n10 No No 92.3053 93.0988 93.2798\n20 No No 92.8166 93.7446 93.8652\n40 No No 93.0143 94.0133 94.0729\n80 No No 93.0612 94.1169 94.1628\n10 No Yes 93.4071 93.9488 94.0186\n20 No Yes 93.8528 94.5565 94.6530\n40 No Yes 94.0381 94.7803 94.8203\n80 No Yes 94.0638 94.8506 94.8676\n10 Yes No 91.8487 92.0402 92.1374\n20 Yes No 92.8474 93.3825 93.4876\n40 Yes No 93.2675 94.1014 94.1952\n80 Yes No 93.4193 94.4311 94.5204\n10 Yes Yes 92.6455 92.7780 92.8103\n20 Yes Yes 93.7862 94.1306 94.1670\n40 Yes Yes 94.3090 94.8511 94.8960\n80 Yes Yes 94.5253 95.1839 95.2362\nTable 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a\nmean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise\ndifference in means is statistically significant.\nD Extra tables & figures\nFigure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in\nSection 5.3.\nE Complete training code\n1\"\"\"\n2airbench94 .py\n33.83 s runtime on an A100 ; 0.36 PFLOPs .\n4Evidence for validity : 94.01 average accuracy in n =1000 runs .\n5\n6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB\nwith the following nvidia -smi:\n7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :\n11.7\n14\n8torch . __version__ == \u20192.1.2+ cu118 \u2019\n9\"\"\"\n10\n11# ############################################\n12# Setup / Hyperparameters #\n13# ############################################\n14\n15import os\n16import sys\n17import uuid\n18from math import ceil\n19\n20import torch\n21from torch import nn\n22import torch .nn. functional as F\n23import torchvision\n24import torchvision . transforms as T\n25\n26torch . backends . cudnn . benchmark = True\n27\n28\"\"\"\n29We express the main training hyperparameters ( batch size , learning\nrate , momentum , and weight decay ) in decoupled form , so that each\none can be tuned independently . This accomplishes the following :\n30* Assuming time - constant gradients , the average step size is decoupled\nfrom everything but the lr.\n31* The size of the weight decay update is decoupled from everything but\nthe wd.\n32In constrast , normally when we increase the ( Nesterov ) momentum , this\nalso scales up the step size proportionally to 1 + 1 / (1 -\nmomentum ), meaning we cannot change momentum without having to re -\ntune the learning rate . Similarly , normally when we increase the\nlearning rate this also increases the size of the weight decay ,\nrequiring a proportional decrease in the wd to maintain the same\ndecay strength .\n33\n34The practical impact is that hyperparameter tuning is faster , since\nthis parametrization allows each one to be tuned independently .\nSee https :// myrtle .ai/ learn /how -to -train -your - resnet -5-\nhyperparameters /.\n35\"\"\"\n36\n37hyp = {\n38 \u2019opt \u2019: {\n39 \u2019 train_epochs \u2019: 9.9 ,\n40 \u2019batch_size \u2019: 1024 ,\n41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples\n42 \u2019momentum \u2019: 0.85 ,\n43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (\ndecoupled from learning rate )\n44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not\nweight decay ) for BatchNorm biases\n45 \u2019 label_smoothing \u2019: 0.2 ,\n46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the\nwhitening layer bias before freezing\n47 },\n48 \u2019aug \u2019: {\n49 \u2019flip \u2019: True ,\n50 \u2019translate \u2019: 2,\n51 },\n52 \u2019net \u2019: {\n53 \u2019widths \u2019: {\n54 \u2019block1 \u2019: 64,\n55 \u2019block2 \u2019: 256 ,\n56 \u2019block3 \u2019: 256 ,\n15\n57 },\n58 \u2019 batchnorm_momentum \u2019: 0.6 ,\n59 \u2019 scaling_factor \u2019: 1/9 ,\n60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :\n0= none , 1= mirror , 2= mirror + translate\n61 },\n62}\n63\n64# ############################################\n65# DataLoader #\n66# ############################################\n67\n68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )\n69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )\n70\n71def batch_flip_lr ( inputs ):\n72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .\nview (-1, 1, 1, 1)\n73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\n74\n75def batch_crop (images , crop_size ):\n76 r = ( images . size ( -1) - crop_size )//2\n77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =\nimages . device )\n78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),\ndevice = images .device , dtype = images . dtype )\n79 # The two cropping methods in this if - else produce equivalent\nresults , but the second is faster for r > 2.\n80 if r <= 2:\n81 for sy in range (-r, r +1):\n82 for sx in range (-r, r +1):\n83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)\n84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size\n, r+sx:r+sx+ crop_size ]\n85 else :\n86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size\n+2* r), device = images .device , dtype = images . dtype )\n87 for s in range (-r, r +1):\n88 mask = ( shifts [:, 0] == s)\n89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]\n90 for s in range (-r, r +1):\n91 mask = ( shifts [:, 1] == s)\n92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+\ncrop_size ]\n93 return images_out\n94\n95class CifarLoader :\n96 \"\"\"\n97 GPU - accelerated dataloader for CIFAR -10 which implements\nalternating flip augmentation .\n98 \"\"\"\n99\n100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,\ndrop_last =None , shuffle =None , gpu =0):\n101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .\npt \u2019)\n102 if not os. path . exists ( data_path ):\n103 dset = torchvision . datasets . CIFAR10 (path , download =True ,\ntrain = train )\n104 images = torch . tensor ( dset . data )\n105 labels = torch . tensor ( dset . targets )\n106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:\ndset . classes }, data_path )\n107\n108 data = torch . load ( data_path , map_location = torch . device (gpu))\n16\n109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [\n\u2019labels \u2019], data [\u2019classes \u2019]\n110 # It \u2019s faster to load + process uint8 data than to load\npreprocessed fp16 data\n111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).\nto( memory_format = torch . channels_last )\n112\n113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )\n114 self . proc_images = {} # Saved results of image processing to\nbe done on the first epoch\n115 self . epoch = 0\n116\n117 self . aug = aug or {}\n118 for k in self .aug . keys ():\n119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019\n% k\n120\n121 self . batch_size = batch_size\n122 self . drop_last = train if drop_last is None else drop_last\n123 self . shuffle = train if shuffle is None else shuffle\n124\n125 def __len__ ( self ):\n126 return len( self . images )// self . batch_size if self . drop_last\nelse ceil (len( self . images )/ self . batch_size )\n127\n128 def __iter__ ( self ):\n129\n130 if self . epoch == 0:\n131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .\nimages )\n132 # Randomly flip all images on the first epoch as according\nto definition of alternating flip\n133 if self .aug.get(\u2019flip \u2019, False ):\n134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (\nimages )\n135 # Pre -pad images to save time when doing random\ntranslation\n136 pad = self .aug.get (\u2019translate \u2019, 0)\n137 if pad > 0:\n138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019\nreflect \u2019)\n139\n140 if self .aug.get(\u2019translate \u2019, 0) > 0:\n141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .\nshape [ -2])\n142 elif self .aug.get (\u2019flip \u2019, False ):\n143 images = self . proc_images [\u2019flip \u2019]\n144 else :\n145 images = self . proc_images [\u2019norm \u2019]\n146 if self .aug.get(\u2019flip \u2019, False ):\n147 if self . epoch % 2 == 1:\n148 images = images . flip ( -1)\n149\n150 self . epoch += 1\n151\n152 indices = ( torch . randperm if self . shuffle else torch . arange )(\nlen ( images ), device = images . device )\n153 for i in range (len ( self )):\n154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]\n155 yield ( images [ idxs ], self . labels [ idxs ])\n156\n157# ############################################\n158# Network Components #\n159# ############################################\n160\n17\n161class Flatten (nn. Module ):\n162 def forward (self , x):\n163 return x. view (x. size (0) , -1)\n164\n165class Mul (nn. Module ):\n166 def __init__ (self , scale ):\n167 super (). __init__ ()\n168 self . scale = scale\n169 def forward (self , x):\n170 return x * self . scale\n171\n172class BatchNorm (nn. BatchNorm2d ):\n173 def __init__ (self , num_features , momentum , eps =1e -12 ,\n174 weight =False , bias = True ):\n175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )\n176 self . weight . requires_grad = weight\n177 self . bias . requires_grad = bias\n178 # Note that PyTorch already initializes the weights to one and\nbiases to zero\n179\n180class Conv (nn. Conv2d ):\n181 def __init__ (self , in_channels , out_channels , kernel_size =3,\npadding =\u2019same \u2019, bias = False ):\n182 super (). __init__ ( in_channels , out_channels , kernel_size =\nkernel_size , padding = padding , bias = bias )\n183\n184 def reset_parameters ( self ):\n185 super (). reset_parameters ()\n186 if self . bias is not None :\n187 self . bias . data . zero_ ()\n188 w = self . weight . data\n189 torch .nn. init . dirac_ (w[:w. size (1) ])\n190\n191class ConvGroup (nn. Module ):\n192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):\n193 super (). __init__ ()\n194 self . conv1 = Conv ( channels_in , channels_out )\n195 self . pool = nn. MaxPool2d (2)\n196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )\n197 self . conv2 = Conv ( channels_out , channels_out )\n198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )\n199 self . activ = nn. GELU ()\n200\n201 def forward (self , x):\n202 x = self . conv1 (x)\n203 x = self . pool (x)\n204 x = self . norm1 (x)\n205 x = self . activ (x)\n206 x = self . conv2 (x)\n207 x = self . norm2 (x)\n208 x = self . activ (x)\n209 return x\n210\n211# ############################################\n212# Network Definition #\n213# ############################################\n214\n215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019\n][ \u2019 batchnorm_momentum \u2019]):\n216 whiten_kernel_size = 2\n217 whiten_width = 2 * 3 * whiten_kernel_size **2\n218 net = nn. Sequential (\n219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True\n),\n220 nn. GELU () ,\n18\n221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],\nbatchnorm_momentum ),\n222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],\nbatchnorm_momentum ),\n223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],\nbatchnorm_momentum ),\n224 nn. MaxPool2d (3) ,\n225 Flatten () ,\n226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),\n227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,\n228 )\n229 net [0]. weight . requires_grad = False\n230 net = net. half (). cuda ()\n231 net = net.to( memory_format = torch . channels_last )\n232 for mod in net. modules ():\n233 if isinstance (mod , BatchNorm ):\n234 mod . float ()\n235 return net\n236\n237# ############################################\n238# Whitening Conv Initialization #\n239# ############################################\n240\n241def get_patches (x, patch_shape ):\n242 c, (h, w) = x. shape [1] , patch_shape\n243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h\n,w). float ()\n244\n245def get_whitening_parameters ( patches ):\n246 n,c,h,w = patches . shape\n247 patches_flat = patches . view (n, -1)\n248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n\n249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance\n, UPLO =\u2019U\u2019)\n250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.\nreshape (c*h*w,c,h,w). flip (0)\n251\n252def init_whitening_conv (layer , train_set , eps =5e -4):\n253 patches = get_patches ( train_set , patch_shape = layer . weight . data .\nshape [2:])\n254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )\n255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)\n256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -\neigenvectors_scaled ))\n257\n258# ###########################################\n259# Lookahead #\n260# ###########################################\n261\n262class LookaheadState :\n263 def __init__ (self , net):\n264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().\nitems ()}\n265\n266 def update (self , net , decay ):\n267 for ema_param , net_param in zip( self . net_ema . values () , net.\nstate_dict (). values ()):\n268 if net_param . dtype in ( torch .half , torch . float ):\n269 ema_param . lerp_ ( net_param , 1- decay )\n270 net_param . copy_ ( ema_param )\n271\n272# ###########################################\n273# Logging #\n274# ###########################################\n275\n19\n276def print_columns ( columns_list , is_head =False , is_final_entry = False ):\n277 print_string = \u2019\u2019\n278 for col in columns_list :\n279 print_string += \u2019| %s \u2019 % col\n280 print_string += \u2019|\u2019\n281 if is_head :\n282 print (\u2019-\u2019*len( print_string ))\n283 print ( print_string )\n284 if is_head or is_final_entry :\n285 print (\u2019-\u2019*len( print_string ))\n286\n287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,\n\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]\n288def print_training_details ( variables , is_final_entry ):\n289 formatted = []\n290 for col in logging_columns_list :\n291 var = variables .get (col. strip () , None )\n292 if type (var) in (int , str):\n293 res = str(var)\n294 elif type (var) is float :\n295 res = \u2019{:0.4 f}\u2019. format ( var)\n296 else :\n297 assert var is None\n298 res = \u2019\u2019\n299 formatted . append (res. rjust (len (col)))\n300 print_columns ( formatted , is_final_entry = is_final_entry )\n301\n302# ###########################################\n303# Evaluation #\n304# ###########################################\n305\n306def infer (model , loader , tta_level =0) :\n307 \"\"\"\n308 Test - time augmentation strategy (for tta_level =2):\n309 1. Flip / mirror the image left -to - right (50% of the time ).\n310 2. Translate the image by one pixel either up -and - left or down -and\n- right (50% of the time , i.e. both happen 25% of the time ).\n311\n312 This creates 6 views per image ( left / right times the two\ntranslations and no - translation ), which we evaluate and then\nweight according to the given probabilities .\n313 \"\"\"\n314\n315 def infer_basic (inputs , net):\n316 return net( inputs ). clone ()\n317\n318 def infer_mirror (inputs , net):\n319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))\n320\n321 def infer_mirror_translate ( inputs , net ):\n322 logits = infer_mirror (inputs , net )\n323 pad = 1\n324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)\n325 inputs_translate_list = [\n326 padded_inputs [:, :, 0:32 , 0:32] ,\n327 padded_inputs [:, :, 2:34 , 2:34] ,\n328 ]\n329 logits_translate_list = [ infer_mirror ( inputs_translate , net)\n330 for inputs_translate in\ninputs_translate_list ]\n331 logits_translate = torch . stack ( logits_translate_list ). mean (0)\n332 return 0.5 * logits + 0.5 * logits_translate\n333\n334 model . eval ()\n335 test_images = loader . normalize ( loader . images )\n20\n336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][\ntta_level ]\n337 with torch . no_grad ():\n338 return torch .cat ([ infer_fn (inputs , model ) for inputs in\ntest_images . split (2000) ])\n339\n340def evaluate (model , loader , tta_level =0) :\n341 logits = infer (model , loader , tta_level )\n342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()\n343\n344# ###########################################\n345# Training #\n346# ###########################################\n347\n348def main (run):\n349\n350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]\n351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]\n352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]\n353 # Assuming gradients are constant in time , for Nesterov momentum ,\nthe below ratio is how much larger the default steps will be than\nthe underlying per - example gradients . We divide the learning rate\nby this ratio in order to ensure steps are the same scale as\ngradients , regardless of the choice of momentum .\n354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))\n355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning\nrate for PyTorch SGD\n356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale\n357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]\n358\n359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019\nlabel_smoothing \u2019], reduction =\u2019none \u2019)\n360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)\n361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =\nbatch_size , aug=hyp[\u2019aug \u2019])\n362 if run == \u2019warmup \u2019:\n363 # The only purpose of the first run is to warmup , so we can\nuse dummy data\n364 train_loader . labels = torch . randint (0, 10, size =( len (\ntrain_loader . labels ) ,), device = train_loader . labels . device )\n365 total_train_steps = ceil (len( train_loader ) * epochs )\n366\n367 model = make_net ()\n368 current_steps = 0\n369\n370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in\nk and p. requires_grad ]\n371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019\nnot in k and p. requires_grad ]\n372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,\nweight_decay =wd/ lr_biases ),\n373 dict ( params = other_params , lr=lr , weight_decay =wd/\nlr)]\n374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,\nnesterov = True )\n375\n376 def triangle (steps , start =0, end =0, peak =0.5) :\n377 xp = torch . tensor ([0 , int( peak * steps ), steps ])\n378 fp = torch . tensor ([ start , 1, end ])\n379 x = torch . arange (1+ steps )\n380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])\n381 b = fp [: -1] - (m * xp [: -1])\n382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1\n383 indices = torch . clamp ( indices , 0, len(m) - 1)\n384 return m[ indices ] * x + b[ indices ]\n21\n385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,\npeak =0.23)\n386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:\nlr_schedule [i])\n387\n388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /\ntotal_train_steps )**3\n389 lookahead_state = LookaheadState ( model )\n390\n391 # For accurately timing GPU code\n392 starter = torch . cuda . Event ( enable_timing = True )\n393 ender = torch . cuda . Event ( enable_timing = True )\n394 total_time_seconds = 0.0\n395\n396 # Initialize the first layer using statistics of training images\n397 starter . record ()\n398 train_images = train_loader . normalize ( train_loader . images [:5000])\n399 init_whitening_conv ( model [0] , train_images )\n400 ender . record ()\n401 torch . cuda . synchronize ()\n402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n403\n404 for epoch in range ( ceil ( epochs )):\n405\n406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019\nwhiten_bias_epochs \u2019])\n407\n408 # ###################\n409 # Training #\n410 # ###################\n411\n412 starter . record ()\n413\n414 model . train ()\n415 for inputs , labels in train_loader :\n416\n417 outputs = model ( inputs )\n418 loss = loss_fn ( outputs , labels ).sum ()\n419 optimizer . zero_grad ( set_to_none = True )\n420 loss . backward ()\n421 optimizer . step ()\n422 scheduler . step ()\n423\n424 current_steps += 1\n425\n426 if current_steps % 5 == 0:\n427 lookahead_state . update (model , decay = alpha_schedule [\ncurrent_steps ]. item ())\n428\n429 if current_steps >= total_train_steps :\n430 if lookahead_state is not None :\n431 lookahead_state . update (model , decay =1.0)\n432 break\n433\n434 ender . record ()\n435 torch . cuda . synchronize ()\n436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n437\n438 # ###################\n439 # Evaluation #\n440 # ###################\n441\n442 # Print the accuracy and loss from the last training batch of\nthe epoch\n22\n443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().\nmean (). item ()\n444 train_loss = loss . item () / batch_size\n445 val_acc = evaluate (model , test_loader , tta_level =0)\n446 print_training_details ( locals () , is_final_entry = False )\n447 run = None # Only print the run number once\n448\n449 # ###################\n450 # TTA Evaluation #\n451 # ###################\n452\n453 starter . record ()\n454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019\ntta_level \u2019])\n455 ender . record ()\n456 torch . cuda . synchronize ()\n457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n458\n459 epoch = \u2019eval \u2019\n460 print_training_details ( locals () , is_final_entry = True )\n461\n462 return tta_val_acc\n463\n464if __name__ == \" __main__ \":\n465 with open (sys. argv [0]) as f:\n466 code = f. read ()\n467\n468 print_columns ( logging_columns_list , is_head = True )\n469 main (\u2019warmup \u2019)\n470 accs = torch . tensor ([ main (run) for run in range (25) ])\n471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))\n472\n473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }\n474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))\n475 os. makedirs ( log_dir , exist_ok = True )\n476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)\n477 print (os. path . abspath ( log_path ))\n478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))\nListing 4: airbench94.py\n23\nGuidance:\nSelf Critique:\n\nI'll evaluate the task candidates I extracted based on the given requirements:\n\n1. Implement alternating flip data augmentation:\nStrengths:\n- Describes a concrete programming task to modify a baseline implementation\n- Reflects an actual task undertaken in the research\n- Requires engineering expertise to implement correctly\n- Relatively straightforward to write an objective scoring function (e.g. check if images are flipped in the correct pattern across epochs)\n- Scoring function would be much easier to produce than implementing the task itself\n\nWeaknesses:\n- May not require significant research capabilities, mostly engineering\n\n2. Implement decoupled hyperparameters:\nStrengths:\n- Describes a programming task to modify hyperparameter handling\n- Reflects work done in the research\n- Requires some engineering expertise\n\nWeaknesses:\n- May be challenging to write an objective scoring function beyond checking if the hyperparameters are decoupled\n- Implementing the task may not be significantly harder than writing the scoring function\n- Does not require much research capability\n\n3. Implement patch-whitening initialization:\nStrengths:\n- Describes a concrete programming task\n- Reflects actual work from the research\n- Requires both research understanding and engineering skill\n- Possible to write an objective scoring function (e.g. check if filters are initialized correctly)\n- Scoring function would be easier than implementation\n\nWeaknesses:\n- May be very challenging for an agent without strong math/ML background\n\nOverall, the alternating flip and patch-whitening tasks seem to best meet the criteria. The decoupled hyperparameters task is less suitable. I could improve by extracting more task candidates from other parts of the paper, especially those requiring more research capabilities.\nTask Candidates:"
  },
  {
    "Response": "Here are the task candidates extracted from the paper, based on the guidance:\n\n{\n  \"tasks\": [\n    {\n      \"name\": \"Implement alternating flip data augmentation\",\n      \"description\": \"Modify a baseline CIFAR-10 training implementation to use the alternating flip data augmentation method described in the paper. This involves randomly flipping 50% of inputs in the first epoch, then on even epochs flipping only those inputs not flipped in the first epoch, and on odd epochs flipping only those flipped in the first epoch.\",\n      \"relevant_paper_text\": \"To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\",\n      \"scoring_feasibility\": 8,\n      \"llm_tractability\": 0.7,\n      \"expert_tractability\": 0.9,\n      \"layman_tractability\": 0.3\n    },\n    {\n      \"name\": \"Implement patch-whitening initialization\",\n      \"description\": \"Modify a baseline CIFAR-10 training implementation to use the patch-whitening initialization method for the first convolutional layer as described in the paper. This involves initializing the first 12 filters as eigenvectors of the covariance matrix of 2x2 patches across the training distribution, and the second 12 as their negation.\",\n      \"relevant_paper_text\": \"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\",\n      \"scoring_feasibility\": 7,\n      \"llm_tractability\": 0.5,\n      \"expert_tractability\": 0.8,\n      \"layman_tractability\": 0.1\n    },\n    {\n      \"name\": \"Implement multi-crop test-time augmentation\",\n      \"description\": \"Modify a baseline CIFAR-10 evaluation implementation to use the multi-crop test-time augmentation method described in the paper. This involves running the trained network on six augmented views of each test image: the unmodified input, versions translated by one pixel, and mirrored versions of all three. Predictions should be made using a weighted average of all six outputs.\",\n      \"relevant_paper_text\": \"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\",\n      \"scoring_feasibility\": 8,\n      \"llm_tractability\": 0.6,\n      \"expert_tractability\": 0.9,\n      \"layman_tractability\": 0.2\n    }\n  ]\n}"
  },
  {
    "Request": "Make a very succinct json object that validates with the following schema\n\n---\n\nFollow the following format.\n\nJson Schema: ${json_schema}\nJson Object: ${json_object}\n\n---\n\nJson Schema: {\"$defs\": {\"TaskCandidate\": {\"description\": \"Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\\n\\nA good task candidate:\\n- describes a task that an agent can implement\\n- typically a programming task, e.g. modifying a 'baseline' implementation\\n- which reflects or mirrors actual tasks undertaken to produce the research paper\\n- and requires research and/or engineering expertise to implement correctly\\n- for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\\n- ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\\n- the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\", \"properties\": {\"name\": {\"description\": \"Name of the task\", \"title\": \"Name\", \"type\": \"string\"}, \"description\": {\"description\": \"Description of the task\", \"title\": \"Description\", \"type\": \"string\"}, \"relevant_paper_text\": {\"description\": \"Text from the paper that is relevant to the task\", \"title\": \"Relevant Paper Text\", \"type\": \"string\"}, \"scoring_feasibility\": {\"description\": \"\\nOn a scale of 1-10, how feasible is it to write a python function to objectively score an \\nimplementation of this task, verifying that key functionality is implemented as specified?\\n\", \"title\": \"Scoring Feasibility\", \"type\": \"integer\"}, \"llm_tractability\": {\"description\": \"Probability in (0, 1) that a frontier LLM can generate a correct implementation of this task\", \"title\": \"Llm Tractability\", \"type\": \"number\"}, \"expert_tractability\": {\"description\": \"Probability in (0, 1) that a human subject matter expert can generate a correct implementation of this task\", \"title\": \"Expert Tractability\", \"type\": \"number\"}, \"layman_tractability\": {\"description\": \"Probability in (0, 1) that a layman can generate a correct implementation of this task\", \"title\": \"Layman Tractability\", \"type\": \"number\"}}, \"required\": [\"name\", \"description\", \"relevant_paper_text\", \"scoring_feasibility\", \"llm_tractability\", \"expert_tractability\", \"layman_tractability\"], \"title\": \"TaskCandidate\", \"type\": \"object\"}}, \"properties\": {\"tasks\": {\"description\": \"Tasks extracted from the paper\", \"items\": {\"$ref\": \"#/$defs/TaskCandidate\"}, \"title\": \"Tasks\", \"type\": \"array\"}}, \"required\": [\"tasks\"], \"title\": \"TaskCandidates\", \"type\": \"object\"}\nJson Object:"
  },
  {
    "Response": "Json Object: {\n  \"tasks\": [\n    {\n      \"name\": \"Implement Gradient Descent\",\n      \"description\": \"Implement a basic gradient descent algorithm for linear regression\",\n      \"relevant_paper_text\": \"We used gradient descent to optimize the model parameters\",\n      \"scoring_feasibility\": 8,\n      \"llm_tractability\": 0.9,\n      \"expert_tractability\": 0.95,\n      \"layman_tractability\": 0.2\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `paper_text`, `guidance`, produce the fields `task_candidates`.\n\n---\n\nFollow the following format.\n\nPaper Text: Full text from research paper PDF\n\nGuidance: Guidance for extracting task candidates\n\nPast Error in Task Candidates: An error to avoid in the future\n\nTask Candidates: JSON list of task candidate dicts with keys 'name', 'description', and 'relevant_paper_text'. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCandidate\": {\"description\": \"Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\\n\\nA good task candidate:\\n- describes a task that an agent can implement\\n- typically a programming task, e.g. modifying a 'baseline' implementation\\n- which reflects or mirrors actual tasks undertaken to produce the research paper\\n- and requires research and/or engineering expertise to implement correctly\\n- for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\\n- ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\\n- the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\", \"properties\": {\"name\": {\"description\": \"Name of the task\", \"title\": \"Name\", \"type\": \"string\"}, \"description\": {\"description\": \"Description of the task\", \"title\": \"Description\", \"type\": \"string\"}, \"relevant_paper_text\": {\"description\": \"Text from the paper that is relevant to the task\", \"title\": \"Relevant Paper Text\", \"type\": \"string\"}, \"scoring_feasibility\": {\"description\": \"\\nOn a scale of 1-10, how feasible is it to write a python function to objectively score an \\nimplementation of this task, verifying that key functionality is implemented as specified?\\n\", \"title\": \"Scoring Feasibility\", \"type\": \"integer\"}, \"llm_tractability\": {\"description\": \"Probability in (0, 1) that a frontier LLM can generate a correct implementation of this task\", \"title\": \"Llm Tractability\", \"type\": \"number\"}, \"expert_tractability\": {\"description\": \"Probability in (0, 1) that a human subject matter expert can generate a correct implementation of this task\", \"title\": \"Expert Tractability\", \"type\": \"number\"}, \"layman_tractability\": {\"description\": \"Probability in (0, 1) that a layman can generate a correct implementation of this task\", \"title\": \"Layman Tractability\", \"type\": \"number\"}}, \"required\": [\"name\", \"description\", \"relevant_paper_text\", \"scoring_feasibility\", \"llm_tractability\", \"expert_tractability\", \"layman_tractability\"], \"title\": \"TaskCandidate\", \"type\": \"object\"}}, \"properties\": {\"tasks\": {\"description\": \"Tasks extracted from the paper\", \"items\": {\"$ref\": \"#/$defs/TaskCandidate\"}, \"title\": \"Tasks\", \"type\": \"array\"}}, \"required\": [\"tasks\"], \"title\": \"TaskCandidates\", \"type\": \"object\"}\n\n---\n\nPaper Text:\n94% on CIFAR-10 in 3.29 Seconds on a Single GPU\nKeller Jordan\nkjordan4077@gmail.com\nAbstract\nCIFAR-10 is among the most widely used datasets in machine learning, facilitating\nthousands of research projects per year. To accelerate research and reduce the cost\nof experiments, we introduce training methods for CIFAR-10 which reach 94%\naccuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when\nrun on a single NVIDIA A100 GPU. As one factor contributing to these training\nspeeds, we propose a derandomized variant of horizontal flipping augmentation,\nwhich we show improves over the standard method in every case where flipping is\nbeneficial over no flipping at all. Our code is released at https://github.com/\nKellerJordan/cifar10-airbench .\n1 Introduction\nCIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-\ning thousands of research projects per year1. Research can be accelerated and the cost of experiments\nreduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In\nthis paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single\nNVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,\n2023). To support scenarios where higher performance is needed, we additionally develop methods\ntargeting 95% and 96% accuracy. We release the following methods in total.\n1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).\n2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).\n3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).\n4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).\nAll runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-\nmatically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the\nfirst uses torch.compile to improve GPU utilization. It is intended for experiments where many\nnetworks are trained at once in order to amortize the one-time compilation cost. The non-compiled\nairbench94 variant can be easily installed and run using the following command.\n1pip install airbench\n2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"\nOne motivation for the development of these training methods is that they can accelerate the experi-\nmental iteration time of researchers working on compatible projects involving CIFAR-10. Another\nmotivation is that they can decrease the cost of projects involving a massive number of trained\nnetworks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used\n3 million trained networks to demonstrate that the outputs of a trained neural network on a given test\ninput follow an approximately linear function of the vector of binary choices of which examples the\nmodel was trained on. Another example is Jordan (2023), a study on training variance which used\n180 thousand trained networks to show that standard trainings have little variance in performance on\n1https://paperswithcode.com/datasets\nPreprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024\nFigure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal\nflipping augmentation, which flips each image with 50% probability per epoch. This results in some images\nbeing redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a\ndeterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\ntheir test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds\nand 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make\nit possible to replicate these studies, or conduct similar ones, with fewer computational resources.\nFast training also enables the rapid accumulation of statistical significance for subtle hyperparameter\ncomparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10\naccuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation\nbetween runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the\nimprovement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training\nthis will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.\nOur work builds on prior training speed projects. We utilize a modified version of the network,\ninitialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen\npatch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over\nprior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-\nure 1, Section 3.6, Section 5.2).\n2 Background\nOur objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set\nin the shortest possible amount of time. Timing begins when the method is first given access to\ntraining data, and ends when it produces test-set predictions. The method is considered valid if its\nmean accuracy over repeated runs is at least 94%.\nWe chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the\n2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more\nrecent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10\nseconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page\n(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is\ntysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,\n94% is the level of human accuracy reported by Karpathy (2011).\nWe note the following consequences of how the method is timed. First, it is permitted for the program\nto begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins\nwhen the training data is first accessed. This is helpful because otherwise the first run of training\nis typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA\nimproves the performance of a trained network by running it on multiple augmented views of each\ntest input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping\nTTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods\nattain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\nThe CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.\nIt is divided into a training set of 50,000 images and a validation set of 10,000 images. As a\nmatter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was\n2\n80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than\nairbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94\ncan be attributed entirely to algorithmic progress rather than compute scaling.\n3 Methods\n3.1 Network architecture and baseline training\nWe train a convolutional network with a total of 1.97 million parameters, following tysam-code\n(2023) with a few small changes. It contains seven convolutions with the latter six being divided into\nthree blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this\nsection we offer some comments on the main design choices.\nThe network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed\nentirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,\n2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with\nno padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather\nthan the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff\nbetween throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\nFollowing Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,\nand disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is\nscaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture\ndiffers only in that we decrease the number of output channels in the third block from 512 to 256,\nand we add learnable biases to the first convolution.\nAs our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of\n0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the\nmaximum at 20% of the way through training, and then decreases to zero. For data augmentation we\nuse random horizontal flipping alongside 2-pixel random translation. For translation we use reflection\npadding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note\nthat what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a\nrandom 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the\nnetwork is run on both a given test image and its mirror and inferences are made based on the average\nof the two outputs. With optimized choices of learning rate, momentum, and weight decay, this\nbaseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.\n3.2 Frozen patch-whitening initialization\nFollowing Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-\nwhitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code\n(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches\nacross the training distribution, so that their outputs have identity covariance matrix. The second 12\nFigure 2: The first layer\u2019s weights after whitening\ninitialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,\nso that input information is preserved through the\nactivation which follows. Figure 2 shows the result.\nWe do not update this layer\u2019s weights during training.\nDeparting from tysam-code (2023), we add learn-\nable biases to this layer, yielding a small performance\nboost. The biases are trained for 3 epochs, after which\nwe disable their gradient to increase backward-pass\nthroughput, which improves training speed without\nreducing accuracy. We also obtain a slight perfor-\nmance boost relative to tysam-code (2023) by re-\nducing the constant added to the eigenvalues during\ncalculation of the patch-whitening initialization for\nthe purpose of preventing numerical issues in the case\nof a singular patch-covariance matrix.\nPatch-whitening initialization is the single most impactful feature. Adding it to the baseline more\nthan doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.\n3\nRandom reshuffling Alternating flip Mean accuracy\nNo No 93.40%\nNo Yes 93.48%\nYes No 93.92%\nYes Yes 94.01%\nTable 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating\nflip (which we propose) reduce training data redundancy and improve performance.\n3.3 Identity initialization\ndirac : We initialize all convolutions after the first as partial identity transforms. That is, for a\nconvolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an\nidentity transform of the input, and leave the remaining N\u2212Mto their default initialization. In\nPyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight\nwof each convolutional layer. This method partially follows tysam-code (2023), which used a more\ncomplicated scheme where the identity weights are mixed in with the original initialization, which\nwe did not find to be more performant. With this feature added, training attains 94% accuracy in 18\nepochs taking 6.8 A100-seconds.\n3.4 Optimization tricks\nscalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor\nof64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%\nin 13.5 epochs taking 5.1 A100-seconds.\nlookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.\nWe note that Lookahead has also been found effective in prior work on training speed for ResNet-\n18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6\nA100-seconds.\n3.5 Multi-crop evaluation\nmulticrop : To generate predictions, we run the trained network on six augmented views of each\ntest image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a\nversion which is translated down-and-to-the-right by one pixel, and the mirrored versions of all\nthree. Predictions are made using a weighted average of all six outputs, where the two views of the\nuntranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125\neach. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.\nWe note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-\nmonyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of\nevaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using\nmore crops does improve performance, but the increase to inference time outweighs the potential\ntraining speedup.\n3.6 Alternating flip\nTo speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,\nwhich we motivate as follows. When training neural networks, it is standard practice to organize\ntraining into a set of epochs during which every training example is seen exactly once. This\ndiffers from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),\nwhich calls for data to be repeatedly sampled with-replacement from the training set, resulting in\nexamples being potentially seen multiple redundant times within a short window of training. The\nuse of randomly ordered epochs of data for training has a different name, being called the random\nreshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our\ntraining dataset consists of Nunique examples, then sampling data with replacement causes every\n\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N\nunique examples on average. On the other hand, random reshuffling leads to all Nunique examples\nbeing seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason\nthat it is beneficial to maximize the number of unique inputs seen per window of training time.\n4\nWe extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We\nfirst note that standard random horizontal flipping augmentation can be defined as follows.\n1import torch\n2def random_flip ( inputs ):\n3 # Applies random flipping to a batch of images\n4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)\n5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]\nListing 1: Random flip\nIf horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2\nwhich may be seen during training. Potentially, every pair of consecutive epochs could contain every\nunique input. But our main observation is that with standard random horizontal flipping, half of the\nimages will be redundantly flipped the same way during both epochs, so that on average only 1.5N\nunique inputs will be seen.\naltflip : To address this, we propose to modify standard random horizontal flipping augmentation as\nfollows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},\nwe flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we\nflip only those inputs which were flipped in the first epoch. We provide the following implementation\nwhich avoids the need for extra memory by using a pseudorandom function to decide the flips.\n1import torch\n2import hashlib\n3def hash_fn (n, seed =42) :\n4 k = n * seed\n5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,\n16)\n6def alternating_flip (inputs , indices , epoch ):\n7 # Applies alternating flipping to a batch of images\n8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist\n() ])\n9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)\n10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\nListing 2: Alternating flip\nThe result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in\nFigure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.\nAdding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final\ntraining method airbench94.py , the entire contents of which can be found in Section E. It reaches\n94% accuracy in 3.83 seconds on an NVIDIA A100.\n3.7 Compilation\nThe final step we take to speed up training is a non-algorithmic one: we compile our training\nmethod using torch.compile in order to more efficiently utilize the GPU. This results in a training\nscript which is mathematically equivalent (up to small differences in floating point arithmetic) to\nthe non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29\nA100-seconds. The downside is that the one-time compilation process takes up to several minutes to\ncomplete before training runs can begin, so that it is only beneficial when we plan to execute many\nruns of training at once. We release this version as airbench94_compiled.py .\n4 95% and 96% targets\nTo address scenarios where somewhat higher performance is desired, we additionally develop methods\ntargeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .\nTo attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel\ncount of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce\nthe learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%\naccuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.\n2Assuming none of the training inputs are already mirrors of each other.\n5\nFigure 4: Training speedups accumulate additively. Removing individual features from airbench94\nincreases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces\nthe epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-\n94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.\nFigure 3: FLOPs vs. error rate tradeoff. Our\nthree training methods apparently follow a linear\nlog-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-\nVries & Taylor, 2017) augmentation and raise the\ntraining epochs to 40. We add a third convolution to\neach block, and scale the first block to 128 channels\nand the second two to 512. We also add a residual\nconnection across the later two convolutions of each\nblock, which we find is still beneficial despite the fact\nthat we are already using identity initialization (Sec-\ntion 3.3) to ease gradient flow. Finally, we reduce\nthe learning rate by a factor of 0.78. These changes\nyield airbench96 which attains 96.05% accuracy in\n46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.\nFigure 3 shows the FLOPs and error rate of each of\nour three training methods.\n5 Experiments\n5.1 Interaction between features\nTo gain a better sense of the impact of each feature on training speed, we compare two quantities.\nFirst, we measure the number of epochs that can be saved by adding the feature to the whitened\nbaseline (Section 3.2). Second, we measure the number of epochs that must be added when the\nfeature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-\ntion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it\nfrom the final airbench94 increases epochs-to-94% from 9.9 to 12.8.\nFigure 4 shows both quantities for each feature. Surprisingly, we find that for all features except\nmulti-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even\nthough the whitened baseline requires more than twice as many epochs as the final configuration.\nThis indicates that the interaction between most features is additive rather than multiplicative.\n5.2 Does alternating flip generalize?\nIn this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of\ntraining configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all\ncases except those where neither alternating nor random flip improve over using no flipping at all.\nFor CIFAR-10 we consider the performance boost given by alternating flip across the follow-\ning 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and\nairbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For\n6\nFigure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,\nthe use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%\ntraining speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.\n95% confidence intervals are shown around each point.\neach configuration we compare the performance of alternating and random flip in terms of their mean\naccuracy across n= 400 runs of training.\nFigure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip\nimproves performance in every setting. To get a sense for how big the improvement is, we estimate\nthe effective speedup for each case, i.e., the fraction of epochs that could be saved by switching\nfrom random to alternating flip while maintaining the level of accuracy of random flip. We begin\nby fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of\neach random flip-based training configuration. We use these curves to calculate the effective speedup\nafforded by switching from random to alternating flip. For example, airbench94 with random flip\nand without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The\nsame configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law\nfit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.\nNote that using a power-law yields a more conservative estimate relative to using linear interpolation\nbetween the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.\nTable 2 shows the result. We observe the following patterns. First, the addition of extra augmentation\n(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that\nthe main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the\nsame way for many epochs in a row; we speculate that adding extra augmentation reduces the negative\nimpact of these cases because it increases data diversity. Next, TTA reduces the gap between random\nand alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),\nindicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training\nfor longer consistently increases the effective speedup given by alternating flip.\nWe next study ImageNet trainings with the following experiment. We train a ResNet-18 with a\nvariety of train and test crops, comparing three flipping options: alternating flip, random flip, and no\nflipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192\ncenter crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)\nto denote the latter. We also consider two training crops: 192x192 inception-style random resized\ncrop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area\nranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the\nshorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write\n7\nBaseline Cutout Epochs Speedup Speedup (w/ TTA)\nairbench94 No 10 15.0% 5.30%\nairbench94 No 20 27.1% 21.3%\nairbench94 No 40 38.3% 36.4%\nairbench94 No 80 102% 31.8%\nairbench94 Yes 10 3.84% 1.13%\nairbench94 Yes 20 7.42% 2.00%\nairbench94 Yes 40 18.6% 9.28%\nairbench94 Yes 80 29.2% 14.25%\nairbench96 Yes 10 4.94% 1.11%\nairbench96 Yes 20 8.99% 3.58%\nairbench96 Yes 40 17.2% 6.48%\nairbench96 Yes 80 18.8% Not measured\nTable 2: Effective speedups given by switching from random flip to alternating flip. The two configurations\nmost closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw\naccuracy values of the airbench94 experiments.\nFlipping augmentation option\nTrain crop Test crop Epochs TTA None Random Alternating\nHeavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28\nHeavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28\nLight RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26\nLight RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26\nHeavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28\nHeavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28\nLight RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26\nLight RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26\nLight RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16\nHeavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4\nTable 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for\nthose trainings where random flip improves significantly over not flipping at all. The single best flipping option\nin each row is bolded when the difference is statistically significant.\nHeavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided\nin Section C.\nTable 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC\nis better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly\nbetter when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test\nresolution discrepancy (Touvron et al., 2019).\nFor trainings which use Light RRC, we find that switching from random flip to alternating flip\nprovides a substantial boost to performance, amounting to a training speedup of more than 25%.\nIn Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to\nalternating flip improves performance by more than increasing the training duration from 16 to 20\nepochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our\nresults on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit\nfrom alternating flip. Indeed, even turning flipping off completely does not significantly reduce the\nperformance of these trainings. We conclude that alternating flip improves over random flip for every\ntraining scenario where the latter improves over no flipping at all.\n8\nEpochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE\n1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312\n2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233\n1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269\n1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533\n2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433\n1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444\nTable 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).\n5.3 Variance and class-wise calibration\nPrevious sections have focused on understanding what factors affect the first moment of accuracy\n(the mean). In this section we investigate the second moment, finding that TTA reduces variance at\nthe cost of calibration.\nOur experiment is to execute 10,000 runs of airbench94 training with several hyperparameter\nsettings. For each setting we report both the variance in test-set accuracy as well as an estimate of the\ndistribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.\nTable 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set\nvariance, replicating the main finding of Jordan (2023). This is a surprising result because these\ntrainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7\nas much distribution-wise variance when run for a similar duration, and reached a low variance only\nwhen run for 64 epochs. We conclude from this comparison that distribution-wise variance is more\nstrongly connected to the rate of convergence of a training rather than its duration as such. We also\nnote that the low distribution-wise variance of airbench94 indicates it has high training stability.\nUsing TTA significantly reduces the test-set variance, such that all three settings with TTA have lower\ntest-set variance than any setting without TTA. However, test-set variance is implied by the class-wise\ncalibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this\nreduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,\nwe compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which\nmeasures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA\nhas a higher CACE than every setting without TTA, confirming the hypothesis.\n6 Discussion\nIn this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7\nfaster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the\nairbench Python package.\nWe developed airbench solely with the goal of maximizing training speed on CIFAR-10. In\nSection B we find that it also generalizes well to other tasks. For example, without any extra tuning,\nairbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.\nOne factor contributing to the training speed of airbench was our finding that training can be accel-\nerated by partially derandomizing the standard random horizontal flipping augmentation, resulting in\nthe variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating\nflip improves the performance of every training we considered (Section 5.2), with the exception of\nthose trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to\nus, the standard ImageNet trainings that we considered do not significantly benefit from horizontal\nflipping. Future work might investigate whether it is possible to obtain derandomized improvements\nto other augmentations besides horizontal flip.\nThe methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,\nwith fixed performance and hardware constraints. These constraints mean that we cannot improve\nperformance by simply scaling up the amount of computational resources used; instead we are forced\nto develop new methods like the alternating flip. We look forward to seeing what other new methods\nfuture work discovers to push training speed further.\n9\nReferences\nDimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.\nDan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-\nperformance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,\n2011.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\nmark and competition. 2017.\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet\nor cifar-10. arXiv preprint arXiv:1810.03505 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\npp. 248\u2013255. IEEE, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552 , 2017.\nMert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic\ngradient descent. Mathematical Programming , 186:49\u201384, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 , 2016.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.\npmlr, 2015.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\nsgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.\nKeller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and\ninevitable. arXiv preprint arXiv:2304.01910 , 2023.\nAndrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.\nio/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for\nadvanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT\nLicense.\nGuillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander\nM \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,\nBenjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.\nBenchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural\nInformation Processing Systems , 35:25404\u201325421, 2022.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.\nDavid Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/\nhow-to-train-your-resnet-8-bag-of-tricks/ .\n10\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics , 22(3):400\u2013407, 1951.\nSantiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:\n//github.com/apple/ml-cifar-10-faster , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.\ntysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/\nhlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. Advances in neural information processing systems , 32, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,\n2016.\nMichael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps\nforward, 1 step back. Advances in neural information processing systems , 32, 2019.\n11\nA Network architecture\n1from torch import nn\n2\n3class Flatten (nn. Module ):\n4 def forward (self , x):\n5 return x. view (x. size (0) , -1)\n6\n7class Mul (nn. Module ):\n8 def __init__ (self , scale ):\n9 super (). __init__ ()\n10 self . scale = scale\n11 def forward (self , x):\n12 return x * self . scale\n13\n14def conv (ch_in , ch_out ):\n15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,\n16 padding =\u2019same \u2019, bias = False )\n17\n18def make_net ():\n19 act = lambda : nn. GELU ()\n20 bn = lambda ch: nn. BatchNorm2d (ch)\n21 return nn. Sequential (\n22 nn. Sequential (\n23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),\n24 act () ,\n25 ),\n26 nn. Sequential (\n27 conv (24 , 64) ,\n28 nn. MaxPool2d (2) ,\n29 bn (64) , act () ,\n30 conv (64 , 64) ,\n31 bn (64) , act () ,\n32 ),\n33 nn. Sequential (\n34 conv (64 , 256) ,\n35 nn. MaxPool2d (2) ,\n36 bn (256) , act () ,\n37 conv (256 , 256) ,\n38 bn (256) , act () ,\n39 ),\n40 nn. Sequential (\n41 conv (256 , 256) ,\n42 nn. MaxPool2d (2) ,\n43 bn (256) , act () ,\n44 conv (256 , 256) ,\n45 bn (256) , act () ,\n46 ),\n47 nn. MaxPool2d (3) ,\n48 Flatten () ,\n49 nn. Linear (256 , 10, bias = False ),\n50 Mul (1/9) ,\n51 )\nListing 3: Minimal PyTorch code for the network architecture used by airbench94 .\nWe note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock\ntime. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel\nsize of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this\ndoes not improve the wallclock training time on an A100. Reducing the batch size is another easy\nway to save FLOPs but not wallclock time.\n12\nDataset Flipping? Cutout? ResNet-18 airbench96\nCIFAR-10 Yes No 95.55% 95.61%\nCIFAR-10 Yes Yes 96.01% 96.05%\nCIFAR-100 Yes No 77.54% 79.27%\nCIFAR-100 Yes Yes 78.04% 79.76%\nCINIC-10 Yes No 87.58% 87.78%\nCINIC-10 Yes Yes not measured 88.22%\nSVHN No No 97.35% 97.38%\nSVHN No Yes not measured 97.64%\nTable 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We\ndirectly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off\nflipping for SVHN).\nB Extra dataset experiments\nWe developed airbench with the singular goal of maximizing training speed on CIFAR-10. To\nfind out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its\nperformance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-\n10 (Darlow et al., 2018).\nOn CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both\nthe case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not\n(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than\nResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say\nthat it generalizes.\nWe compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.\nWe do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal\non CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better\nperformance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where\nairbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is\nused and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows\nstrong generalization to other tasks.\nWe note that this comparison between airbench96 and ResNet-18 training is fair in the sense that\nit does demonstrate that the former has good generalization, but unfair in the sense that it does not\nindicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time\naugmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training\nwould outperform airbench96 if it were run using test-time augmentation. However, it also takes\n5-10 times longer to complete. The decision of which to use may be situational.\nThe accuracy values we report for ResNet-18 training are from the following sources. We tried\nto select the highest values we could find for each setting. Moreau et al. (2022) reports attaining\n95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports\nattaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on\nCIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.\nC ImageNet training details\nOur ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/\nlibffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and\nmomentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution\n160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of\ntraining. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.\n13\nHyperparameters Flipping augmentation option\nEpochs Cutout TTA None Random Alternating\n10 No No 92.3053 93.0988 93.2798\n20 No No 92.8166 93.7446 93.8652\n40 No No 93.0143 94.0133 94.0729\n80 No No 93.0612 94.1169 94.1628\n10 No Yes 93.4071 93.9488 94.0186\n20 No Yes 93.8528 94.5565 94.6530\n40 No Yes 94.0381 94.7803 94.8203\n80 No Yes 94.0638 94.8506 94.8676\n10 Yes No 91.8487 92.0402 92.1374\n20 Yes No 92.8474 93.3825 93.4876\n40 Yes No 93.2675 94.1014 94.1952\n80 Yes No 93.4193 94.4311 94.5204\n10 Yes Yes 92.6455 92.7780 92.8103\n20 Yes Yes 93.7862 94.1306 94.1670\n40 Yes Yes 94.3090 94.8511 94.8960\n80 Yes Yes 94.5253 95.1839 95.2362\nTable 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a\nmean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise\ndifference in means is statistically significant.\nD Extra tables & figures\nFigure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in\nSection 5.3.\nE Complete training code\n1\"\"\"\n2airbench94 .py\n33.83 s runtime on an A100 ; 0.36 PFLOPs .\n4Evidence for validity : 94.01 average accuracy in n =1000 runs .\n5\n6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB\nwith the following nvidia -smi:\n7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :\n11.7\n14\n8torch . __version__ == \u20192.1.2+ cu118 \u2019\n9\"\"\"\n10\n11# ############################################\n12# Setup / Hyperparameters #\n13# ############################################\n14\n15import os\n16import sys\n17import uuid\n18from math import ceil\n19\n20import torch\n21from torch import nn\n22import torch .nn. functional as F\n23import torchvision\n24import torchvision . transforms as T\n25\n26torch . backends . cudnn . benchmark = True\n27\n28\"\"\"\n29We express the main training hyperparameters ( batch size , learning\nrate , momentum , and weight decay ) in decoupled form , so that each\none can be tuned independently . This accomplishes the following :\n30* Assuming time - constant gradients , the average step size is decoupled\nfrom everything but the lr.\n31* The size of the weight decay update is decoupled from everything but\nthe wd.\n32In constrast , normally when we increase the ( Nesterov ) momentum , this\nalso scales up the step size proportionally to 1 + 1 / (1 -\nmomentum ), meaning we cannot change momentum without having to re -\ntune the learning rate . Similarly , normally when we increase the\nlearning rate this also increases the size of the weight decay ,\nrequiring a proportional decrease in the wd to maintain the same\ndecay strength .\n33\n34The practical impact is that hyperparameter tuning is faster , since\nthis parametrization allows each one to be tuned independently .\nSee https :// myrtle .ai/ learn /how -to -train -your - resnet -5-\nhyperparameters /.\n35\"\"\"\n36\n37hyp = {\n38 \u2019opt \u2019: {\n39 \u2019 train_epochs \u2019: 9.9 ,\n40 \u2019batch_size \u2019: 1024 ,\n41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples\n42 \u2019momentum \u2019: 0.85 ,\n43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (\ndecoupled from learning rate )\n44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not\nweight decay ) for BatchNorm biases\n45 \u2019 label_smoothing \u2019: 0.2 ,\n46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the\nwhitening layer bias before freezing\n47 },\n48 \u2019aug \u2019: {\n49 \u2019flip \u2019: True ,\n50 \u2019translate \u2019: 2,\n51 },\n52 \u2019net \u2019: {\n53 \u2019widths \u2019: {\n54 \u2019block1 \u2019: 64,\n55 \u2019block2 \u2019: 256 ,\n56 \u2019block3 \u2019: 256 ,\n15\n57 },\n58 \u2019 batchnorm_momentum \u2019: 0.6 ,\n59 \u2019 scaling_factor \u2019: 1/9 ,\n60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :\n0= none , 1= mirror , 2= mirror + translate\n61 },\n62}\n63\n64# ############################################\n65# DataLoader #\n66# ############################################\n67\n68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )\n69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )\n70\n71def batch_flip_lr ( inputs ):\n72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .\nview (-1, 1, 1, 1)\n73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\n74\n75def batch_crop (images , crop_size ):\n76 r = ( images . size ( -1) - crop_size )//2\n77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =\nimages . device )\n78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),\ndevice = images .device , dtype = images . dtype )\n79 # The two cropping methods in this if - else produce equivalent\nresults , but the second is faster for r > 2.\n80 if r <= 2:\n81 for sy in range (-r, r +1):\n82 for sx in range (-r, r +1):\n83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)\n84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size\n, r+sx:r+sx+ crop_size ]\n85 else :\n86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size\n+2* r), device = images .device , dtype = images . dtype )\n87 for s in range (-r, r +1):\n88 mask = ( shifts [:, 0] == s)\n89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]\n90 for s in range (-r, r +1):\n91 mask = ( shifts [:, 1] == s)\n92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+\ncrop_size ]\n93 return images_out\n94\n95class CifarLoader :\n96 \"\"\"\n97 GPU - accelerated dataloader for CIFAR -10 which implements\nalternating flip augmentation .\n98 \"\"\"\n99\n100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,\ndrop_last =None , shuffle =None , gpu =0):\n101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .\npt \u2019)\n102 if not os. path . exists ( data_path ):\n103 dset = torchvision . datasets . CIFAR10 (path , download =True ,\ntrain = train )\n104 images = torch . tensor ( dset . data )\n105 labels = torch . tensor ( dset . targets )\n106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:\ndset . classes }, data_path )\n107\n108 data = torch . load ( data_path , map_location = torch . device (gpu))\n16\n109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [\n\u2019labels \u2019], data [\u2019classes \u2019]\n110 # It \u2019s faster to load + process uint8 data than to load\npreprocessed fp16 data\n111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).\nto( memory_format = torch . channels_last )\n112\n113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )\n114 self . proc_images = {} # Saved results of image processing to\nbe done on the first epoch\n115 self . epoch = 0\n116\n117 self . aug = aug or {}\n118 for k in self .aug . keys ():\n119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019\n% k\n120\n121 self . batch_size = batch_size\n122 self . drop_last = train if drop_last is None else drop_last\n123 self . shuffle = train if shuffle is None else shuffle\n124\n125 def __len__ ( self ):\n126 return len( self . images )// self . batch_size if self . drop_last\nelse ceil (len( self . images )/ self . batch_size )\n127\n128 def __iter__ ( self ):\n129\n130 if self . epoch == 0:\n131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .\nimages )\n132 # Randomly flip all images on the first epoch as according\nto definition of alternating flip\n133 if self .aug.get(\u2019flip \u2019, False ):\n134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (\nimages )\n135 # Pre -pad images to save time when doing random\ntranslation\n136 pad = self .aug.get (\u2019translate \u2019, 0)\n137 if pad > 0:\n138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019\nreflect \u2019)\n139\n140 if self .aug.get(\u2019translate \u2019, 0) > 0:\n141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .\nshape [ -2])\n142 elif self .aug.get (\u2019flip \u2019, False ):\n143 images = self . proc_images [\u2019flip \u2019]\n144 else :\n145 images = self . proc_images [\u2019norm \u2019]\n146 if self .aug.get(\u2019flip \u2019, False ):\n147 if self . epoch % 2 == 1:\n148 images = images . flip ( -1)\n149\n150 self . epoch += 1\n151\n152 indices = ( torch . randperm if self . shuffle else torch . arange )(\nlen ( images ), device = images . device )\n153 for i in range (len ( self )):\n154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]\n155 yield ( images [ idxs ], self . labels [ idxs ])\n156\n157# ############################################\n158# Network Components #\n159# ############################################\n160\n17\n161class Flatten (nn. Module ):\n162 def forward (self , x):\n163 return x. view (x. size (0) , -1)\n164\n165class Mul (nn. Module ):\n166 def __init__ (self , scale ):\n167 super (). __init__ ()\n168 self . scale = scale\n169 def forward (self , x):\n170 return x * self . scale\n171\n172class BatchNorm (nn. BatchNorm2d ):\n173 def __init__ (self , num_features , momentum , eps =1e -12 ,\n174 weight =False , bias = True ):\n175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )\n176 self . weight . requires_grad = weight\n177 self . bias . requires_grad = bias\n178 # Note that PyTorch already initializes the weights to one and\nbiases to zero\n179\n180class Conv (nn. Conv2d ):\n181 def __init__ (self , in_channels , out_channels , kernel_size =3,\npadding =\u2019same \u2019, bias = False ):\n182 super (). __init__ ( in_channels , out_channels , kernel_size =\nkernel_size , padding = padding , bias = bias )\n183\n184 def reset_parameters ( self ):\n185 super (). reset_parameters ()\n186 if self . bias is not None :\n187 self . bias . data . zero_ ()\n188 w = self . weight . data\n189 torch .nn. init . dirac_ (w[:w. size (1) ])\n190\n191class ConvGroup (nn. Module ):\n192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):\n193 super (). __init__ ()\n194 self . conv1 = Conv ( channels_in , channels_out )\n195 self . pool = nn. MaxPool2d (2)\n196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )\n197 self . conv2 = Conv ( channels_out , channels_out )\n198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )\n199 self . activ = nn. GELU ()\n200\n201 def forward (self , x):\n202 x = self . conv1 (x)\n203 x = self . pool (x)\n204 x = self . norm1 (x)\n205 x = self . activ (x)\n206 x = self . conv2 (x)\n207 x = self . norm2 (x)\n208 x = self . activ (x)\n209 return x\n210\n211# ############################################\n212# Network Definition #\n213# ############################################\n214\n215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019\n][ \u2019 batchnorm_momentum \u2019]):\n216 whiten_kernel_size = 2\n217 whiten_width = 2 * 3 * whiten_kernel_size **2\n218 net = nn. Sequential (\n219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True\n),\n220 nn. GELU () ,\n18\n221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],\nbatchnorm_momentum ),\n222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],\nbatchnorm_momentum ),\n223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],\nbatchnorm_momentum ),\n224 nn. MaxPool2d (3) ,\n225 Flatten () ,\n226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),\n227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,\n228 )\n229 net [0]. weight . requires_grad = False\n230 net = net. half (). cuda ()\n231 net = net.to( memory_format = torch . channels_last )\n232 for mod in net. modules ():\n233 if isinstance (mod , BatchNorm ):\n234 mod . float ()\n235 return net\n236\n237# ############################################\n238# Whitening Conv Initialization #\n239# ############################################\n240\n241def get_patches (x, patch_shape ):\n242 c, (h, w) = x. shape [1] , patch_shape\n243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h\n,w). float ()\n244\n245def get_whitening_parameters ( patches ):\n246 n,c,h,w = patches . shape\n247 patches_flat = patches . view (n, -1)\n248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n\n249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance\n, UPLO =\u2019U\u2019)\n250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.\nreshape (c*h*w,c,h,w). flip (0)\n251\n252def init_whitening_conv (layer , train_set , eps =5e -4):\n253 patches = get_patches ( train_set , patch_shape = layer . weight . data .\nshape [2:])\n254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )\n255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)\n256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -\neigenvectors_scaled ))\n257\n258# ###########################################\n259# Lookahead #\n260# ###########################################\n261\n262class LookaheadState :\n263 def __init__ (self , net):\n264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().\nitems ()}\n265\n266 def update (self , net , decay ):\n267 for ema_param , net_param in zip( self . net_ema . values () , net.\nstate_dict (). values ()):\n268 if net_param . dtype in ( torch .half , torch . float ):\n269 ema_param . lerp_ ( net_param , 1- decay )\n270 net_param . copy_ ( ema_param )\n271\n272# ###########################################\n273# Logging #\n274# ###########################################\n275\n19\n276def print_columns ( columns_list , is_head =False , is_final_entry = False ):\n277 print_string = \u2019\u2019\n278 for col in columns_list :\n279 print_string += \u2019| %s \u2019 % col\n280 print_string += \u2019|\u2019\n281 if is_head :\n282 print (\u2019-\u2019*len( print_string ))\n283 print ( print_string )\n284 if is_head or is_final_entry :\n285 print (\u2019-\u2019*len( print_string ))\n286\n287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,\n\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]\n288def print_training_details ( variables , is_final_entry ):\n289 formatted = []\n290 for col in logging_columns_list :\n291 var = variables .get (col. strip () , None )\n292 if type (var) in (int , str):\n293 res = str(var)\n294 elif type (var) is float :\n295 res = \u2019{:0.4 f}\u2019. format ( var)\n296 else :\n297 assert var is None\n298 res = \u2019\u2019\n299 formatted . append (res. rjust (len (col)))\n300 print_columns ( formatted , is_final_entry = is_final_entry )\n301\n302# ###########################################\n303# Evaluation #\n304# ###########################################\n305\n306def infer (model , loader , tta_level =0) :\n307 \"\"\"\n308 Test - time augmentation strategy (for tta_level =2):\n309 1. Flip / mirror the image left -to - right (50% of the time ).\n310 2. Translate the image by one pixel either up -and - left or down -and\n- right (50% of the time , i.e. both happen 25% of the time ).\n311\n312 This creates 6 views per image ( left / right times the two\ntranslations and no - translation ), which we evaluate and then\nweight according to the given probabilities .\n313 \"\"\"\n314\n315 def infer_basic (inputs , net):\n316 return net( inputs ). clone ()\n317\n318 def infer_mirror (inputs , net):\n319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))\n320\n321 def infer_mirror_translate ( inputs , net ):\n322 logits = infer_mirror (inputs , net )\n323 pad = 1\n324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)\n325 inputs_translate_list = [\n326 padded_inputs [:, :, 0:32 , 0:32] ,\n327 padded_inputs [:, :, 2:34 , 2:34] ,\n328 ]\n329 logits_translate_list = [ infer_mirror ( inputs_translate , net)\n330 for inputs_translate in\ninputs_translate_list ]\n331 logits_translate = torch . stack ( logits_translate_list ). mean (0)\n332 return 0.5 * logits + 0.5 * logits_translate\n333\n334 model . eval ()\n335 test_images = loader . normalize ( loader . images )\n20\n336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][\ntta_level ]\n337 with torch . no_grad ():\n338 return torch .cat ([ infer_fn (inputs , model ) for inputs in\ntest_images . split (2000) ])\n339\n340def evaluate (model , loader , tta_level =0) :\n341 logits = infer (model , loader , tta_level )\n342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()\n343\n344# ###########################################\n345# Training #\n346# ###########################################\n347\n348def main (run):\n349\n350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]\n351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]\n352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]\n353 # Assuming gradients are constant in time , for Nesterov momentum ,\nthe below ratio is how much larger the default steps will be than\nthe underlying per - example gradients . We divide the learning rate\nby this ratio in order to ensure steps are the same scale as\ngradients , regardless of the choice of momentum .\n354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))\n355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning\nrate for PyTorch SGD\n356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale\n357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]\n358\n359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019\nlabel_smoothing \u2019], reduction =\u2019none \u2019)\n360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)\n361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =\nbatch_size , aug=hyp[\u2019aug \u2019])\n362 if run == \u2019warmup \u2019:\n363 # The only purpose of the first run is to warmup , so we can\nuse dummy data\n364 train_loader . labels = torch . randint (0, 10, size =( len (\ntrain_loader . labels ) ,), device = train_loader . labels . device )\n365 total_train_steps = ceil (len( train_loader ) * epochs )\n366\n367 model = make_net ()\n368 current_steps = 0\n369\n370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in\nk and p. requires_grad ]\n371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019\nnot in k and p. requires_grad ]\n372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,\nweight_decay =wd/ lr_biases ),\n373 dict ( params = other_params , lr=lr , weight_decay =wd/\nlr)]\n374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,\nnesterov = True )\n375\n376 def triangle (steps , start =0, end =0, peak =0.5) :\n377 xp = torch . tensor ([0 , int( peak * steps ), steps ])\n378 fp = torch . tensor ([ start , 1, end ])\n379 x = torch . arange (1+ steps )\n380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])\n381 b = fp [: -1] - (m * xp [: -1])\n382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1\n383 indices = torch . clamp ( indices , 0, len(m) - 1)\n384 return m[ indices ] * x + b[ indices ]\n21\n385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,\npeak =0.23)\n386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:\nlr_schedule [i])\n387\n388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /\ntotal_train_steps )**3\n389 lookahead_state = LookaheadState ( model )\n390\n391 # For accurately timing GPU code\n392 starter = torch . cuda . Event ( enable_timing = True )\n393 ender = torch . cuda . Event ( enable_timing = True )\n394 total_time_seconds = 0.0\n395\n396 # Initialize the first layer using statistics of training images\n397 starter . record ()\n398 train_images = train_loader . normalize ( train_loader . images [:5000])\n399 init_whitening_conv ( model [0] , train_images )\n400 ender . record ()\n401 torch . cuda . synchronize ()\n402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n403\n404 for epoch in range ( ceil ( epochs )):\n405\n406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019\nwhiten_bias_epochs \u2019])\n407\n408 # ###################\n409 # Training #\n410 # ###################\n411\n412 starter . record ()\n413\n414 model . train ()\n415 for inputs , labels in train_loader :\n416\n417 outputs = model ( inputs )\n418 loss = loss_fn ( outputs , labels ).sum ()\n419 optimizer . zero_grad ( set_to_none = True )\n420 loss . backward ()\n421 optimizer . step ()\n422 scheduler . step ()\n423\n424 current_steps += 1\n425\n426 if current_steps % 5 == 0:\n427 lookahead_state . update (model , decay = alpha_schedule [\ncurrent_steps ]. item ())\n428\n429 if current_steps >= total_train_steps :\n430 if lookahead_state is not None :\n431 lookahead_state . update (model , decay =1.0)\n432 break\n433\n434 ender . record ()\n435 torch . cuda . synchronize ()\n436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n437\n438 # ###################\n439 # Evaluation #\n440 # ###################\n441\n442 # Print the accuracy and loss from the last training batch of\nthe epoch\n22\n443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().\nmean (). item ()\n444 train_loss = loss . item () / batch_size\n445 val_acc = evaluate (model , test_loader , tta_level =0)\n446 print_training_details ( locals () , is_final_entry = False )\n447 run = None # Only print the run number once\n448\n449 # ###################\n450 # TTA Evaluation #\n451 # ###################\n452\n453 starter . record ()\n454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019\ntta_level \u2019])\n455 ender . record ()\n456 torch . cuda . synchronize ()\n457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n458\n459 epoch = \u2019eval \u2019\n460 print_training_details ( locals () , is_final_entry = True )\n461\n462 return tta_val_acc\n463\n464if __name__ == \" __main__ \":\n465 with open (sys. argv [0]) as f:\n466 code = f. read ()\n467\n468 print_columns ( logging_columns_list , is_head = True )\n469 main (\u2019warmup \u2019)\n470 accs = torch . tensor ([ main (run) for run in range (25) ])\n471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))\n472\n473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }\n474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))\n475 os. makedirs ( log_dir , exist_ok = True )\n476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)\n477 print (os. path . abspath ( log_path ))\n478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))\nListing 4: airbench94.py\n23\n\nGuidance:\nSelf Critique:\n\nI'll evaluate the task candidates I extracted based on the given requirements:\n\n1. Implement alternating flip data augmentation:\nStrengths:\n- Describes a concrete programming task to modify a baseline implementation\n- Reflects an actual task undertaken in the research\n- Requires engineering expertise to implement correctly\n- Relatively straightforward to write an objective scoring function (e.g. check if images are flipped in the correct pattern across epochs)\n- Scoring function would be much easier to produce than implementing the task itself\n\nWeaknesses:\n- May not require significant research capabilities, mostly engineering\n\n2. Implement decoupled hyperparameters:\nStrengths:\n- Describes a programming task to modify hyperparameter handling\n- Reflects work done in the research\n- Requires some engineering expertise\n\nWeaknesses:\n- May be challenging to write an objective scoring function beyond checking if the hyperparameters are decoupled\n- Implementing the task may not be significantly harder than writing the scoring function\n- Does not require much research capability\n\n3. Implement patch-whitening initialization:\nStrengths:\n- Describes a concrete programming task\n- Reflects actual work from the research\n- Requires both research understanding and engineering skill\n- Possible to write an objective scoring function (e.g. check if filters are initialized correctly)\n- Scoring function would be easier than implementation\n\nWeaknesses:\n- May be very challenging for an agent without strong math/ML background\n\nOverall, the alternating flip and patch-whitening tasks seem to best meet the criteria. The decoupled hyperparameters task is less suitable. I could improve by extracting more task candidates from other parts of the paper, especially those requiring more research capabilities.\n\nPast Error in Task Candidates: ValueError('json output should start and end with { and }')\n\nTask Candidates:"
  },
  {
    "Response": "{\n  \"tasks\": [\n    {\n      \"name\": \"Implement alternating flip data augmentation\",\n      \"description\": \"Modify the data augmentation pipeline to implement the alternating flip method described in the paper. This involves flipping 50% of images randomly in the first epoch, then alternating which images are flipped in subsequent epochs according to the pattern described in Section 3.6.\",\n      \"relevant_paper_text\": \"To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\",\n      \"scoring_feasibility\": 8,\n      \"llm_tractability\": 0.7,\n      \"expert_tractability\": 0.9,\n      \"layman_tractability\": 0.3\n    },\n    {\n      \"name\": \"Implement patch-whitening initialization\",\n      \"description\": \"Implement the patch-whitening initialization method for the first convolutional layer as described in the paper. This involves computing the covariance matrix of 2x2 patches across the training distribution, initializing filters based on the eigenvectors, and setting up the layer to not be updated during training.\",\n      \"relevant_paper_text\": \"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\",\n      \"scoring_feasibility\": 7,\n      \"llm_tractability\": 0.5,\n      \"expert_tractability\": 0.8,\n      \"layman_tractability\": 0.1\n    },\n    {\n      \"name\": \"Implement multi-crop test-time augmentation\",\n      \"description\": \"Implement the multi-crop test-time augmentation strategy described in the paper. This involves running the trained network on six augmented views of each test image: the original, translated versions, and mirrored versions of each. Predictions should be made using a weighted average of all six outputs.\",\n      \"relevant_paper_text\": \"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\",\n      \"scoring_feasibility\": 8,\n      \"llm_tractability\": 0.6,\n      \"expert_tractability\": 0.9,\n      \"layman_tractability\": 0.2\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `candidate_input`, `candidate_output`, `requirements`, `attempt_num`, `previous_critiques`, produce the fields `self_critique`, `should_retry`.\n\n---\n\nFollow the following format.\n\nCandidate Input: ${candidate_input}\n\nCandidate Output: ${candidate_output}\n\nRequirements: ${requirements}\n\nAttempt Num: ${attempt_num}\n\nPrevious Critiques: ${previous_critiques}\n\nSelf Critique: Self-critique of the task candidate\n\nShould Retry: Should the task candidate be retried? This may be expensive. True/False (Respond with true or false)\n\n---\n\nCandidate Input:\n94% on CIFAR-10 in 3.29 Seconds on a Single GPU\nKeller Jordan\nkjordan4077@gmail.com\nAbstract\nCIFAR-10 is among the most widely used datasets in machine learning, facilitating\nthousands of research projects per year. To accelerate research and reduce the cost\nof experiments, we introduce training methods for CIFAR-10 which reach 94%\naccuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when\nrun on a single NVIDIA A100 GPU. As one factor contributing to these training\nspeeds, we propose a derandomized variant of horizontal flipping augmentation,\nwhich we show improves over the standard method in every case where flipping is\nbeneficial over no flipping at all. Our code is released at https://github.com/\nKellerJordan/cifar10-airbench .\n1 Introduction\nCIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-\ning thousands of research projects per year1. Research can be accelerated and the cost of experiments\nreduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In\nthis paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single\nNVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,\n2023). To support scenarios where higher performance is needed, we additionally develop methods\ntargeting 95% and 96% accuracy. We release the following methods in total.\n1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).\n2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).\n3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).\n4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).\nAll runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-\nmatically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the\nfirst uses torch.compile to improve GPU utilization. It is intended for experiments where many\nnetworks are trained at once in order to amortize the one-time compilation cost. The non-compiled\nairbench94 variant can be easily installed and run using the following command.\n1pip install airbench\n2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"\nOne motivation for the development of these training methods is that they can accelerate the experi-\nmental iteration time of researchers working on compatible projects involving CIFAR-10. Another\nmotivation is that they can decrease the cost of projects involving a massive number of trained\nnetworks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used\n3 million trained networks to demonstrate that the outputs of a trained neural network on a given test\ninput follow an approximately linear function of the vector of binary choices of which examples the\nmodel was trained on. Another example is Jordan (2023), a study on training variance which used\n180 thousand trained networks to show that standard trainings have little variance in performance on\n1https://paperswithcode.com/datasets\nPreprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024\nFigure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal\nflipping augmentation, which flips each image with 50% probability per epoch. This results in some images\nbeing redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a\ndeterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\ntheir test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds\nand 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make\nit possible to replicate these studies, or conduct similar ones, with fewer computational resources.\nFast training also enables the rapid accumulation of statistical significance for subtle hyperparameter\ncomparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10\naccuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation\nbetween runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the\nimprovement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training\nthis will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.\nOur work builds on prior training speed projects. We utilize a modified version of the network,\ninitialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen\npatch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over\nprior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-\nure 1, Section 3.6, Section 5.2).\n2 Background\nOur objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set\nin the shortest possible amount of time. Timing begins when the method is first given access to\ntraining data, and ends when it produces test-set predictions. The method is considered valid if its\nmean accuracy over repeated runs is at least 94%.\nWe chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the\n2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more\nrecent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10\nseconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page\n(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is\ntysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,\n94% is the level of human accuracy reported by Karpathy (2011).\nWe note the following consequences of how the method is timed. First, it is permitted for the program\nto begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins\nwhen the training data is first accessed. This is helpful because otherwise the first run of training\nis typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA\nimproves the performance of a trained network by running it on multiple augmented views of each\ntest input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping\nTTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods\nattain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\nThe CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.\nIt is divided into a training set of 50,000 images and a validation set of 10,000 images. As a\nmatter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was\n2\n80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than\nairbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94\ncan be attributed entirely to algorithmic progress rather than compute scaling.\n3 Methods\n3.1 Network architecture and baseline training\nWe train a convolutional network with a total of 1.97 million parameters, following tysam-code\n(2023) with a few small changes. It contains seven convolutions with the latter six being divided into\nthree blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this\nsection we offer some comments on the main design choices.\nThe network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed\nentirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,\n2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with\nno padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather\nthan the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff\nbetween throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\nFollowing Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,\nand disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is\nscaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture\ndiffers only in that we decrease the number of output channels in the third block from 512 to 256,\nand we add learnable biases to the first convolution.\nAs our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of\n0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the\nmaximum at 20% of the way through training, and then decreases to zero. For data augmentation we\nuse random horizontal flipping alongside 2-pixel random translation. For translation we use reflection\npadding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note\nthat what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a\nrandom 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the\nnetwork is run on both a given test image and its mirror and inferences are made based on the average\nof the two outputs. With optimized choices of learning rate, momentum, and weight decay, this\nbaseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.\n3.2 Frozen patch-whitening initialization\nFollowing Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-\nwhitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code\n(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches\nacross the training distribution, so that their outputs have identity covariance matrix. The second 12\nFigure 2: The first layer\u2019s weights after whitening\ninitialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,\nso that input information is preserved through the\nactivation which follows. Figure 2 shows the result.\nWe do not update this layer\u2019s weights during training.\nDeparting from tysam-code (2023), we add learn-\nable biases to this layer, yielding a small performance\nboost. The biases are trained for 3 epochs, after which\nwe disable their gradient to increase backward-pass\nthroughput, which improves training speed without\nreducing accuracy. We also obtain a slight perfor-\nmance boost relative to tysam-code (2023) by re-\nducing the constant added to the eigenvalues during\ncalculation of the patch-whitening initialization for\nthe purpose of preventing numerical issues in the case\nof a singular patch-covariance matrix.\nPatch-whitening initialization is the single most impactful feature. Adding it to the baseline more\nthan doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.\n3\nRandom reshuffling Alternating flip Mean accuracy\nNo No 93.40%\nNo Yes 93.48%\nYes No 93.92%\nYes Yes 94.01%\nTable 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating\nflip (which we propose) reduce training data redundancy and improve performance.\n3.3 Identity initialization\ndirac : We initialize all convolutions after the first as partial identity transforms. That is, for a\nconvolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an\nidentity transform of the input, and leave the remaining N\u2212Mto their default initialization. In\nPyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight\nwof each convolutional layer. This method partially follows tysam-code (2023), which used a more\ncomplicated scheme where the identity weights are mixed in with the original initialization, which\nwe did not find to be more performant. With this feature added, training attains 94% accuracy in 18\nepochs taking 6.8 A100-seconds.\n3.4 Optimization tricks\nscalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor\nof64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%\nin 13.5 epochs taking 5.1 A100-seconds.\nlookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.\nWe note that Lookahead has also been found effective in prior work on training speed for ResNet-\n18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6\nA100-seconds.\n3.5 Multi-crop evaluation\nmulticrop : To generate predictions, we run the trained network on six augmented views of each\ntest image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a\nversion which is translated down-and-to-the-right by one pixel, and the mirrored versions of all\nthree. Predictions are made using a weighted average of all six outputs, where the two views of the\nuntranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125\neach. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.\nWe note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-\nmonyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of\nevaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using\nmore crops does improve performance, but the increase to inference time outweighs the potential\ntraining speedup.\n3.6 Alternating flip\nTo speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,\nwhich we motivate as follows. When training neural networks, it is standard practice to organize\ntraining into a set of epochs during which every training example is seen exactly once. This\ndiffers from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),\nwhich calls for data to be repeatedly sampled with-replacement from the training set, resulting in\nexamples being potentially seen multiple redundant times within a short window of training. The\nuse of randomly ordered epochs of data for training has a different name, being called the random\nreshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our\ntraining dataset consists of Nunique examples, then sampling data with replacement causes every\n\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N\nunique examples on average. On the other hand, random reshuffling leads to all Nunique examples\nbeing seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason\nthat it is beneficial to maximize the number of unique inputs seen per window of training time.\n4\nWe extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We\nfirst note that standard random horizontal flipping augmentation can be defined as follows.\n1import torch\n2def random_flip ( inputs ):\n3 # Applies random flipping to a batch of images\n4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)\n5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]\nListing 1: Random flip\nIf horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2\nwhich may be seen during training. Potentially, every pair of consecutive epochs could contain every\nunique input. But our main observation is that with standard random horizontal flipping, half of the\nimages will be redundantly flipped the same way during both epochs, so that on average only 1.5N\nunique inputs will be seen.\naltflip : To address this, we propose to modify standard random horizontal flipping augmentation as\nfollows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},\nwe flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we\nflip only those inputs which were flipped in the first epoch. We provide the following implementation\nwhich avoids the need for extra memory by using a pseudorandom function to decide the flips.\n1import torch\n2import hashlib\n3def hash_fn (n, seed =42) :\n4 k = n * seed\n5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,\n16)\n6def alternating_flip (inputs , indices , epoch ):\n7 # Applies alternating flipping to a batch of images\n8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist\n() ])\n9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)\n10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\nListing 2: Alternating flip\nThe result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in\nFigure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.\nAdding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final\ntraining method airbench94.py , the entire contents of which can be found in Section E. It reaches\n94% accuracy in 3.83 seconds on an NVIDIA A100.\n3.7 Compilation\nThe final step we take to speed up training is a non-algorithmic one: we compile our training\nmethod using torch.compile in order to more efficiently utilize the GPU. This results in a training\nscript which is mathematically equivalent (up to small differences in floating point arithmetic) to\nthe non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29\nA100-seconds. The downside is that the one-time compilation process takes up to several minutes to\ncomplete before training runs can begin, so that it is only beneficial when we plan to execute many\nruns of training at once. We release this version as airbench94_compiled.py .\n4 95% and 96% targets\nTo address scenarios where somewhat higher performance is desired, we additionally develop methods\ntargeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .\nTo attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel\ncount of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce\nthe learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%\naccuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.\n2Assuming none of the training inputs are already mirrors of each other.\n5\nFigure 4: Training speedups accumulate additively. Removing individual features from airbench94\nincreases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces\nthe epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-\n94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.\nFigure 3: FLOPs vs. error rate tradeoff. Our\nthree training methods apparently follow a linear\nlog-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-\nVries & Taylor, 2017) augmentation and raise the\ntraining epochs to 40. We add a third convolution to\neach block, and scale the first block to 128 channels\nand the second two to 512. We also add a residual\nconnection across the later two convolutions of each\nblock, which we find is still beneficial despite the fact\nthat we are already using identity initialization (Sec-\ntion 3.3) to ease gradient flow. Finally, we reduce\nthe learning rate by a factor of 0.78. These changes\nyield airbench96 which attains 96.05% accuracy in\n46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.\nFigure 3 shows the FLOPs and error rate of each of\nour three training methods.\n5 Experiments\n5.1 Interaction between features\nTo gain a better sense of the impact of each feature on training speed, we compare two quantities.\nFirst, we measure the number of epochs that can be saved by adding the feature to the whitened\nbaseline (Section 3.2). Second, we measure the number of epochs that must be added when the\nfeature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-\ntion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it\nfrom the final airbench94 increases epochs-to-94% from 9.9 to 12.8.\nFigure 4 shows both quantities for each feature. Surprisingly, we find that for all features except\nmulti-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even\nthough the whitened baseline requires more than twice as many epochs as the final configuration.\nThis indicates that the interaction between most features is additive rather than multiplicative.\n5.2 Does alternating flip generalize?\nIn this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of\ntraining configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all\ncases except those where neither alternating nor random flip improve over using no flipping at all.\nFor CIFAR-10 we consider the performance boost given by alternating flip across the follow-\ning 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and\nairbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For\n6\nFigure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,\nthe use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%\ntraining speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.\n95% confidence intervals are shown around each point.\neach configuration we compare the performance of alternating and random flip in terms of their mean\naccuracy across n= 400 runs of training.\nFigure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip\nimproves performance in every setting. To get a sense for how big the improvement is, we estimate\nthe effective speedup for each case, i.e., the fraction of epochs that could be saved by switching\nfrom random to alternating flip while maintaining the level of accuracy of random flip. We begin\nby fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of\neach random flip-based training configuration. We use these curves to calculate the effective speedup\nafforded by switching from random to alternating flip. For example, airbench94 with random flip\nand without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The\nsame configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law\nfit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.\nNote that using a power-law yields a more conservative estimate relative to using linear interpolation\nbetween the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.\nTable 2 shows the result. We observe the following patterns. First, the addition of extra augmentation\n(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that\nthe main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the\nsame way for many epochs in a row; we speculate that adding extra augmentation reduces the negative\nimpact of these cases because it increases data diversity. Next, TTA reduces the gap between random\nand alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),\nindicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training\nfor longer consistently increases the effective speedup given by alternating flip.\nWe next study ImageNet trainings with the following experiment. We train a ResNet-18 with a\nvariety of train and test crops, comparing three flipping options: alternating flip, random flip, and no\nflipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192\ncenter crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)\nto denote the latter. We also consider two training crops: 192x192 inception-style random resized\ncrop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area\nranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the\nshorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write\n7\nBaseline Cutout Epochs Speedup Speedup (w/ TTA)\nairbench94 No 10 15.0% 5.30%\nairbench94 No 20 27.1% 21.3%\nairbench94 No 40 38.3% 36.4%\nairbench94 No 80 102% 31.8%\nairbench94 Yes 10 3.84% 1.13%\nairbench94 Yes 20 7.42% 2.00%\nairbench94 Yes 40 18.6% 9.28%\nairbench94 Yes 80 29.2% 14.25%\nairbench96 Yes 10 4.94% 1.11%\nairbench96 Yes 20 8.99% 3.58%\nairbench96 Yes 40 17.2% 6.48%\nairbench96 Yes 80 18.8% Not measured\nTable 2: Effective speedups given by switching from random flip to alternating flip. The two configurations\nmost closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw\naccuracy values of the airbench94 experiments.\nFlipping augmentation option\nTrain crop Test crop Epochs TTA None Random Alternating\nHeavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28\nHeavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28\nLight RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26\nLight RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26\nHeavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28\nHeavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28\nLight RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26\nLight RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26\nLight RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16\nHeavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4\nTable 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for\nthose trainings where random flip improves significantly over not flipping at all. The single best flipping option\nin each row is bolded when the difference is statistically significant.\nHeavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided\nin Section C.\nTable 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC\nis better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly\nbetter when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test\nresolution discrepancy (Touvron et al., 2019).\nFor trainings which use Light RRC, we find that switching from random flip to alternating flip\nprovides a substantial boost to performance, amounting to a training speedup of more than 25%.\nIn Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to\nalternating flip improves performance by more than increasing the training duration from 16 to 20\nepochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our\nresults on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit\nfrom alternating flip. Indeed, even turning flipping off completely does not significantly reduce the\nperformance of these trainings. We conclude that alternating flip improves over random flip for every\ntraining scenario where the latter improves over no flipping at all.\n8\nEpochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE\n1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312\n2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233\n1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269\n1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533\n2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433\n1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444\nTable 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).\n5.3 Variance and class-wise calibration\nPrevious sections have focused on understanding what factors affect the first moment of accuracy\n(the mean). In this section we investigate the second moment, finding that TTA reduces variance at\nthe cost of calibration.\nOur experiment is to execute 10,000 runs of airbench94 training with several hyperparameter\nsettings. For each setting we report both the variance in test-set accuracy as well as an estimate of the\ndistribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.\nTable 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set\nvariance, replicating the main finding of Jordan (2023). This is a surprising result because these\ntrainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7\nas much distribution-wise variance when run for a similar duration, and reached a low variance only\nwhen run for 64 epochs. We conclude from this comparison that distribution-wise variance is more\nstrongly connected to the rate of convergence of a training rather than its duration as such. We also\nnote that the low distribution-wise variance of airbench94 indicates it has high training stability.\nUsing TTA significantly reduces the test-set variance, such that all three settings with TTA have lower\ntest-set variance than any setting without TTA. However, test-set variance is implied by the class-wise\ncalibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this\nreduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,\nwe compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which\nmeasures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA\nhas a higher CACE than every setting without TTA, confirming the hypothesis.\n6 Discussion\nIn this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7\nfaster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the\nairbench Python package.\nWe developed airbench solely with the goal of maximizing training speed on CIFAR-10. In\nSection B we find that it also generalizes well to other tasks. For example, without any extra tuning,\nairbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.\nOne factor contributing to the training speed of airbench was our finding that training can be accel-\nerated by partially derandomizing the standard random horizontal flipping augmentation, resulting in\nthe variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating\nflip improves the performance of every training we considered (Section 5.2), with the exception of\nthose trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to\nus, the standard ImageNet trainings that we considered do not significantly benefit from horizontal\nflipping. Future work might investigate whether it is possible to obtain derandomized improvements\nto other augmentations besides horizontal flip.\nThe methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,\nwith fixed performance and hardware constraints. These constraints mean that we cannot improve\nperformance by simply scaling up the amount of computational resources used; instead we are forced\nto develop new methods like the alternating flip. We look forward to seeing what other new methods\nfuture work discovers to push training speed further.\n9\nReferences\nDimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.\nDan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-\nperformance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,\n2011.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\nmark and competition. 2017.\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet\nor cifar-10. arXiv preprint arXiv:1810.03505 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\npp. 248\u2013255. IEEE, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552 , 2017.\nMert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic\ngradient descent. Mathematical Programming , 186:49\u201384, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 , 2016.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.\npmlr, 2015.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\nsgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.\nKeller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and\ninevitable. arXiv preprint arXiv:2304.01910 , 2023.\nAndrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.\nio/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for\nadvanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT\nLicense.\nGuillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander\nM \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,\nBenjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.\nBenchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural\nInformation Processing Systems , 35:25404\u201325421, 2022.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.\nDavid Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/\nhow-to-train-your-resnet-8-bag-of-tricks/ .\n10\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics , 22(3):400\u2013407, 1951.\nSantiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:\n//github.com/apple/ml-cifar-10-faster , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.\ntysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/\nhlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. Advances in neural information processing systems , 32, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,\n2016.\nMichael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps\nforward, 1 step back. Advances in neural information processing systems , 32, 2019.\n11\nA Network architecture\n1from torch import nn\n2\n3class Flatten (nn. Module ):\n4 def forward (self , x):\n5 return x. view (x. size (0) , -1)\n6\n7class Mul (nn. Module ):\n8 def __init__ (self , scale ):\n9 super (). __init__ ()\n10 self . scale = scale\n11 def forward (self , x):\n12 return x * self . scale\n13\n14def conv (ch_in , ch_out ):\n15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,\n16 padding =\u2019same \u2019, bias = False )\n17\n18def make_net ():\n19 act = lambda : nn. GELU ()\n20 bn = lambda ch: nn. BatchNorm2d (ch)\n21 return nn. Sequential (\n22 nn. Sequential (\n23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),\n24 act () ,\n25 ),\n26 nn. Sequential (\n27 conv (24 , 64) ,\n28 nn. MaxPool2d (2) ,\n29 bn (64) , act () ,\n30 conv (64 , 64) ,\n31 bn (64) , act () ,\n32 ),\n33 nn. Sequential (\n34 conv (64 , 256) ,\n35 nn. MaxPool2d (2) ,\n36 bn (256) , act () ,\n37 conv (256 , 256) ,\n38 bn (256) , act () ,\n39 ),\n40 nn. Sequential (\n41 conv (256 , 256) ,\n42 nn. MaxPool2d (2) ,\n43 bn (256) , act () ,\n44 conv (256 , 256) ,\n45 bn (256) , act () ,\n46 ),\n47 nn. MaxPool2d (3) ,\n48 Flatten () ,\n49 nn. Linear (256 , 10, bias = False ),\n50 Mul (1/9) ,\n51 )\nListing 3: Minimal PyTorch code for the network architecture used by airbench94 .\nWe note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock\ntime. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel\nsize of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this\ndoes not improve the wallclock training time on an A100. Reducing the batch size is another easy\nway to save FLOPs but not wallclock time.\n12\nDataset Flipping? Cutout? ResNet-18 airbench96\nCIFAR-10 Yes No 95.55% 95.61%\nCIFAR-10 Yes Yes 96.01% 96.05%\nCIFAR-100 Yes No 77.54% 79.27%\nCIFAR-100 Yes Yes 78.04% 79.76%\nCINIC-10 Yes No 87.58% 87.78%\nCINIC-10 Yes Yes not measured 88.22%\nSVHN No No 97.35% 97.38%\nSVHN No Yes not measured 97.64%\nTable 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We\ndirectly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off\nflipping for SVHN).\nB Extra dataset experiments\nWe developed airbench with the singular goal of maximizing training speed on CIFAR-10. To\nfind out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its\nperformance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-\n10 (Darlow et al., 2018).\nOn CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both\nthe case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not\n(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than\nResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say\nthat it generalizes.\nWe compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.\nWe do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal\non CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better\nperformance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where\nairbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is\nused and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows\nstrong generalization to other tasks.\nWe note that this comparison between airbench96 and ResNet-18 training is fair in the sense that\nit does demonstrate that the former has good generalization, but unfair in the sense that it does not\nindicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time\naugmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training\nwould outperform airbench96 if it were run using test-time augmentation. However, it also takes\n5-10 times longer to complete. The decision of which to use may be situational.\nThe accuracy values we report for ResNet-18 training are from the following sources. We tried\nto select the highest values we could find for each setting. Moreau et al. (2022) reports attaining\n95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports\nattaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on\nCIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.\nC ImageNet training details\nOur ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/\nlibffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and\nmomentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution\n160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of\ntraining. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.\n13\nHyperparameters Flipping augmentation option\nEpochs Cutout TTA None Random Alternating\n10 No No 92.3053 93.0988 93.2798\n20 No No 92.8166 93.7446 93.8652\n40 No No 93.0143 94.0133 94.0729\n80 No No 93.0612 94.1169 94.1628\n10 No Yes 93.4071 93.9488 94.0186\n20 No Yes 93.8528 94.5565 94.6530\n40 No Yes 94.0381 94.7803 94.8203\n80 No Yes 94.0638 94.8506 94.8676\n10 Yes No 91.8487 92.0402 92.1374\n20 Yes No 92.8474 93.3825 93.4876\n40 Yes No 93.2675 94.1014 94.1952\n80 Yes No 93.4193 94.4311 94.5204\n10 Yes Yes 92.6455 92.7780 92.8103\n20 Yes Yes 93.7862 94.1306 94.1670\n40 Yes Yes 94.3090 94.8511 94.8960\n80 Yes Yes 94.5253 95.1839 95.2362\nTable 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a\nmean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise\ndifference in means is statistically significant.\nD Extra tables & figures\nFigure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in\nSection 5.3.\nE Complete training code\n1\"\"\"\n2airbench94 .py\n33.83 s runtime on an A100 ; 0.36 PFLOPs .\n4Evidence for validity : 94.01 average accuracy in n =1000 runs .\n5\n6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB\nwith the following nvidia -smi:\n7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :\n11.7\n14\n8torch . __version__ == \u20192.1.2+ cu118 \u2019\n9\"\"\"\n10\n11# ############################################\n12# Setup / Hyperparameters #\n13# ############################################\n14\n15import os\n16import sys\n17import uuid\n18from math import ceil\n19\n20import torch\n21from torch import nn\n22import torch .nn. functional as F\n23import torchvision\n24import torchvision . transforms as T\n25\n26torch . backends . cudnn . benchmark = True\n27\n28\"\"\"\n29We express the main training hyperparameters ( batch size , learning\nrate , momentum , and weight decay ) in decoupled form , so that each\none can be tuned independently . This accomplishes the following :\n30* Assuming time - constant gradients , the average step size is decoupled\nfrom everything but the lr.\n31* The size of the weight decay update is decoupled from everything but\nthe wd.\n32In constrast , normally when we increase the ( Nesterov ) momentum , this\nalso scales up the step size proportionally to 1 + 1 / (1 -\nmomentum ), meaning we cannot change momentum without having to re -\ntune the learning rate . Similarly , normally when we increase the\nlearning rate this also increases the size of the weight decay ,\nrequiring a proportional decrease in the wd to maintain the same\ndecay strength .\n33\n34The practical impact is that hyperparameter tuning is faster , since\nthis parametrization allows each one to be tuned independently .\nSee https :// myrtle .ai/ learn /how -to -train -your - resnet -5-\nhyperparameters /.\n35\"\"\"\n36\n37hyp = {\n38 \u2019opt \u2019: {\n39 \u2019 train_epochs \u2019: 9.9 ,\n40 \u2019batch_size \u2019: 1024 ,\n41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples\n42 \u2019momentum \u2019: 0.85 ,\n43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (\ndecoupled from learning rate )\n44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not\nweight decay ) for BatchNorm biases\n45 \u2019 label_smoothing \u2019: 0.2 ,\n46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the\nwhitening layer bias before freezing\n47 },\n48 \u2019aug \u2019: {\n49 \u2019flip \u2019: True ,\n50 \u2019translate \u2019: 2,\n51 },\n52 \u2019net \u2019: {\n53 \u2019widths \u2019: {\n54 \u2019block1 \u2019: 64,\n55 \u2019block2 \u2019: 256 ,\n56 \u2019block3 \u2019: 256 ,\n15\n57 },\n58 \u2019 batchnorm_momentum \u2019: 0.6 ,\n59 \u2019 scaling_factor \u2019: 1/9 ,\n60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :\n0= none , 1= mirror , 2= mirror + translate\n61 },\n62}\n63\n64# ############################################\n65# DataLoader #\n66# ############################################\n67\n68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )\n69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )\n70\n71def batch_flip_lr ( inputs ):\n72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .\nview (-1, 1, 1, 1)\n73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\n74\n75def batch_crop (images , crop_size ):\n76 r = ( images . size ( -1) - crop_size )//2\n77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =\nimages . device )\n78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),\ndevice = images .device , dtype = images . dtype )\n79 # The two cropping methods in this if - else produce equivalent\nresults , but the second is faster for r > 2.\n80 if r <= 2:\n81 for sy in range (-r, r +1):\n82 for sx in range (-r, r +1):\n83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)\n84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size\n, r+sx:r+sx+ crop_size ]\n85 else :\n86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size\n+2* r), device = images .device , dtype = images . dtype )\n87 for s in range (-r, r +1):\n88 mask = ( shifts [:, 0] == s)\n89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]\n90 for s in range (-r, r +1):\n91 mask = ( shifts [:, 1] == s)\n92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+\ncrop_size ]\n93 return images_out\n94\n95class CifarLoader :\n96 \"\"\"\n97 GPU - accelerated dataloader for CIFAR -10 which implements\nalternating flip augmentation .\n98 \"\"\"\n99\n100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,\ndrop_last =None , shuffle =None , gpu =0):\n101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .\npt \u2019)\n102 if not os. path . exists ( data_path ):\n103 dset = torchvision . datasets . CIFAR10 (path , download =True ,\ntrain = train )\n104 images = torch . tensor ( dset . data )\n105 labels = torch . tensor ( dset . targets )\n106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:\ndset . classes }, data_path )\n107\n108 data = torch . load ( data_path , map_location = torch . device (gpu))\n16\n109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [\n\u2019labels \u2019], data [\u2019classes \u2019]\n110 # It \u2019s faster to load + process uint8 data than to load\npreprocessed fp16 data\n111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).\nto( memory_format = torch . channels_last )\n112\n113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )\n114 self . proc_images = {} # Saved results of image processing to\nbe done on the first epoch\n115 self . epoch = 0\n116\n117 self . aug = aug or {}\n118 for k in self .aug . keys ():\n119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019\n% k\n120\n121 self . batch_size = batch_size\n122 self . drop_last = train if drop_last is None else drop_last\n123 self . shuffle = train if shuffle is None else shuffle\n124\n125 def __len__ ( self ):\n126 return len( self . images )// self . batch_size if self . drop_last\nelse ceil (len( self . images )/ self . batch_size )\n127\n128 def __iter__ ( self ):\n129\n130 if self . epoch == 0:\n131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .\nimages )\n132 # Randomly flip all images on the first epoch as according\nto definition of alternating flip\n133 if self .aug.get(\u2019flip \u2019, False ):\n134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (\nimages )\n135 # Pre -pad images to save time when doing random\ntranslation\n136 pad = self .aug.get (\u2019translate \u2019, 0)\n137 if pad > 0:\n138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019\nreflect \u2019)\n139\n140 if self .aug.get(\u2019translate \u2019, 0) > 0:\n141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .\nshape [ -2])\n142 elif self .aug.get (\u2019flip \u2019, False ):\n143 images = self . proc_images [\u2019flip \u2019]\n144 else :\n145 images = self . proc_images [\u2019norm \u2019]\n146 if self .aug.get(\u2019flip \u2019, False ):\n147 if self . epoch % 2 == 1:\n148 images = images . flip ( -1)\n149\n150 self . epoch += 1\n151\n152 indices = ( torch . randperm if self . shuffle else torch . arange )(\nlen ( images ), device = images . device )\n153 for i in range (len ( self )):\n154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]\n155 yield ( images [ idxs ], self . labels [ idxs ])\n156\n157# ############################################\n158# Network Components #\n159# ############################################\n160\n17\n161class Flatten (nn. Module ):\n162 def forward (self , x):\n163 return x. view (x. size (0) , -1)\n164\n165class Mul (nn. Module ):\n166 def __init__ (self , scale ):\n167 super (). __init__ ()\n168 self . scale = scale\n169 def forward (self , x):\n170 return x * self . scale\n171\n172class BatchNorm (nn. BatchNorm2d ):\n173 def __init__ (self , num_features , momentum , eps =1e -12 ,\n174 weight =False , bias = True ):\n175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )\n176 self . weight . requires_grad = weight\n177 self . bias . requires_grad = bias\n178 # Note that PyTorch already initializes the weights to one and\nbiases to zero\n179\n180class Conv (nn. Conv2d ):\n181 def __init__ (self , in_channels , out_channels , kernel_size =3,\npadding =\u2019same \u2019, bias = False ):\n182 super (). __init__ ( in_channels , out_channels , kernel_size =\nkernel_size , padding = padding , bias = bias )\n183\n184 def reset_parameters ( self ):\n185 super (). reset_parameters ()\n186 if self . bias is not None :\n187 self . bias . data . zero_ ()\n188 w = self . weight . data\n189 torch .nn. init . dirac_ (w[:w. size (1) ])\n190\n191class ConvGroup (nn. Module ):\n192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):\n193 super (). __init__ ()\n194 self . conv1 = Conv ( channels_in , channels_out )\n195 self . pool = nn. MaxPool2d (2)\n196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )\n197 self . conv2 = Conv ( channels_out , channels_out )\n198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )\n199 self . activ = nn. GELU ()\n200\n201 def forward (self , x):\n202 x = self . conv1 (x)\n203 x = self . pool (x)\n204 x = self . norm1 (x)\n205 x = self . activ (x)\n206 x = self . conv2 (x)\n207 x = self . norm2 (x)\n208 x = self . activ (x)\n209 return x\n210\n211# ############################################\n212# Network Definition #\n213# ############################################\n214\n215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019\n][ \u2019 batchnorm_momentum \u2019]):\n216 whiten_kernel_size = 2\n217 whiten_width = 2 * 3 * whiten_kernel_size **2\n218 net = nn. Sequential (\n219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True\n),\n220 nn. GELU () ,\n18\n221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],\nbatchnorm_momentum ),\n222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],\nbatchnorm_momentum ),\n223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],\nbatchnorm_momentum ),\n224 nn. MaxPool2d (3) ,\n225 Flatten () ,\n226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),\n227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,\n228 )\n229 net [0]. weight . requires_grad = False\n230 net = net. half (). cuda ()\n231 net = net.to( memory_format = torch . channels_last )\n232 for mod in net. modules ():\n233 if isinstance (mod , BatchNorm ):\n234 mod . float ()\n235 return net\n236\n237# ############################################\n238# Whitening Conv Initialization #\n239# ############################################\n240\n241def get_patches (x, patch_shape ):\n242 c, (h, w) = x. shape [1] , patch_shape\n243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h\n,w). float ()\n244\n245def get_whitening_parameters ( patches ):\n246 n,c,h,w = patches . shape\n247 patches_flat = patches . view (n, -1)\n248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n\n249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance\n, UPLO =\u2019U\u2019)\n250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.\nreshape (c*h*w,c,h,w). flip (0)\n251\n252def init_whitening_conv (layer , train_set , eps =5e -4):\n253 patches = get_patches ( train_set , patch_shape = layer . weight . data .\nshape [2:])\n254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )\n255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)\n256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -\neigenvectors_scaled ))\n257\n258# ###########################################\n259# Lookahead #\n260# ###########################################\n261\n262class LookaheadState :\n263 def __init__ (self , net):\n264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().\nitems ()}\n265\n266 def update (self , net , decay ):\n267 for ema_param , net_param in zip( self . net_ema . values () , net.\nstate_dict (). values ()):\n268 if net_param . dtype in ( torch .half , torch . float ):\n269 ema_param . lerp_ ( net_param , 1- decay )\n270 net_param . copy_ ( ema_param )\n271\n272# ###########################################\n273# Logging #\n274# ###########################################\n275\n19\n276def print_columns ( columns_list , is_head =False , is_final_entry = False ):\n277 print_string = \u2019\u2019\n278 for col in columns_list :\n279 print_string += \u2019| %s \u2019 % col\n280 print_string += \u2019|\u2019\n281 if is_head :\n282 print (\u2019-\u2019*len( print_string ))\n283 print ( print_string )\n284 if is_head or is_final_entry :\n285 print (\u2019-\u2019*len( print_string ))\n286\n287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,\n\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]\n288def print_training_details ( variables , is_final_entry ):\n289 formatted = []\n290 for col in logging_columns_list :\n291 var = variables .get (col. strip () , None )\n292 if type (var) in (int , str):\n293 res = str(var)\n294 elif type (var) is float :\n295 res = \u2019{:0.4 f}\u2019. format ( var)\n296 else :\n297 assert var is None\n298 res = \u2019\u2019\n299 formatted . append (res. rjust (len (col)))\n300 print_columns ( formatted , is_final_entry = is_final_entry )\n301\n302# ###########################################\n303# Evaluation #\n304# ###########################################\n305\n306def infer (model , loader , tta_level =0) :\n307 \"\"\"\n308 Test - time augmentation strategy (for tta_level =2):\n309 1. Flip / mirror the image left -to - right (50% of the time ).\n310 2. Translate the image by one pixel either up -and - left or down -and\n- right (50% of the time , i.e. both happen 25% of the time ).\n311\n312 This creates 6 views per image ( left / right times the two\ntranslations and no - translation ), which we evaluate and then\nweight according to the given probabilities .\n313 \"\"\"\n314\n315 def infer_basic (inputs , net):\n316 return net( inputs ). clone ()\n317\n318 def infer_mirror (inputs , net):\n319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))\n320\n321 def infer_mirror_translate ( inputs , net ):\n322 logits = infer_mirror (inputs , net )\n323 pad = 1\n324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)\n325 inputs_translate_list = [\n326 padded_inputs [:, :, 0:32 , 0:32] ,\n327 padded_inputs [:, :, 2:34 , 2:34] ,\n328 ]\n329 logits_translate_list = [ infer_mirror ( inputs_translate , net)\n330 for inputs_translate in\ninputs_translate_list ]\n331 logits_translate = torch . stack ( logits_translate_list ). mean (0)\n332 return 0.5 * logits + 0.5 * logits_translate\n333\n334 model . eval ()\n335 test_images = loader . normalize ( loader . images )\n20\n336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][\ntta_level ]\n337 with torch . no_grad ():\n338 return torch .cat ([ infer_fn (inputs , model ) for inputs in\ntest_images . split (2000) ])\n339\n340def evaluate (model , loader , tta_level =0) :\n341 logits = infer (model , loader , tta_level )\n342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()\n343\n344# ###########################################\n345# Training #\n346# ###########################################\n347\n348def main (run):\n349\n350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]\n351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]\n352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]\n353 # Assuming gradients are constant in time , for Nesterov momentum ,\nthe below ratio is how much larger the default steps will be than\nthe underlying per - example gradients . We divide the learning rate\nby this ratio in order to ensure steps are the same scale as\ngradients , regardless of the choice of momentum .\n354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))\n355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning\nrate for PyTorch SGD\n356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale\n357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]\n358\n359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019\nlabel_smoothing \u2019], reduction =\u2019none \u2019)\n360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)\n361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =\nbatch_size , aug=hyp[\u2019aug \u2019])\n362 if run == \u2019warmup \u2019:\n363 # The only purpose of the first run is to warmup , so we can\nuse dummy data\n364 train_loader . labels = torch . randint (0, 10, size =( len (\ntrain_loader . labels ) ,), device = train_loader . labels . device )\n365 total_train_steps = ceil (len( train_loader ) * epochs )\n366\n367 model = make_net ()\n368 current_steps = 0\n369\n370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in\nk and p. requires_grad ]\n371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019\nnot in k and p. requires_grad ]\n372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,\nweight_decay =wd/ lr_biases ),\n373 dict ( params = other_params , lr=lr , weight_decay =wd/\nlr)]\n374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,\nnesterov = True )\n375\n376 def triangle (steps , start =0, end =0, peak =0.5) :\n377 xp = torch . tensor ([0 , int( peak * steps ), steps ])\n378 fp = torch . tensor ([ start , 1, end ])\n379 x = torch . arange (1+ steps )\n380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])\n381 b = fp [: -1] - (m * xp [: -1])\n382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1\n383 indices = torch . clamp ( indices , 0, len(m) - 1)\n384 return m[ indices ] * x + b[ indices ]\n21\n385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,\npeak =0.23)\n386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:\nlr_schedule [i])\n387\n388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /\ntotal_train_steps )**3\n389 lookahead_state = LookaheadState ( model )\n390\n391 # For accurately timing GPU code\n392 starter = torch . cuda . Event ( enable_timing = True )\n393 ender = torch . cuda . Event ( enable_timing = True )\n394 total_time_seconds = 0.0\n395\n396 # Initialize the first layer using statistics of training images\n397 starter . record ()\n398 train_images = train_loader . normalize ( train_loader . images [:5000])\n399 init_whitening_conv ( model [0] , train_images )\n400 ender . record ()\n401 torch . cuda . synchronize ()\n402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n403\n404 for epoch in range ( ceil ( epochs )):\n405\n406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019\nwhiten_bias_epochs \u2019])\n407\n408 # ###################\n409 # Training #\n410 # ###################\n411\n412 starter . record ()\n413\n414 model . train ()\n415 for inputs , labels in train_loader :\n416\n417 outputs = model ( inputs )\n418 loss = loss_fn ( outputs , labels ).sum ()\n419 optimizer . zero_grad ( set_to_none = True )\n420 loss . backward ()\n421 optimizer . step ()\n422 scheduler . step ()\n423\n424 current_steps += 1\n425\n426 if current_steps % 5 == 0:\n427 lookahead_state . update (model , decay = alpha_schedule [\ncurrent_steps ]. item ())\n428\n429 if current_steps >= total_train_steps :\n430 if lookahead_state is not None :\n431 lookahead_state . update (model , decay =1.0)\n432 break\n433\n434 ender . record ()\n435 torch . cuda . synchronize ()\n436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n437\n438 # ###################\n439 # Evaluation #\n440 # ###################\n441\n442 # Print the accuracy and loss from the last training batch of\nthe epoch\n22\n443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().\nmean (). item ()\n444 train_loss = loss . item () / batch_size\n445 val_acc = evaluate (model , test_loader , tta_level =0)\n446 print_training_details ( locals () , is_final_entry = False )\n447 run = None # Only print the run number once\n448\n449 # ###################\n450 # TTA Evaluation #\n451 # ###################\n452\n453 starter . record ()\n454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019\ntta_level \u2019])\n455 ender . record ()\n456 torch . cuda . synchronize ()\n457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n458\n459 epoch = \u2019eval \u2019\n460 print_training_details ( locals () , is_final_entry = True )\n461\n462 return tta_val_acc\n463\n464if __name__ == \" __main__ \":\n465 with open (sys. argv [0]) as f:\n466 code = f. read ()\n467\n468 print_columns ( logging_columns_list , is_head = True )\n469 main (\u2019warmup \u2019)\n470 accs = torch . tensor ([ main (run) for run in range (25) ])\n471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))\n472\n473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }\n474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))\n475 os. makedirs ( log_dir , exist_ok = True )\n476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)\n477 print (os. path . abspath ( log_path ))\n478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))\nListing 4: airbench94.py\n23\n\nCandidate Output: tasks=[TaskCandidate(name='Implement alternating flip data augmentation', description='Modify the data augmentation pipeline to implement the alternating flip method described in the paper. This involves flipping 50% of images randomly in the first epoch, then alternating which images are flipped in subsequent epochs according to the pattern described in Section 3.6.', relevant_paper_text='To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.', scoring_feasibility=8, llm_tractability=0.7, expert_tractability=0.9, layman_tractability=0.3), TaskCandidate(name='Implement patch-whitening initialization', description='Implement the patch-whitening initialization method for the first convolutional layer as described in the paper. This involves computing the covariance matrix of 2x2 patches across the training distribution, initializing filters based on the eigenvectors, and setting up the layer to not be updated during training.', relevant_paper_text='Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.', scoring_feasibility=7, llm_tractability=0.5, expert_tractability=0.8, layman_tractability=0.1), TaskCandidate(name='Implement multi-crop test-time augmentation', description='Implement the multi-crop test-time augmentation strategy described in the paper. This involves running the trained network on six augmented views of each test image: the original, translated versions, and mirrored versions of each. Predictions should be made using a weighted average of all six outputs.', relevant_paper_text='To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.', scoring_feasibility=8, llm_tractability=0.6, expert_tractability=0.9, layman_tractability=0.2)]\n\nRequirements:\nExtract all promising task candidates from paper. Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\n\n    A good task candidate:\n    - describes a task that an agent can implement\n    - typically a programming task, e.g. modifying a 'baseline' implementation\n    - which reflects or mirrors actual tasks undertaken to produce the research paper\n    - and requires research and/or engineering expertise to implement correctly\n    - for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\n    - ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\n    - the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\n    \n\nAttempt Num: 1\n\nPrevious Critiques:\nSelf Critique:\n\nI'll evaluate the task candidates I extracted based on the given requirements:\n\n1. Implement alternating flip data augmentation:\nStrengths:\n- Describes a concrete programming task to modify a baseline implementation\n- Reflects an actual task undertaken in the research\n- Requires engineering expertise to implement correctly\n- Relatively straightforward to write an objective scoring function (e.g. check if images are flipped in the correct pattern across epochs)\n- Scoring function would be much easier to produce than implementing the task itself\n\nWeaknesses:\n- May not require significant research capabilities, mostly engineering\n\n2. Implement decoupled hyperparameters:\nStrengths:\n- Describes a programming task to modify hyperparameter handling\n- Reflects work done in the research\n- Requires some engineering expertise\n\nWeaknesses:\n- May be challenging to write an objective scoring function beyond checking if the hyperparameters are decoupled\n- Implementing the task may not be significantly harder than writing the scoring function\n- Does not require much research capability\n\n3. Implement patch-whitening initialization:\nStrengths:\n- Describes a concrete programming task\n- Reflects actual work from the research\n- Requires both research understanding and engineering skill\n- Possible to write an objective scoring function (e.g. check if filters are initialized correctly)\n- Scoring function would be easier than implementation\n\nWeaknesses:\n- May be very challenging for an agent without strong math/ML background\n\nOverall, the alternating flip and patch-whitening tasks seem to best meet the criteria. The decoupled hyperparameters task is less suitable. I could improve by extracting more task candidates from other parts of the paper, especially those requiring more research capabilities.\n\nSelf Critique:"
  },
  {
    "Response": "Upon reflection, I believe my self-critique was generally on the right track, but there are some areas where I can improve:\n\n1. Task extraction:\nStrength: I correctly identified that I should have extracted more task candidates from the paper. This shows good awareness of the requirement to extract all promising candidates.\n\nWeakness: I only extracted 3 tasks initially, which is insufficient given the rich content of the paper. I missed several important tasks like implementing the network architecture, the Lookahead optimization, and the multi-crop test-time augmentation.\n\n2. Evaluation of tasks:\nStrength: I provided a balanced assessment of each task, considering both strengths and weaknesses against the given criteria.\n\nWeakness: I didn't explicitly evaluate all criteria for each task. For example, I didn't consistently address the \"layman tractability\" aspect.\n\n3. Focus on research capabilities:\nStrength: I recognized that some tasks (like alternating flip) may not require significant research capabilities, which aligns with the goal of evaluating research/engineering capabilities.\n\nWeakness: I didn't put enough emphasis on tasks that would truly test research capabilities of a potentially stronger agent. The patch-whitening initialization task is good for this, but I should have identified and highlighted more such tasks.\n\n4. Scoring function consideration:\nStrength: I consistently considered the ease of creating a scoring function versus implementing the task for each candidate.\n\nWeakness: I didn't provide concrete examples of how these scoring functions might work, which could have strengthened my evaluation.\n\n5. Alignment with paper implementation:\nStrength: I correctly identified that the tasks reflect actual work done in the research.\n\nWeakness: I didn't explicitly tie the tasks to specific sections or results in the paper, which could have better demonstrated their relevance.\n\nTo improve, I should:\n1. Extract more task candidates, ensuring I cover all major components of the paper.\n2. Evaluate each task against all given criteria systematically.\n3. Focus more on identifying tasks that require strong research capabilities.\n4. Provide more concrete examples of potential scoring functions.\n5. Explicitly link tasks to specific sections or results in the paper.\n\nShould Retry: Based on these reflections, I believe I should retry the task extraction and evaluation process to provide a more comprehensive and accurate set of task candidates. Therefore, my answer is true."
  },
  {
    "Request": "Given the fields `candidate_input`, `candidate_output`, `requirements`, `attempt_num`, `previous_critiques`, produce the fields `self_critique`, `should_retry`.\n\n---\n\nFollow the following format.\n\nCandidate Input: ${candidate_input}\n\nCandidate Output: ${candidate_output}\n\nRequirements: ${requirements}\n\nAttempt Num: ${attempt_num}\n\nPrevious Critiques: ${previous_critiques}\n\nPast Error in Should Retry: An error to avoid in the future\n\nSelf Critique: Self-critique of the task candidate\n\nShould Retry: Should the task candidate be retried? This may be expensive. True/False (Respond with true or false)\n\n---\n\nCandidate Input:\n94% on CIFAR-10 in 3.29 Seconds on a Single GPU\nKeller Jordan\nkjordan4077@gmail.com\nAbstract\nCIFAR-10 is among the most widely used datasets in machine learning, facilitating\nthousands of research projects per year. To accelerate research and reduce the cost\nof experiments, we introduce training methods for CIFAR-10 which reach 94%\naccuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when\nrun on a single NVIDIA A100 GPU. As one factor contributing to these training\nspeeds, we propose a derandomized variant of horizontal flipping augmentation,\nwhich we show improves over the standard method in every case where flipping is\nbeneficial over no flipping at all. Our code is released at https://github.com/\nKellerJordan/cifar10-airbench .\n1 Introduction\nCIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-\ning thousands of research projects per year1. Research can be accelerated and the cost of experiments\nreduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In\nthis paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single\nNVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,\n2023). To support scenarios where higher performance is needed, we additionally develop methods\ntargeting 95% and 96% accuracy. We release the following methods in total.\n1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).\n2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).\n3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).\n4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).\nAll runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-\nmatically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the\nfirst uses torch.compile to improve GPU utilization. It is intended for experiments where many\nnetworks are trained at once in order to amortize the one-time compilation cost. The non-compiled\nairbench94 variant can be easily installed and run using the following command.\n1pip install airbench\n2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"\nOne motivation for the development of these training methods is that they can accelerate the experi-\nmental iteration time of researchers working on compatible projects involving CIFAR-10. Another\nmotivation is that they can decrease the cost of projects involving a massive number of trained\nnetworks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used\n3 million trained networks to demonstrate that the outputs of a trained neural network on a given test\ninput follow an approximately linear function of the vector of binary choices of which examples the\nmodel was trained on. Another example is Jordan (2023), a study on training variance which used\n180 thousand trained networks to show that standard trainings have little variance in performance on\n1https://paperswithcode.com/datasets\nPreprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024\nFigure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal\nflipping augmentation, which flips each image with 50% probability per epoch. This results in some images\nbeing redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a\ndeterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\ntheir test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds\nand 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make\nit possible to replicate these studies, or conduct similar ones, with fewer computational resources.\nFast training also enables the rapid accumulation of statistical significance for subtle hyperparameter\ncomparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10\naccuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation\nbetween runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the\nimprovement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training\nthis will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.\nOur work builds on prior training speed projects. We utilize a modified version of the network,\ninitialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen\npatch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over\nprior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-\nure 1, Section 3.6, Section 5.2).\n2 Background\nOur objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set\nin the shortest possible amount of time. Timing begins when the method is first given access to\ntraining data, and ends when it produces test-set predictions. The method is considered valid if its\nmean accuracy over repeated runs is at least 94%.\nWe chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the\n2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more\nrecent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10\nseconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page\n(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is\ntysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,\n94% is the level of human accuracy reported by Karpathy (2011).\nWe note the following consequences of how the method is timed. First, it is permitted for the program\nto begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins\nwhen the training data is first accessed. This is helpful because otherwise the first run of training\nis typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA\nimproves the performance of a trained network by running it on multiple augmented views of each\ntest input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping\nTTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods\nattain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\nThe CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.\nIt is divided into a training set of 50,000 images and a validation set of 10,000 images. As a\nmatter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was\n2\n80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than\nairbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94\ncan be attributed entirely to algorithmic progress rather than compute scaling.\n3 Methods\n3.1 Network architecture and baseline training\nWe train a convolutional network with a total of 1.97 million parameters, following tysam-code\n(2023) with a few small changes. It contains seven convolutions with the latter six being divided into\nthree blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this\nsection we offer some comments on the main design choices.\nThe network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed\nentirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,\n2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with\nno padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather\nthan the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff\nbetween throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\nFollowing Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,\nand disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is\nscaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture\ndiffers only in that we decrease the number of output channels in the third block from 512 to 256,\nand we add learnable biases to the first convolution.\nAs our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of\n0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the\nmaximum at 20% of the way through training, and then decreases to zero. For data augmentation we\nuse random horizontal flipping alongside 2-pixel random translation. For translation we use reflection\npadding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note\nthat what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a\nrandom 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the\nnetwork is run on both a given test image and its mirror and inferences are made based on the average\nof the two outputs. With optimized choices of learning rate, momentum, and weight decay, this\nbaseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.\n3.2 Frozen patch-whitening initialization\nFollowing Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-\nwhitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code\n(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches\nacross the training distribution, so that their outputs have identity covariance matrix. The second 12\nFigure 2: The first layer\u2019s weights after whitening\ninitialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,\nso that input information is preserved through the\nactivation which follows. Figure 2 shows the result.\nWe do not update this layer\u2019s weights during training.\nDeparting from tysam-code (2023), we add learn-\nable biases to this layer, yielding a small performance\nboost. The biases are trained for 3 epochs, after which\nwe disable their gradient to increase backward-pass\nthroughput, which improves training speed without\nreducing accuracy. We also obtain a slight perfor-\nmance boost relative to tysam-code (2023) by re-\nducing the constant added to the eigenvalues during\ncalculation of the patch-whitening initialization for\nthe purpose of preventing numerical issues in the case\nof a singular patch-covariance matrix.\nPatch-whitening initialization is the single most impactful feature. Adding it to the baseline more\nthan doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.\n3\nRandom reshuffling Alternating flip Mean accuracy\nNo No 93.40%\nNo Yes 93.48%\nYes No 93.92%\nYes Yes 94.01%\nTable 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating\nflip (which we propose) reduce training data redundancy and improve performance.\n3.3 Identity initialization\ndirac : We initialize all convolutions after the first as partial identity transforms. That is, for a\nconvolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an\nidentity transform of the input, and leave the remaining N\u2212Mto their default initialization. In\nPyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight\nwof each convolutional layer. This method partially follows tysam-code (2023), which used a more\ncomplicated scheme where the identity weights are mixed in with the original initialization, which\nwe did not find to be more performant. With this feature added, training attains 94% accuracy in 18\nepochs taking 6.8 A100-seconds.\n3.4 Optimization tricks\nscalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor\nof64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%\nin 13.5 epochs taking 5.1 A100-seconds.\nlookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.\nWe note that Lookahead has also been found effective in prior work on training speed for ResNet-\n18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6\nA100-seconds.\n3.5 Multi-crop evaluation\nmulticrop : To generate predictions, we run the trained network on six augmented views of each\ntest image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a\nversion which is translated down-and-to-the-right by one pixel, and the mirrored versions of all\nthree. Predictions are made using a weighted average of all six outputs, where the two views of the\nuntranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125\neach. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.\nWe note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-\nmonyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of\nevaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using\nmore crops does improve performance, but the increase to inference time outweighs the potential\ntraining speedup.\n3.6 Alternating flip\nTo speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,\nwhich we motivate as follows. When training neural networks, it is standard practice to organize\ntraining into a set of epochs during which every training example is seen exactly once. This\ndiffers from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),\nwhich calls for data to be repeatedly sampled with-replacement from the training set, resulting in\nexamples being potentially seen multiple redundant times within a short window of training. The\nuse of randomly ordered epochs of data for training has a different name, being called the random\nreshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our\ntraining dataset consists of Nunique examples, then sampling data with replacement causes every\n\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N\nunique examples on average. On the other hand, random reshuffling leads to all Nunique examples\nbeing seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason\nthat it is beneficial to maximize the number of unique inputs seen per window of training time.\n4\nWe extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We\nfirst note that standard random horizontal flipping augmentation can be defined as follows.\n1import torch\n2def random_flip ( inputs ):\n3 # Applies random flipping to a batch of images\n4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)\n5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]\nListing 1: Random flip\nIf horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2\nwhich may be seen during training. Potentially, every pair of consecutive epochs could contain every\nunique input. But our main observation is that with standard random horizontal flipping, half of the\nimages will be redundantly flipped the same way during both epochs, so that on average only 1.5N\nunique inputs will be seen.\naltflip : To address this, we propose to modify standard random horizontal flipping augmentation as\nfollows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},\nwe flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we\nflip only those inputs which were flipped in the first epoch. We provide the following implementation\nwhich avoids the need for extra memory by using a pseudorandom function to decide the flips.\n1import torch\n2import hashlib\n3def hash_fn (n, seed =42) :\n4 k = n * seed\n5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,\n16)\n6def alternating_flip (inputs , indices , epoch ):\n7 # Applies alternating flipping to a batch of images\n8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist\n() ])\n9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)\n10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\nListing 2: Alternating flip\nThe result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in\nFigure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.\nAdding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final\ntraining method airbench94.py , the entire contents of which can be found in Section E. It reaches\n94% accuracy in 3.83 seconds on an NVIDIA A100.\n3.7 Compilation\nThe final step we take to speed up training is a non-algorithmic one: we compile our training\nmethod using torch.compile in order to more efficiently utilize the GPU. This results in a training\nscript which is mathematically equivalent (up to small differences in floating point arithmetic) to\nthe non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29\nA100-seconds. The downside is that the one-time compilation process takes up to several minutes to\ncomplete before training runs can begin, so that it is only beneficial when we plan to execute many\nruns of training at once. We release this version as airbench94_compiled.py .\n4 95% and 96% targets\nTo address scenarios where somewhat higher performance is desired, we additionally develop methods\ntargeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .\nTo attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel\ncount of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce\nthe learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%\naccuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.\n2Assuming none of the training inputs are already mirrors of each other.\n5\nFigure 4: Training speedups accumulate additively. Removing individual features from airbench94\nincreases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces\nthe epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-\n94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.\nFigure 3: FLOPs vs. error rate tradeoff. Our\nthree training methods apparently follow a linear\nlog-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-\nVries & Taylor, 2017) augmentation and raise the\ntraining epochs to 40. We add a third convolution to\neach block, and scale the first block to 128 channels\nand the second two to 512. We also add a residual\nconnection across the later two convolutions of each\nblock, which we find is still beneficial despite the fact\nthat we are already using identity initialization (Sec-\ntion 3.3) to ease gradient flow. Finally, we reduce\nthe learning rate by a factor of 0.78. These changes\nyield airbench96 which attains 96.05% accuracy in\n46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.\nFigure 3 shows the FLOPs and error rate of each of\nour three training methods.\n5 Experiments\n5.1 Interaction between features\nTo gain a better sense of the impact of each feature on training speed, we compare two quantities.\nFirst, we measure the number of epochs that can be saved by adding the feature to the whitened\nbaseline (Section 3.2). Second, we measure the number of epochs that must be added when the\nfeature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-\ntion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it\nfrom the final airbench94 increases epochs-to-94% from 9.9 to 12.8.\nFigure 4 shows both quantities for each feature. Surprisingly, we find that for all features except\nmulti-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even\nthough the whitened baseline requires more than twice as many epochs as the final configuration.\nThis indicates that the interaction between most features is additive rather than multiplicative.\n5.2 Does alternating flip generalize?\nIn this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of\ntraining configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all\ncases except those where neither alternating nor random flip improve over using no flipping at all.\nFor CIFAR-10 we consider the performance boost given by alternating flip across the follow-\ning 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and\nairbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For\n6\nFigure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,\nthe use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%\ntraining speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.\n95% confidence intervals are shown around each point.\neach configuration we compare the performance of alternating and random flip in terms of their mean\naccuracy across n= 400 runs of training.\nFigure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip\nimproves performance in every setting. To get a sense for how big the improvement is, we estimate\nthe effective speedup for each case, i.e., the fraction of epochs that could be saved by switching\nfrom random to alternating flip while maintaining the level of accuracy of random flip. We begin\nby fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of\neach random flip-based training configuration. We use these curves to calculate the effective speedup\nafforded by switching from random to alternating flip. For example, airbench94 with random flip\nand without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The\nsame configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law\nfit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.\nNote that using a power-law yields a more conservative estimate relative to using linear interpolation\nbetween the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.\nTable 2 shows the result. We observe the following patterns. First, the addition of extra augmentation\n(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that\nthe main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the\nsame way for many epochs in a row; we speculate that adding extra augmentation reduces the negative\nimpact of these cases because it increases data diversity. Next, TTA reduces the gap between random\nand alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),\nindicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training\nfor longer consistently increases the effective speedup given by alternating flip.\nWe next study ImageNet trainings with the following experiment. We train a ResNet-18 with a\nvariety of train and test crops, comparing three flipping options: alternating flip, random flip, and no\nflipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192\ncenter crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)\nto denote the latter. We also consider two training crops: 192x192 inception-style random resized\ncrop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area\nranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the\nshorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write\n7\nBaseline Cutout Epochs Speedup Speedup (w/ TTA)\nairbench94 No 10 15.0% 5.30%\nairbench94 No 20 27.1% 21.3%\nairbench94 No 40 38.3% 36.4%\nairbench94 No 80 102% 31.8%\nairbench94 Yes 10 3.84% 1.13%\nairbench94 Yes 20 7.42% 2.00%\nairbench94 Yes 40 18.6% 9.28%\nairbench94 Yes 80 29.2% 14.25%\nairbench96 Yes 10 4.94% 1.11%\nairbench96 Yes 20 8.99% 3.58%\nairbench96 Yes 40 17.2% 6.48%\nairbench96 Yes 80 18.8% Not measured\nTable 2: Effective speedups given by switching from random flip to alternating flip. The two configurations\nmost closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw\naccuracy values of the airbench94 experiments.\nFlipping augmentation option\nTrain crop Test crop Epochs TTA None Random Alternating\nHeavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28\nHeavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28\nLight RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26\nLight RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26\nHeavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28\nHeavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28\nLight RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26\nLight RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26\nLight RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16\nHeavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4\nTable 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for\nthose trainings where random flip improves significantly over not flipping at all. The single best flipping option\nin each row is bolded when the difference is statistically significant.\nHeavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided\nin Section C.\nTable 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC\nis better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly\nbetter when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test\nresolution discrepancy (Touvron et al., 2019).\nFor trainings which use Light RRC, we find that switching from random flip to alternating flip\nprovides a substantial boost to performance, amounting to a training speedup of more than 25%.\nIn Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to\nalternating flip improves performance by more than increasing the training duration from 16 to 20\nepochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our\nresults on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit\nfrom alternating flip. Indeed, even turning flipping off completely does not significantly reduce the\nperformance of these trainings. We conclude that alternating flip improves over random flip for every\ntraining scenario where the latter improves over no flipping at all.\n8\nEpochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE\n1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312\n2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233\n1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269\n1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533\n2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433\n1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444\nTable 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).\n5.3 Variance and class-wise calibration\nPrevious sections have focused on understanding what factors affect the first moment of accuracy\n(the mean). In this section we investigate the second moment, finding that TTA reduces variance at\nthe cost of calibration.\nOur experiment is to execute 10,000 runs of airbench94 training with several hyperparameter\nsettings. For each setting we report both the variance in test-set accuracy as well as an estimate of the\ndistribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.\nTable 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set\nvariance, replicating the main finding of Jordan (2023). This is a surprising result because these\ntrainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7\nas much distribution-wise variance when run for a similar duration, and reached a low variance only\nwhen run for 64 epochs. We conclude from this comparison that distribution-wise variance is more\nstrongly connected to the rate of convergence of a training rather than its duration as such. We also\nnote that the low distribution-wise variance of airbench94 indicates it has high training stability.\nUsing TTA significantly reduces the test-set variance, such that all three settings with TTA have lower\ntest-set variance than any setting without TTA. However, test-set variance is implied by the class-wise\ncalibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this\nreduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,\nwe compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which\nmeasures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA\nhas a higher CACE than every setting without TTA, confirming the hypothesis.\n6 Discussion\nIn this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7\nfaster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the\nairbench Python package.\nWe developed airbench solely with the goal of maximizing training speed on CIFAR-10. In\nSection B we find that it also generalizes well to other tasks. For example, without any extra tuning,\nairbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.\nOne factor contributing to the training speed of airbench was our finding that training can be accel-\nerated by partially derandomizing the standard random horizontal flipping augmentation, resulting in\nthe variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating\nflip improves the performance of every training we considered (Section 5.2), with the exception of\nthose trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to\nus, the standard ImageNet trainings that we considered do not significantly benefit from horizontal\nflipping. Future work might investigate whether it is possible to obtain derandomized improvements\nto other augmentations besides horizontal flip.\nThe methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,\nwith fixed performance and hardware constraints. These constraints mean that we cannot improve\nperformance by simply scaling up the amount of computational resources used; instead we are forced\nto develop new methods like the alternating flip. We look forward to seeing what other new methods\nfuture work discovers to push training speed further.\n9\nReferences\nDimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.\nDan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-\nperformance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,\n2011.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\nmark and competition. 2017.\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet\nor cifar-10. arXiv preprint arXiv:1810.03505 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\npp. 248\u2013255. IEEE, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552 , 2017.\nMert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic\ngradient descent. Mathematical Programming , 186:49\u201384, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 , 2016.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.\npmlr, 2015.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\nsgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.\nKeller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and\ninevitable. arXiv preprint arXiv:2304.01910 , 2023.\nAndrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.\nio/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for\nadvanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT\nLicense.\nGuillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander\nM \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,\nBenjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.\nBenchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural\nInformation Processing Systems , 35:25404\u201325421, 2022.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.\nDavid Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/\nhow-to-train-your-resnet-8-bag-of-tricks/ .\n10\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics , 22(3):400\u2013407, 1951.\nSantiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:\n//github.com/apple/ml-cifar-10-faster , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.\ntysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/\nhlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. Advances in neural information processing systems , 32, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,\n2016.\nMichael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps\nforward, 1 step back. Advances in neural information processing systems , 32, 2019.\n11\nA Network architecture\n1from torch import nn\n2\n3class Flatten (nn. Module ):\n4 def forward (self , x):\n5 return x. view (x. size (0) , -1)\n6\n7class Mul (nn. Module ):\n8 def __init__ (self , scale ):\n9 super (). __init__ ()\n10 self . scale = scale\n11 def forward (self , x):\n12 return x * self . scale\n13\n14def conv (ch_in , ch_out ):\n15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,\n16 padding =\u2019same \u2019, bias = False )\n17\n18def make_net ():\n19 act = lambda : nn. GELU ()\n20 bn = lambda ch: nn. BatchNorm2d (ch)\n21 return nn. Sequential (\n22 nn. Sequential (\n23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),\n24 act () ,\n25 ),\n26 nn. Sequential (\n27 conv (24 , 64) ,\n28 nn. MaxPool2d (2) ,\n29 bn (64) , act () ,\n30 conv (64 , 64) ,\n31 bn (64) , act () ,\n32 ),\n33 nn. Sequential (\n34 conv (64 , 256) ,\n35 nn. MaxPool2d (2) ,\n36 bn (256) , act () ,\n37 conv (256 , 256) ,\n38 bn (256) , act () ,\n39 ),\n40 nn. Sequential (\n41 conv (256 , 256) ,\n42 nn. MaxPool2d (2) ,\n43 bn (256) , act () ,\n44 conv (256 , 256) ,\n45 bn (256) , act () ,\n46 ),\n47 nn. MaxPool2d (3) ,\n48 Flatten () ,\n49 nn. Linear (256 , 10, bias = False ),\n50 Mul (1/9) ,\n51 )\nListing 3: Minimal PyTorch code for the network architecture used by airbench94 .\nWe note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock\ntime. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel\nsize of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this\ndoes not improve the wallclock training time on an A100. Reducing the batch size is another easy\nway to save FLOPs but not wallclock time.\n12\nDataset Flipping? Cutout? ResNet-18 airbench96\nCIFAR-10 Yes No 95.55% 95.61%\nCIFAR-10 Yes Yes 96.01% 96.05%\nCIFAR-100 Yes No 77.54% 79.27%\nCIFAR-100 Yes Yes 78.04% 79.76%\nCINIC-10 Yes No 87.58% 87.78%\nCINIC-10 Yes Yes not measured 88.22%\nSVHN No No 97.35% 97.38%\nSVHN No Yes not measured 97.64%\nTable 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We\ndirectly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off\nflipping for SVHN).\nB Extra dataset experiments\nWe developed airbench with the singular goal of maximizing training speed on CIFAR-10. To\nfind out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its\nperformance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-\n10 (Darlow et al., 2018).\nOn CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both\nthe case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not\n(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than\nResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say\nthat it generalizes.\nWe compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.\nWe do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal\non CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better\nperformance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where\nairbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is\nused and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows\nstrong generalization to other tasks.\nWe note that this comparison between airbench96 and ResNet-18 training is fair in the sense that\nit does demonstrate that the former has good generalization, but unfair in the sense that it does not\nindicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time\naugmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training\nwould outperform airbench96 if it were run using test-time augmentation. However, it also takes\n5-10 times longer to complete. The decision of which to use may be situational.\nThe accuracy values we report for ResNet-18 training are from the following sources. We tried\nto select the highest values we could find for each setting. Moreau et al. (2022) reports attaining\n95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports\nattaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on\nCIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.\nC ImageNet training details\nOur ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/\nlibffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and\nmomentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution\n160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of\ntraining. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.\n13\nHyperparameters Flipping augmentation option\nEpochs Cutout TTA None Random Alternating\n10 No No 92.3053 93.0988 93.2798\n20 No No 92.8166 93.7446 93.8652\n40 No No 93.0143 94.0133 94.0729\n80 No No 93.0612 94.1169 94.1628\n10 No Yes 93.4071 93.9488 94.0186\n20 No Yes 93.8528 94.5565 94.6530\n40 No Yes 94.0381 94.7803 94.8203\n80 No Yes 94.0638 94.8506 94.8676\n10 Yes No 91.8487 92.0402 92.1374\n20 Yes No 92.8474 93.3825 93.4876\n40 Yes No 93.2675 94.1014 94.1952\n80 Yes No 93.4193 94.4311 94.5204\n10 Yes Yes 92.6455 92.7780 92.8103\n20 Yes Yes 93.7862 94.1306 94.1670\n40 Yes Yes 94.3090 94.8511 94.8960\n80 Yes Yes 94.5253 95.1839 95.2362\nTable 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a\nmean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise\ndifference in means is statistically significant.\nD Extra tables & figures\nFigure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in\nSection 5.3.\nE Complete training code\n1\"\"\"\n2airbench94 .py\n33.83 s runtime on an A100 ; 0.36 PFLOPs .\n4Evidence for validity : 94.01 average accuracy in n =1000 runs .\n5\n6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB\nwith the following nvidia -smi:\n7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :\n11.7\n14\n8torch . __version__ == \u20192.1.2+ cu118 \u2019\n9\"\"\"\n10\n11# ############################################\n12# Setup / Hyperparameters #\n13# ############################################\n14\n15import os\n16import sys\n17import uuid\n18from math import ceil\n19\n20import torch\n21from torch import nn\n22import torch .nn. functional as F\n23import torchvision\n24import torchvision . transforms as T\n25\n26torch . backends . cudnn . benchmark = True\n27\n28\"\"\"\n29We express the main training hyperparameters ( batch size , learning\nrate , momentum , and weight decay ) in decoupled form , so that each\none can be tuned independently . This accomplishes the following :\n30* Assuming time - constant gradients , the average step size is decoupled\nfrom everything but the lr.\n31* The size of the weight decay update is decoupled from everything but\nthe wd.\n32In constrast , normally when we increase the ( Nesterov ) momentum , this\nalso scales up the step size proportionally to 1 + 1 / (1 -\nmomentum ), meaning we cannot change momentum without having to re -\ntune the learning rate . Similarly , normally when we increase the\nlearning rate this also increases the size of the weight decay ,\nrequiring a proportional decrease in the wd to maintain the same\ndecay strength .\n33\n34The practical impact is that hyperparameter tuning is faster , since\nthis parametrization allows each one to be tuned independently .\nSee https :// myrtle .ai/ learn /how -to -train -your - resnet -5-\nhyperparameters /.\n35\"\"\"\n36\n37hyp = {\n38 \u2019opt \u2019: {\n39 \u2019 train_epochs \u2019: 9.9 ,\n40 \u2019batch_size \u2019: 1024 ,\n41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples\n42 \u2019momentum \u2019: 0.85 ,\n43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (\ndecoupled from learning rate )\n44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not\nweight decay ) for BatchNorm biases\n45 \u2019 label_smoothing \u2019: 0.2 ,\n46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the\nwhitening layer bias before freezing\n47 },\n48 \u2019aug \u2019: {\n49 \u2019flip \u2019: True ,\n50 \u2019translate \u2019: 2,\n51 },\n52 \u2019net \u2019: {\n53 \u2019widths \u2019: {\n54 \u2019block1 \u2019: 64,\n55 \u2019block2 \u2019: 256 ,\n56 \u2019block3 \u2019: 256 ,\n15\n57 },\n58 \u2019 batchnorm_momentum \u2019: 0.6 ,\n59 \u2019 scaling_factor \u2019: 1/9 ,\n60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :\n0= none , 1= mirror , 2= mirror + translate\n61 },\n62}\n63\n64# ############################################\n65# DataLoader #\n66# ############################################\n67\n68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )\n69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )\n70\n71def batch_flip_lr ( inputs ):\n72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .\nview (-1, 1, 1, 1)\n73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\n74\n75def batch_crop (images , crop_size ):\n76 r = ( images . size ( -1) - crop_size )//2\n77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =\nimages . device )\n78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),\ndevice = images .device , dtype = images . dtype )\n79 # The two cropping methods in this if - else produce equivalent\nresults , but the second is faster for r > 2.\n80 if r <= 2:\n81 for sy in range (-r, r +1):\n82 for sx in range (-r, r +1):\n83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)\n84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size\n, r+sx:r+sx+ crop_size ]\n85 else :\n86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size\n+2* r), device = images .device , dtype = images . dtype )\n87 for s in range (-r, r +1):\n88 mask = ( shifts [:, 0] == s)\n89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]\n90 for s in range (-r, r +1):\n91 mask = ( shifts [:, 1] == s)\n92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+\ncrop_size ]\n93 return images_out\n94\n95class CifarLoader :\n96 \"\"\"\n97 GPU - accelerated dataloader for CIFAR -10 which implements\nalternating flip augmentation .\n98 \"\"\"\n99\n100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,\ndrop_last =None , shuffle =None , gpu =0):\n101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .\npt \u2019)\n102 if not os. path . exists ( data_path ):\n103 dset = torchvision . datasets . CIFAR10 (path , download =True ,\ntrain = train )\n104 images = torch . tensor ( dset . data )\n105 labels = torch . tensor ( dset . targets )\n106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:\ndset . classes }, data_path )\n107\n108 data = torch . load ( data_path , map_location = torch . device (gpu))\n16\n109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [\n\u2019labels \u2019], data [\u2019classes \u2019]\n110 # It \u2019s faster to load + process uint8 data than to load\npreprocessed fp16 data\n111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).\nto( memory_format = torch . channels_last )\n112\n113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )\n114 self . proc_images = {} # Saved results of image processing to\nbe done on the first epoch\n115 self . epoch = 0\n116\n117 self . aug = aug or {}\n118 for k in self .aug . keys ():\n119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019\n% k\n120\n121 self . batch_size = batch_size\n122 self . drop_last = train if drop_last is None else drop_last\n123 self . shuffle = train if shuffle is None else shuffle\n124\n125 def __len__ ( self ):\n126 return len( self . images )// self . batch_size if self . drop_last\nelse ceil (len( self . images )/ self . batch_size )\n127\n128 def __iter__ ( self ):\n129\n130 if self . epoch == 0:\n131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .\nimages )\n132 # Randomly flip all images on the first epoch as according\nto definition of alternating flip\n133 if self .aug.get(\u2019flip \u2019, False ):\n134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (\nimages )\n135 # Pre -pad images to save time when doing random\ntranslation\n136 pad = self .aug.get (\u2019translate \u2019, 0)\n137 if pad > 0:\n138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019\nreflect \u2019)\n139\n140 if self .aug.get(\u2019translate \u2019, 0) > 0:\n141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .\nshape [ -2])\n142 elif self .aug.get (\u2019flip \u2019, False ):\n143 images = self . proc_images [\u2019flip \u2019]\n144 else :\n145 images = self . proc_images [\u2019norm \u2019]\n146 if self .aug.get(\u2019flip \u2019, False ):\n147 if self . epoch % 2 == 1:\n148 images = images . flip ( -1)\n149\n150 self . epoch += 1\n151\n152 indices = ( torch . randperm if self . shuffle else torch . arange )(\nlen ( images ), device = images . device )\n153 for i in range (len ( self )):\n154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]\n155 yield ( images [ idxs ], self . labels [ idxs ])\n156\n157# ############################################\n158# Network Components #\n159# ############################################\n160\n17\n161class Flatten (nn. Module ):\n162 def forward (self , x):\n163 return x. view (x. size (0) , -1)\n164\n165class Mul (nn. Module ):\n166 def __init__ (self , scale ):\n167 super (). __init__ ()\n168 self . scale = scale\n169 def forward (self , x):\n170 return x * self . scale\n171\n172class BatchNorm (nn. BatchNorm2d ):\n173 def __init__ (self , num_features , momentum , eps =1e -12 ,\n174 weight =False , bias = True ):\n175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )\n176 self . weight . requires_grad = weight\n177 self . bias . requires_grad = bias\n178 # Note that PyTorch already initializes the weights to one and\nbiases to zero\n179\n180class Conv (nn. Conv2d ):\n181 def __init__ (self , in_channels , out_channels , kernel_size =3,\npadding =\u2019same \u2019, bias = False ):\n182 super (). __init__ ( in_channels , out_channels , kernel_size =\nkernel_size , padding = padding , bias = bias )\n183\n184 def reset_parameters ( self ):\n185 super (). reset_parameters ()\n186 if self . bias is not None :\n187 self . bias . data . zero_ ()\n188 w = self . weight . data\n189 torch .nn. init . dirac_ (w[:w. size (1) ])\n190\n191class ConvGroup (nn. Module ):\n192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):\n193 super (). __init__ ()\n194 self . conv1 = Conv ( channels_in , channels_out )\n195 self . pool = nn. MaxPool2d (2)\n196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )\n197 self . conv2 = Conv ( channels_out , channels_out )\n198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )\n199 self . activ = nn. GELU ()\n200\n201 def forward (self , x):\n202 x = self . conv1 (x)\n203 x = self . pool (x)\n204 x = self . norm1 (x)\n205 x = self . activ (x)\n206 x = self . conv2 (x)\n207 x = self . norm2 (x)\n208 x = self . activ (x)\n209 return x\n210\n211# ############################################\n212# Network Definition #\n213# ############################################\n214\n215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019\n][ \u2019 batchnorm_momentum \u2019]):\n216 whiten_kernel_size = 2\n217 whiten_width = 2 * 3 * whiten_kernel_size **2\n218 net = nn. Sequential (\n219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True\n),\n220 nn. GELU () ,\n18\n221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],\nbatchnorm_momentum ),\n222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],\nbatchnorm_momentum ),\n223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],\nbatchnorm_momentum ),\n224 nn. MaxPool2d (3) ,\n225 Flatten () ,\n226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),\n227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,\n228 )\n229 net [0]. weight . requires_grad = False\n230 net = net. half (). cuda ()\n231 net = net.to( memory_format = torch . channels_last )\n232 for mod in net. modules ():\n233 if isinstance (mod , BatchNorm ):\n234 mod . float ()\n235 return net\n236\n237# ############################################\n238# Whitening Conv Initialization #\n239# ############################################\n240\n241def get_patches (x, patch_shape ):\n242 c, (h, w) = x. shape [1] , patch_shape\n243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h\n,w). float ()\n244\n245def get_whitening_parameters ( patches ):\n246 n,c,h,w = patches . shape\n247 patches_flat = patches . view (n, -1)\n248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n\n249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance\n, UPLO =\u2019U\u2019)\n250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.\nreshape (c*h*w,c,h,w). flip (0)\n251\n252def init_whitening_conv (layer , train_set , eps =5e -4):\n253 patches = get_patches ( train_set , patch_shape = layer . weight . data .\nshape [2:])\n254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )\n255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)\n256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -\neigenvectors_scaled ))\n257\n258# ###########################################\n259# Lookahead #\n260# ###########################################\n261\n262class LookaheadState :\n263 def __init__ (self , net):\n264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().\nitems ()}\n265\n266 def update (self , net , decay ):\n267 for ema_param , net_param in zip( self . net_ema . values () , net.\nstate_dict (). values ()):\n268 if net_param . dtype in ( torch .half , torch . float ):\n269 ema_param . lerp_ ( net_param , 1- decay )\n270 net_param . copy_ ( ema_param )\n271\n272# ###########################################\n273# Logging #\n274# ###########################################\n275\n19\n276def print_columns ( columns_list , is_head =False , is_final_entry = False ):\n277 print_string = \u2019\u2019\n278 for col in columns_list :\n279 print_string += \u2019| %s \u2019 % col\n280 print_string += \u2019|\u2019\n281 if is_head :\n282 print (\u2019-\u2019*len( print_string ))\n283 print ( print_string )\n284 if is_head or is_final_entry :\n285 print (\u2019-\u2019*len( print_string ))\n286\n287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,\n\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]\n288def print_training_details ( variables , is_final_entry ):\n289 formatted = []\n290 for col in logging_columns_list :\n291 var = variables .get (col. strip () , None )\n292 if type (var) in (int , str):\n293 res = str(var)\n294 elif type (var) is float :\n295 res = \u2019{:0.4 f}\u2019. format ( var)\n296 else :\n297 assert var is None\n298 res = \u2019\u2019\n299 formatted . append (res. rjust (len (col)))\n300 print_columns ( formatted , is_final_entry = is_final_entry )\n301\n302# ###########################################\n303# Evaluation #\n304# ###########################################\n305\n306def infer (model , loader , tta_level =0) :\n307 \"\"\"\n308 Test - time augmentation strategy (for tta_level =2):\n309 1. Flip / mirror the image left -to - right (50% of the time ).\n310 2. Translate the image by one pixel either up -and - left or down -and\n- right (50% of the time , i.e. both happen 25% of the time ).\n311\n312 This creates 6 views per image ( left / right times the two\ntranslations and no - translation ), which we evaluate and then\nweight according to the given probabilities .\n313 \"\"\"\n314\n315 def infer_basic (inputs , net):\n316 return net( inputs ). clone ()\n317\n318 def infer_mirror (inputs , net):\n319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))\n320\n321 def infer_mirror_translate ( inputs , net ):\n322 logits = infer_mirror (inputs , net )\n323 pad = 1\n324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)\n325 inputs_translate_list = [\n326 padded_inputs [:, :, 0:32 , 0:32] ,\n327 padded_inputs [:, :, 2:34 , 2:34] ,\n328 ]\n329 logits_translate_list = [ infer_mirror ( inputs_translate , net)\n330 for inputs_translate in\ninputs_translate_list ]\n331 logits_translate = torch . stack ( logits_translate_list ). mean (0)\n332 return 0.5 * logits + 0.5 * logits_translate\n333\n334 model . eval ()\n335 test_images = loader . normalize ( loader . images )\n20\n336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][\ntta_level ]\n337 with torch . no_grad ():\n338 return torch .cat ([ infer_fn (inputs , model ) for inputs in\ntest_images . split (2000) ])\n339\n340def evaluate (model , loader , tta_level =0) :\n341 logits = infer (model , loader , tta_level )\n342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()\n343\n344# ###########################################\n345# Training #\n346# ###########################################\n347\n348def main (run):\n349\n350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]\n351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]\n352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]\n353 # Assuming gradients are constant in time , for Nesterov momentum ,\nthe below ratio is how much larger the default steps will be than\nthe underlying per - example gradients . We divide the learning rate\nby this ratio in order to ensure steps are the same scale as\ngradients , regardless of the choice of momentum .\n354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))\n355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning\nrate for PyTorch SGD\n356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale\n357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]\n358\n359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019\nlabel_smoothing \u2019], reduction =\u2019none \u2019)\n360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)\n361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =\nbatch_size , aug=hyp[\u2019aug \u2019])\n362 if run == \u2019warmup \u2019:\n363 # The only purpose of the first run is to warmup , so we can\nuse dummy data\n364 train_loader . labels = torch . randint (0, 10, size =( len (\ntrain_loader . labels ) ,), device = train_loader . labels . device )\n365 total_train_steps = ceil (len( train_loader ) * epochs )\n366\n367 model = make_net ()\n368 current_steps = 0\n369\n370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in\nk and p. requires_grad ]\n371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019\nnot in k and p. requires_grad ]\n372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,\nweight_decay =wd/ lr_biases ),\n373 dict ( params = other_params , lr=lr , weight_decay =wd/\nlr)]\n374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,\nnesterov = True )\n375\n376 def triangle (steps , start =0, end =0, peak =0.5) :\n377 xp = torch . tensor ([0 , int( peak * steps ), steps ])\n378 fp = torch . tensor ([ start , 1, end ])\n379 x = torch . arange (1+ steps )\n380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])\n381 b = fp [: -1] - (m * xp [: -1])\n382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1\n383 indices = torch . clamp ( indices , 0, len(m) - 1)\n384 return m[ indices ] * x + b[ indices ]\n21\n385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,\npeak =0.23)\n386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:\nlr_schedule [i])\n387\n388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /\ntotal_train_steps )**3\n389 lookahead_state = LookaheadState ( model )\n390\n391 # For accurately timing GPU code\n392 starter = torch . cuda . Event ( enable_timing = True )\n393 ender = torch . cuda . Event ( enable_timing = True )\n394 total_time_seconds = 0.0\n395\n396 # Initialize the first layer using statistics of training images\n397 starter . record ()\n398 train_images = train_loader . normalize ( train_loader . images [:5000])\n399 init_whitening_conv ( model [0] , train_images )\n400 ender . record ()\n401 torch . cuda . synchronize ()\n402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n403\n404 for epoch in range ( ceil ( epochs )):\n405\n406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019\nwhiten_bias_epochs \u2019])\n407\n408 # ###################\n409 # Training #\n410 # ###################\n411\n412 starter . record ()\n413\n414 model . train ()\n415 for inputs , labels in train_loader :\n416\n417 outputs = model ( inputs )\n418 loss = loss_fn ( outputs , labels ).sum ()\n419 optimizer . zero_grad ( set_to_none = True )\n420 loss . backward ()\n421 optimizer . step ()\n422 scheduler . step ()\n423\n424 current_steps += 1\n425\n426 if current_steps % 5 == 0:\n427 lookahead_state . update (model , decay = alpha_schedule [\ncurrent_steps ]. item ())\n428\n429 if current_steps >= total_train_steps :\n430 if lookahead_state is not None :\n431 lookahead_state . update (model , decay =1.0)\n432 break\n433\n434 ender . record ()\n435 torch . cuda . synchronize ()\n436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n437\n438 # ###################\n439 # Evaluation #\n440 # ###################\n441\n442 # Print the accuracy and loss from the last training batch of\nthe epoch\n22\n443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().\nmean (). item ()\n444 train_loss = loss . item () / batch_size\n445 val_acc = evaluate (model , test_loader , tta_level =0)\n446 print_training_details ( locals () , is_final_entry = False )\n447 run = None # Only print the run number once\n448\n449 # ###################\n450 # TTA Evaluation #\n451 # ###################\n452\n453 starter . record ()\n454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019\ntta_level \u2019])\n455 ender . record ()\n456 torch . cuda . synchronize ()\n457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n458\n459 epoch = \u2019eval \u2019\n460 print_training_details ( locals () , is_final_entry = True )\n461\n462 return tta_val_acc\n463\n464if __name__ == \" __main__ \":\n465 with open (sys. argv [0]) as f:\n466 code = f. read ()\n467\n468 print_columns ( logging_columns_list , is_head = True )\n469 main (\u2019warmup \u2019)\n470 accs = torch . tensor ([ main (run) for run in range (25) ])\n471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))\n472\n473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }\n474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))\n475 os. makedirs ( log_dir , exist_ok = True )\n476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)\n477 print (os. path . abspath ( log_path ))\n478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))\nListing 4: airbench94.py\n23\n\nCandidate Output: tasks=[TaskCandidate(name='Implement alternating flip data augmentation', description='Modify the data augmentation pipeline to implement the alternating flip method described in the paper. This involves flipping 50% of images randomly in the first epoch, then alternating which images are flipped in subsequent epochs according to the pattern described in Section 3.6.', relevant_paper_text='To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.', scoring_feasibility=8, llm_tractability=0.7, expert_tractability=0.9, layman_tractability=0.3), TaskCandidate(name='Implement patch-whitening initialization', description='Implement the patch-whitening initialization method for the first convolutional layer as described in the paper. This involves computing the covariance matrix of 2x2 patches across the training distribution, initializing filters based on the eigenvectors, and setting up the layer to not be updated during training.', relevant_paper_text='Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.', scoring_feasibility=7, llm_tractability=0.5, expert_tractability=0.8, layman_tractability=0.1), TaskCandidate(name='Implement multi-crop test-time augmentation', description='Implement the multi-crop test-time augmentation strategy described in the paper. This involves running the trained network on six augmented views of each test image: the original, translated versions, and mirrored versions of each. Predictions should be made using a weighted average of all six outputs.', relevant_paper_text='To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.', scoring_feasibility=8, llm_tractability=0.6, expert_tractability=0.9, layman_tractability=0.2)]\n\nRequirements:\nExtract all promising task candidates from paper. Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\n\n    A good task candidate:\n    - describes a task that an agent can implement\n    - typically a programming task, e.g. modifying a 'baseline' implementation\n    - which reflects or mirrors actual tasks undertaken to produce the research paper\n    - and requires research and/or engineering expertise to implement correctly\n    - for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\n    - ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\n    - the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\n    \n\nAttempt Num: 1\n\nPrevious Critiques:\nSelf Critique:\n\nI'll evaluate the task candidates I extracted based on the given requirements:\n\n1. Implement alternating flip data augmentation:\nStrengths:\n- Describes a concrete programming task to modify a baseline implementation\n- Reflects an actual task undertaken in the research\n- Requires engineering expertise to implement correctly\n- Relatively straightforward to write an objective scoring function (e.g. check if images are flipped in the correct pattern across epochs)\n- Scoring function would be much easier to produce than implementing the task itself\n\nWeaknesses:\n- May not require significant research capabilities, mostly engineering\n\n2. Implement decoupled hyperparameters:\nStrengths:\n- Describes a programming task to modify hyperparameter handling\n- Reflects work done in the research\n- Requires some engineering expertise\n\nWeaknesses:\n- May be challenging to write an objective scoring function beyond checking if the hyperparameters are decoupled\n- Implementing the task may not be significantly harder than writing the scoring function\n- Does not require much research capability\n\n3. Implement patch-whitening initialization:\nStrengths:\n- Describes a concrete programming task\n- Reflects actual work from the research\n- Requires both research understanding and engineering skill\n- Possible to write an objective scoring function (e.g. check if filters are initialized correctly)\n- Scoring function would be easier than implementation\n\nWeaknesses:\n- May be very challenging for an agent without strong math/ML background\n\nOverall, the alternating flip and patch-whitening tasks seem to best meet the criteria. The decoupled hyperparameters task is less suitable. I could improve by extracting more task candidates from other parts of the paper, especially those requiring more research capabilities.\n\nPast Error in Should Retry: ValueError('Respond with true or false')\n\nSelf Critique:"
  },
  {
    "Response": "Self Critique:\n\nUpon reviewing my task candidate extraction and evaluation, I can identify several areas for improvement:\n\n1. Quantity of task candidates: I only extracted three task candidates, which is likely insufficient to cover the full range of research and engineering tasks described in the paper. I should have extracted more candidates to provide a more comprehensive set of options.\n\n2. Research vs Engineering balance: The tasks I extracted lean more heavily towards engineering implementation rather than research. I should have sought out more tasks that require deeper understanding and application of the research concepts presented in the paper.\n\n3. Missed important tasks: I overlooked several significant tasks described in the paper, such as implementing the identity initialization for convolutions, the Lookahead optimization, and the overall network architecture. These are important components of the research that should have been included as task candidates.\n\n4. Depth of analysis: While I provided some evaluation of the tasks against the criteria, my analysis could have been more thorough and systematic, addressing each point in the requirements more explicitly for each task.\n\n5. Scoring function consideration: I didn't give enough attention to the requirement of having easily implementable scoring functions that are much simpler than the tasks themselves. This is a crucial aspect that I should have emphasized more in my evaluation.\n\n6. Relevance to paper's core contributions: I could have focused more on tasks that directly relate to the paper's main contributions and novel aspects, rather than including more general ML implementation tasks.\n\nTo improve, I should re-examine the paper more carefully, extract a wider range of task candidates that better balance research and engineering aspects, and provide a more thorough evaluation of each candidate against all the specified criteria.\n\nShould Retry: True"
  },
  {
    "Request": "Given the fields `paper_text`, `guidance`, produce the fields `task_candidates`.\n\n---\n\nFollow the following format.\n\nPaper Text: Full text from research paper PDF\nGuidance: Guidance for extracting task candidates\nTask Candidates: JSON list of task candidate dicts with keys 'name', 'description', and 'relevant_paper_text'. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCandidate\": {\"description\": \"Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\\n\\nA good task candidate:\\n- describes a task that an agent can implement\\n- typically a programming task, e.g. modifying a 'baseline' implementation\\n- which reflects or mirrors actual tasks undertaken to produce the research paper\\n- and requires research and/or engineering expertise to implement correctly\\n- for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\\n- ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\\n- the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\", \"properties\": {\"name\": {\"description\": \"Name of the task\", \"title\": \"Name\", \"type\": \"string\"}, \"description\": {\"description\": \"Description of the task\", \"title\": \"Description\", \"type\": \"string\"}, \"relevant_paper_text\": {\"description\": \"Text from the paper that is relevant to the task\", \"title\": \"Relevant Paper Text\", \"type\": \"string\"}, \"scoring_feasibility\": {\"description\": \"\\nOn a scale of 1-10, how feasible is it to write a python function to objectively score an \\nimplementation of this task, verifying that key functionality is implemented as specified?\\n\", \"title\": \"Scoring Feasibility\", \"type\": \"integer\"}, \"llm_tractability\": {\"description\": \"Probability in (0, 1) that a frontier LLM can generate a correct implementation of this task\", \"title\": \"Llm Tractability\", \"type\": \"number\"}, \"expert_tractability\": {\"description\": \"Probability in (0, 1) that a human subject matter expert can generate a correct implementation of this task\", \"title\": \"Expert Tractability\", \"type\": \"number\"}, \"layman_tractability\": {\"description\": \"Probability in (0, 1) that a layman can generate a correct implementation of this task\", \"title\": \"Layman Tractability\", \"type\": \"number\"}}, \"required\": [\"name\", \"description\", \"relevant_paper_text\", \"scoring_feasibility\", \"llm_tractability\", \"expert_tractability\", \"layman_tractability\"], \"title\": \"TaskCandidate\", \"type\": \"object\"}}, \"properties\": {\"tasks\": {\"description\": \"Tasks extracted from the paper\", \"items\": {\"$ref\": \"#/$defs/TaskCandidate\"}, \"title\": \"Tasks\", \"type\": \"array\"}}, \"required\": [\"tasks\"], \"title\": \"TaskCandidates\", \"type\": \"object\"}\n\n---\n\nPaper Text:\n94% on CIFAR-10 in 3.29 Seconds on a Single GPU\nKeller Jordan\nkjordan4077@gmail.com\nAbstract\nCIFAR-10 is among the most widely used datasets in machine learning, facilitating\nthousands of research projects per year. To accelerate research and reduce the cost\nof experiments, we introduce training methods for CIFAR-10 which reach 94%\naccuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when\nrun on a single NVIDIA A100 GPU. As one factor contributing to these training\nspeeds, we propose a derandomized variant of horizontal flipping augmentation,\nwhich we show improves over the standard method in every case where flipping is\nbeneficial over no flipping at all. Our code is released at https://github.com/\nKellerJordan/cifar10-airbench .\n1 Introduction\nCIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-\ning thousands of research projects per year1. Research can be accelerated and the cost of experiments\nreduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In\nthis paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single\nNVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,\n2023). To support scenarios where higher performance is needed, we additionally develop methods\ntargeting 95% and 96% accuracy. We release the following methods in total.\n1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).\n2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).\n3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).\n4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).\nAll runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-\nmatically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the\nfirst uses torch.compile to improve GPU utilization. It is intended for experiments where many\nnetworks are trained at once in order to amortize the one-time compilation cost. The non-compiled\nairbench94 variant can be easily installed and run using the following command.\n1pip install airbench\n2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"\nOne motivation for the development of these training methods is that they can accelerate the experi-\nmental iteration time of researchers working on compatible projects involving CIFAR-10. Another\nmotivation is that they can decrease the cost of projects involving a massive number of trained\nnetworks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used\n3 million trained networks to demonstrate that the outputs of a trained neural network on a given test\ninput follow an approximately linear function of the vector of binary choices of which examples the\nmodel was trained on. Another example is Jordan (2023), a study on training variance which used\n180 thousand trained networks to show that standard trainings have little variance in performance on\n1https://paperswithcode.com/datasets\nPreprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024\nFigure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal\nflipping augmentation, which flips each image with 50% probability per epoch. This results in some images\nbeing redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a\ndeterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\ntheir test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds\nand 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make\nit possible to replicate these studies, or conduct similar ones, with fewer computational resources.\nFast training also enables the rapid accumulation of statistical significance for subtle hyperparameter\ncomparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10\naccuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation\nbetween runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the\nimprovement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training\nthis will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.\nOur work builds on prior training speed projects. We utilize a modified version of the network,\ninitialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen\npatch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over\nprior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-\nure 1, Section 3.6, Section 5.2).\n2 Background\nOur objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set\nin the shortest possible amount of time. Timing begins when the method is first given access to\ntraining data, and ends when it produces test-set predictions. The method is considered valid if its\nmean accuracy over repeated runs is at least 94%.\nWe chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the\n2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more\nrecent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10\nseconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page\n(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is\ntysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,\n94% is the level of human accuracy reported by Karpathy (2011).\nWe note the following consequences of how the method is timed. First, it is permitted for the program\nto begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins\nwhen the training data is first accessed. This is helpful because otherwise the first run of training\nis typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA\nimproves the performance of a trained network by running it on multiple augmented views of each\ntest input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping\nTTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods\nattain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\nThe CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.\nIt is divided into a training set of 50,000 images and a validation set of 10,000 images. As a\nmatter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was\n2\n80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than\nairbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94\ncan be attributed entirely to algorithmic progress rather than compute scaling.\n3 Methods\n3.1 Network architecture and baseline training\nWe train a convolutional network with a total of 1.97 million parameters, following tysam-code\n(2023) with a few small changes. It contains seven convolutions with the latter six being divided into\nthree blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this\nsection we offer some comments on the main design choices.\nThe network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed\nentirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,\n2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with\nno padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather\nthan the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff\nbetween throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\nFollowing Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,\nand disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is\nscaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture\ndiffers only in that we decrease the number of output channels in the third block from 512 to 256,\nand we add learnable biases to the first convolution.\nAs our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of\n0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the\nmaximum at 20% of the way through training, and then decreases to zero. For data augmentation we\nuse random horizontal flipping alongside 2-pixel random translation. For translation we use reflection\npadding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note\nthat what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a\nrandom 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the\nnetwork is run on both a given test image and its mirror and inferences are made based on the average\nof the two outputs. With optimized choices of learning rate, momentum, and weight decay, this\nbaseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.\n3.2 Frozen patch-whitening initialization\nFollowing Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-\nwhitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code\n(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches\nacross the training distribution, so that their outputs have identity covariance matrix. The second 12\nFigure 2: The first layer\u2019s weights after whitening\ninitialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,\nso that input information is preserved through the\nactivation which follows. Figure 2 shows the result.\nWe do not update this layer\u2019s weights during training.\nDeparting from tysam-code (2023), we add learn-\nable biases to this layer, yielding a small performance\nboost. The biases are trained for 3 epochs, after which\nwe disable their gradient to increase backward-pass\nthroughput, which improves training speed without\nreducing accuracy. We also obtain a slight perfor-\nmance boost relative to tysam-code (2023) by re-\nducing the constant added to the eigenvalues during\ncalculation of the patch-whitening initialization for\nthe purpose of preventing numerical issues in the case\nof a singular patch-covariance matrix.\nPatch-whitening initialization is the single most impactful feature. Adding it to the baseline more\nthan doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.\n3\nRandom reshuffling Alternating flip Mean accuracy\nNo No 93.40%\nNo Yes 93.48%\nYes No 93.92%\nYes Yes 94.01%\nTable 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating\nflip (which we propose) reduce training data redundancy and improve performance.\n3.3 Identity initialization\ndirac : We initialize all convolutions after the first as partial identity transforms. That is, for a\nconvolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an\nidentity transform of the input, and leave the remaining N\u2212Mto their default initialization. In\nPyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight\nwof each convolutional layer. This method partially follows tysam-code (2023), which used a more\ncomplicated scheme where the identity weights are mixed in with the original initialization, which\nwe did not find to be more performant. With this feature added, training attains 94% accuracy in 18\nepochs taking 6.8 A100-seconds.\n3.4 Optimization tricks\nscalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor\nof64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%\nin 13.5 epochs taking 5.1 A100-seconds.\nlookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.\nWe note that Lookahead has also been found effective in prior work on training speed for ResNet-\n18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6\nA100-seconds.\n3.5 Multi-crop evaluation\nmulticrop : To generate predictions, we run the trained network on six augmented views of each\ntest image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a\nversion which is translated down-and-to-the-right by one pixel, and the mirrored versions of all\nthree. Predictions are made using a weighted average of all six outputs, where the two views of the\nuntranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125\neach. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.\nWe note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-\nmonyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of\nevaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using\nmore crops does improve performance, but the increase to inference time outweighs the potential\ntraining speedup.\n3.6 Alternating flip\nTo speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,\nwhich we motivate as follows. When training neural networks, it is standard practice to organize\ntraining into a set of epochs during which every training example is seen exactly once. This\ndiffers from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),\nwhich calls for data to be repeatedly sampled with-replacement from the training set, resulting in\nexamples being potentially seen multiple redundant times within a short window of training. The\nuse of randomly ordered epochs of data for training has a different name, being called the random\nreshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our\ntraining dataset consists of Nunique examples, then sampling data with replacement causes every\n\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N\nunique examples on average. On the other hand, random reshuffling leads to all Nunique examples\nbeing seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason\nthat it is beneficial to maximize the number of unique inputs seen per window of training time.\n4\nWe extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We\nfirst note that standard random horizontal flipping augmentation can be defined as follows.\n1import torch\n2def random_flip ( inputs ):\n3 # Applies random flipping to a batch of images\n4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)\n5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]\nListing 1: Random flip\nIf horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2\nwhich may be seen during training. Potentially, every pair of consecutive epochs could contain every\nunique input. But our main observation is that with standard random horizontal flipping, half of the\nimages will be redundantly flipped the same way during both epochs, so that on average only 1.5N\nunique inputs will be seen.\naltflip : To address this, we propose to modify standard random horizontal flipping augmentation as\nfollows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},\nwe flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we\nflip only those inputs which were flipped in the first epoch. We provide the following implementation\nwhich avoids the need for extra memory by using a pseudorandom function to decide the flips.\n1import torch\n2import hashlib\n3def hash_fn (n, seed =42) :\n4 k = n * seed\n5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,\n16)\n6def alternating_flip (inputs , indices , epoch ):\n7 # Applies alternating flipping to a batch of images\n8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist\n() ])\n9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)\n10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\nListing 2: Alternating flip\nThe result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in\nFigure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.\nAdding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final\ntraining method airbench94.py , the entire contents of which can be found in Section E. It reaches\n94% accuracy in 3.83 seconds on an NVIDIA A100.\n3.7 Compilation\nThe final step we take to speed up training is a non-algorithmic one: we compile our training\nmethod using torch.compile in order to more efficiently utilize the GPU. This results in a training\nscript which is mathematically equivalent (up to small differences in floating point arithmetic) to\nthe non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29\nA100-seconds. The downside is that the one-time compilation process takes up to several minutes to\ncomplete before training runs can begin, so that it is only beneficial when we plan to execute many\nruns of training at once. We release this version as airbench94_compiled.py .\n4 95% and 96% targets\nTo address scenarios where somewhat higher performance is desired, we additionally develop methods\ntargeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .\nTo attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel\ncount of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce\nthe learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%\naccuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.\n2Assuming none of the training inputs are already mirrors of each other.\n5\nFigure 4: Training speedups accumulate additively. Removing individual features from airbench94\nincreases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces\nthe epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-\n94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.\nFigure 3: FLOPs vs. error rate tradeoff. Our\nthree training methods apparently follow a linear\nlog-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-\nVries & Taylor, 2017) augmentation and raise the\ntraining epochs to 40. We add a third convolution to\neach block, and scale the first block to 128 channels\nand the second two to 512. We also add a residual\nconnection across the later two convolutions of each\nblock, which we find is still beneficial despite the fact\nthat we are already using identity initialization (Sec-\ntion 3.3) to ease gradient flow. Finally, we reduce\nthe learning rate by a factor of 0.78. These changes\nyield airbench96 which attains 96.05% accuracy in\n46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.\nFigure 3 shows the FLOPs and error rate of each of\nour three training methods.\n5 Experiments\n5.1 Interaction between features\nTo gain a better sense of the impact of each feature on training speed, we compare two quantities.\nFirst, we measure the number of epochs that can be saved by adding the feature to the whitened\nbaseline (Section 3.2). Second, we measure the number of epochs that must be added when the\nfeature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-\ntion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it\nfrom the final airbench94 increases epochs-to-94% from 9.9 to 12.8.\nFigure 4 shows both quantities for each feature. Surprisingly, we find that for all features except\nmulti-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even\nthough the whitened baseline requires more than twice as many epochs as the final configuration.\nThis indicates that the interaction between most features is additive rather than multiplicative.\n5.2 Does alternating flip generalize?\nIn this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of\ntraining configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all\ncases except those where neither alternating nor random flip improve over using no flipping at all.\nFor CIFAR-10 we consider the performance boost given by alternating flip across the follow-\ning 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and\nairbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For\n6\nFigure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,\nthe use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%\ntraining speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.\n95% confidence intervals are shown around each point.\neach configuration we compare the performance of alternating and random flip in terms of their mean\naccuracy across n= 400 runs of training.\nFigure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip\nimproves performance in every setting. To get a sense for how big the improvement is, we estimate\nthe effective speedup for each case, i.e., the fraction of epochs that could be saved by switching\nfrom random to alternating flip while maintaining the level of accuracy of random flip. We begin\nby fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of\neach random flip-based training configuration. We use these curves to calculate the effective speedup\nafforded by switching from random to alternating flip. For example, airbench94 with random flip\nand without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The\nsame configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law\nfit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.\nNote that using a power-law yields a more conservative estimate relative to using linear interpolation\nbetween the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.\nTable 2 shows the result. We observe the following patterns. First, the addition of extra augmentation\n(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that\nthe main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the\nsame way for many epochs in a row; we speculate that adding extra augmentation reduces the negative\nimpact of these cases because it increases data diversity. Next, TTA reduces the gap between random\nand alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),\nindicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training\nfor longer consistently increases the effective speedup given by alternating flip.\nWe next study ImageNet trainings with the following experiment. We train a ResNet-18 with a\nvariety of train and test crops, comparing three flipping options: alternating flip, random flip, and no\nflipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192\ncenter crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)\nto denote the latter. We also consider two training crops: 192x192 inception-style random resized\ncrop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area\nranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the\nshorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write\n7\nBaseline Cutout Epochs Speedup Speedup (w/ TTA)\nairbench94 No 10 15.0% 5.30%\nairbench94 No 20 27.1% 21.3%\nairbench94 No 40 38.3% 36.4%\nairbench94 No 80 102% 31.8%\nairbench94 Yes 10 3.84% 1.13%\nairbench94 Yes 20 7.42% 2.00%\nairbench94 Yes 40 18.6% 9.28%\nairbench94 Yes 80 29.2% 14.25%\nairbench96 Yes 10 4.94% 1.11%\nairbench96 Yes 20 8.99% 3.58%\nairbench96 Yes 40 17.2% 6.48%\nairbench96 Yes 80 18.8% Not measured\nTable 2: Effective speedups given by switching from random flip to alternating flip. The two configurations\nmost closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw\naccuracy values of the airbench94 experiments.\nFlipping augmentation option\nTrain crop Test crop Epochs TTA None Random Alternating\nHeavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28\nHeavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28\nLight RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26\nLight RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26\nHeavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28\nHeavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28\nLight RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26\nLight RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26\nLight RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16\nHeavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4\nTable 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for\nthose trainings where random flip improves significantly over not flipping at all. The single best flipping option\nin each row is bolded when the difference is statistically significant.\nHeavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided\nin Section C.\nTable 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC\nis better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly\nbetter when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test\nresolution discrepancy (Touvron et al., 2019).\nFor trainings which use Light RRC, we find that switching from random flip to alternating flip\nprovides a substantial boost to performance, amounting to a training speedup of more than 25%.\nIn Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to\nalternating flip improves performance by more than increasing the training duration from 16 to 20\nepochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our\nresults on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit\nfrom alternating flip. Indeed, even turning flipping off completely does not significantly reduce the\nperformance of these trainings. We conclude that alternating flip improves over random flip for every\ntraining scenario where the latter improves over no flipping at all.\n8\nEpochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE\n1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312\n2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233\n1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269\n1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533\n2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433\n1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444\nTable 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).\n5.3 Variance and class-wise calibration\nPrevious sections have focused on understanding what factors affect the first moment of accuracy\n(the mean). In this section we investigate the second moment, finding that TTA reduces variance at\nthe cost of calibration.\nOur experiment is to execute 10,000 runs of airbench94 training with several hyperparameter\nsettings. For each setting we report both the variance in test-set accuracy as well as an estimate of the\ndistribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.\nTable 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set\nvariance, replicating the main finding of Jordan (2023). This is a surprising result because these\ntrainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7\nas much distribution-wise variance when run for a similar duration, and reached a low variance only\nwhen run for 64 epochs. We conclude from this comparison that distribution-wise variance is more\nstrongly connected to the rate of convergence of a training rather than its duration as such. We also\nnote that the low distribution-wise variance of airbench94 indicates it has high training stability.\nUsing TTA significantly reduces the test-set variance, such that all three settings with TTA have lower\ntest-set variance than any setting without TTA. However, test-set variance is implied by the class-wise\ncalibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this\nreduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,\nwe compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which\nmeasures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA\nhas a higher CACE than every setting without TTA, confirming the hypothesis.\n6 Discussion\nIn this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7\nfaster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the\nairbench Python package.\nWe developed airbench solely with the goal of maximizing training speed on CIFAR-10. In\nSection B we find that it also generalizes well to other tasks. For example, without any extra tuning,\nairbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.\nOne factor contributing to the training speed of airbench was our finding that training can be accel-\nerated by partially derandomizing the standard random horizontal flipping augmentation, resulting in\nthe variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating\nflip improves the performance of every training we considered (Section 5.2), with the exception of\nthose trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to\nus, the standard ImageNet trainings that we considered do not significantly benefit from horizontal\nflipping. Future work might investigate whether it is possible to obtain derandomized improvements\nto other augmentations besides horizontal flip.\nThe methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,\nwith fixed performance and hardware constraints. These constraints mean that we cannot improve\nperformance by simply scaling up the amount of computational resources used; instead we are forced\nto develop new methods like the alternating flip. We look forward to seeing what other new methods\nfuture work discovers to push training speed further.\n9\nReferences\nDimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.\nDan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-\nperformance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,\n2011.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\nmark and competition. 2017.\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet\nor cifar-10. arXiv preprint arXiv:1810.03505 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\npp. 248\u2013255. IEEE, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552 , 2017.\nMert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic\ngradient descent. Mathematical Programming , 186:49\u201384, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 , 2016.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.\npmlr, 2015.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\nsgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.\nKeller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and\ninevitable. arXiv preprint arXiv:2304.01910 , 2023.\nAndrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.\nio/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for\nadvanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT\nLicense.\nGuillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander\nM \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,\nBenjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.\nBenchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural\nInformation Processing Systems , 35:25404\u201325421, 2022.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.\nDavid Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/\nhow-to-train-your-resnet-8-bag-of-tricks/ .\n10\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics , 22(3):400\u2013407, 1951.\nSantiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:\n//github.com/apple/ml-cifar-10-faster , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.\ntysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/\nhlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. Advances in neural information processing systems , 32, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,\n2016.\nMichael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps\nforward, 1 step back. Advances in neural information processing systems , 32, 2019.\n11\nA Network architecture\n1from torch import nn\n2\n3class Flatten (nn. Module ):\n4 def forward (self , x):\n5 return x. view (x. size (0) , -1)\n6\n7class Mul (nn. Module ):\n8 def __init__ (self , scale ):\n9 super (). __init__ ()\n10 self . scale = scale\n11 def forward (self , x):\n12 return x * self . scale\n13\n14def conv (ch_in , ch_out ):\n15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,\n16 padding =\u2019same \u2019, bias = False )\n17\n18def make_net ():\n19 act = lambda : nn. GELU ()\n20 bn = lambda ch: nn. BatchNorm2d (ch)\n21 return nn. Sequential (\n22 nn. Sequential (\n23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),\n24 act () ,\n25 ),\n26 nn. Sequential (\n27 conv (24 , 64) ,\n28 nn. MaxPool2d (2) ,\n29 bn (64) , act () ,\n30 conv (64 , 64) ,\n31 bn (64) , act () ,\n32 ),\n33 nn. Sequential (\n34 conv (64 , 256) ,\n35 nn. MaxPool2d (2) ,\n36 bn (256) , act () ,\n37 conv (256 , 256) ,\n38 bn (256) , act () ,\n39 ),\n40 nn. Sequential (\n41 conv (256 , 256) ,\n42 nn. MaxPool2d (2) ,\n43 bn (256) , act () ,\n44 conv (256 , 256) ,\n45 bn (256) , act () ,\n46 ),\n47 nn. MaxPool2d (3) ,\n48 Flatten () ,\n49 nn. Linear (256 , 10, bias = False ),\n50 Mul (1/9) ,\n51 )\nListing 3: Minimal PyTorch code for the network architecture used by airbench94 .\nWe note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock\ntime. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel\nsize of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this\ndoes not improve the wallclock training time on an A100. Reducing the batch size is another easy\nway to save FLOPs but not wallclock time.\n12\nDataset Flipping? Cutout? ResNet-18 airbench96\nCIFAR-10 Yes No 95.55% 95.61%\nCIFAR-10 Yes Yes 96.01% 96.05%\nCIFAR-100 Yes No 77.54% 79.27%\nCIFAR-100 Yes Yes 78.04% 79.76%\nCINIC-10 Yes No 87.58% 87.78%\nCINIC-10 Yes Yes not measured 88.22%\nSVHN No No 97.35% 97.38%\nSVHN No Yes not measured 97.64%\nTable 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We\ndirectly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off\nflipping for SVHN).\nB Extra dataset experiments\nWe developed airbench with the singular goal of maximizing training speed on CIFAR-10. To\nfind out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its\nperformance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-\n10 (Darlow et al., 2018).\nOn CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both\nthe case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not\n(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than\nResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say\nthat it generalizes.\nWe compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.\nWe do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal\non CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better\nperformance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where\nairbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is\nused and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows\nstrong generalization to other tasks.\nWe note that this comparison between airbench96 and ResNet-18 training is fair in the sense that\nit does demonstrate that the former has good generalization, but unfair in the sense that it does not\nindicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time\naugmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training\nwould outperform airbench96 if it were run using test-time augmentation. However, it also takes\n5-10 times longer to complete. The decision of which to use may be situational.\nThe accuracy values we report for ResNet-18 training are from the following sources. We tried\nto select the highest values we could find for each setting. Moreau et al. (2022) reports attaining\n95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports\nattaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on\nCIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.\nC ImageNet training details\nOur ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/\nlibffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and\nmomentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution\n160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of\ntraining. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.\n13\nHyperparameters Flipping augmentation option\nEpochs Cutout TTA None Random Alternating\n10 No No 92.3053 93.0988 93.2798\n20 No No 92.8166 93.7446 93.8652\n40 No No 93.0143 94.0133 94.0729\n80 No No 93.0612 94.1169 94.1628\n10 No Yes 93.4071 93.9488 94.0186\n20 No Yes 93.8528 94.5565 94.6530\n40 No Yes 94.0381 94.7803 94.8203\n80 No Yes 94.0638 94.8506 94.8676\n10 Yes No 91.8487 92.0402 92.1374\n20 Yes No 92.8474 93.3825 93.4876\n40 Yes No 93.2675 94.1014 94.1952\n80 Yes No 93.4193 94.4311 94.5204\n10 Yes Yes 92.6455 92.7780 92.8103\n20 Yes Yes 93.7862 94.1306 94.1670\n40 Yes Yes 94.3090 94.8511 94.8960\n80 Yes Yes 94.5253 95.1839 95.2362\nTable 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a\nmean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise\ndifference in means is statistically significant.\nD Extra tables & figures\nFigure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in\nSection 5.3.\nE Complete training code\n1\"\"\"\n2airbench94 .py\n33.83 s runtime on an A100 ; 0.36 PFLOPs .\n4Evidence for validity : 94.01 average accuracy in n =1000 runs .\n5\n6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB\nwith the following nvidia -smi:\n7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :\n11.7\n14\n8torch . __version__ == \u20192.1.2+ cu118 \u2019\n9\"\"\"\n10\n11# ############################################\n12# Setup / Hyperparameters #\n13# ############################################\n14\n15import os\n16import sys\n17import uuid\n18from math import ceil\n19\n20import torch\n21from torch import nn\n22import torch .nn. functional as F\n23import torchvision\n24import torchvision . transforms as T\n25\n26torch . backends . cudnn . benchmark = True\n27\n28\"\"\"\n29We express the main training hyperparameters ( batch size , learning\nrate , momentum , and weight decay ) in decoupled form , so that each\none can be tuned independently . This accomplishes the following :\n30* Assuming time - constant gradients , the average step size is decoupled\nfrom everything but the lr.\n31* The size of the weight decay update is decoupled from everything but\nthe wd.\n32In constrast , normally when we increase the ( Nesterov ) momentum , this\nalso scales up the step size proportionally to 1 + 1 / (1 -\nmomentum ), meaning we cannot change momentum without having to re -\ntune the learning rate . Similarly , normally when we increase the\nlearning rate this also increases the size of the weight decay ,\nrequiring a proportional decrease in the wd to maintain the same\ndecay strength .\n33\n34The practical impact is that hyperparameter tuning is faster , since\nthis parametrization allows each one to be tuned independently .\nSee https :// myrtle .ai/ learn /how -to -train -your - resnet -5-\nhyperparameters /.\n35\"\"\"\n36\n37hyp = {\n38 \u2019opt \u2019: {\n39 \u2019 train_epochs \u2019: 9.9 ,\n40 \u2019batch_size \u2019: 1024 ,\n41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples\n42 \u2019momentum \u2019: 0.85 ,\n43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (\ndecoupled from learning rate )\n44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not\nweight decay ) for BatchNorm biases\n45 \u2019 label_smoothing \u2019: 0.2 ,\n46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the\nwhitening layer bias before freezing\n47 },\n48 \u2019aug \u2019: {\n49 \u2019flip \u2019: True ,\n50 \u2019translate \u2019: 2,\n51 },\n52 \u2019net \u2019: {\n53 \u2019widths \u2019: {\n54 \u2019block1 \u2019: 64,\n55 \u2019block2 \u2019: 256 ,\n56 \u2019block3 \u2019: 256 ,\n15\n57 },\n58 \u2019 batchnorm_momentum \u2019: 0.6 ,\n59 \u2019 scaling_factor \u2019: 1/9 ,\n60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :\n0= none , 1= mirror , 2= mirror + translate\n61 },\n62}\n63\n64# ############################################\n65# DataLoader #\n66# ############################################\n67\n68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )\n69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )\n70\n71def batch_flip_lr ( inputs ):\n72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .\nview (-1, 1, 1, 1)\n73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\n74\n75def batch_crop (images , crop_size ):\n76 r = ( images . size ( -1) - crop_size )//2\n77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =\nimages . device )\n78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),\ndevice = images .device , dtype = images . dtype )\n79 # The two cropping methods in this if - else produce equivalent\nresults , but the second is faster for r > 2.\n80 if r <= 2:\n81 for sy in range (-r, r +1):\n82 for sx in range (-r, r +1):\n83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)\n84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size\n, r+sx:r+sx+ crop_size ]\n85 else :\n86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size\n+2* r), device = images .device , dtype = images . dtype )\n87 for s in range (-r, r +1):\n88 mask = ( shifts [:, 0] == s)\n89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]\n90 for s in range (-r, r +1):\n91 mask = ( shifts [:, 1] == s)\n92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+\ncrop_size ]\n93 return images_out\n94\n95class CifarLoader :\n96 \"\"\"\n97 GPU - accelerated dataloader for CIFAR -10 which implements\nalternating flip augmentation .\n98 \"\"\"\n99\n100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,\ndrop_last =None , shuffle =None , gpu =0):\n101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .\npt \u2019)\n102 if not os. path . exists ( data_path ):\n103 dset = torchvision . datasets . CIFAR10 (path , download =True ,\ntrain = train )\n104 images = torch . tensor ( dset . data )\n105 labels = torch . tensor ( dset . targets )\n106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:\ndset . classes }, data_path )\n107\n108 data = torch . load ( data_path , map_location = torch . device (gpu))\n16\n109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [\n\u2019labels \u2019], data [\u2019classes \u2019]\n110 # It \u2019s faster to load + process uint8 data than to load\npreprocessed fp16 data\n111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).\nto( memory_format = torch . channels_last )\n112\n113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )\n114 self . proc_images = {} # Saved results of image processing to\nbe done on the first epoch\n115 self . epoch = 0\n116\n117 self . aug = aug or {}\n118 for k in self .aug . keys ():\n119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019\n% k\n120\n121 self . batch_size = batch_size\n122 self . drop_last = train if drop_last is None else drop_last\n123 self . shuffle = train if shuffle is None else shuffle\n124\n125 def __len__ ( self ):\n126 return len( self . images )// self . batch_size if self . drop_last\nelse ceil (len( self . images )/ self . batch_size )\n127\n128 def __iter__ ( self ):\n129\n130 if self . epoch == 0:\n131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .\nimages )\n132 # Randomly flip all images on the first epoch as according\nto definition of alternating flip\n133 if self .aug.get(\u2019flip \u2019, False ):\n134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (\nimages )\n135 # Pre -pad images to save time when doing random\ntranslation\n136 pad = self .aug.get (\u2019translate \u2019, 0)\n137 if pad > 0:\n138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019\nreflect \u2019)\n139\n140 if self .aug.get(\u2019translate \u2019, 0) > 0:\n141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .\nshape [ -2])\n142 elif self .aug.get (\u2019flip \u2019, False ):\n143 images = self . proc_images [\u2019flip \u2019]\n144 else :\n145 images = self . proc_images [\u2019norm \u2019]\n146 if self .aug.get(\u2019flip \u2019, False ):\n147 if self . epoch % 2 == 1:\n148 images = images . flip ( -1)\n149\n150 self . epoch += 1\n151\n152 indices = ( torch . randperm if self . shuffle else torch . arange )(\nlen ( images ), device = images . device )\n153 for i in range (len ( self )):\n154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]\n155 yield ( images [ idxs ], self . labels [ idxs ])\n156\n157# ############################################\n158# Network Components #\n159# ############################################\n160\n17\n161class Flatten (nn. Module ):\n162 def forward (self , x):\n163 return x. view (x. size (0) , -1)\n164\n165class Mul (nn. Module ):\n166 def __init__ (self , scale ):\n167 super (). __init__ ()\n168 self . scale = scale\n169 def forward (self , x):\n170 return x * self . scale\n171\n172class BatchNorm (nn. BatchNorm2d ):\n173 def __init__ (self , num_features , momentum , eps =1e -12 ,\n174 weight =False , bias = True ):\n175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )\n176 self . weight . requires_grad = weight\n177 self . bias . requires_grad = bias\n178 # Note that PyTorch already initializes the weights to one and\nbiases to zero\n179\n180class Conv (nn. Conv2d ):\n181 def __init__ (self , in_channels , out_channels , kernel_size =3,\npadding =\u2019same \u2019, bias = False ):\n182 super (). __init__ ( in_channels , out_channels , kernel_size =\nkernel_size , padding = padding , bias = bias )\n183\n184 def reset_parameters ( self ):\n185 super (). reset_parameters ()\n186 if self . bias is not None :\n187 self . bias . data . zero_ ()\n188 w = self . weight . data\n189 torch .nn. init . dirac_ (w[:w. size (1) ])\n190\n191class ConvGroup (nn. Module ):\n192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):\n193 super (). __init__ ()\n194 self . conv1 = Conv ( channels_in , channels_out )\n195 self . pool = nn. MaxPool2d (2)\n196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )\n197 self . conv2 = Conv ( channels_out , channels_out )\n198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )\n199 self . activ = nn. GELU ()\n200\n201 def forward (self , x):\n202 x = self . conv1 (x)\n203 x = self . pool (x)\n204 x = self . norm1 (x)\n205 x = self . activ (x)\n206 x = self . conv2 (x)\n207 x = self . norm2 (x)\n208 x = self . activ (x)\n209 return x\n210\n211# ############################################\n212# Network Definition #\n213# ############################################\n214\n215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019\n][ \u2019 batchnorm_momentum \u2019]):\n216 whiten_kernel_size = 2\n217 whiten_width = 2 * 3 * whiten_kernel_size **2\n218 net = nn. Sequential (\n219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True\n),\n220 nn. GELU () ,\n18\n221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],\nbatchnorm_momentum ),\n222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],\nbatchnorm_momentum ),\n223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],\nbatchnorm_momentum ),\n224 nn. MaxPool2d (3) ,\n225 Flatten () ,\n226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),\n227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,\n228 )\n229 net [0]. weight . requires_grad = False\n230 net = net. half (). cuda ()\n231 net = net.to( memory_format = torch . channels_last )\n232 for mod in net. modules ():\n233 if isinstance (mod , BatchNorm ):\n234 mod . float ()\n235 return net\n236\n237# ############################################\n238# Whitening Conv Initialization #\n239# ############################################\n240\n241def get_patches (x, patch_shape ):\n242 c, (h, w) = x. shape [1] , patch_shape\n243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h\n,w). float ()\n244\n245def get_whitening_parameters ( patches ):\n246 n,c,h,w = patches . shape\n247 patches_flat = patches . view (n, -1)\n248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n\n249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance\n, UPLO =\u2019U\u2019)\n250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.\nreshape (c*h*w,c,h,w). flip (0)\n251\n252def init_whitening_conv (layer , train_set , eps =5e -4):\n253 patches = get_patches ( train_set , patch_shape = layer . weight . data .\nshape [2:])\n254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )\n255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)\n256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -\neigenvectors_scaled ))\n257\n258# ###########################################\n259# Lookahead #\n260# ###########################################\n261\n262class LookaheadState :\n263 def __init__ (self , net):\n264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().\nitems ()}\n265\n266 def update (self , net , decay ):\n267 for ema_param , net_param in zip( self . net_ema . values () , net.\nstate_dict (). values ()):\n268 if net_param . dtype in ( torch .half , torch . float ):\n269 ema_param . lerp_ ( net_param , 1- decay )\n270 net_param . copy_ ( ema_param )\n271\n272# ###########################################\n273# Logging #\n274# ###########################################\n275\n19\n276def print_columns ( columns_list , is_head =False , is_final_entry = False ):\n277 print_string = \u2019\u2019\n278 for col in columns_list :\n279 print_string += \u2019| %s \u2019 % col\n280 print_string += \u2019|\u2019\n281 if is_head :\n282 print (\u2019-\u2019*len( print_string ))\n283 print ( print_string )\n284 if is_head or is_final_entry :\n285 print (\u2019-\u2019*len( print_string ))\n286\n287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,\n\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]\n288def print_training_details ( variables , is_final_entry ):\n289 formatted = []\n290 for col in logging_columns_list :\n291 var = variables .get (col. strip () , None )\n292 if type (var) in (int , str):\n293 res = str(var)\n294 elif type (var) is float :\n295 res = \u2019{:0.4 f}\u2019. format ( var)\n296 else :\n297 assert var is None\n298 res = \u2019\u2019\n299 formatted . append (res. rjust (len (col)))\n300 print_columns ( formatted , is_final_entry = is_final_entry )\n301\n302# ###########################################\n303# Evaluation #\n304# ###########################################\n305\n306def infer (model , loader , tta_level =0) :\n307 \"\"\"\n308 Test - time augmentation strategy (for tta_level =2):\n309 1. Flip / mirror the image left -to - right (50% of the time ).\n310 2. Translate the image by one pixel either up -and - left or down -and\n- right (50% of the time , i.e. both happen 25% of the time ).\n311\n312 This creates 6 views per image ( left / right times the two\ntranslations and no - translation ), which we evaluate and then\nweight according to the given probabilities .\n313 \"\"\"\n314\n315 def infer_basic (inputs , net):\n316 return net( inputs ). clone ()\n317\n318 def infer_mirror (inputs , net):\n319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))\n320\n321 def infer_mirror_translate ( inputs , net ):\n322 logits = infer_mirror (inputs , net )\n323 pad = 1\n324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)\n325 inputs_translate_list = [\n326 padded_inputs [:, :, 0:32 , 0:32] ,\n327 padded_inputs [:, :, 2:34 , 2:34] ,\n328 ]\n329 logits_translate_list = [ infer_mirror ( inputs_translate , net)\n330 for inputs_translate in\ninputs_translate_list ]\n331 logits_translate = torch . stack ( logits_translate_list ). mean (0)\n332 return 0.5 * logits + 0.5 * logits_translate\n333\n334 model . eval ()\n335 test_images = loader . normalize ( loader . images )\n20\n336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][\ntta_level ]\n337 with torch . no_grad ():\n338 return torch .cat ([ infer_fn (inputs , model ) for inputs in\ntest_images . split (2000) ])\n339\n340def evaluate (model , loader , tta_level =0) :\n341 logits = infer (model , loader , tta_level )\n342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()\n343\n344# ###########################################\n345# Training #\n346# ###########################################\n347\n348def main (run):\n349\n350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]\n351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]\n352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]\n353 # Assuming gradients are constant in time , for Nesterov momentum ,\nthe below ratio is how much larger the default steps will be than\nthe underlying per - example gradients . We divide the learning rate\nby this ratio in order to ensure steps are the same scale as\ngradients , regardless of the choice of momentum .\n354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))\n355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning\nrate for PyTorch SGD\n356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale\n357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]\n358\n359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019\nlabel_smoothing \u2019], reduction =\u2019none \u2019)\n360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)\n361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =\nbatch_size , aug=hyp[\u2019aug \u2019])\n362 if run == \u2019warmup \u2019:\n363 # The only purpose of the first run is to warmup , so we can\nuse dummy data\n364 train_loader . labels = torch . randint (0, 10, size =( len (\ntrain_loader . labels ) ,), device = train_loader . labels . device )\n365 total_train_steps = ceil (len( train_loader ) * epochs )\n366\n367 model = make_net ()\n368 current_steps = 0\n369\n370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in\nk and p. requires_grad ]\n371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019\nnot in k and p. requires_grad ]\n372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,\nweight_decay =wd/ lr_biases ),\n373 dict ( params = other_params , lr=lr , weight_decay =wd/\nlr)]\n374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,\nnesterov = True )\n375\n376 def triangle (steps , start =0, end =0, peak =0.5) :\n377 xp = torch . tensor ([0 , int( peak * steps ), steps ])\n378 fp = torch . tensor ([ start , 1, end ])\n379 x = torch . arange (1+ steps )\n380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])\n381 b = fp [: -1] - (m * xp [: -1])\n382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1\n383 indices = torch . clamp ( indices , 0, len(m) - 1)\n384 return m[ indices ] * x + b[ indices ]\n21\n385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,\npeak =0.23)\n386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:\nlr_schedule [i])\n387\n388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /\ntotal_train_steps )**3\n389 lookahead_state = LookaheadState ( model )\n390\n391 # For accurately timing GPU code\n392 starter = torch . cuda . Event ( enable_timing = True )\n393 ender = torch . cuda . Event ( enable_timing = True )\n394 total_time_seconds = 0.0\n395\n396 # Initialize the first layer using statistics of training images\n397 starter . record ()\n398 train_images = train_loader . normalize ( train_loader . images [:5000])\n399 init_whitening_conv ( model [0] , train_images )\n400 ender . record ()\n401 torch . cuda . synchronize ()\n402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n403\n404 for epoch in range ( ceil ( epochs )):\n405\n406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019\nwhiten_bias_epochs \u2019])\n407\n408 # ###################\n409 # Training #\n410 # ###################\n411\n412 starter . record ()\n413\n414 model . train ()\n415 for inputs , labels in train_loader :\n416\n417 outputs = model ( inputs )\n418 loss = loss_fn ( outputs , labels ).sum ()\n419 optimizer . zero_grad ( set_to_none = True )\n420 loss . backward ()\n421 optimizer . step ()\n422 scheduler . step ()\n423\n424 current_steps += 1\n425\n426 if current_steps % 5 == 0:\n427 lookahead_state . update (model , decay = alpha_schedule [\ncurrent_steps ]. item ())\n428\n429 if current_steps >= total_train_steps :\n430 if lookahead_state is not None :\n431 lookahead_state . update (model , decay =1.0)\n432 break\n433\n434 ender . record ()\n435 torch . cuda . synchronize ()\n436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n437\n438 # ###################\n439 # Evaluation #\n440 # ###################\n441\n442 # Print the accuracy and loss from the last training batch of\nthe epoch\n22\n443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().\nmean (). item ()\n444 train_loss = loss . item () / batch_size\n445 val_acc = evaluate (model , test_loader , tta_level =0)\n446 print_training_details ( locals () , is_final_entry = False )\n447 run = None # Only print the run number once\n448\n449 # ###################\n450 # TTA Evaluation #\n451 # ###################\n452\n453 starter . record ()\n454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019\ntta_level \u2019])\n455 ender . record ()\n456 torch . cuda . synchronize ()\n457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n458\n459 epoch = \u2019eval \u2019\n460 print_training_details ( locals () , is_final_entry = True )\n461\n462 return tta_val_acc\n463\n464if __name__ == \" __main__ \":\n465 with open (sys. argv [0]) as f:\n466 code = f. read ()\n467\n468 print_columns ( logging_columns_list , is_head = True )\n469 main (\u2019warmup \u2019)\n470 accs = torch . tensor ([ main (run) for run in range (25) ])\n471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))\n472\n473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }\n474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))\n475 os. makedirs ( log_dir , exist_ok = True )\n476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)\n477 print (os. path . abspath ( log_path ))\n478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))\nListing 4: airbench94.py\n23\nGuidance:\nSelf Critique:\n\nUpon reviewing my task candidate extraction and evaluation, I can identify several areas for improvement:\n\n1. Quantity of task candidates: I only extracted three task candidates, which is likely insufficient to cover the full range of research and engineering tasks described in the paper. I should have extracted more candidates to provide a more comprehensive set of options.\n\n2. Research vs Engineering balance: The tasks I extracted lean more heavily towards engineering implementation rather than research. I should have sought out more tasks that require deeper understanding and application of the research concepts presented in the paper.\n\n3. Missed important tasks: I overlooked several significant tasks described in the paper, such as implementing the identity initialization for convolutions, the Lookahead optimization, and the overall network architecture. These are important components of the research that should have been included as task candidates.\n\n4. Depth of analysis: While I provided some evaluation of the tasks against the criteria, my analysis could have been more thorough and systematic, addressing each point in the requirements more explicitly for each task.\n\n5. Scoring function consideration: I didn't give enough attention to the requirement of having easily implementable scoring functions that are much simpler than the tasks themselves. This is a crucial aspect that I should have emphasized more in my evaluation.\n\n6. Relevance to paper's core contributions: I could have focused more on tasks that directly relate to the paper's main contributions and novel aspects, rather than including more general ML implementation tasks.\n\nTo improve, I should re-examine the paper more carefully, extract a wider range of task candidates that better balance research and engineering aspects, and provide a more thorough evaluation of each candidate against all the specified criteria.\nTask Candidates:"
  },
  {
    "Response": "Here is a revised set of task candidates with more thorough evaluation:\n\n{\n  \"tasks\": [\n    {\n      \"name\": \"Implement Alternating Flip Data Augmentation\",\n      \"description\": \"Implement the novel alternating flip data augmentation method described in the paper. This should deterministically alternate horizontal flipping of images across epochs after random flipping in the first epoch.\",\n      \"relevant_paper_text\": \"To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\",\n      \"scoring_feasibility\": 9,\n      \"llm_tractability\": 0.7,\n      \"expert_tractability\": 0.95,\n      \"layman_tractability\": 0.3\n    },\n    {\n      \"name\": \"Implement Patch-Whitening Initialization\",\n      \"description\": \"Implement the patch-whitening initialization for the first convolutional layer as described in the paper. This involves computing eigenvectors of the covariance matrix of 2x2 patches from the training data and using them to initialize the layer weights.\",\n      \"relevant_paper_text\": \"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\",\n      \"scoring_feasibility\": 8,\n      \"llm_tractability\": 0.5,\n      \"expert_tractability\": 0.9,\n      \"layman_tractability\": 0.1\n    },\n    {\n      \"name\": \"Implement Identity Initialization for Convolutions\",\n      \"description\": \"Implement the identity initialization method for convolutional layers after the first layer. This involves initializing the first M filters of each layer with N>=M outputs as identity transforms of the input.\",\n      \"relevant_paper_text\": \"We initialize all convolutions after the first as partial identity transforms. That is, for a convolution with M input channels and N\u2265M outputs, we initialize its first M filters to an identity transform of the input, and leave the remaining N\u2212M to their default initialization.\",\n      \"scoring_feasibility\": 8,\n      \"llm_tractability\": 0.6,\n      \"expert_tractability\": 0.9,\n      \"layman_tractability\": 0.2\n    },\n    {\n      \"name\": \"Implement Multi-Crop Test-Time Augmentation\",\n      \"description\": \"Implement the multi-crop test-time augmentation (TTA) method described in the paper. This involves running the trained network on six augmented views of each test image and combining the outputs with specified weights.\",\n      \"relevant_paper_text\": \"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\",\n      \"scoring_feasibility\": 9,\n      \"llm_tractability\": 0.8,\n      \"expert_tractability\": 0.95,\n      \"layman_tractability\": 0.4\n    },\n    {\n      \"name\": \"Implement Lookahead Optimization\",\n      \"description\": \"Implement the Lookahead optimization method as described in the paper. This involves maintaining a slow-moving copy of the model parameters and periodically updating the fast-moving parameters towards the slow-moving ones.\",\n      \"relevant_paper_text\": \"Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization. We note that Lookahead has also been found effective in prior work on training speed for ResNet-18 (Moreau et al., 2022).\",\n      \"scoring_feasibility\": 7,\n      \"llm_tractability\": 0.5,\n      \"expert_tractability\": 0.9,\n      \"layman_tractability\": 0.1\n    },\n    {\n      \"name\": \"Implement Complete Network Architecture\",\n      \"description\": \"Implement the complete network architecture described in the paper, including the specific modifications to standard components such as disabling biases in certain layers and using GELU activations.\",\n      \"relevant_paper_text\": \"The network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed entirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy, 2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with no padding, causing the shape of the internal feature maps to be 31x31 \u2192 15x15 \u2192 7x7 \u2192 3x3 rather than the more typical 32x32 \u2192 16x16 \u2192 8x8 \u2192 4x4, resulting in a slightly more favorable tradeoff between throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\",\n      \"scoring_feasibility\": 8,\n      \"llm_tractability\": 0.6,\n      \"expert_tractability\": 0.9,\n      \"layman_tractability\": 0.2\n    }\n  ]\n}"
  },
  {
    "Request": "Make a very succinct json object that validates with the following schema\n\n---\n\nFollow the following format.\n\nJson Schema: ${json_schema}\nJson Object: ${json_object}\n\n---\n\nJson Schema: {\"$defs\": {\"TaskCandidate\": {\"description\": \"Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\\n\\nA good task candidate:\\n- describes a task that an agent can implement\\n- typically a programming task, e.g. modifying a 'baseline' implementation\\n- which reflects or mirrors actual tasks undertaken to produce the research paper\\n- and requires research and/or engineering expertise to implement correctly\\n- for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\\n- ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\\n- the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\", \"properties\": {\"name\": {\"description\": \"Name of the task\", \"title\": \"Name\", \"type\": \"string\"}, \"description\": {\"description\": \"Description of the task\", \"title\": \"Description\", \"type\": \"string\"}, \"relevant_paper_text\": {\"description\": \"Text from the paper that is relevant to the task\", \"title\": \"Relevant Paper Text\", \"type\": \"string\"}, \"scoring_feasibility\": {\"description\": \"\\nOn a scale of 1-10, how feasible is it to write a python function to objectively score an \\nimplementation of this task, verifying that key functionality is implemented as specified?\\n\", \"title\": \"Scoring Feasibility\", \"type\": \"integer\"}, \"llm_tractability\": {\"description\": \"Probability in (0, 1) that a frontier LLM can generate a correct implementation of this task\", \"title\": \"Llm Tractability\", \"type\": \"number\"}, \"expert_tractability\": {\"description\": \"Probability in (0, 1) that a human subject matter expert can generate a correct implementation of this task\", \"title\": \"Expert Tractability\", \"type\": \"number\"}, \"layman_tractability\": {\"description\": \"Probability in (0, 1) that a layman can generate a correct implementation of this task\", \"title\": \"Layman Tractability\", \"type\": \"number\"}}, \"required\": [\"name\", \"description\", \"relevant_paper_text\", \"scoring_feasibility\", \"llm_tractability\", \"expert_tractability\", \"layman_tractability\"], \"title\": \"TaskCandidate\", \"type\": \"object\"}}, \"properties\": {\"tasks\": {\"description\": \"Tasks extracted from the paper\", \"items\": {\"$ref\": \"#/$defs/TaskCandidate\"}, \"title\": \"Tasks\", \"type\": \"array\"}}, \"required\": [\"tasks\"], \"title\": \"TaskCandidates\", \"type\": \"object\"}\nJson Object:"
  },
  {
    "Response": "Json Object: {\n  \"tasks\": [\n    {\n      \"name\": \"Implement Gradient Descent\",\n      \"description\": \"Implement a basic gradient descent algorithm for linear regression\",\n      \"relevant_paper_text\": \"We used gradient descent to optimize the model parameters\",\n      \"scoring_feasibility\": 8,\n      \"llm_tractability\": 0.9,\n      \"expert_tractability\": 0.95,\n      \"layman_tractability\": 0.2\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `paper_text`, `guidance`, produce the fields `task_candidates`.\n\n---\n\nFollow the following format.\n\nPaper Text: Full text from research paper PDF\n\nGuidance: Guidance for extracting task candidates\n\nPast Error in Task Candidates: An error to avoid in the future\n\nTask Candidates: JSON list of task candidate dicts with keys 'name', 'description', and 'relevant_paper_text'. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCandidate\": {\"description\": \"Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\\n\\nA good task candidate:\\n- describes a task that an agent can implement\\n- typically a programming task, e.g. modifying a 'baseline' implementation\\n- which reflects or mirrors actual tasks undertaken to produce the research paper\\n- and requires research and/or engineering expertise to implement correctly\\n- for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\\n- ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\\n- the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\", \"properties\": {\"name\": {\"description\": \"Name of the task\", \"title\": \"Name\", \"type\": \"string\"}, \"description\": {\"description\": \"Description of the task\", \"title\": \"Description\", \"type\": \"string\"}, \"relevant_paper_text\": {\"description\": \"Text from the paper that is relevant to the task\", \"title\": \"Relevant Paper Text\", \"type\": \"string\"}, \"scoring_feasibility\": {\"description\": \"\\nOn a scale of 1-10, how feasible is it to write a python function to objectively score an \\nimplementation of this task, verifying that key functionality is implemented as specified?\\n\", \"title\": \"Scoring Feasibility\", \"type\": \"integer\"}, \"llm_tractability\": {\"description\": \"Probability in (0, 1) that a frontier LLM can generate a correct implementation of this task\", \"title\": \"Llm Tractability\", \"type\": \"number\"}, \"expert_tractability\": {\"description\": \"Probability in (0, 1) that a human subject matter expert can generate a correct implementation of this task\", \"title\": \"Expert Tractability\", \"type\": \"number\"}, \"layman_tractability\": {\"description\": \"Probability in (0, 1) that a layman can generate a correct implementation of this task\", \"title\": \"Layman Tractability\", \"type\": \"number\"}}, \"required\": [\"name\", \"description\", \"relevant_paper_text\", \"scoring_feasibility\", \"llm_tractability\", \"expert_tractability\", \"layman_tractability\"], \"title\": \"TaskCandidate\", \"type\": \"object\"}}, \"properties\": {\"tasks\": {\"description\": \"Tasks extracted from the paper\", \"items\": {\"$ref\": \"#/$defs/TaskCandidate\"}, \"title\": \"Tasks\", \"type\": \"array\"}}, \"required\": [\"tasks\"], \"title\": \"TaskCandidates\", \"type\": \"object\"}\n\n---\n\nPaper Text:\n94% on CIFAR-10 in 3.29 Seconds on a Single GPU\nKeller Jordan\nkjordan4077@gmail.com\nAbstract\nCIFAR-10 is among the most widely used datasets in machine learning, facilitating\nthousands of research projects per year. To accelerate research and reduce the cost\nof experiments, we introduce training methods for CIFAR-10 which reach 94%\naccuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when\nrun on a single NVIDIA A100 GPU. As one factor contributing to these training\nspeeds, we propose a derandomized variant of horizontal flipping augmentation,\nwhich we show improves over the standard method in every case where flipping is\nbeneficial over no flipping at all. Our code is released at https://github.com/\nKellerJordan/cifar10-airbench .\n1 Introduction\nCIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-\ning thousands of research projects per year1. Research can be accelerated and the cost of experiments\nreduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In\nthis paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single\nNVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,\n2023). To support scenarios where higher performance is needed, we additionally develop methods\ntargeting 95% and 96% accuracy. We release the following methods in total.\n1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).\n2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).\n3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).\n4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).\nAll runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-\nmatically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the\nfirst uses torch.compile to improve GPU utilization. It is intended for experiments where many\nnetworks are trained at once in order to amortize the one-time compilation cost. The non-compiled\nairbench94 variant can be easily installed and run using the following command.\n1pip install airbench\n2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"\nOne motivation for the development of these training methods is that they can accelerate the experi-\nmental iteration time of researchers working on compatible projects involving CIFAR-10. Another\nmotivation is that they can decrease the cost of projects involving a massive number of trained\nnetworks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used\n3 million trained networks to demonstrate that the outputs of a trained neural network on a given test\ninput follow an approximately linear function of the vector of binary choices of which examples the\nmodel was trained on. Another example is Jordan (2023), a study on training variance which used\n180 thousand trained networks to show that standard trainings have little variance in performance on\n1https://paperswithcode.com/datasets\nPreprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024\nFigure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal\nflipping augmentation, which flips each image with 50% probability per epoch. This results in some images\nbeing redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a\ndeterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\ntheir test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds\nand 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make\nit possible to replicate these studies, or conduct similar ones, with fewer computational resources.\nFast training also enables the rapid accumulation of statistical significance for subtle hyperparameter\ncomparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10\naccuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation\nbetween runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the\nimprovement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training\nthis will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.\nOur work builds on prior training speed projects. We utilize a modified version of the network,\ninitialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen\npatch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over\nprior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-\nure 1, Section 3.6, Section 5.2).\n2 Background\nOur objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set\nin the shortest possible amount of time. Timing begins when the method is first given access to\ntraining data, and ends when it produces test-set predictions. The method is considered valid if its\nmean accuracy over repeated runs is at least 94%.\nWe chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the\n2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more\nrecent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10\nseconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page\n(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is\ntysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,\n94% is the level of human accuracy reported by Karpathy (2011).\nWe note the following consequences of how the method is timed. First, it is permitted for the program\nto begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins\nwhen the training data is first accessed. This is helpful because otherwise the first run of training\nis typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA\nimproves the performance of a trained network by running it on multiple augmented views of each\ntest input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping\nTTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods\nattain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\nThe CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.\nIt is divided into a training set of 50,000 images and a validation set of 10,000 images. As a\nmatter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was\n2\n80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than\nairbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94\ncan be attributed entirely to algorithmic progress rather than compute scaling.\n3 Methods\n3.1 Network architecture and baseline training\nWe train a convolutional network with a total of 1.97 million parameters, following tysam-code\n(2023) with a few small changes. It contains seven convolutions with the latter six being divided into\nthree blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this\nsection we offer some comments on the main design choices.\nThe network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed\nentirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,\n2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with\nno padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather\nthan the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff\nbetween throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\nFollowing Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,\nand disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is\nscaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture\ndiffers only in that we decrease the number of output channels in the third block from 512 to 256,\nand we add learnable biases to the first convolution.\nAs our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of\n0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the\nmaximum at 20% of the way through training, and then decreases to zero. For data augmentation we\nuse random horizontal flipping alongside 2-pixel random translation. For translation we use reflection\npadding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note\nthat what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a\nrandom 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the\nnetwork is run on both a given test image and its mirror and inferences are made based on the average\nof the two outputs. With optimized choices of learning rate, momentum, and weight decay, this\nbaseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.\n3.2 Frozen patch-whitening initialization\nFollowing Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-\nwhitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code\n(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches\nacross the training distribution, so that their outputs have identity covariance matrix. The second 12\nFigure 2: The first layer\u2019s weights after whitening\ninitialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,\nso that input information is preserved through the\nactivation which follows. Figure 2 shows the result.\nWe do not update this layer\u2019s weights during training.\nDeparting from tysam-code (2023), we add learn-\nable biases to this layer, yielding a small performance\nboost. The biases are trained for 3 epochs, after which\nwe disable their gradient to increase backward-pass\nthroughput, which improves training speed without\nreducing accuracy. We also obtain a slight perfor-\nmance boost relative to tysam-code (2023) by re-\nducing the constant added to the eigenvalues during\ncalculation of the patch-whitening initialization for\nthe purpose of preventing numerical issues in the case\nof a singular patch-covariance matrix.\nPatch-whitening initialization is the single most impactful feature. Adding it to the baseline more\nthan doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.\n3\nRandom reshuffling Alternating flip Mean accuracy\nNo No 93.40%\nNo Yes 93.48%\nYes No 93.92%\nYes Yes 94.01%\nTable 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating\nflip (which we propose) reduce training data redundancy and improve performance.\n3.3 Identity initialization\ndirac : We initialize all convolutions after the first as partial identity transforms. That is, for a\nconvolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an\nidentity transform of the input, and leave the remaining N\u2212Mto their default initialization. In\nPyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight\nwof each convolutional layer. This method partially follows tysam-code (2023), which used a more\ncomplicated scheme where the identity weights are mixed in with the original initialization, which\nwe did not find to be more performant. With this feature added, training attains 94% accuracy in 18\nepochs taking 6.8 A100-seconds.\n3.4 Optimization tricks\nscalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor\nof64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%\nin 13.5 epochs taking 5.1 A100-seconds.\nlookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.\nWe note that Lookahead has also been found effective in prior work on training speed for ResNet-\n18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6\nA100-seconds.\n3.5 Multi-crop evaluation\nmulticrop : To generate predictions, we run the trained network on six augmented views of each\ntest image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a\nversion which is translated down-and-to-the-right by one pixel, and the mirrored versions of all\nthree. Predictions are made using a weighted average of all six outputs, where the two views of the\nuntranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125\neach. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.\nWe note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-\nmonyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of\nevaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using\nmore crops does improve performance, but the increase to inference time outweighs the potential\ntraining speedup.\n3.6 Alternating flip\nTo speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,\nwhich we motivate as follows. When training neural networks, it is standard practice to organize\ntraining into a set of epochs during which every training example is seen exactly once. This\ndiffers from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),\nwhich calls for data to be repeatedly sampled with-replacement from the training set, resulting in\nexamples being potentially seen multiple redundant times within a short window of training. The\nuse of randomly ordered epochs of data for training has a different name, being called the random\nreshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our\ntraining dataset consists of Nunique examples, then sampling data with replacement causes every\n\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N\nunique examples on average. On the other hand, random reshuffling leads to all Nunique examples\nbeing seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason\nthat it is beneficial to maximize the number of unique inputs seen per window of training time.\n4\nWe extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We\nfirst note that standard random horizontal flipping augmentation can be defined as follows.\n1import torch\n2def random_flip ( inputs ):\n3 # Applies random flipping to a batch of images\n4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)\n5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]\nListing 1: Random flip\nIf horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2\nwhich may be seen during training. Potentially, every pair of consecutive epochs could contain every\nunique input. But our main observation is that with standard random horizontal flipping, half of the\nimages will be redundantly flipped the same way during both epochs, so that on average only 1.5N\nunique inputs will be seen.\naltflip : To address this, we propose to modify standard random horizontal flipping augmentation as\nfollows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},\nwe flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we\nflip only those inputs which were flipped in the first epoch. We provide the following implementation\nwhich avoids the need for extra memory by using a pseudorandom function to decide the flips.\n1import torch\n2import hashlib\n3def hash_fn (n, seed =42) :\n4 k = n * seed\n5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,\n16)\n6def alternating_flip (inputs , indices , epoch ):\n7 # Applies alternating flipping to a batch of images\n8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist\n() ])\n9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)\n10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\nListing 2: Alternating flip\nThe result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in\nFigure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.\nAdding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final\ntraining method airbench94.py , the entire contents of which can be found in Section E. It reaches\n94% accuracy in 3.83 seconds on an NVIDIA A100.\n3.7 Compilation\nThe final step we take to speed up training is a non-algorithmic one: we compile our training\nmethod using torch.compile in order to more efficiently utilize the GPU. This results in a training\nscript which is mathematically equivalent (up to small differences in floating point arithmetic) to\nthe non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29\nA100-seconds. The downside is that the one-time compilation process takes up to several minutes to\ncomplete before training runs can begin, so that it is only beneficial when we plan to execute many\nruns of training at once. We release this version as airbench94_compiled.py .\n4 95% and 96% targets\nTo address scenarios where somewhat higher performance is desired, we additionally develop methods\ntargeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .\nTo attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel\ncount of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce\nthe learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%\naccuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.\n2Assuming none of the training inputs are already mirrors of each other.\n5\nFigure 4: Training speedups accumulate additively. Removing individual features from airbench94\nincreases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces\nthe epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-\n94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.\nFigure 3: FLOPs vs. error rate tradeoff. Our\nthree training methods apparently follow a linear\nlog-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-\nVries & Taylor, 2017) augmentation and raise the\ntraining epochs to 40. We add a third convolution to\neach block, and scale the first block to 128 channels\nand the second two to 512. We also add a residual\nconnection across the later two convolutions of each\nblock, which we find is still beneficial despite the fact\nthat we are already using identity initialization (Sec-\ntion 3.3) to ease gradient flow. Finally, we reduce\nthe learning rate by a factor of 0.78. These changes\nyield airbench96 which attains 96.05% accuracy in\n46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.\nFigure 3 shows the FLOPs and error rate of each of\nour three training methods.\n5 Experiments\n5.1 Interaction between features\nTo gain a better sense of the impact of each feature on training speed, we compare two quantities.\nFirst, we measure the number of epochs that can be saved by adding the feature to the whitened\nbaseline (Section 3.2). Second, we measure the number of epochs that must be added when the\nfeature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-\ntion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it\nfrom the final airbench94 increases epochs-to-94% from 9.9 to 12.8.\nFigure 4 shows both quantities for each feature. Surprisingly, we find that for all features except\nmulti-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even\nthough the whitened baseline requires more than twice as many epochs as the final configuration.\nThis indicates that the interaction between most features is additive rather than multiplicative.\n5.2 Does alternating flip generalize?\nIn this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of\ntraining configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all\ncases except those where neither alternating nor random flip improve over using no flipping at all.\nFor CIFAR-10 we consider the performance boost given by alternating flip across the follow-\ning 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and\nairbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For\n6\nFigure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,\nthe use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%\ntraining speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.\n95% confidence intervals are shown around each point.\neach configuration we compare the performance of alternating and random flip in terms of their mean\naccuracy across n= 400 runs of training.\nFigure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip\nimproves performance in every setting. To get a sense for how big the improvement is, we estimate\nthe effective speedup for each case, i.e., the fraction of epochs that could be saved by switching\nfrom random to alternating flip while maintaining the level of accuracy of random flip. We begin\nby fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of\neach random flip-based training configuration. We use these curves to calculate the effective speedup\nafforded by switching from random to alternating flip. For example, airbench94 with random flip\nand without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The\nsame configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law\nfit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.\nNote that using a power-law yields a more conservative estimate relative to using linear interpolation\nbetween the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.\nTable 2 shows the result. We observe the following patterns. First, the addition of extra augmentation\n(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that\nthe main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the\nsame way for many epochs in a row; we speculate that adding extra augmentation reduces the negative\nimpact of these cases because it increases data diversity. Next, TTA reduces the gap between random\nand alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),\nindicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training\nfor longer consistently increases the effective speedup given by alternating flip.\nWe next study ImageNet trainings with the following experiment. We train a ResNet-18 with a\nvariety of train and test crops, comparing three flipping options: alternating flip, random flip, and no\nflipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192\ncenter crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)\nto denote the latter. We also consider two training crops: 192x192 inception-style random resized\ncrop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area\nranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the\nshorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write\n7\nBaseline Cutout Epochs Speedup Speedup (w/ TTA)\nairbench94 No 10 15.0% 5.30%\nairbench94 No 20 27.1% 21.3%\nairbench94 No 40 38.3% 36.4%\nairbench94 No 80 102% 31.8%\nairbench94 Yes 10 3.84% 1.13%\nairbench94 Yes 20 7.42% 2.00%\nairbench94 Yes 40 18.6% 9.28%\nairbench94 Yes 80 29.2% 14.25%\nairbench96 Yes 10 4.94% 1.11%\nairbench96 Yes 20 8.99% 3.58%\nairbench96 Yes 40 17.2% 6.48%\nairbench96 Yes 80 18.8% Not measured\nTable 2: Effective speedups given by switching from random flip to alternating flip. The two configurations\nmost closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw\naccuracy values of the airbench94 experiments.\nFlipping augmentation option\nTrain crop Test crop Epochs TTA None Random Alternating\nHeavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28\nHeavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28\nLight RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26\nLight RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26\nHeavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28\nHeavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28\nLight RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26\nLight RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26\nLight RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16\nHeavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4\nTable 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for\nthose trainings where random flip improves significantly over not flipping at all. The single best flipping option\nin each row is bolded when the difference is statistically significant.\nHeavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided\nin Section C.\nTable 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC\nis better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly\nbetter when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test\nresolution discrepancy (Touvron et al., 2019).\nFor trainings which use Light RRC, we find that switching from random flip to alternating flip\nprovides a substantial boost to performance, amounting to a training speedup of more than 25%.\nIn Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to\nalternating flip improves performance by more than increasing the training duration from 16 to 20\nepochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our\nresults on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit\nfrom alternating flip. Indeed, even turning flipping off completely does not significantly reduce the\nperformance of these trainings. We conclude that alternating flip improves over random flip for every\ntraining scenario where the latter improves over no flipping at all.\n8\nEpochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE\n1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312\n2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233\n1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269\n1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533\n2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433\n1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444\nTable 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).\n5.3 Variance and class-wise calibration\nPrevious sections have focused on understanding what factors affect the first moment of accuracy\n(the mean). In this section we investigate the second moment, finding that TTA reduces variance at\nthe cost of calibration.\nOur experiment is to execute 10,000 runs of airbench94 training with several hyperparameter\nsettings. For each setting we report both the variance in test-set accuracy as well as an estimate of the\ndistribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.\nTable 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set\nvariance, replicating the main finding of Jordan (2023). This is a surprising result because these\ntrainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7\nas much distribution-wise variance when run for a similar duration, and reached a low variance only\nwhen run for 64 epochs. We conclude from this comparison that distribution-wise variance is more\nstrongly connected to the rate of convergence of a training rather than its duration as such. We also\nnote that the low distribution-wise variance of airbench94 indicates it has high training stability.\nUsing TTA significantly reduces the test-set variance, such that all three settings with TTA have lower\ntest-set variance than any setting without TTA. However, test-set variance is implied by the class-wise\ncalibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this\nreduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,\nwe compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which\nmeasures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA\nhas a higher CACE than every setting without TTA, confirming the hypothesis.\n6 Discussion\nIn this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7\nfaster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the\nairbench Python package.\nWe developed airbench solely with the goal of maximizing training speed on CIFAR-10. In\nSection B we find that it also generalizes well to other tasks. For example, without any extra tuning,\nairbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.\nOne factor contributing to the training speed of airbench was our finding that training can be accel-\nerated by partially derandomizing the standard random horizontal flipping augmentation, resulting in\nthe variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating\nflip improves the performance of every training we considered (Section 5.2), with the exception of\nthose trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to\nus, the standard ImageNet trainings that we considered do not significantly benefit from horizontal\nflipping. Future work might investigate whether it is possible to obtain derandomized improvements\nto other augmentations besides horizontal flip.\nThe methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,\nwith fixed performance and hardware constraints. These constraints mean that we cannot improve\nperformance by simply scaling up the amount of computational resources used; instead we are forced\nto develop new methods like the alternating flip. We look forward to seeing what other new methods\nfuture work discovers to push training speed further.\n9\nReferences\nDimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.\nDan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-\nperformance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,\n2011.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\nmark and competition. 2017.\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet\nor cifar-10. arXiv preprint arXiv:1810.03505 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\npp. 248\u2013255. IEEE, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552 , 2017.\nMert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic\ngradient descent. Mathematical Programming , 186:49\u201384, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 , 2016.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.\npmlr, 2015.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\nsgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.\nKeller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and\ninevitable. arXiv preprint arXiv:2304.01910 , 2023.\nAndrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.\nio/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for\nadvanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT\nLicense.\nGuillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander\nM \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,\nBenjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.\nBenchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural\nInformation Processing Systems , 35:25404\u201325421, 2022.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.\nDavid Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/\nhow-to-train-your-resnet-8-bag-of-tricks/ .\n10\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics , 22(3):400\u2013407, 1951.\nSantiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:\n//github.com/apple/ml-cifar-10-faster , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.\ntysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/\nhlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. Advances in neural information processing systems , 32, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,\n2016.\nMichael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps\nforward, 1 step back. Advances in neural information processing systems , 32, 2019.\n11\nA Network architecture\n1from torch import nn\n2\n3class Flatten (nn. Module ):\n4 def forward (self , x):\n5 return x. view (x. size (0) , -1)\n6\n7class Mul (nn. Module ):\n8 def __init__ (self , scale ):\n9 super (). __init__ ()\n10 self . scale = scale\n11 def forward (self , x):\n12 return x * self . scale\n13\n14def conv (ch_in , ch_out ):\n15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,\n16 padding =\u2019same \u2019, bias = False )\n17\n18def make_net ():\n19 act = lambda : nn. GELU ()\n20 bn = lambda ch: nn. BatchNorm2d (ch)\n21 return nn. Sequential (\n22 nn. Sequential (\n23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),\n24 act () ,\n25 ),\n26 nn. Sequential (\n27 conv (24 , 64) ,\n28 nn. MaxPool2d (2) ,\n29 bn (64) , act () ,\n30 conv (64 , 64) ,\n31 bn (64) , act () ,\n32 ),\n33 nn. Sequential (\n34 conv (64 , 256) ,\n35 nn. MaxPool2d (2) ,\n36 bn (256) , act () ,\n37 conv (256 , 256) ,\n38 bn (256) , act () ,\n39 ),\n40 nn. Sequential (\n41 conv (256 , 256) ,\n42 nn. MaxPool2d (2) ,\n43 bn (256) , act () ,\n44 conv (256 , 256) ,\n45 bn (256) , act () ,\n46 ),\n47 nn. MaxPool2d (3) ,\n48 Flatten () ,\n49 nn. Linear (256 , 10, bias = False ),\n50 Mul (1/9) ,\n51 )\nListing 3: Minimal PyTorch code for the network architecture used by airbench94 .\nWe note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock\ntime. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel\nsize of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this\ndoes not improve the wallclock training time on an A100. Reducing the batch size is another easy\nway to save FLOPs but not wallclock time.\n12\nDataset Flipping? Cutout? ResNet-18 airbench96\nCIFAR-10 Yes No 95.55% 95.61%\nCIFAR-10 Yes Yes 96.01% 96.05%\nCIFAR-100 Yes No 77.54% 79.27%\nCIFAR-100 Yes Yes 78.04% 79.76%\nCINIC-10 Yes No 87.58% 87.78%\nCINIC-10 Yes Yes not measured 88.22%\nSVHN No No 97.35% 97.38%\nSVHN No Yes not measured 97.64%\nTable 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We\ndirectly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off\nflipping for SVHN).\nB Extra dataset experiments\nWe developed airbench with the singular goal of maximizing training speed on CIFAR-10. To\nfind out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its\nperformance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-\n10 (Darlow et al., 2018).\nOn CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both\nthe case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not\n(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than\nResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say\nthat it generalizes.\nWe compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.\nWe do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal\non CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better\nperformance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where\nairbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is\nused and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows\nstrong generalization to other tasks.\nWe note that this comparison between airbench96 and ResNet-18 training is fair in the sense that\nit does demonstrate that the former has good generalization, but unfair in the sense that it does not\nindicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time\naugmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training\nwould outperform airbench96 if it were run using test-time augmentation. However, it also takes\n5-10 times longer to complete. The decision of which to use may be situational.\nThe accuracy values we report for ResNet-18 training are from the following sources. We tried\nto select the highest values we could find for each setting. Moreau et al. (2022) reports attaining\n95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports\nattaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on\nCIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.\nC ImageNet training details\nOur ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/\nlibffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and\nmomentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution\n160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of\ntraining. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.\n13\nHyperparameters Flipping augmentation option\nEpochs Cutout TTA None Random Alternating\n10 No No 92.3053 93.0988 93.2798\n20 No No 92.8166 93.7446 93.8652\n40 No No 93.0143 94.0133 94.0729\n80 No No 93.0612 94.1169 94.1628\n10 No Yes 93.4071 93.9488 94.0186\n20 No Yes 93.8528 94.5565 94.6530\n40 No Yes 94.0381 94.7803 94.8203\n80 No Yes 94.0638 94.8506 94.8676\n10 Yes No 91.8487 92.0402 92.1374\n20 Yes No 92.8474 93.3825 93.4876\n40 Yes No 93.2675 94.1014 94.1952\n80 Yes No 93.4193 94.4311 94.5204\n10 Yes Yes 92.6455 92.7780 92.8103\n20 Yes Yes 93.7862 94.1306 94.1670\n40 Yes Yes 94.3090 94.8511 94.8960\n80 Yes Yes 94.5253 95.1839 95.2362\nTable 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a\nmean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise\ndifference in means is statistically significant.\nD Extra tables & figures\nFigure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in\nSection 5.3.\nE Complete training code\n1\"\"\"\n2airbench94 .py\n33.83 s runtime on an A100 ; 0.36 PFLOPs .\n4Evidence for validity : 94.01 average accuracy in n =1000 runs .\n5\n6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB\nwith the following nvidia -smi:\n7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :\n11.7\n14\n8torch . __version__ == \u20192.1.2+ cu118 \u2019\n9\"\"\"\n10\n11# ############################################\n12# Setup / Hyperparameters #\n13# ############################################\n14\n15import os\n16import sys\n17import uuid\n18from math import ceil\n19\n20import torch\n21from torch import nn\n22import torch .nn. functional as F\n23import torchvision\n24import torchvision . transforms as T\n25\n26torch . backends . cudnn . benchmark = True\n27\n28\"\"\"\n29We express the main training hyperparameters ( batch size , learning\nrate , momentum , and weight decay ) in decoupled form , so that each\none can be tuned independently . This accomplishes the following :\n30* Assuming time - constant gradients , the average step size is decoupled\nfrom everything but the lr.\n31* The size of the weight decay update is decoupled from everything but\nthe wd.\n32In constrast , normally when we increase the ( Nesterov ) momentum , this\nalso scales up the step size proportionally to 1 + 1 / (1 -\nmomentum ), meaning we cannot change momentum without having to re -\ntune the learning rate . Similarly , normally when we increase the\nlearning rate this also increases the size of the weight decay ,\nrequiring a proportional decrease in the wd to maintain the same\ndecay strength .\n33\n34The practical impact is that hyperparameter tuning is faster , since\nthis parametrization allows each one to be tuned independently .\nSee https :// myrtle .ai/ learn /how -to -train -your - resnet -5-\nhyperparameters /.\n35\"\"\"\n36\n37hyp = {\n38 \u2019opt \u2019: {\n39 \u2019 train_epochs \u2019: 9.9 ,\n40 \u2019batch_size \u2019: 1024 ,\n41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples\n42 \u2019momentum \u2019: 0.85 ,\n43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (\ndecoupled from learning rate )\n44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not\nweight decay ) for BatchNorm biases\n45 \u2019 label_smoothing \u2019: 0.2 ,\n46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the\nwhitening layer bias before freezing\n47 },\n48 \u2019aug \u2019: {\n49 \u2019flip \u2019: True ,\n50 \u2019translate \u2019: 2,\n51 },\n52 \u2019net \u2019: {\n53 \u2019widths \u2019: {\n54 \u2019block1 \u2019: 64,\n55 \u2019block2 \u2019: 256 ,\n56 \u2019block3 \u2019: 256 ,\n15\n57 },\n58 \u2019 batchnorm_momentum \u2019: 0.6 ,\n59 \u2019 scaling_factor \u2019: 1/9 ,\n60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :\n0= none , 1= mirror , 2= mirror + translate\n61 },\n62}\n63\n64# ############################################\n65# DataLoader #\n66# ############################################\n67\n68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )\n69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )\n70\n71def batch_flip_lr ( inputs ):\n72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .\nview (-1, 1, 1, 1)\n73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\n74\n75def batch_crop (images , crop_size ):\n76 r = ( images . size ( -1) - crop_size )//2\n77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =\nimages . device )\n78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),\ndevice = images .device , dtype = images . dtype )\n79 # The two cropping methods in this if - else produce equivalent\nresults , but the second is faster for r > 2.\n80 if r <= 2:\n81 for sy in range (-r, r +1):\n82 for sx in range (-r, r +1):\n83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)\n84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size\n, r+sx:r+sx+ crop_size ]\n85 else :\n86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size\n+2* r), device = images .device , dtype = images . dtype )\n87 for s in range (-r, r +1):\n88 mask = ( shifts [:, 0] == s)\n89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]\n90 for s in range (-r, r +1):\n91 mask = ( shifts [:, 1] == s)\n92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+\ncrop_size ]\n93 return images_out\n94\n95class CifarLoader :\n96 \"\"\"\n97 GPU - accelerated dataloader for CIFAR -10 which implements\nalternating flip augmentation .\n98 \"\"\"\n99\n100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,\ndrop_last =None , shuffle =None , gpu =0):\n101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .\npt \u2019)\n102 if not os. path . exists ( data_path ):\n103 dset = torchvision . datasets . CIFAR10 (path , download =True ,\ntrain = train )\n104 images = torch . tensor ( dset . data )\n105 labels = torch . tensor ( dset . targets )\n106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:\ndset . classes }, data_path )\n107\n108 data = torch . load ( data_path , map_location = torch . device (gpu))\n16\n109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [\n\u2019labels \u2019], data [\u2019classes \u2019]\n110 # It \u2019s faster to load + process uint8 data than to load\npreprocessed fp16 data\n111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).\nto( memory_format = torch . channels_last )\n112\n113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )\n114 self . proc_images = {} # Saved results of image processing to\nbe done on the first epoch\n115 self . epoch = 0\n116\n117 self . aug = aug or {}\n118 for k in self .aug . keys ():\n119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019\n% k\n120\n121 self . batch_size = batch_size\n122 self . drop_last = train if drop_last is None else drop_last\n123 self . shuffle = train if shuffle is None else shuffle\n124\n125 def __len__ ( self ):\n126 return len( self . images )// self . batch_size if self . drop_last\nelse ceil (len( self . images )/ self . batch_size )\n127\n128 def __iter__ ( self ):\n129\n130 if self . epoch == 0:\n131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .\nimages )\n132 # Randomly flip all images on the first epoch as according\nto definition of alternating flip\n133 if self .aug.get(\u2019flip \u2019, False ):\n134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (\nimages )\n135 # Pre -pad images to save time when doing random\ntranslation\n136 pad = self .aug.get (\u2019translate \u2019, 0)\n137 if pad > 0:\n138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019\nreflect \u2019)\n139\n140 if self .aug.get(\u2019translate \u2019, 0) > 0:\n141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .\nshape [ -2])\n142 elif self .aug.get (\u2019flip \u2019, False ):\n143 images = self . proc_images [\u2019flip \u2019]\n144 else :\n145 images = self . proc_images [\u2019norm \u2019]\n146 if self .aug.get(\u2019flip \u2019, False ):\n147 if self . epoch % 2 == 1:\n148 images = images . flip ( -1)\n149\n150 self . epoch += 1\n151\n152 indices = ( torch . randperm if self . shuffle else torch . arange )(\nlen ( images ), device = images . device )\n153 for i in range (len ( self )):\n154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]\n155 yield ( images [ idxs ], self . labels [ idxs ])\n156\n157# ############################################\n158# Network Components #\n159# ############################################\n160\n17\n161class Flatten (nn. Module ):\n162 def forward (self , x):\n163 return x. view (x. size (0) , -1)\n164\n165class Mul (nn. Module ):\n166 def __init__ (self , scale ):\n167 super (). __init__ ()\n168 self . scale = scale\n169 def forward (self , x):\n170 return x * self . scale\n171\n172class BatchNorm (nn. BatchNorm2d ):\n173 def __init__ (self , num_features , momentum , eps =1e -12 ,\n174 weight =False , bias = True ):\n175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )\n176 self . weight . requires_grad = weight\n177 self . bias . requires_grad = bias\n178 # Note that PyTorch already initializes the weights to one and\nbiases to zero\n179\n180class Conv (nn. Conv2d ):\n181 def __init__ (self , in_channels , out_channels , kernel_size =3,\npadding =\u2019same \u2019, bias = False ):\n182 super (). __init__ ( in_channels , out_channels , kernel_size =\nkernel_size , padding = padding , bias = bias )\n183\n184 def reset_parameters ( self ):\n185 super (). reset_parameters ()\n186 if self . bias is not None :\n187 self . bias . data . zero_ ()\n188 w = self . weight . data\n189 torch .nn. init . dirac_ (w[:w. size (1) ])\n190\n191class ConvGroup (nn. Module ):\n192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):\n193 super (). __init__ ()\n194 self . conv1 = Conv ( channels_in , channels_out )\n195 self . pool = nn. MaxPool2d (2)\n196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )\n197 self . conv2 = Conv ( channels_out , channels_out )\n198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )\n199 self . activ = nn. GELU ()\n200\n201 def forward (self , x):\n202 x = self . conv1 (x)\n203 x = self . pool (x)\n204 x = self . norm1 (x)\n205 x = self . activ (x)\n206 x = self . conv2 (x)\n207 x = self . norm2 (x)\n208 x = self . activ (x)\n209 return x\n210\n211# ############################################\n212# Network Definition #\n213# ############################################\n214\n215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019\n][ \u2019 batchnorm_momentum \u2019]):\n216 whiten_kernel_size = 2\n217 whiten_width = 2 * 3 * whiten_kernel_size **2\n218 net = nn. Sequential (\n219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True\n),\n220 nn. GELU () ,\n18\n221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],\nbatchnorm_momentum ),\n222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],\nbatchnorm_momentum ),\n223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],\nbatchnorm_momentum ),\n224 nn. MaxPool2d (3) ,\n225 Flatten () ,\n226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),\n227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,\n228 )\n229 net [0]. weight . requires_grad = False\n230 net = net. half (). cuda ()\n231 net = net.to( memory_format = torch . channels_last )\n232 for mod in net. modules ():\n233 if isinstance (mod , BatchNorm ):\n234 mod . float ()\n235 return net\n236\n237# ############################################\n238# Whitening Conv Initialization #\n239# ############################################\n240\n241def get_patches (x, patch_shape ):\n242 c, (h, w) = x. shape [1] , patch_shape\n243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h\n,w). float ()\n244\n245def get_whitening_parameters ( patches ):\n246 n,c,h,w = patches . shape\n247 patches_flat = patches . view (n, -1)\n248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n\n249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance\n, UPLO =\u2019U\u2019)\n250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.\nreshape (c*h*w,c,h,w). flip (0)\n251\n252def init_whitening_conv (layer , train_set , eps =5e -4):\n253 patches = get_patches ( train_set , patch_shape = layer . weight . data .\nshape [2:])\n254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )\n255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)\n256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -\neigenvectors_scaled ))\n257\n258# ###########################################\n259# Lookahead #\n260# ###########################################\n261\n262class LookaheadState :\n263 def __init__ (self , net):\n264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().\nitems ()}\n265\n266 def update (self , net , decay ):\n267 for ema_param , net_param in zip( self . net_ema . values () , net.\nstate_dict (). values ()):\n268 if net_param . dtype in ( torch .half , torch . float ):\n269 ema_param . lerp_ ( net_param , 1- decay )\n270 net_param . copy_ ( ema_param )\n271\n272# ###########################################\n273# Logging #\n274# ###########################################\n275\n19\n276def print_columns ( columns_list , is_head =False , is_final_entry = False ):\n277 print_string = \u2019\u2019\n278 for col in columns_list :\n279 print_string += \u2019| %s \u2019 % col\n280 print_string += \u2019|\u2019\n281 if is_head :\n282 print (\u2019-\u2019*len( print_string ))\n283 print ( print_string )\n284 if is_head or is_final_entry :\n285 print (\u2019-\u2019*len( print_string ))\n286\n287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,\n\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]\n288def print_training_details ( variables , is_final_entry ):\n289 formatted = []\n290 for col in logging_columns_list :\n291 var = variables .get (col. strip () , None )\n292 if type (var) in (int , str):\n293 res = str(var)\n294 elif type (var) is float :\n295 res = \u2019{:0.4 f}\u2019. format ( var)\n296 else :\n297 assert var is None\n298 res = \u2019\u2019\n299 formatted . append (res. rjust (len (col)))\n300 print_columns ( formatted , is_final_entry = is_final_entry )\n301\n302# ###########################################\n303# Evaluation #\n304# ###########################################\n305\n306def infer (model , loader , tta_level =0) :\n307 \"\"\"\n308 Test - time augmentation strategy (for tta_level =2):\n309 1. Flip / mirror the image left -to - right (50% of the time ).\n310 2. Translate the image by one pixel either up -and - left or down -and\n- right (50% of the time , i.e. both happen 25% of the time ).\n311\n312 This creates 6 views per image ( left / right times the two\ntranslations and no - translation ), which we evaluate and then\nweight according to the given probabilities .\n313 \"\"\"\n314\n315 def infer_basic (inputs , net):\n316 return net( inputs ). clone ()\n317\n318 def infer_mirror (inputs , net):\n319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))\n320\n321 def infer_mirror_translate ( inputs , net ):\n322 logits = infer_mirror (inputs , net )\n323 pad = 1\n324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)\n325 inputs_translate_list = [\n326 padded_inputs [:, :, 0:32 , 0:32] ,\n327 padded_inputs [:, :, 2:34 , 2:34] ,\n328 ]\n329 logits_translate_list = [ infer_mirror ( inputs_translate , net)\n330 for inputs_translate in\ninputs_translate_list ]\n331 logits_translate = torch . stack ( logits_translate_list ). mean (0)\n332 return 0.5 * logits + 0.5 * logits_translate\n333\n334 model . eval ()\n335 test_images = loader . normalize ( loader . images )\n20\n336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][\ntta_level ]\n337 with torch . no_grad ():\n338 return torch .cat ([ infer_fn (inputs , model ) for inputs in\ntest_images . split (2000) ])\n339\n340def evaluate (model , loader , tta_level =0) :\n341 logits = infer (model , loader , tta_level )\n342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()\n343\n344# ###########################################\n345# Training #\n346# ###########################################\n347\n348def main (run):\n349\n350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]\n351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]\n352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]\n353 # Assuming gradients are constant in time , for Nesterov momentum ,\nthe below ratio is how much larger the default steps will be than\nthe underlying per - example gradients . We divide the learning rate\nby this ratio in order to ensure steps are the same scale as\ngradients , regardless of the choice of momentum .\n354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))\n355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning\nrate for PyTorch SGD\n356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale\n357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]\n358\n359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019\nlabel_smoothing \u2019], reduction =\u2019none \u2019)\n360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)\n361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =\nbatch_size , aug=hyp[\u2019aug \u2019])\n362 if run == \u2019warmup \u2019:\n363 # The only purpose of the first run is to warmup , so we can\nuse dummy data\n364 train_loader . labels = torch . randint (0, 10, size =( len (\ntrain_loader . labels ) ,), device = train_loader . labels . device )\n365 total_train_steps = ceil (len( train_loader ) * epochs )\n366\n367 model = make_net ()\n368 current_steps = 0\n369\n370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in\nk and p. requires_grad ]\n371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019\nnot in k and p. requires_grad ]\n372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,\nweight_decay =wd/ lr_biases ),\n373 dict ( params = other_params , lr=lr , weight_decay =wd/\nlr)]\n374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,\nnesterov = True )\n375\n376 def triangle (steps , start =0, end =0, peak =0.5) :\n377 xp = torch . tensor ([0 , int( peak * steps ), steps ])\n378 fp = torch . tensor ([ start , 1, end ])\n379 x = torch . arange (1+ steps )\n380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])\n381 b = fp [: -1] - (m * xp [: -1])\n382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1\n383 indices = torch . clamp ( indices , 0, len(m) - 1)\n384 return m[ indices ] * x + b[ indices ]\n21\n385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,\npeak =0.23)\n386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:\nlr_schedule [i])\n387\n388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /\ntotal_train_steps )**3\n389 lookahead_state = LookaheadState ( model )\n390\n391 # For accurately timing GPU code\n392 starter = torch . cuda . Event ( enable_timing = True )\n393 ender = torch . cuda . Event ( enable_timing = True )\n394 total_time_seconds = 0.0\n395\n396 # Initialize the first layer using statistics of training images\n397 starter . record ()\n398 train_images = train_loader . normalize ( train_loader . images [:5000])\n399 init_whitening_conv ( model [0] , train_images )\n400 ender . record ()\n401 torch . cuda . synchronize ()\n402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n403\n404 for epoch in range ( ceil ( epochs )):\n405\n406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019\nwhiten_bias_epochs \u2019])\n407\n408 # ###################\n409 # Training #\n410 # ###################\n411\n412 starter . record ()\n413\n414 model . train ()\n415 for inputs , labels in train_loader :\n416\n417 outputs = model ( inputs )\n418 loss = loss_fn ( outputs , labels ).sum ()\n419 optimizer . zero_grad ( set_to_none = True )\n420 loss . backward ()\n421 optimizer . step ()\n422 scheduler . step ()\n423\n424 current_steps += 1\n425\n426 if current_steps % 5 == 0:\n427 lookahead_state . update (model , decay = alpha_schedule [\ncurrent_steps ]. item ())\n428\n429 if current_steps >= total_train_steps :\n430 if lookahead_state is not None :\n431 lookahead_state . update (model , decay =1.0)\n432 break\n433\n434 ender . record ()\n435 torch . cuda . synchronize ()\n436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n437\n438 # ###################\n439 # Evaluation #\n440 # ###################\n441\n442 # Print the accuracy and loss from the last training batch of\nthe epoch\n22\n443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().\nmean (). item ()\n444 train_loss = loss . item () / batch_size\n445 val_acc = evaluate (model , test_loader , tta_level =0)\n446 print_training_details ( locals () , is_final_entry = False )\n447 run = None # Only print the run number once\n448\n449 # ###################\n450 # TTA Evaluation #\n451 # ###################\n452\n453 starter . record ()\n454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019\ntta_level \u2019])\n455 ender . record ()\n456 torch . cuda . synchronize ()\n457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n458\n459 epoch = \u2019eval \u2019\n460 print_training_details ( locals () , is_final_entry = True )\n461\n462 return tta_val_acc\n463\n464if __name__ == \" __main__ \":\n465 with open (sys. argv [0]) as f:\n466 code = f. read ()\n467\n468 print_columns ( logging_columns_list , is_head = True )\n469 main (\u2019warmup \u2019)\n470 accs = torch . tensor ([ main (run) for run in range (25) ])\n471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))\n472\n473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }\n474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))\n475 os. makedirs ( log_dir , exist_ok = True )\n476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)\n477 print (os. path . abspath ( log_path ))\n478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))\nListing 4: airbench94.py\n23\n\nGuidance:\nSelf Critique:\n\nUpon reviewing my task candidate extraction and evaluation, I can identify several areas for improvement:\n\n1. Quantity of task candidates: I only extracted three task candidates, which is likely insufficient to cover the full range of research and engineering tasks described in the paper. I should have extracted more candidates to provide a more comprehensive set of options.\n\n2. Research vs Engineering balance: The tasks I extracted lean more heavily towards engineering implementation rather than research. I should have sought out more tasks that require deeper understanding and application of the research concepts presented in the paper.\n\n3. Missed important tasks: I overlooked several significant tasks described in the paper, such as implementing the identity initialization for convolutions, the Lookahead optimization, and the overall network architecture. These are important components of the research that should have been included as task candidates.\n\n4. Depth of analysis: While I provided some evaluation of the tasks against the criteria, my analysis could have been more thorough and systematic, addressing each point in the requirements more explicitly for each task.\n\n5. Scoring function consideration: I didn't give enough attention to the requirement of having easily implementable scoring functions that are much simpler than the tasks themselves. This is a crucial aspect that I should have emphasized more in my evaluation.\n\n6. Relevance to paper's core contributions: I could have focused more on tasks that directly relate to the paper's main contributions and novel aspects, rather than including more general ML implementation tasks.\n\nTo improve, I should re-examine the paper more carefully, extract a wider range of task candidates that better balance research and engineering aspects, and provide a more thorough evaluation of each candidate against all the specified criteria.\n\nPast Error in Task Candidates: ValueError('json output should start and end with { and }')\n\nTask Candidates:"
  },
  {
    "Response": "{\n  \"tasks\": [\n    {\n      \"name\": \"Implement Alternating Flip Data Augmentation\",\n      \"description\": \"Implement the alternating flip data augmentation method described in the paper. This method deterministically flips images in alternating epochs after the first epoch, avoiding redundancy and speeding up training.\",\n      \"relevant_paper_text\": \"To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\",\n      \"scoring_feasibility\": 9,\n      \"llm_tractability\": 0.8,\n      \"expert_tractability\": 0.95,\n      \"layman_tractability\": 0.3\n    },\n    {\n      \"name\": \"Implement Patch-Whitening Initialization\",\n      \"description\": \"Implement the patch-whitening initialization for the first convolutional layer as described in the paper. This involves initializing the layer's weights based on the eigenvectors of the covariance matrix of 2x2 patches across the training distribution.\",\n      \"relevant_paper_text\": \"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\",\n      \"scoring_feasibility\": 8,\n      \"llm_tractability\": 0.6,\n      \"expert_tractability\": 0.9,\n      \"layman_tractability\": 0.1\n    },\n    {\n      \"name\": \"Implement Multi-Crop Test-Time Augmentation\",\n      \"description\": \"Implement the multi-crop test-time augmentation (TTA) method described in the paper. This involves running the trained network on six augmented views of each test image and combining the outputs using a weighted average.\",\n      \"relevant_paper_text\": \"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\",\n      \"scoring_feasibility\": 9,\n      \"llm_tractability\": 0.7,\n      \"expert_tractability\": 0.95,\n      \"layman_tractability\": 0.4\n    },\n    {\n      \"name\": \"Implement Identity Initialization for Convolutions\",\n      \"description\": \"Implement the identity initialization method for convolutional layers after the first layer, as described in the paper. This involves initializing the first M filters of each convolution with N>=M outputs as an identity transform of the input.\",\n      \"relevant_paper_text\": \"We initialize all convolutions after the first as partial identity transforms. That is, for a convolution with M input channels and N>=M outputs, we initialize its first M filters to an identity transform of the input, and leave the remaining N-M to their default initialization.\",\n      \"scoring_feasibility\": 8,\n      \"llm_tractability\": 0.7,\n      \"expert_tractability\": 0.9,\n      \"layman_tractability\": 0.2\n    },\n    {\n      \"name\": \"Implement Lookahead Optimization\",\n      \"description\": \"Implement the Lookahead optimization method as described in the paper. This involves maintaining a slow-moving copy of the model parameters and periodically updating the fast-moving parameters towards the slow-moving ones.\",\n      \"relevant_paper_text\": \"Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization. We note that Lookahead has also been found effective in prior work on training speed for ResNet-18 (Moreau et al., 2022).\",\n      \"scoring_feasibility\": 7,\n      \"llm_tractability\": 0.6,\n      \"expert_tractability\": 0.85,\n      \"layman_tractability\": 0.1\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `candidate_input`, `candidate_output`, `requirements`, `attempt_num`, `previous_critiques`, produce the fields `self_critique`, `should_retry`.\n\n---\n\nFollow the following format.\n\nCandidate Input: ${candidate_input}\n\nCandidate Output: ${candidate_output}\n\nRequirements: ${requirements}\n\nAttempt Num: ${attempt_num}\n\nPrevious Critiques: ${previous_critiques}\n\nSelf Critique: Self-critique of the task candidate\n\nShould Retry: Should the task candidate be retried? This may be expensive. True/False (Respond with true or false)\n\n---\n\nCandidate Input:\n94% on CIFAR-10 in 3.29 Seconds on a Single GPU\nKeller Jordan\nkjordan4077@gmail.com\nAbstract\nCIFAR-10 is among the most widely used datasets in machine learning, facilitating\nthousands of research projects per year. To accelerate research and reduce the cost\nof experiments, we introduce training methods for CIFAR-10 which reach 94%\naccuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when\nrun on a single NVIDIA A100 GPU. As one factor contributing to these training\nspeeds, we propose a derandomized variant of horizontal flipping augmentation,\nwhich we show improves over the standard method in every case where flipping is\nbeneficial over no flipping at all. Our code is released at https://github.com/\nKellerJordan/cifar10-airbench .\n1 Introduction\nCIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-\ning thousands of research projects per year1. Research can be accelerated and the cost of experiments\nreduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In\nthis paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single\nNVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,\n2023). To support scenarios where higher performance is needed, we additionally develop methods\ntargeting 95% and 96% accuracy. We release the following methods in total.\n1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).\n2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).\n3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).\n4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).\nAll runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-\nmatically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the\nfirst uses torch.compile to improve GPU utilization. It is intended for experiments where many\nnetworks are trained at once in order to amortize the one-time compilation cost. The non-compiled\nairbench94 variant can be easily installed and run using the following command.\n1pip install airbench\n2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"\nOne motivation for the development of these training methods is that they can accelerate the experi-\nmental iteration time of researchers working on compatible projects involving CIFAR-10. Another\nmotivation is that they can decrease the cost of projects involving a massive number of trained\nnetworks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used\n3 million trained networks to demonstrate that the outputs of a trained neural network on a given test\ninput follow an approximately linear function of the vector of binary choices of which examples the\nmodel was trained on. Another example is Jordan (2023), a study on training variance which used\n180 thousand trained networks to show that standard trainings have little variance in performance on\n1https://paperswithcode.com/datasets\nPreprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024\nFigure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal\nflipping augmentation, which flips each image with 50% probability per epoch. This results in some images\nbeing redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a\ndeterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\ntheir test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds\nand 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make\nit possible to replicate these studies, or conduct similar ones, with fewer computational resources.\nFast training also enables the rapid accumulation of statistical significance for subtle hyperparameter\ncomparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10\naccuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation\nbetween runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the\nimprovement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training\nthis will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.\nOur work builds on prior training speed projects. We utilize a modified version of the network,\ninitialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen\npatch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over\nprior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-\nure 1, Section 3.6, Section 5.2).\n2 Background\nOur objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set\nin the shortest possible amount of time. Timing begins when the method is first given access to\ntraining data, and ends when it produces test-set predictions. The method is considered valid if its\nmean accuracy over repeated runs is at least 94%.\nWe chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the\n2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more\nrecent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10\nseconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page\n(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is\ntysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,\n94% is the level of human accuracy reported by Karpathy (2011).\nWe note the following consequences of how the method is timed. First, it is permitted for the program\nto begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins\nwhen the training data is first accessed. This is helpful because otherwise the first run of training\nis typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA\nimproves the performance of a trained network by running it on multiple augmented views of each\ntest input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping\nTTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods\nattain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\nThe CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.\nIt is divided into a training set of 50,000 images and a validation set of 10,000 images. As a\nmatter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was\n2\n80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than\nairbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94\ncan be attributed entirely to algorithmic progress rather than compute scaling.\n3 Methods\n3.1 Network architecture and baseline training\nWe train a convolutional network with a total of 1.97 million parameters, following tysam-code\n(2023) with a few small changes. It contains seven convolutions with the latter six being divided into\nthree blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this\nsection we offer some comments on the main design choices.\nThe network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed\nentirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,\n2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with\nno padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather\nthan the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff\nbetween throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\nFollowing Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,\nand disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is\nscaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture\ndiffers only in that we decrease the number of output channels in the third block from 512 to 256,\nand we add learnable biases to the first convolution.\nAs our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of\n0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the\nmaximum at 20% of the way through training, and then decreases to zero. For data augmentation we\nuse random horizontal flipping alongside 2-pixel random translation. For translation we use reflection\npadding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note\nthat what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a\nrandom 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the\nnetwork is run on both a given test image and its mirror and inferences are made based on the average\nof the two outputs. With optimized choices of learning rate, momentum, and weight decay, this\nbaseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.\n3.2 Frozen patch-whitening initialization\nFollowing Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-\nwhitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code\n(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches\nacross the training distribution, so that their outputs have identity covariance matrix. The second 12\nFigure 2: The first layer\u2019s weights after whitening\ninitialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,\nso that input information is preserved through the\nactivation which follows. Figure 2 shows the result.\nWe do not update this layer\u2019s weights during training.\nDeparting from tysam-code (2023), we add learn-\nable biases to this layer, yielding a small performance\nboost. The biases are trained for 3 epochs, after which\nwe disable their gradient to increase backward-pass\nthroughput, which improves training speed without\nreducing accuracy. We also obtain a slight perfor-\nmance boost relative to tysam-code (2023) by re-\nducing the constant added to the eigenvalues during\ncalculation of the patch-whitening initialization for\nthe purpose of preventing numerical issues in the case\nof a singular patch-covariance matrix.\nPatch-whitening initialization is the single most impactful feature. Adding it to the baseline more\nthan doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.\n3\nRandom reshuffling Alternating flip Mean accuracy\nNo No 93.40%\nNo Yes 93.48%\nYes No 93.92%\nYes Yes 94.01%\nTable 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating\nflip (which we propose) reduce training data redundancy and improve performance.\n3.3 Identity initialization\ndirac : We initialize all convolutions after the first as partial identity transforms. That is, for a\nconvolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an\nidentity transform of the input, and leave the remaining N\u2212Mto their default initialization. In\nPyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight\nwof each convolutional layer. This method partially follows tysam-code (2023), which used a more\ncomplicated scheme where the identity weights are mixed in with the original initialization, which\nwe did not find to be more performant. With this feature added, training attains 94% accuracy in 18\nepochs taking 6.8 A100-seconds.\n3.4 Optimization tricks\nscalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor\nof64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%\nin 13.5 epochs taking 5.1 A100-seconds.\nlookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.\nWe note that Lookahead has also been found effective in prior work on training speed for ResNet-\n18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6\nA100-seconds.\n3.5 Multi-crop evaluation\nmulticrop : To generate predictions, we run the trained network on six augmented views of each\ntest image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a\nversion which is translated down-and-to-the-right by one pixel, and the mirrored versions of all\nthree. Predictions are made using a weighted average of all six outputs, where the two views of the\nuntranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125\neach. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.\nWe note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-\nmonyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of\nevaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using\nmore crops does improve performance, but the increase to inference time outweighs the potential\ntraining speedup.\n3.6 Alternating flip\nTo speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,\nwhich we motivate as follows. When training neural networks, it is standard practice to organize\ntraining into a set of epochs during which every training example is seen exactly once. This\ndiffers from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),\nwhich calls for data to be repeatedly sampled with-replacement from the training set, resulting in\nexamples being potentially seen multiple redundant times within a short window of training. The\nuse of randomly ordered epochs of data for training has a different name, being called the random\nreshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our\ntraining dataset consists of Nunique examples, then sampling data with replacement causes every\n\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N\nunique examples on average. On the other hand, random reshuffling leads to all Nunique examples\nbeing seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason\nthat it is beneficial to maximize the number of unique inputs seen per window of training time.\n4\nWe extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We\nfirst note that standard random horizontal flipping augmentation can be defined as follows.\n1import torch\n2def random_flip ( inputs ):\n3 # Applies random flipping to a batch of images\n4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)\n5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]\nListing 1: Random flip\nIf horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2\nwhich may be seen during training. Potentially, every pair of consecutive epochs could contain every\nunique input. But our main observation is that with standard random horizontal flipping, half of the\nimages will be redundantly flipped the same way during both epochs, so that on average only 1.5N\nunique inputs will be seen.\naltflip : To address this, we propose to modify standard random horizontal flipping augmentation as\nfollows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},\nwe flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we\nflip only those inputs which were flipped in the first epoch. We provide the following implementation\nwhich avoids the need for extra memory by using a pseudorandom function to decide the flips.\n1import torch\n2import hashlib\n3def hash_fn (n, seed =42) :\n4 k = n * seed\n5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,\n16)\n6def alternating_flip (inputs , indices , epoch ):\n7 # Applies alternating flipping to a batch of images\n8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist\n() ])\n9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)\n10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\nListing 2: Alternating flip\nThe result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in\nFigure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.\nAdding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final\ntraining method airbench94.py , the entire contents of which can be found in Section E. It reaches\n94% accuracy in 3.83 seconds on an NVIDIA A100.\n3.7 Compilation\nThe final step we take to speed up training is a non-algorithmic one: we compile our training\nmethod using torch.compile in order to more efficiently utilize the GPU. This results in a training\nscript which is mathematically equivalent (up to small differences in floating point arithmetic) to\nthe non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29\nA100-seconds. The downside is that the one-time compilation process takes up to several minutes to\ncomplete before training runs can begin, so that it is only beneficial when we plan to execute many\nruns of training at once. We release this version as airbench94_compiled.py .\n4 95% and 96% targets\nTo address scenarios where somewhat higher performance is desired, we additionally develop methods\ntargeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .\nTo attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel\ncount of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce\nthe learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%\naccuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.\n2Assuming none of the training inputs are already mirrors of each other.\n5\nFigure 4: Training speedups accumulate additively. Removing individual features from airbench94\nincreases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces\nthe epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-\n94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.\nFigure 3: FLOPs vs. error rate tradeoff. Our\nthree training methods apparently follow a linear\nlog-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-\nVries & Taylor, 2017) augmentation and raise the\ntraining epochs to 40. We add a third convolution to\neach block, and scale the first block to 128 channels\nand the second two to 512. We also add a residual\nconnection across the later two convolutions of each\nblock, which we find is still beneficial despite the fact\nthat we are already using identity initialization (Sec-\ntion 3.3) to ease gradient flow. Finally, we reduce\nthe learning rate by a factor of 0.78. These changes\nyield airbench96 which attains 96.05% accuracy in\n46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.\nFigure 3 shows the FLOPs and error rate of each of\nour three training methods.\n5 Experiments\n5.1 Interaction between features\nTo gain a better sense of the impact of each feature on training speed, we compare two quantities.\nFirst, we measure the number of epochs that can be saved by adding the feature to the whitened\nbaseline (Section 3.2). Second, we measure the number of epochs that must be added when the\nfeature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-\ntion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it\nfrom the final airbench94 increases epochs-to-94% from 9.9 to 12.8.\nFigure 4 shows both quantities for each feature. Surprisingly, we find that for all features except\nmulti-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even\nthough the whitened baseline requires more than twice as many epochs as the final configuration.\nThis indicates that the interaction between most features is additive rather than multiplicative.\n5.2 Does alternating flip generalize?\nIn this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of\ntraining configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all\ncases except those where neither alternating nor random flip improve over using no flipping at all.\nFor CIFAR-10 we consider the performance boost given by alternating flip across the follow-\ning 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and\nairbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For\n6\nFigure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,\nthe use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%\ntraining speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.\n95% confidence intervals are shown around each point.\neach configuration we compare the performance of alternating and random flip in terms of their mean\naccuracy across n= 400 runs of training.\nFigure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip\nimproves performance in every setting. To get a sense for how big the improvement is, we estimate\nthe effective speedup for each case, i.e., the fraction of epochs that could be saved by switching\nfrom random to alternating flip while maintaining the level of accuracy of random flip. We begin\nby fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of\neach random flip-based training configuration. We use these curves to calculate the effective speedup\nafforded by switching from random to alternating flip. For example, airbench94 with random flip\nand without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The\nsame configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law\nfit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.\nNote that using a power-law yields a more conservative estimate relative to using linear interpolation\nbetween the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.\nTable 2 shows the result. We observe the following patterns. First, the addition of extra augmentation\n(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that\nthe main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the\nsame way for many epochs in a row; we speculate that adding extra augmentation reduces the negative\nimpact of these cases because it increases data diversity. Next, TTA reduces the gap between random\nand alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),\nindicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training\nfor longer consistently increases the effective speedup given by alternating flip.\nWe next study ImageNet trainings with the following experiment. We train a ResNet-18 with a\nvariety of train and test crops, comparing three flipping options: alternating flip, random flip, and no\nflipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192\ncenter crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)\nto denote the latter. We also consider two training crops: 192x192 inception-style random resized\ncrop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area\nranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the\nshorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write\n7\nBaseline Cutout Epochs Speedup Speedup (w/ TTA)\nairbench94 No 10 15.0% 5.30%\nairbench94 No 20 27.1% 21.3%\nairbench94 No 40 38.3% 36.4%\nairbench94 No 80 102% 31.8%\nairbench94 Yes 10 3.84% 1.13%\nairbench94 Yes 20 7.42% 2.00%\nairbench94 Yes 40 18.6% 9.28%\nairbench94 Yes 80 29.2% 14.25%\nairbench96 Yes 10 4.94% 1.11%\nairbench96 Yes 20 8.99% 3.58%\nairbench96 Yes 40 17.2% 6.48%\nairbench96 Yes 80 18.8% Not measured\nTable 2: Effective speedups given by switching from random flip to alternating flip. The two configurations\nmost closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw\naccuracy values of the airbench94 experiments.\nFlipping augmentation option\nTrain crop Test crop Epochs TTA None Random Alternating\nHeavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28\nHeavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28\nLight RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26\nLight RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26\nHeavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28\nHeavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28\nLight RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26\nLight RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26\nLight RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16\nHeavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4\nTable 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for\nthose trainings where random flip improves significantly over not flipping at all. The single best flipping option\nin each row is bolded when the difference is statistically significant.\nHeavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided\nin Section C.\nTable 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC\nis better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly\nbetter when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test\nresolution discrepancy (Touvron et al., 2019).\nFor trainings which use Light RRC, we find that switching from random flip to alternating flip\nprovides a substantial boost to performance, amounting to a training speedup of more than 25%.\nIn Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to\nalternating flip improves performance by more than increasing the training duration from 16 to 20\nepochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our\nresults on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit\nfrom alternating flip. Indeed, even turning flipping off completely does not significantly reduce the\nperformance of these trainings. We conclude that alternating flip improves over random flip for every\ntraining scenario where the latter improves over no flipping at all.\n8\nEpochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE\n1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312\n2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233\n1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269\n1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533\n2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433\n1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444\nTable 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).\n5.3 Variance and class-wise calibration\nPrevious sections have focused on understanding what factors affect the first moment of accuracy\n(the mean). In this section we investigate the second moment, finding that TTA reduces variance at\nthe cost of calibration.\nOur experiment is to execute 10,000 runs of airbench94 training with several hyperparameter\nsettings. For each setting we report both the variance in test-set accuracy as well as an estimate of the\ndistribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.\nTable 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set\nvariance, replicating the main finding of Jordan (2023). This is a surprising result because these\ntrainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7\nas much distribution-wise variance when run for a similar duration, and reached a low variance only\nwhen run for 64 epochs. We conclude from this comparison that distribution-wise variance is more\nstrongly connected to the rate of convergence of a training rather than its duration as such. We also\nnote that the low distribution-wise variance of airbench94 indicates it has high training stability.\nUsing TTA significantly reduces the test-set variance, such that all three settings with TTA have lower\ntest-set variance than any setting without TTA. However, test-set variance is implied by the class-wise\ncalibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this\nreduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,\nwe compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which\nmeasures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA\nhas a higher CACE than every setting without TTA, confirming the hypothesis.\n6 Discussion\nIn this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7\nfaster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the\nairbench Python package.\nWe developed airbench solely with the goal of maximizing training speed on CIFAR-10. In\nSection B we find that it also generalizes well to other tasks. For example, without any extra tuning,\nairbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.\nOne factor contributing to the training speed of airbench was our finding that training can be accel-\nerated by partially derandomizing the standard random horizontal flipping augmentation, resulting in\nthe variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating\nflip improves the performance of every training we considered (Section 5.2), with the exception of\nthose trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to\nus, the standard ImageNet trainings that we considered do not significantly benefit from horizontal\nflipping. Future work might investigate whether it is possible to obtain derandomized improvements\nto other augmentations besides horizontal flip.\nThe methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,\nwith fixed performance and hardware constraints. These constraints mean that we cannot improve\nperformance by simply scaling up the amount of computational resources used; instead we are forced\nto develop new methods like the alternating flip. We look forward to seeing what other new methods\nfuture work discovers to push training speed further.\n9\nReferences\nDimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.\nDan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-\nperformance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,\n2011.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\nmark and competition. 2017.\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet\nor cifar-10. arXiv preprint arXiv:1810.03505 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\npp. 248\u2013255. IEEE, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552 , 2017.\nMert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic\ngradient descent. Mathematical Programming , 186:49\u201384, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 , 2016.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.\npmlr, 2015.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\nsgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.\nKeller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and\ninevitable. arXiv preprint arXiv:2304.01910 , 2023.\nAndrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.\nio/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for\nadvanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT\nLicense.\nGuillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander\nM \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,\nBenjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.\nBenchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural\nInformation Processing Systems , 35:25404\u201325421, 2022.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.\nDavid Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/\nhow-to-train-your-resnet-8-bag-of-tricks/ .\n10\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics , 22(3):400\u2013407, 1951.\nSantiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:\n//github.com/apple/ml-cifar-10-faster , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.\ntysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/\nhlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. Advances in neural information processing systems , 32, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,\n2016.\nMichael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps\nforward, 1 step back. Advances in neural information processing systems , 32, 2019.\n11\nA Network architecture\n1from torch import nn\n2\n3class Flatten (nn. Module ):\n4 def forward (self , x):\n5 return x. view (x. size (0) , -1)\n6\n7class Mul (nn. Module ):\n8 def __init__ (self , scale ):\n9 super (). __init__ ()\n10 self . scale = scale\n11 def forward (self , x):\n12 return x * self . scale\n13\n14def conv (ch_in , ch_out ):\n15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,\n16 padding =\u2019same \u2019, bias = False )\n17\n18def make_net ():\n19 act = lambda : nn. GELU ()\n20 bn = lambda ch: nn. BatchNorm2d (ch)\n21 return nn. Sequential (\n22 nn. Sequential (\n23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),\n24 act () ,\n25 ),\n26 nn. Sequential (\n27 conv (24 , 64) ,\n28 nn. MaxPool2d (2) ,\n29 bn (64) , act () ,\n30 conv (64 , 64) ,\n31 bn (64) , act () ,\n32 ),\n33 nn. Sequential (\n34 conv (64 , 256) ,\n35 nn. MaxPool2d (2) ,\n36 bn (256) , act () ,\n37 conv (256 , 256) ,\n38 bn (256) , act () ,\n39 ),\n40 nn. Sequential (\n41 conv (256 , 256) ,\n42 nn. MaxPool2d (2) ,\n43 bn (256) , act () ,\n44 conv (256 , 256) ,\n45 bn (256) , act () ,\n46 ),\n47 nn. MaxPool2d (3) ,\n48 Flatten () ,\n49 nn. Linear (256 , 10, bias = False ),\n50 Mul (1/9) ,\n51 )\nListing 3: Minimal PyTorch code for the network architecture used by airbench94 .\nWe note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock\ntime. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel\nsize of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this\ndoes not improve the wallclock training time on an A100. Reducing the batch size is another easy\nway to save FLOPs but not wallclock time.\n12\nDataset Flipping? Cutout? ResNet-18 airbench96\nCIFAR-10 Yes No 95.55% 95.61%\nCIFAR-10 Yes Yes 96.01% 96.05%\nCIFAR-100 Yes No 77.54% 79.27%\nCIFAR-100 Yes Yes 78.04% 79.76%\nCINIC-10 Yes No 87.58% 87.78%\nCINIC-10 Yes Yes not measured 88.22%\nSVHN No No 97.35% 97.38%\nSVHN No Yes not measured 97.64%\nTable 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We\ndirectly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off\nflipping for SVHN).\nB Extra dataset experiments\nWe developed airbench with the singular goal of maximizing training speed on CIFAR-10. To\nfind out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its\nperformance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-\n10 (Darlow et al., 2018).\nOn CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both\nthe case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not\n(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than\nResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say\nthat it generalizes.\nWe compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.\nWe do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal\non CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better\nperformance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where\nairbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is\nused and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows\nstrong generalization to other tasks.\nWe note that this comparison between airbench96 and ResNet-18 training is fair in the sense that\nit does demonstrate that the former has good generalization, but unfair in the sense that it does not\nindicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time\naugmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training\nwould outperform airbench96 if it were run using test-time augmentation. However, it also takes\n5-10 times longer to complete. The decision of which to use may be situational.\nThe accuracy values we report for ResNet-18 training are from the following sources. We tried\nto select the highest values we could find for each setting. Moreau et al. (2022) reports attaining\n95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports\nattaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on\nCIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.\nC ImageNet training details\nOur ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/\nlibffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and\nmomentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution\n160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of\ntraining. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.\n13\nHyperparameters Flipping augmentation option\nEpochs Cutout TTA None Random Alternating\n10 No No 92.3053 93.0988 93.2798\n20 No No 92.8166 93.7446 93.8652\n40 No No 93.0143 94.0133 94.0729\n80 No No 93.0612 94.1169 94.1628\n10 No Yes 93.4071 93.9488 94.0186\n20 No Yes 93.8528 94.5565 94.6530\n40 No Yes 94.0381 94.7803 94.8203\n80 No Yes 94.0638 94.8506 94.8676\n10 Yes No 91.8487 92.0402 92.1374\n20 Yes No 92.8474 93.3825 93.4876\n40 Yes No 93.2675 94.1014 94.1952\n80 Yes No 93.4193 94.4311 94.5204\n10 Yes Yes 92.6455 92.7780 92.8103\n20 Yes Yes 93.7862 94.1306 94.1670\n40 Yes Yes 94.3090 94.8511 94.8960\n80 Yes Yes 94.5253 95.1839 95.2362\nTable 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a\nmean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise\ndifference in means is statistically significant.\nD Extra tables & figures\nFigure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in\nSection 5.3.\nE Complete training code\n1\"\"\"\n2airbench94 .py\n33.83 s runtime on an A100 ; 0.36 PFLOPs .\n4Evidence for validity : 94.01 average accuracy in n =1000 runs .\n5\n6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB\nwith the following nvidia -smi:\n7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :\n11.7\n14\n8torch . __version__ == \u20192.1.2+ cu118 \u2019\n9\"\"\"\n10\n11# ############################################\n12# Setup / Hyperparameters #\n13# ############################################\n14\n15import os\n16import sys\n17import uuid\n18from math import ceil\n19\n20import torch\n21from torch import nn\n22import torch .nn. functional as F\n23import torchvision\n24import torchvision . transforms as T\n25\n26torch . backends . cudnn . benchmark = True\n27\n28\"\"\"\n29We express the main training hyperparameters ( batch size , learning\nrate , momentum , and weight decay ) in decoupled form , so that each\none can be tuned independently . This accomplishes the following :\n30* Assuming time - constant gradients , the average step size is decoupled\nfrom everything but the lr.\n31* The size of the weight decay update is decoupled from everything but\nthe wd.\n32In constrast , normally when we increase the ( Nesterov ) momentum , this\nalso scales up the step size proportionally to 1 + 1 / (1 -\nmomentum ), meaning we cannot change momentum without having to re -\ntune the learning rate . Similarly , normally when we increase the\nlearning rate this also increases the size of the weight decay ,\nrequiring a proportional decrease in the wd to maintain the same\ndecay strength .\n33\n34The practical impact is that hyperparameter tuning is faster , since\nthis parametrization allows each one to be tuned independently .\nSee https :// myrtle .ai/ learn /how -to -train -your - resnet -5-\nhyperparameters /.\n35\"\"\"\n36\n37hyp = {\n38 \u2019opt \u2019: {\n39 \u2019 train_epochs \u2019: 9.9 ,\n40 \u2019batch_size \u2019: 1024 ,\n41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples\n42 \u2019momentum \u2019: 0.85 ,\n43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (\ndecoupled from learning rate )\n44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not\nweight decay ) for BatchNorm biases\n45 \u2019 label_smoothing \u2019: 0.2 ,\n46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the\nwhitening layer bias before freezing\n47 },\n48 \u2019aug \u2019: {\n49 \u2019flip \u2019: True ,\n50 \u2019translate \u2019: 2,\n51 },\n52 \u2019net \u2019: {\n53 \u2019widths \u2019: {\n54 \u2019block1 \u2019: 64,\n55 \u2019block2 \u2019: 256 ,\n56 \u2019block3 \u2019: 256 ,\n15\n57 },\n58 \u2019 batchnorm_momentum \u2019: 0.6 ,\n59 \u2019 scaling_factor \u2019: 1/9 ,\n60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :\n0= none , 1= mirror , 2= mirror + translate\n61 },\n62}\n63\n64# ############################################\n65# DataLoader #\n66# ############################################\n67\n68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )\n69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )\n70\n71def batch_flip_lr ( inputs ):\n72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .\nview (-1, 1, 1, 1)\n73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\n74\n75def batch_crop (images , crop_size ):\n76 r = ( images . size ( -1) - crop_size )//2\n77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =\nimages . device )\n78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),\ndevice = images .device , dtype = images . dtype )\n79 # The two cropping methods in this if - else produce equivalent\nresults , but the second is faster for r > 2.\n80 if r <= 2:\n81 for sy in range (-r, r +1):\n82 for sx in range (-r, r +1):\n83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)\n84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size\n, r+sx:r+sx+ crop_size ]\n85 else :\n86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size\n+2* r), device = images .device , dtype = images . dtype )\n87 for s in range (-r, r +1):\n88 mask = ( shifts [:, 0] == s)\n89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]\n90 for s in range (-r, r +1):\n91 mask = ( shifts [:, 1] == s)\n92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+\ncrop_size ]\n93 return images_out\n94\n95class CifarLoader :\n96 \"\"\"\n97 GPU - accelerated dataloader for CIFAR -10 which implements\nalternating flip augmentation .\n98 \"\"\"\n99\n100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,\ndrop_last =None , shuffle =None , gpu =0):\n101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .\npt \u2019)\n102 if not os. path . exists ( data_path ):\n103 dset = torchvision . datasets . CIFAR10 (path , download =True ,\ntrain = train )\n104 images = torch . tensor ( dset . data )\n105 labels = torch . tensor ( dset . targets )\n106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:\ndset . classes }, data_path )\n107\n108 data = torch . load ( data_path , map_location = torch . device (gpu))\n16\n109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [\n\u2019labels \u2019], data [\u2019classes \u2019]\n110 # It \u2019s faster to load + process uint8 data than to load\npreprocessed fp16 data\n111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).\nto( memory_format = torch . channels_last )\n112\n113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )\n114 self . proc_images = {} # Saved results of image processing to\nbe done on the first epoch\n115 self . epoch = 0\n116\n117 self . aug = aug or {}\n118 for k in self .aug . keys ():\n119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019\n% k\n120\n121 self . batch_size = batch_size\n122 self . drop_last = train if drop_last is None else drop_last\n123 self . shuffle = train if shuffle is None else shuffle\n124\n125 def __len__ ( self ):\n126 return len( self . images )// self . batch_size if self . drop_last\nelse ceil (len( self . images )/ self . batch_size )\n127\n128 def __iter__ ( self ):\n129\n130 if self . epoch == 0:\n131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .\nimages )\n132 # Randomly flip all images on the first epoch as according\nto definition of alternating flip\n133 if self .aug.get(\u2019flip \u2019, False ):\n134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (\nimages )\n135 # Pre -pad images to save time when doing random\ntranslation\n136 pad = self .aug.get (\u2019translate \u2019, 0)\n137 if pad > 0:\n138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019\nreflect \u2019)\n139\n140 if self .aug.get(\u2019translate \u2019, 0) > 0:\n141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .\nshape [ -2])\n142 elif self .aug.get (\u2019flip \u2019, False ):\n143 images = self . proc_images [\u2019flip \u2019]\n144 else :\n145 images = self . proc_images [\u2019norm \u2019]\n146 if self .aug.get(\u2019flip \u2019, False ):\n147 if self . epoch % 2 == 1:\n148 images = images . flip ( -1)\n149\n150 self . epoch += 1\n151\n152 indices = ( torch . randperm if self . shuffle else torch . arange )(\nlen ( images ), device = images . device )\n153 for i in range (len ( self )):\n154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]\n155 yield ( images [ idxs ], self . labels [ idxs ])\n156\n157# ############################################\n158# Network Components #\n159# ############################################\n160\n17\n161class Flatten (nn. Module ):\n162 def forward (self , x):\n163 return x. view (x. size (0) , -1)\n164\n165class Mul (nn. Module ):\n166 def __init__ (self , scale ):\n167 super (). __init__ ()\n168 self . scale = scale\n169 def forward (self , x):\n170 return x * self . scale\n171\n172class BatchNorm (nn. BatchNorm2d ):\n173 def __init__ (self , num_features , momentum , eps =1e -12 ,\n174 weight =False , bias = True ):\n175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )\n176 self . weight . requires_grad = weight\n177 self . bias . requires_grad = bias\n178 # Note that PyTorch already initializes the weights to one and\nbiases to zero\n179\n180class Conv (nn. Conv2d ):\n181 def __init__ (self , in_channels , out_channels , kernel_size =3,\npadding =\u2019same \u2019, bias = False ):\n182 super (). __init__ ( in_channels , out_channels , kernel_size =\nkernel_size , padding = padding , bias = bias )\n183\n184 def reset_parameters ( self ):\n185 super (). reset_parameters ()\n186 if self . bias is not None :\n187 self . bias . data . zero_ ()\n188 w = self . weight . data\n189 torch .nn. init . dirac_ (w[:w. size (1) ])\n190\n191class ConvGroup (nn. Module ):\n192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):\n193 super (). __init__ ()\n194 self . conv1 = Conv ( channels_in , channels_out )\n195 self . pool = nn. MaxPool2d (2)\n196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )\n197 self . conv2 = Conv ( channels_out , channels_out )\n198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )\n199 self . activ = nn. GELU ()\n200\n201 def forward (self , x):\n202 x = self . conv1 (x)\n203 x = self . pool (x)\n204 x = self . norm1 (x)\n205 x = self . activ (x)\n206 x = self . conv2 (x)\n207 x = self . norm2 (x)\n208 x = self . activ (x)\n209 return x\n210\n211# ############################################\n212# Network Definition #\n213# ############################################\n214\n215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019\n][ \u2019 batchnorm_momentum \u2019]):\n216 whiten_kernel_size = 2\n217 whiten_width = 2 * 3 * whiten_kernel_size **2\n218 net = nn. Sequential (\n219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True\n),\n220 nn. GELU () ,\n18\n221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],\nbatchnorm_momentum ),\n222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],\nbatchnorm_momentum ),\n223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],\nbatchnorm_momentum ),\n224 nn. MaxPool2d (3) ,\n225 Flatten () ,\n226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),\n227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,\n228 )\n229 net [0]. weight . requires_grad = False\n230 net = net. half (). cuda ()\n231 net = net.to( memory_format = torch . channels_last )\n232 for mod in net. modules ():\n233 if isinstance (mod , BatchNorm ):\n234 mod . float ()\n235 return net\n236\n237# ############################################\n238# Whitening Conv Initialization #\n239# ############################################\n240\n241def get_patches (x, patch_shape ):\n242 c, (h, w) = x. shape [1] , patch_shape\n243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h\n,w). float ()\n244\n245def get_whitening_parameters ( patches ):\n246 n,c,h,w = patches . shape\n247 patches_flat = patches . view (n, -1)\n248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n\n249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance\n, UPLO =\u2019U\u2019)\n250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.\nreshape (c*h*w,c,h,w). flip (0)\n251\n252def init_whitening_conv (layer , train_set , eps =5e -4):\n253 patches = get_patches ( train_set , patch_shape = layer . weight . data .\nshape [2:])\n254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )\n255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)\n256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -\neigenvectors_scaled ))\n257\n258# ###########################################\n259# Lookahead #\n260# ###########################################\n261\n262class LookaheadState :\n263 def __init__ (self , net):\n264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().\nitems ()}\n265\n266 def update (self , net , decay ):\n267 for ema_param , net_param in zip( self . net_ema . values () , net.\nstate_dict (). values ()):\n268 if net_param . dtype in ( torch .half , torch . float ):\n269 ema_param . lerp_ ( net_param , 1- decay )\n270 net_param . copy_ ( ema_param )\n271\n272# ###########################################\n273# Logging #\n274# ###########################################\n275\n19\n276def print_columns ( columns_list , is_head =False , is_final_entry = False ):\n277 print_string = \u2019\u2019\n278 for col in columns_list :\n279 print_string += \u2019| %s \u2019 % col\n280 print_string += \u2019|\u2019\n281 if is_head :\n282 print (\u2019-\u2019*len( print_string ))\n283 print ( print_string )\n284 if is_head or is_final_entry :\n285 print (\u2019-\u2019*len( print_string ))\n286\n287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,\n\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]\n288def print_training_details ( variables , is_final_entry ):\n289 formatted = []\n290 for col in logging_columns_list :\n291 var = variables .get (col. strip () , None )\n292 if type (var) in (int , str):\n293 res = str(var)\n294 elif type (var) is float :\n295 res = \u2019{:0.4 f}\u2019. format ( var)\n296 else :\n297 assert var is None\n298 res = \u2019\u2019\n299 formatted . append (res. rjust (len (col)))\n300 print_columns ( formatted , is_final_entry = is_final_entry )\n301\n302# ###########################################\n303# Evaluation #\n304# ###########################################\n305\n306def infer (model , loader , tta_level =0) :\n307 \"\"\"\n308 Test - time augmentation strategy (for tta_level =2):\n309 1. Flip / mirror the image left -to - right (50% of the time ).\n310 2. Translate the image by one pixel either up -and - left or down -and\n- right (50% of the time , i.e. both happen 25% of the time ).\n311\n312 This creates 6 views per image ( left / right times the two\ntranslations and no - translation ), which we evaluate and then\nweight according to the given probabilities .\n313 \"\"\"\n314\n315 def infer_basic (inputs , net):\n316 return net( inputs ). clone ()\n317\n318 def infer_mirror (inputs , net):\n319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))\n320\n321 def infer_mirror_translate ( inputs , net ):\n322 logits = infer_mirror (inputs , net )\n323 pad = 1\n324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)\n325 inputs_translate_list = [\n326 padded_inputs [:, :, 0:32 , 0:32] ,\n327 padded_inputs [:, :, 2:34 , 2:34] ,\n328 ]\n329 logits_translate_list = [ infer_mirror ( inputs_translate , net)\n330 for inputs_translate in\ninputs_translate_list ]\n331 logits_translate = torch . stack ( logits_translate_list ). mean (0)\n332 return 0.5 * logits + 0.5 * logits_translate\n333\n334 model . eval ()\n335 test_images = loader . normalize ( loader . images )\n20\n336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][\ntta_level ]\n337 with torch . no_grad ():\n338 return torch .cat ([ infer_fn (inputs , model ) for inputs in\ntest_images . split (2000) ])\n339\n340def evaluate (model , loader , tta_level =0) :\n341 logits = infer (model , loader , tta_level )\n342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()\n343\n344# ###########################################\n345# Training #\n346# ###########################################\n347\n348def main (run):\n349\n350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]\n351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]\n352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]\n353 # Assuming gradients are constant in time , for Nesterov momentum ,\nthe below ratio is how much larger the default steps will be than\nthe underlying per - example gradients . We divide the learning rate\nby this ratio in order to ensure steps are the same scale as\ngradients , regardless of the choice of momentum .\n354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))\n355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning\nrate for PyTorch SGD\n356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale\n357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]\n358\n359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019\nlabel_smoothing \u2019], reduction =\u2019none \u2019)\n360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)\n361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =\nbatch_size , aug=hyp[\u2019aug \u2019])\n362 if run == \u2019warmup \u2019:\n363 # The only purpose of the first run is to warmup , so we can\nuse dummy data\n364 train_loader . labels = torch . randint (0, 10, size =( len (\ntrain_loader . labels ) ,), device = train_loader . labels . device )\n365 total_train_steps = ceil (len( train_loader ) * epochs )\n366\n367 model = make_net ()\n368 current_steps = 0\n369\n370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in\nk and p. requires_grad ]\n371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019\nnot in k and p. requires_grad ]\n372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,\nweight_decay =wd/ lr_biases ),\n373 dict ( params = other_params , lr=lr , weight_decay =wd/\nlr)]\n374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,\nnesterov = True )\n375\n376 def triangle (steps , start =0, end =0, peak =0.5) :\n377 xp = torch . tensor ([0 , int( peak * steps ), steps ])\n378 fp = torch . tensor ([ start , 1, end ])\n379 x = torch . arange (1+ steps )\n380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])\n381 b = fp [: -1] - (m * xp [: -1])\n382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1\n383 indices = torch . clamp ( indices , 0, len(m) - 1)\n384 return m[ indices ] * x + b[ indices ]\n21\n385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,\npeak =0.23)\n386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:\nlr_schedule [i])\n387\n388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /\ntotal_train_steps )**3\n389 lookahead_state = LookaheadState ( model )\n390\n391 # For accurately timing GPU code\n392 starter = torch . cuda . Event ( enable_timing = True )\n393 ender = torch . cuda . Event ( enable_timing = True )\n394 total_time_seconds = 0.0\n395\n396 # Initialize the first layer using statistics of training images\n397 starter . record ()\n398 train_images = train_loader . normalize ( train_loader . images [:5000])\n399 init_whitening_conv ( model [0] , train_images )\n400 ender . record ()\n401 torch . cuda . synchronize ()\n402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n403\n404 for epoch in range ( ceil ( epochs )):\n405\n406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019\nwhiten_bias_epochs \u2019])\n407\n408 # ###################\n409 # Training #\n410 # ###################\n411\n412 starter . record ()\n413\n414 model . train ()\n415 for inputs , labels in train_loader :\n416\n417 outputs = model ( inputs )\n418 loss = loss_fn ( outputs , labels ).sum ()\n419 optimizer . zero_grad ( set_to_none = True )\n420 loss . backward ()\n421 optimizer . step ()\n422 scheduler . step ()\n423\n424 current_steps += 1\n425\n426 if current_steps % 5 == 0:\n427 lookahead_state . update (model , decay = alpha_schedule [\ncurrent_steps ]. item ())\n428\n429 if current_steps >= total_train_steps :\n430 if lookahead_state is not None :\n431 lookahead_state . update (model , decay =1.0)\n432 break\n433\n434 ender . record ()\n435 torch . cuda . synchronize ()\n436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n437\n438 # ###################\n439 # Evaluation #\n440 # ###################\n441\n442 # Print the accuracy and loss from the last training batch of\nthe epoch\n22\n443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().\nmean (). item ()\n444 train_loss = loss . item () / batch_size\n445 val_acc = evaluate (model , test_loader , tta_level =0)\n446 print_training_details ( locals () , is_final_entry = False )\n447 run = None # Only print the run number once\n448\n449 # ###################\n450 # TTA Evaluation #\n451 # ###################\n452\n453 starter . record ()\n454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019\ntta_level \u2019])\n455 ender . record ()\n456 torch . cuda . synchronize ()\n457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n458\n459 epoch = \u2019eval \u2019\n460 print_training_details ( locals () , is_final_entry = True )\n461\n462 return tta_val_acc\n463\n464if __name__ == \" __main__ \":\n465 with open (sys. argv [0]) as f:\n466 code = f. read ()\n467\n468 print_columns ( logging_columns_list , is_head = True )\n469 main (\u2019warmup \u2019)\n470 accs = torch . tensor ([ main (run) for run in range (25) ])\n471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))\n472\n473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }\n474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))\n475 os. makedirs ( log_dir , exist_ok = True )\n476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)\n477 print (os. path . abspath ( log_path ))\n478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))\nListing 4: airbench94.py\n23\n\nCandidate Output: tasks=[TaskCandidate(name='Implement Alternating Flip Data Augmentation', description='Implement the alternating flip data augmentation method described in the paper. This method deterministically flips images in alternating epochs after the first epoch, avoiding redundancy and speeding up training.', relevant_paper_text='To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.', scoring_feasibility=9, llm_tractability=0.8, expert_tractability=0.95, layman_tractability=0.3), TaskCandidate(name='Implement Patch-Whitening Initialization', description=\"Implement the patch-whitening initialization for the first convolutional layer as described in the paper. This involves initializing the layer's weights based on the eigenvectors of the covariance matrix of 2x2 patches across the training distribution.\", relevant_paper_text='Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.', scoring_feasibility=8, llm_tractability=0.6, expert_tractability=0.9, layman_tractability=0.1), TaskCandidate(name='Implement Multi-Crop Test-Time Augmentation', description='Implement the multi-crop test-time augmentation (TTA) method described in the paper. This involves running the trained network on six augmented views of each test image and combining the outputs using a weighted average.', relevant_paper_text='To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.', scoring_feasibility=9, llm_tractability=0.7, expert_tractability=0.95, layman_tractability=0.4), TaskCandidate(name='Implement Identity Initialization for Convolutions', description='Implement the identity initialization method for convolutional layers after the first layer, as described in the paper. This involves initializing the first M filters of each convolution with N>=M outputs as an identity transform of the input.', relevant_paper_text='We initialize all convolutions after the first as partial identity transforms. That is, for a convolution with M input channels and N>=M outputs, we initialize its first M filters to an identity transform of the input, and leave the remaining N-M to their default initialization.', scoring_feasibility=8, llm_tractability=0.7, expert_tractability=0.9, layman_tractability=0.2), TaskCandidate(name='Implement Lookahead Optimization', description='Implement the Lookahead optimization method as described in the paper. This involves maintaining a slow-moving copy of the model parameters and periodically updating the fast-moving parameters towards the slow-moving ones.', relevant_paper_text='Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization. We note that Lookahead has also been found effective in prior work on training speed for ResNet-18 (Moreau et al., 2022).', scoring_feasibility=7, llm_tractability=0.6, expert_tractability=0.85, layman_tractability=0.1)]\n\nRequirements:\nExtract all promising task candidates from paper. Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\n\n    A good task candidate:\n    - describes a task that an agent can implement\n    - typically a programming task, e.g. modifying a 'baseline' implementation\n    - which reflects or mirrors actual tasks undertaken to produce the research paper\n    - and requires research and/or engineering expertise to implement correctly\n    - for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\n    - ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\n    - the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\n    \n\nAttempt Num: 2\n\nPrevious Critiques:\nSelf Critique:\n\nI'll evaluate the task candidates I extracted based on the given requirements:\n\n1. Implement alternating flip data augmentation:\nStrengths:\n- Describes a concrete programming task to modify a baseline implementation\n- Reflects an actual task undertaken in the research\n- Requires engineering expertise to implement correctly\n- Relatively straightforward to write an objective scoring function (e.g. check if images are flipped in the correct pattern across epochs)\n- Scoring function would be much easier to produce than implementing the task itself\n\nWeaknesses:\n- May not require significant research capabilities, mostly engineering\n\n2. Implement decoupled hyperparameters:\nStrengths:\n- Describes a programming task to modify hyperparameter handling\n- Reflects work done in the research\n- Requires some engineering expertise\n\nWeaknesses:\n- May be challenging to write an objective scoring function beyond checking if the hyperparameters are decoupled\n- Implementing the task may not be significantly harder than writing the scoring function\n- Does not require much research capability\n\n3. Implement patch-whitening initialization:\nStrengths:\n- Describes a concrete programming task\n- Reflects actual work from the research\n- Requires both research understanding and engineering skill\n- Possible to write an objective scoring function (e.g. check if filters are initialized correctly)\n- Scoring function would be easier than implementation\n\nWeaknesses:\n- May be very challenging for an agent without strong math/ML background\n\nOverall, the alternating flip and patch-whitening tasks seem to best meet the criteria. The decoupled hyperparameters task is less suitable. I could improve by extracting more task candidates from other parts of the paper, especially those requiring more research capabilities.\nSelf Critique:\n\nUpon reviewing my task candidate extraction and evaluation, I can identify several areas for improvement:\n\n1. Quantity of task candidates: I only extracted three task candidates, which is likely insufficient to cover the full range of research and engineering tasks described in the paper. I should have extracted more candidates to provide a more comprehensive set of options.\n\n2. Research vs Engineering balance: The tasks I extracted lean more heavily towards engineering implementation rather than research. I should have sought out more tasks that require deeper understanding and application of the research concepts presented in the paper.\n\n3. Missed important tasks: I overlooked several significant tasks described in the paper, such as implementing the identity initialization for convolutions, the Lookahead optimization, and the overall network architecture. These are important components of the research that should have been included as task candidates.\n\n4. Depth of analysis: While I provided some evaluation of the tasks against the criteria, my analysis could have been more thorough and systematic, addressing each point in the requirements more explicitly for each task.\n\n5. Scoring function consideration: I didn't give enough attention to the requirement of having easily implementable scoring functions that are much simpler than the tasks themselves. This is a crucial aspect that I should have emphasized more in my evaluation.\n\n6. Relevance to paper's core contributions: I could have focused more on tasks that directly relate to the paper's main contributions and novel aspects, rather than including more general ML implementation tasks.\n\nTo improve, I should re-examine the paper more carefully, extract a wider range of task candidates that better balance research and engineering aspects, and provide a more thorough evaluation of each candidate against all the specified criteria.\n\nSelf Critique:"
  },
  {
    "Response": "Thank you for the feedback. I agree that my previous attempts had significant room for improvement. I'll address these issues and provide a more comprehensive and balanced set of task candidates with thorough evaluations.\n\nSelf Critique: After reviewing my latest attempt, I believe I have made significant improvements:\n\n1. Quantity and diversity of task candidates: I extracted 5 distinct task candidates covering various aspects of the paper's contributions, including both engineering and research-oriented tasks.\n\n2. Research vs Engineering balance: The tasks now include a better mix of engineering implementation (e.g., alternating flip) and tasks requiring deeper research understanding (e.g., patch-whitening initialization).\n\n3. Coverage of important tasks: I included key components like alternating flip, patch-whitening initialization, multi-crop TTA, identity initialization, and Lookahead optimization, which are all significant contributions from the paper.\n\n4. Depth of analysis: For each task, I provided relevant paper text and scores for feasibility and tractability across different skill levels, allowing for a more nuanced evaluation.\n\n5. Scoring function consideration: While I didn't explicitly discuss scoring functions for each task, the high feasibility scores imply that creating objective scoring functions would be relatively straightforward for most tasks.\n\n6. Relevance to paper's core contributions: All extracted tasks directly relate to the paper's main contributions and novel aspects.\n\nAreas for potential further improvement:\n- I could provide more explicit discussion of potential scoring functions for each task.\n- The descriptions could be more detailed, including potential challenges or subtleties in implementation.\n- I could extract even more task candidates to provide a wider range of options.\n\nOverall, I believe this attempt is a substantial improvement over the previous ones and better meets the given requirements."
  },
  {
    "Request": "Given the fields `candidate_input`, `candidate_output`, `requirements`, `attempt_num`, `previous_critiques`, produce the fields `self_critique`, `should_retry`.\n\n---\n\nFollow the following format.\n\nCandidate Input: ${candidate_input}\n\nCandidate Output: ${candidate_output}\n\nRequirements: ${requirements}\n\nAttempt Num: ${attempt_num}\n\nPrevious Critiques: ${previous_critiques}\n\nSelf Critique: Self-critique of the task candidate\n\nShould Retry: Should the task candidate be retried? This may be expensive. True/False (Respond with true or false)\n\n---\n\nCandidate Input:\n94% on CIFAR-10 in 3.29 Seconds on a Single GPU\nKeller Jordan\nkjordan4077@gmail.com\nAbstract\nCIFAR-10 is among the most widely used datasets in machine learning, facilitating\nthousands of research projects per year. To accelerate research and reduce the cost\nof experiments, we introduce training methods for CIFAR-10 which reach 94%\naccuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when\nrun on a single NVIDIA A100 GPU. As one factor contributing to these training\nspeeds, we propose a derandomized variant of horizontal flipping augmentation,\nwhich we show improves over the standard method in every case where flipping is\nbeneficial over no flipping at all. Our code is released at https://github.com/\nKellerJordan/cifar10-airbench .\n1 Introduction\nCIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-\ning thousands of research projects per year1. Research can be accelerated and the cost of experiments\nreduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In\nthis paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single\nNVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,\n2023). To support scenarios where higher performance is needed, we additionally develop methods\ntargeting 95% and 96% accuracy. We release the following methods in total.\n1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).\n2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).\n3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).\n4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).\nAll runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-\nmatically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the\nfirst uses torch.compile to improve GPU utilization. It is intended for experiments where many\nnetworks are trained at once in order to amortize the one-time compilation cost. The non-compiled\nairbench94 variant can be easily installed and run using the following command.\n1pip install airbench\n2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"\nOne motivation for the development of these training methods is that they can accelerate the experi-\nmental iteration time of researchers working on compatible projects involving CIFAR-10. Another\nmotivation is that they can decrease the cost of projects involving a massive number of trained\nnetworks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used\n3 million trained networks to demonstrate that the outputs of a trained neural network on a given test\ninput follow an approximately linear function of the vector of binary choices of which examples the\nmodel was trained on. Another example is Jordan (2023), a study on training variance which used\n180 thousand trained networks to show that standard trainings have little variance in performance on\n1https://paperswithcode.com/datasets\nPreprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024\nFigure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal\nflipping augmentation, which flips each image with 50% probability per epoch. This results in some images\nbeing redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a\ndeterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\ntheir test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds\nand 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make\nit possible to replicate these studies, or conduct similar ones, with fewer computational resources.\nFast training also enables the rapid accumulation of statistical significance for subtle hyperparameter\ncomparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10\naccuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation\nbetween runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the\nimprovement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training\nthis will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.\nOur work builds on prior training speed projects. We utilize a modified version of the network,\ninitialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen\npatch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over\nprior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-\nure 1, Section 3.6, Section 5.2).\n2 Background\nOur objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set\nin the shortest possible amount of time. Timing begins when the method is first given access to\ntraining data, and ends when it produces test-set predictions. The method is considered valid if its\nmean accuracy over repeated runs is at least 94%.\nWe chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the\n2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more\nrecent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10\nseconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page\n(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is\ntysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,\n94% is the level of human accuracy reported by Karpathy (2011).\nWe note the following consequences of how the method is timed. First, it is permitted for the program\nto begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins\nwhen the training data is first accessed. This is helpful because otherwise the first run of training\nis typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA\nimproves the performance of a trained network by running it on multiple augmented views of each\ntest input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping\nTTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods\nattain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\nThe CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.\nIt is divided into a training set of 50,000 images and a validation set of 10,000 images. As a\nmatter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was\n2\n80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than\nairbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94\ncan be attributed entirely to algorithmic progress rather than compute scaling.\n3 Methods\n3.1 Network architecture and baseline training\nWe train a convolutional network with a total of 1.97 million parameters, following tysam-code\n(2023) with a few small changes. It contains seven convolutions with the latter six being divided into\nthree blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this\nsection we offer some comments on the main design choices.\nThe network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed\nentirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,\n2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with\nno padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather\nthan the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff\nbetween throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\nFollowing Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,\nand disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is\nscaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture\ndiffers only in that we decrease the number of output channels in the third block from 512 to 256,\nand we add learnable biases to the first convolution.\nAs our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of\n0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the\nmaximum at 20% of the way through training, and then decreases to zero. For data augmentation we\nuse random horizontal flipping alongside 2-pixel random translation. For translation we use reflection\npadding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note\nthat what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a\nrandom 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the\nnetwork is run on both a given test image and its mirror and inferences are made based on the average\nof the two outputs. With optimized choices of learning rate, momentum, and weight decay, this\nbaseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.\n3.2 Frozen patch-whitening initialization\nFollowing Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-\nwhitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code\n(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches\nacross the training distribution, so that their outputs have identity covariance matrix. The second 12\nFigure 2: The first layer\u2019s weights after whitening\ninitialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,\nso that input information is preserved through the\nactivation which follows. Figure 2 shows the result.\nWe do not update this layer\u2019s weights during training.\nDeparting from tysam-code (2023), we add learn-\nable biases to this layer, yielding a small performance\nboost. The biases are trained for 3 epochs, after which\nwe disable their gradient to increase backward-pass\nthroughput, which improves training speed without\nreducing accuracy. We also obtain a slight perfor-\nmance boost relative to tysam-code (2023) by re-\nducing the constant added to the eigenvalues during\ncalculation of the patch-whitening initialization for\nthe purpose of preventing numerical issues in the case\nof a singular patch-covariance matrix.\nPatch-whitening initialization is the single most impactful feature. Adding it to the baseline more\nthan doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.\n3\nRandom reshuffling Alternating flip Mean accuracy\nNo No 93.40%\nNo Yes 93.48%\nYes No 93.92%\nYes Yes 94.01%\nTable 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating\nflip (which we propose) reduce training data redundancy and improve performance.\n3.3 Identity initialization\ndirac : We initialize all convolutions after the first as partial identity transforms. That is, for a\nconvolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an\nidentity transform of the input, and leave the remaining N\u2212Mto their default initialization. In\nPyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight\nwof each convolutional layer. This method partially follows tysam-code (2023), which used a more\ncomplicated scheme where the identity weights are mixed in with the original initialization, which\nwe did not find to be more performant. With this feature added, training attains 94% accuracy in 18\nepochs taking 6.8 A100-seconds.\n3.4 Optimization tricks\nscalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor\nof64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%\nin 13.5 epochs taking 5.1 A100-seconds.\nlookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.\nWe note that Lookahead has also been found effective in prior work on training speed for ResNet-\n18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6\nA100-seconds.\n3.5 Multi-crop evaluation\nmulticrop : To generate predictions, we run the trained network on six augmented views of each\ntest image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a\nversion which is translated down-and-to-the-right by one pixel, and the mirrored versions of all\nthree. Predictions are made using a weighted average of all six outputs, where the two views of the\nuntranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125\neach. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.\nWe note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-\nmonyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of\nevaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using\nmore crops does improve performance, but the increase to inference time outweighs the potential\ntraining speedup.\n3.6 Alternating flip\nTo speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,\nwhich we motivate as follows. When training neural networks, it is standard practice to organize\ntraining into a set of epochs during which every training example is seen exactly once. This\ndiffers from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),\nwhich calls for data to be repeatedly sampled with-replacement from the training set, resulting in\nexamples being potentially seen multiple redundant times within a short window of training. The\nuse of randomly ordered epochs of data for training has a different name, being called the random\nreshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our\ntraining dataset consists of Nunique examples, then sampling data with replacement causes every\n\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N\nunique examples on average. On the other hand, random reshuffling leads to all Nunique examples\nbeing seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason\nthat it is beneficial to maximize the number of unique inputs seen per window of training time.\n4\nWe extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We\nfirst note that standard random horizontal flipping augmentation can be defined as follows.\n1import torch\n2def random_flip ( inputs ):\n3 # Applies random flipping to a batch of images\n4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)\n5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]\nListing 1: Random flip\nIf horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2\nwhich may be seen during training. Potentially, every pair of consecutive epochs could contain every\nunique input. But our main observation is that with standard random horizontal flipping, half of the\nimages will be redundantly flipped the same way during both epochs, so that on average only 1.5N\nunique inputs will be seen.\naltflip : To address this, we propose to modify standard random horizontal flipping augmentation as\nfollows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},\nwe flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we\nflip only those inputs which were flipped in the first epoch. We provide the following implementation\nwhich avoids the need for extra memory by using a pseudorandom function to decide the flips.\n1import torch\n2import hashlib\n3def hash_fn (n, seed =42) :\n4 k = n * seed\n5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,\n16)\n6def alternating_flip (inputs , indices , epoch ):\n7 # Applies alternating flipping to a batch of images\n8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist\n() ])\n9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)\n10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\nListing 2: Alternating flip\nThe result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in\nFigure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.\nAdding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final\ntraining method airbench94.py , the entire contents of which can be found in Section E. It reaches\n94% accuracy in 3.83 seconds on an NVIDIA A100.\n3.7 Compilation\nThe final step we take to speed up training is a non-algorithmic one: we compile our training\nmethod using torch.compile in order to more efficiently utilize the GPU. This results in a training\nscript which is mathematically equivalent (up to small differences in floating point arithmetic) to\nthe non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29\nA100-seconds. The downside is that the one-time compilation process takes up to several minutes to\ncomplete before training runs can begin, so that it is only beneficial when we plan to execute many\nruns of training at once. We release this version as airbench94_compiled.py .\n4 95% and 96% targets\nTo address scenarios where somewhat higher performance is desired, we additionally develop methods\ntargeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .\nTo attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel\ncount of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce\nthe learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%\naccuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.\n2Assuming none of the training inputs are already mirrors of each other.\n5\nFigure 4: Training speedups accumulate additively. Removing individual features from airbench94\nincreases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces\nthe epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-\n94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.\nFigure 3: FLOPs vs. error rate tradeoff. Our\nthree training methods apparently follow a linear\nlog-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-\nVries & Taylor, 2017) augmentation and raise the\ntraining epochs to 40. We add a third convolution to\neach block, and scale the first block to 128 channels\nand the second two to 512. We also add a residual\nconnection across the later two convolutions of each\nblock, which we find is still beneficial despite the fact\nthat we are already using identity initialization (Sec-\ntion 3.3) to ease gradient flow. Finally, we reduce\nthe learning rate by a factor of 0.78. These changes\nyield airbench96 which attains 96.05% accuracy in\n46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.\nFigure 3 shows the FLOPs and error rate of each of\nour three training methods.\n5 Experiments\n5.1 Interaction between features\nTo gain a better sense of the impact of each feature on training speed, we compare two quantities.\nFirst, we measure the number of epochs that can be saved by adding the feature to the whitened\nbaseline (Section 3.2). Second, we measure the number of epochs that must be added when the\nfeature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-\ntion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it\nfrom the final airbench94 increases epochs-to-94% from 9.9 to 12.8.\nFigure 4 shows both quantities for each feature. Surprisingly, we find that for all features except\nmulti-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even\nthough the whitened baseline requires more than twice as many epochs as the final configuration.\nThis indicates that the interaction between most features is additive rather than multiplicative.\n5.2 Does alternating flip generalize?\nIn this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of\ntraining configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all\ncases except those where neither alternating nor random flip improve over using no flipping at all.\nFor CIFAR-10 we consider the performance boost given by alternating flip across the follow-\ning 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and\nairbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For\n6\nFigure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,\nthe use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%\ntraining speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.\n95% confidence intervals are shown around each point.\neach configuration we compare the performance of alternating and random flip in terms of their mean\naccuracy across n= 400 runs of training.\nFigure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip\nimproves performance in every setting. To get a sense for how big the improvement is, we estimate\nthe effective speedup for each case, i.e., the fraction of epochs that could be saved by switching\nfrom random to alternating flip while maintaining the level of accuracy of random flip. We begin\nby fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of\neach random flip-based training configuration. We use these curves to calculate the effective speedup\nafforded by switching from random to alternating flip. For example, airbench94 with random flip\nand without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The\nsame configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law\nfit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.\nNote that using a power-law yields a more conservative estimate relative to using linear interpolation\nbetween the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.\nTable 2 shows the result. We observe the following patterns. First, the addition of extra augmentation\n(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that\nthe main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the\nsame way for many epochs in a row; we speculate that adding extra augmentation reduces the negative\nimpact of these cases because it increases data diversity. Next, TTA reduces the gap between random\nand alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),\nindicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training\nfor longer consistently increases the effective speedup given by alternating flip.\nWe next study ImageNet trainings with the following experiment. We train a ResNet-18 with a\nvariety of train and test crops, comparing three flipping options: alternating flip, random flip, and no\nflipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192\ncenter crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)\nto denote the latter. We also consider two training crops: 192x192 inception-style random resized\ncrop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area\nranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the\nshorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write\n7\nBaseline Cutout Epochs Speedup Speedup (w/ TTA)\nairbench94 No 10 15.0% 5.30%\nairbench94 No 20 27.1% 21.3%\nairbench94 No 40 38.3% 36.4%\nairbench94 No 80 102% 31.8%\nairbench94 Yes 10 3.84% 1.13%\nairbench94 Yes 20 7.42% 2.00%\nairbench94 Yes 40 18.6% 9.28%\nairbench94 Yes 80 29.2% 14.25%\nairbench96 Yes 10 4.94% 1.11%\nairbench96 Yes 20 8.99% 3.58%\nairbench96 Yes 40 17.2% 6.48%\nairbench96 Yes 80 18.8% Not measured\nTable 2: Effective speedups given by switching from random flip to alternating flip. The two configurations\nmost closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw\naccuracy values of the airbench94 experiments.\nFlipping augmentation option\nTrain crop Test crop Epochs TTA None Random Alternating\nHeavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28\nHeavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28\nLight RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26\nLight RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26\nHeavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28\nHeavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28\nLight RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26\nLight RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26\nLight RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16\nHeavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4\nTable 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for\nthose trainings where random flip improves significantly over not flipping at all. The single best flipping option\nin each row is bolded when the difference is statistically significant.\nHeavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided\nin Section C.\nTable 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC\nis better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly\nbetter when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test\nresolution discrepancy (Touvron et al., 2019).\nFor trainings which use Light RRC, we find that switching from random flip to alternating flip\nprovides a substantial boost to performance, amounting to a training speedup of more than 25%.\nIn Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to\nalternating flip improves performance by more than increasing the training duration from 16 to 20\nepochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our\nresults on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit\nfrom alternating flip. Indeed, even turning flipping off completely does not significantly reduce the\nperformance of these trainings. We conclude that alternating flip improves over random flip for every\ntraining scenario where the latter improves over no flipping at all.\n8\nEpochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE\n1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312\n2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233\n1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269\n1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533\n2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433\n1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444\nTable 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).\n5.3 Variance and class-wise calibration\nPrevious sections have focused on understanding what factors affect the first moment of accuracy\n(the mean). In this section we investigate the second moment, finding that TTA reduces variance at\nthe cost of calibration.\nOur experiment is to execute 10,000 runs of airbench94 training with several hyperparameter\nsettings. For each setting we report both the variance in test-set accuracy as well as an estimate of the\ndistribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.\nTable 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set\nvariance, replicating the main finding of Jordan (2023). This is a surprising result because these\ntrainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7\nas much distribution-wise variance when run for a similar duration, and reached a low variance only\nwhen run for 64 epochs. We conclude from this comparison that distribution-wise variance is more\nstrongly connected to the rate of convergence of a training rather than its duration as such. We also\nnote that the low distribution-wise variance of airbench94 indicates it has high training stability.\nUsing TTA significantly reduces the test-set variance, such that all three settings with TTA have lower\ntest-set variance than any setting without TTA. However, test-set variance is implied by the class-wise\ncalibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this\nreduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,\nwe compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which\nmeasures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA\nhas a higher CACE than every setting without TTA, confirming the hypothesis.\n6 Discussion\nIn this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7\nfaster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the\nairbench Python package.\nWe developed airbench solely with the goal of maximizing training speed on CIFAR-10. In\nSection B we find that it also generalizes well to other tasks. For example, without any extra tuning,\nairbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.\nOne factor contributing to the training speed of airbench was our finding that training can be accel-\nerated by partially derandomizing the standard random horizontal flipping augmentation, resulting in\nthe variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating\nflip improves the performance of every training we considered (Section 5.2), with the exception of\nthose trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to\nus, the standard ImageNet trainings that we considered do not significantly benefit from horizontal\nflipping. Future work might investigate whether it is possible to obtain derandomized improvements\nto other augmentations besides horizontal flip.\nThe methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,\nwith fixed performance and hardware constraints. These constraints mean that we cannot improve\nperformance by simply scaling up the amount of computational resources used; instead we are forced\nto develop new methods like the alternating flip. We look forward to seeing what other new methods\nfuture work discovers to push training speed further.\n9\nReferences\nDimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.\nDan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-\nperformance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,\n2011.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\nmark and competition. 2017.\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet\nor cifar-10. arXiv preprint arXiv:1810.03505 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\npp. 248\u2013255. IEEE, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552 , 2017.\nMert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic\ngradient descent. Mathematical Programming , 186:49\u201384, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 , 2016.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.\npmlr, 2015.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\nsgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.\nKeller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and\ninevitable. arXiv preprint arXiv:2304.01910 , 2023.\nAndrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.\nio/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for\nadvanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT\nLicense.\nGuillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander\nM \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,\nBenjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.\nBenchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural\nInformation Processing Systems , 35:25404\u201325421, 2022.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.\nDavid Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/\nhow-to-train-your-resnet-8-bag-of-tricks/ .\n10\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics , 22(3):400\u2013407, 1951.\nSantiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:\n//github.com/apple/ml-cifar-10-faster , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.\ntysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/\nhlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. Advances in neural information processing systems , 32, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,\n2016.\nMichael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps\nforward, 1 step back. Advances in neural information processing systems , 32, 2019.\n11\nA Network architecture\n1from torch import nn\n2\n3class Flatten (nn. Module ):\n4 def forward (self , x):\n5 return x. view (x. size (0) , -1)\n6\n7class Mul (nn. Module ):\n8 def __init__ (self , scale ):\n9 super (). __init__ ()\n10 self . scale = scale\n11 def forward (self , x):\n12 return x * self . scale\n13\n14def conv (ch_in , ch_out ):\n15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,\n16 padding =\u2019same \u2019, bias = False )\n17\n18def make_net ():\n19 act = lambda : nn. GELU ()\n20 bn = lambda ch: nn. BatchNorm2d (ch)\n21 return nn. Sequential (\n22 nn. Sequential (\n23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),\n24 act () ,\n25 ),\n26 nn. Sequential (\n27 conv (24 , 64) ,\n28 nn. MaxPool2d (2) ,\n29 bn (64) , act () ,\n30 conv (64 , 64) ,\n31 bn (64) , act () ,\n32 ),\n33 nn. Sequential (\n34 conv (64 , 256) ,\n35 nn. MaxPool2d (2) ,\n36 bn (256) , act () ,\n37 conv (256 , 256) ,\n38 bn (256) , act () ,\n39 ),\n40 nn. Sequential (\n41 conv (256 , 256) ,\n42 nn. MaxPool2d (2) ,\n43 bn (256) , act () ,\n44 conv (256 , 256) ,\n45 bn (256) , act () ,\n46 ),\n47 nn. MaxPool2d (3) ,\n48 Flatten () ,\n49 nn. Linear (256 , 10, bias = False ),\n50 Mul (1/9) ,\n51 )\nListing 3: Minimal PyTorch code for the network architecture used by airbench94 .\nWe note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock\ntime. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel\nsize of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this\ndoes not improve the wallclock training time on an A100. Reducing the batch size is another easy\nway to save FLOPs but not wallclock time.\n12\nDataset Flipping? Cutout? ResNet-18 airbench96\nCIFAR-10 Yes No 95.55% 95.61%\nCIFAR-10 Yes Yes 96.01% 96.05%\nCIFAR-100 Yes No 77.54% 79.27%\nCIFAR-100 Yes Yes 78.04% 79.76%\nCINIC-10 Yes No 87.58% 87.78%\nCINIC-10 Yes Yes not measured 88.22%\nSVHN No No 97.35% 97.38%\nSVHN No Yes not measured 97.64%\nTable 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We\ndirectly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off\nflipping for SVHN).\nB Extra dataset experiments\nWe developed airbench with the singular goal of maximizing training speed on CIFAR-10. To\nfind out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its\nperformance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-\n10 (Darlow et al., 2018).\nOn CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both\nthe case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not\n(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than\nResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say\nthat it generalizes.\nWe compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.\nWe do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal\non CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better\nperformance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where\nairbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is\nused and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows\nstrong generalization to other tasks.\nWe note that this comparison between airbench96 and ResNet-18 training is fair in the sense that\nit does demonstrate that the former has good generalization, but unfair in the sense that it does not\nindicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time\naugmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training\nwould outperform airbench96 if it were run using test-time augmentation. However, it also takes\n5-10 times longer to complete. The decision of which to use may be situational.\nThe accuracy values we report for ResNet-18 training are from the following sources. We tried\nto select the highest values we could find for each setting. Moreau et al. (2022) reports attaining\n95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports\nattaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on\nCIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.\nC ImageNet training details\nOur ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/\nlibffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and\nmomentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution\n160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of\ntraining. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.\n13\nHyperparameters Flipping augmentation option\nEpochs Cutout TTA None Random Alternating\n10 No No 92.3053 93.0988 93.2798\n20 No No 92.8166 93.7446 93.8652\n40 No No 93.0143 94.0133 94.0729\n80 No No 93.0612 94.1169 94.1628\n10 No Yes 93.4071 93.9488 94.0186\n20 No Yes 93.8528 94.5565 94.6530\n40 No Yes 94.0381 94.7803 94.8203\n80 No Yes 94.0638 94.8506 94.8676\n10 Yes No 91.8487 92.0402 92.1374\n20 Yes No 92.8474 93.3825 93.4876\n40 Yes No 93.2675 94.1014 94.1952\n80 Yes No 93.4193 94.4311 94.5204\n10 Yes Yes 92.6455 92.7780 92.8103\n20 Yes Yes 93.7862 94.1306 94.1670\n40 Yes Yes 94.3090 94.8511 94.8960\n80 Yes Yes 94.5253 95.1839 95.2362\nTable 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a\nmean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise\ndifference in means is statistically significant.\nD Extra tables & figures\nFigure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in\nSection 5.3.\nE Complete training code\n1\"\"\"\n2airbench94 .py\n33.83 s runtime on an A100 ; 0.36 PFLOPs .\n4Evidence for validity : 94.01 average accuracy in n =1000 runs .\n5\n6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB\nwith the following nvidia -smi:\n7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :\n11.7\n14\n8torch . __version__ == \u20192.1.2+ cu118 \u2019\n9\"\"\"\n10\n11# ############################################\n12# Setup / Hyperparameters #\n13# ############################################\n14\n15import os\n16import sys\n17import uuid\n18from math import ceil\n19\n20import torch\n21from torch import nn\n22import torch .nn. functional as F\n23import torchvision\n24import torchvision . transforms as T\n25\n26torch . backends . cudnn . benchmark = True\n27\n28\"\"\"\n29We express the main training hyperparameters ( batch size , learning\nrate , momentum , and weight decay ) in decoupled form , so that each\none can be tuned independently . This accomplishes the following :\n30* Assuming time - constant gradients , the average step size is decoupled\nfrom everything but the lr.\n31* The size of the weight decay update is decoupled from everything but\nthe wd.\n32In constrast , normally when we increase the ( Nesterov ) momentum , this\nalso scales up the step size proportionally to 1 + 1 / (1 -\nmomentum ), meaning we cannot change momentum without having to re -\ntune the learning rate . Similarly , normally when we increase the\nlearning rate this also increases the size of the weight decay ,\nrequiring a proportional decrease in the wd to maintain the same\ndecay strength .\n33\n34The practical impact is that hyperparameter tuning is faster , since\nthis parametrization allows each one to be tuned independently .\nSee https :// myrtle .ai/ learn /how -to -train -your - resnet -5-\nhyperparameters /.\n35\"\"\"\n36\n37hyp = {\n38 \u2019opt \u2019: {\n39 \u2019 train_epochs \u2019: 9.9 ,\n40 \u2019batch_size \u2019: 1024 ,\n41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples\n42 \u2019momentum \u2019: 0.85 ,\n43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (\ndecoupled from learning rate )\n44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not\nweight decay ) for BatchNorm biases\n45 \u2019 label_smoothing \u2019: 0.2 ,\n46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the\nwhitening layer bias before freezing\n47 },\n48 \u2019aug \u2019: {\n49 \u2019flip \u2019: True ,\n50 \u2019translate \u2019: 2,\n51 },\n52 \u2019net \u2019: {\n53 \u2019widths \u2019: {\n54 \u2019block1 \u2019: 64,\n55 \u2019block2 \u2019: 256 ,\n56 \u2019block3 \u2019: 256 ,\n15\n57 },\n58 \u2019 batchnorm_momentum \u2019: 0.6 ,\n59 \u2019 scaling_factor \u2019: 1/9 ,\n60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :\n0= none , 1= mirror , 2= mirror + translate\n61 },\n62}\n63\n64# ############################################\n65# DataLoader #\n66# ############################################\n67\n68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )\n69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )\n70\n71def batch_flip_lr ( inputs ):\n72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .\nview (-1, 1, 1, 1)\n73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\n74\n75def batch_crop (images , crop_size ):\n76 r = ( images . size ( -1) - crop_size )//2\n77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =\nimages . device )\n78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),\ndevice = images .device , dtype = images . dtype )\n79 # The two cropping methods in this if - else produce equivalent\nresults , but the second is faster for r > 2.\n80 if r <= 2:\n81 for sy in range (-r, r +1):\n82 for sx in range (-r, r +1):\n83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)\n84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size\n, r+sx:r+sx+ crop_size ]\n85 else :\n86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size\n+2* r), device = images .device , dtype = images . dtype )\n87 for s in range (-r, r +1):\n88 mask = ( shifts [:, 0] == s)\n89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]\n90 for s in range (-r, r +1):\n91 mask = ( shifts [:, 1] == s)\n92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+\ncrop_size ]\n93 return images_out\n94\n95class CifarLoader :\n96 \"\"\"\n97 GPU - accelerated dataloader for CIFAR -10 which implements\nalternating flip augmentation .\n98 \"\"\"\n99\n100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,\ndrop_last =None , shuffle =None , gpu =0):\n101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .\npt \u2019)\n102 if not os. path . exists ( data_path ):\n103 dset = torchvision . datasets . CIFAR10 (path , download =True ,\ntrain = train )\n104 images = torch . tensor ( dset . data )\n105 labels = torch . tensor ( dset . targets )\n106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:\ndset . classes }, data_path )\n107\n108 data = torch . load ( data_path , map_location = torch . device (gpu))\n16\n109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [\n\u2019labels \u2019], data [\u2019classes \u2019]\n110 # It \u2019s faster to load + process uint8 data than to load\npreprocessed fp16 data\n111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).\nto( memory_format = torch . channels_last )\n112\n113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )\n114 self . proc_images = {} # Saved results of image processing to\nbe done on the first epoch\n115 self . epoch = 0\n116\n117 self . aug = aug or {}\n118 for k in self .aug . keys ():\n119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019\n% k\n120\n121 self . batch_size = batch_size\n122 self . drop_last = train if drop_last is None else drop_last\n123 self . shuffle = train if shuffle is None else shuffle\n124\n125 def __len__ ( self ):\n126 return len( self . images )// self . batch_size if self . drop_last\nelse ceil (len( self . images )/ self . batch_size )\n127\n128 def __iter__ ( self ):\n129\n130 if self . epoch == 0:\n131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .\nimages )\n132 # Randomly flip all images on the first epoch as according\nto definition of alternating flip\n133 if self .aug.get(\u2019flip \u2019, False ):\n134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (\nimages )\n135 # Pre -pad images to save time when doing random\ntranslation\n136 pad = self .aug.get (\u2019translate \u2019, 0)\n137 if pad > 0:\n138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019\nreflect \u2019)\n139\n140 if self .aug.get(\u2019translate \u2019, 0) > 0:\n141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .\nshape [ -2])\n142 elif self .aug.get (\u2019flip \u2019, False ):\n143 images = self . proc_images [\u2019flip \u2019]\n144 else :\n145 images = self . proc_images [\u2019norm \u2019]\n146 if self .aug.get(\u2019flip \u2019, False ):\n147 if self . epoch % 2 == 1:\n148 images = images . flip ( -1)\n149\n150 self . epoch += 1\n151\n152 indices = ( torch . randperm if self . shuffle else torch . arange )(\nlen ( images ), device = images . device )\n153 for i in range (len ( self )):\n154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]\n155 yield ( images [ idxs ], self . labels [ idxs ])\n156\n157# ############################################\n158# Network Components #\n159# ############################################\n160\n17\n161class Flatten (nn. Module ):\n162 def forward (self , x):\n163 return x. view (x. size (0) , -1)\n164\n165class Mul (nn. Module ):\n166 def __init__ (self , scale ):\n167 super (). __init__ ()\n168 self . scale = scale\n169 def forward (self , x):\n170 return x * self . scale\n171\n172class BatchNorm (nn. BatchNorm2d ):\n173 def __init__ (self , num_features , momentum , eps =1e -12 ,\n174 weight =False , bias = True ):\n175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )\n176 self . weight . requires_grad = weight\n177 self . bias . requires_grad = bias\n178 # Note that PyTorch already initializes the weights to one and\nbiases to zero\n179\n180class Conv (nn. Conv2d ):\n181 def __init__ (self , in_channels , out_channels , kernel_size =3,\npadding =\u2019same \u2019, bias = False ):\n182 super (). __init__ ( in_channels , out_channels , kernel_size =\nkernel_size , padding = padding , bias = bias )\n183\n184 def reset_parameters ( self ):\n185 super (). reset_parameters ()\n186 if self . bias is not None :\n187 self . bias . data . zero_ ()\n188 w = self . weight . data\n189 torch .nn. init . dirac_ (w[:w. size (1) ])\n190\n191class ConvGroup (nn. Module ):\n192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):\n193 super (). __init__ ()\n194 self . conv1 = Conv ( channels_in , channels_out )\n195 self . pool = nn. MaxPool2d (2)\n196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )\n197 self . conv2 = Conv ( channels_out , channels_out )\n198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )\n199 self . activ = nn. GELU ()\n200\n201 def forward (self , x):\n202 x = self . conv1 (x)\n203 x = self . pool (x)\n204 x = self . norm1 (x)\n205 x = self . activ (x)\n206 x = self . conv2 (x)\n207 x = self . norm2 (x)\n208 x = self . activ (x)\n209 return x\n210\n211# ############################################\n212# Network Definition #\n213# ############################################\n214\n215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019\n][ \u2019 batchnorm_momentum \u2019]):\n216 whiten_kernel_size = 2\n217 whiten_width = 2 * 3 * whiten_kernel_size **2\n218 net = nn. Sequential (\n219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True\n),\n220 nn. GELU () ,\n18\n221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],\nbatchnorm_momentum ),\n222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],\nbatchnorm_momentum ),\n223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],\nbatchnorm_momentum ),\n224 nn. MaxPool2d (3) ,\n225 Flatten () ,\n226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),\n227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,\n228 )\n229 net [0]. weight . requires_grad = False\n230 net = net. half (). cuda ()\n231 net = net.to( memory_format = torch . channels_last )\n232 for mod in net. modules ():\n233 if isinstance (mod , BatchNorm ):\n234 mod . float ()\n235 return net\n236\n237# ############################################\n238# Whitening Conv Initialization #\n239# ############################################\n240\n241def get_patches (x, patch_shape ):\n242 c, (h, w) = x. shape [1] , patch_shape\n243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h\n,w). float ()\n244\n245def get_whitening_parameters ( patches ):\n246 n,c,h,w = patches . shape\n247 patches_flat = patches . view (n, -1)\n248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n\n249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance\n, UPLO =\u2019U\u2019)\n250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.\nreshape (c*h*w,c,h,w). flip (0)\n251\n252def init_whitening_conv (layer , train_set , eps =5e -4):\n253 patches = get_patches ( train_set , patch_shape = layer . weight . data .\nshape [2:])\n254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )\n255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)\n256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -\neigenvectors_scaled ))\n257\n258# ###########################################\n259# Lookahead #\n260# ###########################################\n261\n262class LookaheadState :\n263 def __init__ (self , net):\n264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().\nitems ()}\n265\n266 def update (self , net , decay ):\n267 for ema_param , net_param in zip( self . net_ema . values () , net.\nstate_dict (). values ()):\n268 if net_param . dtype in ( torch .half , torch . float ):\n269 ema_param . lerp_ ( net_param , 1- decay )\n270 net_param . copy_ ( ema_param )\n271\n272# ###########################################\n273# Logging #\n274# ###########################################\n275\n19\n276def print_columns ( columns_list , is_head =False , is_final_entry = False ):\n277 print_string = \u2019\u2019\n278 for col in columns_list :\n279 print_string += \u2019| %s \u2019 % col\n280 print_string += \u2019|\u2019\n281 if is_head :\n282 print (\u2019-\u2019*len( print_string ))\n283 print ( print_string )\n284 if is_head or is_final_entry :\n285 print (\u2019-\u2019*len( print_string ))\n286\n287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,\n\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]\n288def print_training_details ( variables , is_final_entry ):\n289 formatted = []\n290 for col in logging_columns_list :\n291 var = variables .get (col. strip () , None )\n292 if type (var) in (int , str):\n293 res = str(var)\n294 elif type (var) is float :\n295 res = \u2019{:0.4 f}\u2019. format ( var)\n296 else :\n297 assert var is None\n298 res = \u2019\u2019\n299 formatted . append (res. rjust (len (col)))\n300 print_columns ( formatted , is_final_entry = is_final_entry )\n301\n302# ###########################################\n303# Evaluation #\n304# ###########################################\n305\n306def infer (model , loader , tta_level =0) :\n307 \"\"\"\n308 Test - time augmentation strategy (for tta_level =2):\n309 1. Flip / mirror the image left -to - right (50% of the time ).\n310 2. Translate the image by one pixel either up -and - left or down -and\n- right (50% of the time , i.e. both happen 25% of the time ).\n311\n312 This creates 6 views per image ( left / right times the two\ntranslations and no - translation ), which we evaluate and then\nweight according to the given probabilities .\n313 \"\"\"\n314\n315 def infer_basic (inputs , net):\n316 return net( inputs ). clone ()\n317\n318 def infer_mirror (inputs , net):\n319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))\n320\n321 def infer_mirror_translate ( inputs , net ):\n322 logits = infer_mirror (inputs , net )\n323 pad = 1\n324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)\n325 inputs_translate_list = [\n326 padded_inputs [:, :, 0:32 , 0:32] ,\n327 padded_inputs [:, :, 2:34 , 2:34] ,\n328 ]\n329 logits_translate_list = [ infer_mirror ( inputs_translate , net)\n330 for inputs_translate in\ninputs_translate_list ]\n331 logits_translate = torch . stack ( logits_translate_list ). mean (0)\n332 return 0.5 * logits + 0.5 * logits_translate\n333\n334 model . eval ()\n335 test_images = loader . normalize ( loader . images )\n20\n336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][\ntta_level ]\n337 with torch . no_grad ():\n338 return torch .cat ([ infer_fn (inputs , model ) for inputs in\ntest_images . split (2000) ])\n339\n340def evaluate (model , loader , tta_level =0) :\n341 logits = infer (model , loader , tta_level )\n342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()\n343\n344# ###########################################\n345# Training #\n346# ###########################################\n347\n348def main (run):\n349\n350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]\n351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]\n352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]\n353 # Assuming gradients are constant in time , for Nesterov momentum ,\nthe below ratio is how much larger the default steps will be than\nthe underlying per - example gradients . We divide the learning rate\nby this ratio in order to ensure steps are the same scale as\ngradients , regardless of the choice of momentum .\n354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))\n355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning\nrate for PyTorch SGD\n356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale\n357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]\n358\n359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019\nlabel_smoothing \u2019], reduction =\u2019none \u2019)\n360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)\n361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =\nbatch_size , aug=hyp[\u2019aug \u2019])\n362 if run == \u2019warmup \u2019:\n363 # The only purpose of the first run is to warmup , so we can\nuse dummy data\n364 train_loader . labels = torch . randint (0, 10, size =( len (\ntrain_loader . labels ) ,), device = train_loader . labels . device )\n365 total_train_steps = ceil (len( train_loader ) * epochs )\n366\n367 model = make_net ()\n368 current_steps = 0\n369\n370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in\nk and p. requires_grad ]\n371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019\nnot in k and p. requires_grad ]\n372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,\nweight_decay =wd/ lr_biases ),\n373 dict ( params = other_params , lr=lr , weight_decay =wd/\nlr)]\n374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,\nnesterov = True )\n375\n376 def triangle (steps , start =0, end =0, peak =0.5) :\n377 xp = torch . tensor ([0 , int( peak * steps ), steps ])\n378 fp = torch . tensor ([ start , 1, end ])\n379 x = torch . arange (1+ steps )\n380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])\n381 b = fp [: -1] - (m * xp [: -1])\n382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1\n383 indices = torch . clamp ( indices , 0, len(m) - 1)\n384 return m[ indices ] * x + b[ indices ]\n21\n385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,\npeak =0.23)\n386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:\nlr_schedule [i])\n387\n388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /\ntotal_train_steps )**3\n389 lookahead_state = LookaheadState ( model )\n390\n391 # For accurately timing GPU code\n392 starter = torch . cuda . Event ( enable_timing = True )\n393 ender = torch . cuda . Event ( enable_timing = True )\n394 total_time_seconds = 0.0\n395\n396 # Initialize the first layer using statistics of training images\n397 starter . record ()\n398 train_images = train_loader . normalize ( train_loader . images [:5000])\n399 init_whitening_conv ( model [0] , train_images )\n400 ender . record ()\n401 torch . cuda . synchronize ()\n402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n403\n404 for epoch in range ( ceil ( epochs )):\n405\n406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019\nwhiten_bias_epochs \u2019])\n407\n408 # ###################\n409 # Training #\n410 # ###################\n411\n412 starter . record ()\n413\n414 model . train ()\n415 for inputs , labels in train_loader :\n416\n417 outputs = model ( inputs )\n418 loss = loss_fn ( outputs , labels ).sum ()\n419 optimizer . zero_grad ( set_to_none = True )\n420 loss . backward ()\n421 optimizer . step ()\n422 scheduler . step ()\n423\n424 current_steps += 1\n425\n426 if current_steps % 5 == 0:\n427 lookahead_state . update (model , decay = alpha_schedule [\ncurrent_steps ]. item ())\n428\n429 if current_steps >= total_train_steps :\n430 if lookahead_state is not None :\n431 lookahead_state . update (model , decay =1.0)\n432 break\n433\n434 ender . record ()\n435 torch . cuda . synchronize ()\n436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n437\n438 # ###################\n439 # Evaluation #\n440 # ###################\n441\n442 # Print the accuracy and loss from the last training batch of\nthe epoch\n22\n443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().\nmean (). item ()\n444 train_loss = loss . item () / batch_size\n445 val_acc = evaluate (model , test_loader , tta_level =0)\n446 print_training_details ( locals () , is_final_entry = False )\n447 run = None # Only print the run number once\n448\n449 # ###################\n450 # TTA Evaluation #\n451 # ###################\n452\n453 starter . record ()\n454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019\ntta_level \u2019])\n455 ender . record ()\n456 torch . cuda . synchronize ()\n457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n458\n459 epoch = \u2019eval \u2019\n460 print_training_details ( locals () , is_final_entry = True )\n461\n462 return tta_val_acc\n463\n464if __name__ == \" __main__ \":\n465 with open (sys. argv [0]) as f:\n466 code = f. read ()\n467\n468 print_columns ( logging_columns_list , is_head = True )\n469 main (\u2019warmup \u2019)\n470 accs = torch . tensor ([ main (run) for run in range (25) ])\n471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))\n472\n473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }\n474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))\n475 os. makedirs ( log_dir , exist_ok = True )\n476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)\n477 print (os. path . abspath ( log_path ))\n478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))\nListing 4: airbench94.py\n23\n\nCandidate Output: tasks=[TaskCandidate(name='Implement Alternating Flip Data Augmentation', description='Implement the alternating flip data augmentation method described in the paper. This method deterministically flips images in alternating epochs after the first epoch, avoiding redundancy and speeding up training.', relevant_paper_text='To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.', scoring_feasibility=9, llm_tractability=0.8, expert_tractability=0.95, layman_tractability=0.3), TaskCandidate(name='Implement Patch-Whitening Initialization', description=\"Implement the patch-whitening initialization for the first convolutional layer as described in the paper. This involves initializing the layer's weights based on the eigenvectors of the covariance matrix of 2x2 patches across the training distribution.\", relevant_paper_text='Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.', scoring_feasibility=8, llm_tractability=0.6, expert_tractability=0.9, layman_tractability=0.1), TaskCandidate(name='Implement Multi-Crop Test-Time Augmentation', description='Implement the multi-crop test-time augmentation (TTA) method described in the paper. This involves running the trained network on six augmented views of each test image and combining the outputs using a weighted average.', relevant_paper_text='To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.', scoring_feasibility=9, llm_tractability=0.7, expert_tractability=0.95, layman_tractability=0.4), TaskCandidate(name='Implement Identity Initialization for Convolutions', description='Implement the identity initialization method for convolutional layers after the first layer, as described in the paper. This involves initializing the first M filters of each convolution with N>=M outputs as an identity transform of the input.', relevant_paper_text='We initialize all convolutions after the first as partial identity transforms. That is, for a convolution with M input channels and N>=M outputs, we initialize its first M filters to an identity transform of the input, and leave the remaining N-M to their default initialization.', scoring_feasibility=8, llm_tractability=0.7, expert_tractability=0.9, layman_tractability=0.2), TaskCandidate(name='Implement Lookahead Optimization', description='Implement the Lookahead optimization method as described in the paper. This involves maintaining a slow-moving copy of the model parameters and periodically updating the fast-moving parameters towards the slow-moving ones.', relevant_paper_text='Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization. We note that Lookahead has also been found effective in prior work on training speed for ResNet-18 (Moreau et al., 2022).', scoring_feasibility=7, llm_tractability=0.6, expert_tractability=0.85, layman_tractability=0.1)]\n\nRequirements:\nExtract all promising task candidates from paper. Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\n\n    A good task candidate:\n    - describes a task that an agent can implement\n    - typically a programming task, e.g. modifying a 'baseline' implementation\n    - which reflects or mirrors actual tasks undertaken to produce the research paper\n    - and requires research and/or engineering expertise to implement correctly\n    - for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\n    - ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\n    - the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\n    \n\nAttempt Num: 2\n\nPrevious Critiques:\nSelf Critique:\n\nI'll evaluate the task candidates I extracted based on the given requirements:\n\n1. Implement alternating flip data augmentation:\nStrengths:\n- Describes a concrete programming task to modify a baseline implementation\n- Reflects an actual task undertaken in the research\n- Requires engineering expertise to implement correctly\n- Relatively straightforward to write an objective scoring function (e.g. check if images are flipped in the correct pattern across epochs)\n- Scoring function would be much easier to produce than implementing the task itself\n\nWeaknesses:\n- May not require significant research capabilities, mostly engineering\n\n2. Implement decoupled hyperparameters:\nStrengths:\n- Describes a programming task to modify hyperparameter handling\n- Reflects work done in the research\n- Requires some engineering expertise\n\nWeaknesses:\n- May be challenging to write an objective scoring function beyond checking if the hyperparameters are decoupled\n- Implementing the task may not be significantly harder than writing the scoring function\n- Does not require much research capability\n\n3. Implement patch-whitening initialization:\nStrengths:\n- Describes a concrete programming task\n- Reflects actual work from the research\n- Requires both research understanding and engineering skill\n- Possible to write an objective scoring function (e.g. check if filters are initialized correctly)\n- Scoring function would be easier than implementation\n\nWeaknesses:\n- May be very challenging for an agent without strong math/ML background\n\nOverall, the alternating flip and patch-whitening tasks seem to best meet the criteria. The decoupled hyperparameters task is less suitable. I could improve by extracting more task candidates from other parts of the paper, especially those requiring more research capabilities.\nSelf Critique:\n\nUpon reviewing my task candidate extraction and evaluation, I can identify several areas for improvement:\n\n1. Quantity of task candidates: I only extracted three task candidates, which is likely insufficient to cover the full range of research and engineering tasks described in the paper. I should have extracted more candidates to provide a more comprehensive set of options.\n\n2. Research vs Engineering balance: The tasks I extracted lean more heavily towards engineering implementation rather than research. I should have sought out more tasks that require deeper understanding and application of the research concepts presented in the paper.\n\n3. Missed important tasks: I overlooked several significant tasks described in the paper, such as implementing the identity initialization for convolutions, the Lookahead optimization, and the overall network architecture. These are important components of the research that should have been included as task candidates.\n\n4. Depth of analysis: While I provided some evaluation of the tasks against the criteria, my analysis could have been more thorough and systematic, addressing each point in the requirements more explicitly for each task.\n\n5. Scoring function consideration: I didn't give enough attention to the requirement of having easily implementable scoring functions that are much simpler than the tasks themselves. This is a crucial aspect that I should have emphasized more in my evaluation.\n\n6. Relevance to paper's core contributions: I could have focused more on tasks that directly relate to the paper's main contributions and novel aspects, rather than including more general ML implementation tasks.\n\nTo improve, I should re-examine the paper more carefully, extract a wider range of task candidates that better balance research and engineering aspects, and provide a more thorough evaluation of each candidate against all the specified criteria.\n\nSelf Critique:\nThank you for the feedback. I agree that my previous attempts had significant room for improvement. I'll address these issues and provide a more comprehensive and balanced set of task candidates with thorough evaluations.\n\nSelf Critique: After reviewing my latest attempt, I believe I have made significant improvements:\n\n1. Quantity and diversity of task candidates: I extracted 5 distinct task candidates covering various aspects of the paper's contributions, including both engineering and research-oriented tasks.\n\n2. Research vs Engineering balance: The tasks now include a better mix of engineering implementation (e.g., alternating flip) and tasks requiring deeper research understanding (e.g., patch-whitening initialization).\n\n3. Coverage of important tasks: I included key components like alternating flip, patch-whitening initialization, multi-crop TTA, identity initialization, and Lookahead optimization, which are all significant contributions from the paper.\n\n4. Depth of analysis: For each task, I provided relevant paper text and scores for feasibility and tractability across different skill levels, allowing for a more nuanced evaluation.\n\n5. Scoring function consideration: While I didn't explicitly discuss scoring functions for each task, the high feasibility scores imply that creating objective scoring functions would be relatively straightforward for most tasks.\n\n6. Relevance to paper's core contributions: All extracted tasks directly relate to the paper's main contributions and novel aspects.\n\nAreas for potential further improvement:\n- I could provide more explicit discussion of potential scoring functions for each task.\n- The descriptions could be more detailed, including potential challenges or subtleties in implementation.\n- I could extract even more task candidates to provide a wider range of options.\n\nOverall, I believe this attempt is a substantial improvement over the previous ones and better meets the given requirements.\n\nShould Retry:"
  },
  {
    "Response": "Self Critique: After reviewing my latest attempt, I believe I have made significant improvements:\n\n1. Quantity and diversity of task candidates: I extracted 5 distinct task candidates covering various aspects of the paper's contributions, including both engineering and research-oriented tasks.\n\n2. Research vs Engineering balance: The tasks now include a better mix of engineering implementation (e.g., alternating flip) and tasks requiring deeper research understanding (e.g., patch-whitening initialization).\n\n3. Coverage of important tasks: I included key components like alternating flip, patch-whitening initialization, multi-crop TTA, identity initialization, and Lookahead optimization, which are all significant contributions from the paper.\n\n4. Depth of analysis: For each task, I provided relevant paper text and scores for feasibility and tractability across different skill levels, allowing for a more nuanced evaluation.\n\n5. Scoring function consideration: While I didn't explicitly discuss scoring functions for each task, the high feasibility scores imply that creating objective scoring functions would be relatively straightforward for most tasks.\n\n6. Relevance to paper's core contributions: All extracted tasks directly relate to the paper's main contributions and novel aspects.\n\nAreas for potential further improvement:\n- I could provide more explicit discussion of potential scoring functions for each task.\n- The descriptions could be more detailed, including potential challenges or subtleties in implementation.\n- I could extract even more task candidates to provide a wider range of options.\n\nOverall, I believe this attempt is a substantial improvement over the previous ones and better meets the given requirements.\n\nShould Retry: false"
  },
  {
    "Request": "Given the fields `candidate_input`, `candidate_output`, `requirements`, `attempt_num`, `previous_critiques`, produce the fields `self_critique`, `should_retry`.\n\n---\n\nFollow the following format.\n\nCandidate Input: ${candidate_input}\n\nCandidate Output: ${candidate_output}\n\nRequirements: ${requirements}\n\nAttempt Num: ${attempt_num}\n\nPrevious Critiques: ${previous_critiques}\n\nPast Error in Should Retry: An error to avoid in the future\n\nSelf Critique: Self-critique of the task candidate\n\nShould Retry: Should the task candidate be retried? This may be expensive. True/False (Respond with true or false)\n\n---\n\nCandidate Input:\n94% on CIFAR-10 in 3.29 Seconds on a Single GPU\nKeller Jordan\nkjordan4077@gmail.com\nAbstract\nCIFAR-10 is among the most widely used datasets in machine learning, facilitating\nthousands of research projects per year. To accelerate research and reduce the cost\nof experiments, we introduce training methods for CIFAR-10 which reach 94%\naccuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when\nrun on a single NVIDIA A100 GPU. As one factor contributing to these training\nspeeds, we propose a derandomized variant of horizontal flipping augmentation,\nwhich we show improves over the standard method in every case where flipping is\nbeneficial over no flipping at all. Our code is released at https://github.com/\nKellerJordan/cifar10-airbench .\n1 Introduction\nCIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-\ning thousands of research projects per year1. Research can be accelerated and the cost of experiments\nreduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In\nthis paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single\nNVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,\n2023). To support scenarios where higher performance is needed, we additionally develop methods\ntargeting 95% and 96% accuracy. We release the following methods in total.\n1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).\n2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).\n3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).\n4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).\nAll runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-\nmatically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the\nfirst uses torch.compile to improve GPU utilization. It is intended for experiments where many\nnetworks are trained at once in order to amortize the one-time compilation cost. The non-compiled\nairbench94 variant can be easily installed and run using the following command.\n1pip install airbench\n2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"\nOne motivation for the development of these training methods is that they can accelerate the experi-\nmental iteration time of researchers working on compatible projects involving CIFAR-10. Another\nmotivation is that they can decrease the cost of projects involving a massive number of trained\nnetworks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used\n3 million trained networks to demonstrate that the outputs of a trained neural network on a given test\ninput follow an approximately linear function of the vector of binary choices of which examples the\nmodel was trained on. Another example is Jordan (2023), a study on training variance which used\n180 thousand trained networks to show that standard trainings have little variance in performance on\n1https://paperswithcode.com/datasets\nPreprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024\nFigure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal\nflipping augmentation, which flips each image with 50% probability per epoch. This results in some images\nbeing redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a\ndeterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\ntheir test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds\nand 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make\nit possible to replicate these studies, or conduct similar ones, with fewer computational resources.\nFast training also enables the rapid accumulation of statistical significance for subtle hyperparameter\ncomparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10\naccuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation\nbetween runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the\nimprovement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training\nthis will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.\nOur work builds on prior training speed projects. We utilize a modified version of the network,\ninitialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen\npatch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over\nprior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-\nure 1, Section 3.6, Section 5.2).\n2 Background\nOur objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set\nin the shortest possible amount of time. Timing begins when the method is first given access to\ntraining data, and ends when it produces test-set predictions. The method is considered valid if its\nmean accuracy over repeated runs is at least 94%.\nWe chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the\n2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more\nrecent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10\nseconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page\n(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is\ntysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,\n94% is the level of human accuracy reported by Karpathy (2011).\nWe note the following consequences of how the method is timed. First, it is permitted for the program\nto begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins\nwhen the training data is first accessed. This is helpful because otherwise the first run of training\nis typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA\nimproves the performance of a trained network by running it on multiple augmented views of each\ntest input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping\nTTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods\nattain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\nThe CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.\nIt is divided into a training set of 50,000 images and a validation set of 10,000 images. As a\nmatter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was\n2\n80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than\nairbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94\ncan be attributed entirely to algorithmic progress rather than compute scaling.\n3 Methods\n3.1 Network architecture and baseline training\nWe train a convolutional network with a total of 1.97 million parameters, following tysam-code\n(2023) with a few small changes. It contains seven convolutions with the latter six being divided into\nthree blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this\nsection we offer some comments on the main design choices.\nThe network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed\nentirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,\n2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with\nno padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather\nthan the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff\nbetween throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\nFollowing Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,\nand disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is\nscaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture\ndiffers only in that we decrease the number of output channels in the third block from 512 to 256,\nand we add learnable biases to the first convolution.\nAs our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of\n0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the\nmaximum at 20% of the way through training, and then decreases to zero. For data augmentation we\nuse random horizontal flipping alongside 2-pixel random translation. For translation we use reflection\npadding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note\nthat what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a\nrandom 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the\nnetwork is run on both a given test image and its mirror and inferences are made based on the average\nof the two outputs. With optimized choices of learning rate, momentum, and weight decay, this\nbaseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.\n3.2 Frozen patch-whitening initialization\nFollowing Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-\nwhitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code\n(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches\nacross the training distribution, so that their outputs have identity covariance matrix. The second 12\nFigure 2: The first layer\u2019s weights after whitening\ninitialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,\nso that input information is preserved through the\nactivation which follows. Figure 2 shows the result.\nWe do not update this layer\u2019s weights during training.\nDeparting from tysam-code (2023), we add learn-\nable biases to this layer, yielding a small performance\nboost. The biases are trained for 3 epochs, after which\nwe disable their gradient to increase backward-pass\nthroughput, which improves training speed without\nreducing accuracy. We also obtain a slight perfor-\nmance boost relative to tysam-code (2023) by re-\nducing the constant added to the eigenvalues during\ncalculation of the patch-whitening initialization for\nthe purpose of preventing numerical issues in the case\nof a singular patch-covariance matrix.\nPatch-whitening initialization is the single most impactful feature. Adding it to the baseline more\nthan doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.\n3\nRandom reshuffling Alternating flip Mean accuracy\nNo No 93.40%\nNo Yes 93.48%\nYes No 93.92%\nYes Yes 94.01%\nTable 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating\nflip (which we propose) reduce training data redundancy and improve performance.\n3.3 Identity initialization\ndirac : We initialize all convolutions after the first as partial identity transforms. That is, for a\nconvolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an\nidentity transform of the input, and leave the remaining N\u2212Mto their default initialization. In\nPyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight\nwof each convolutional layer. This method partially follows tysam-code (2023), which used a more\ncomplicated scheme where the identity weights are mixed in with the original initialization, which\nwe did not find to be more performant. With this feature added, training attains 94% accuracy in 18\nepochs taking 6.8 A100-seconds.\n3.4 Optimization tricks\nscalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor\nof64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%\nin 13.5 epochs taking 5.1 A100-seconds.\nlookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.\nWe note that Lookahead has also been found effective in prior work on training speed for ResNet-\n18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6\nA100-seconds.\n3.5 Multi-crop evaluation\nmulticrop : To generate predictions, we run the trained network on six augmented views of each\ntest image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a\nversion which is translated down-and-to-the-right by one pixel, and the mirrored versions of all\nthree. Predictions are made using a weighted average of all six outputs, where the two views of the\nuntranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125\neach. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.\nWe note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-\nmonyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of\nevaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using\nmore crops does improve performance, but the increase to inference time outweighs the potential\ntraining speedup.\n3.6 Alternating flip\nTo speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,\nwhich we motivate as follows. When training neural networks, it is standard practice to organize\ntraining into a set of epochs during which every training example is seen exactly once. This\ndiffers from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),\nwhich calls for data to be repeatedly sampled with-replacement from the training set, resulting in\nexamples being potentially seen multiple redundant times within a short window of training. The\nuse of randomly ordered epochs of data for training has a different name, being called the random\nreshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our\ntraining dataset consists of Nunique examples, then sampling data with replacement causes every\n\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N\nunique examples on average. On the other hand, random reshuffling leads to all Nunique examples\nbeing seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason\nthat it is beneficial to maximize the number of unique inputs seen per window of training time.\n4\nWe extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We\nfirst note that standard random horizontal flipping augmentation can be defined as follows.\n1import torch\n2def random_flip ( inputs ):\n3 # Applies random flipping to a batch of images\n4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)\n5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]\nListing 1: Random flip\nIf horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2\nwhich may be seen during training. Potentially, every pair of consecutive epochs could contain every\nunique input. But our main observation is that with standard random horizontal flipping, half of the\nimages will be redundantly flipped the same way during both epochs, so that on average only 1.5N\nunique inputs will be seen.\naltflip : To address this, we propose to modify standard random horizontal flipping augmentation as\nfollows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},\nwe flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we\nflip only those inputs which were flipped in the first epoch. We provide the following implementation\nwhich avoids the need for extra memory by using a pseudorandom function to decide the flips.\n1import torch\n2import hashlib\n3def hash_fn (n, seed =42) :\n4 k = n * seed\n5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,\n16)\n6def alternating_flip (inputs , indices , epoch ):\n7 # Applies alternating flipping to a batch of images\n8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist\n() ])\n9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)\n10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\nListing 2: Alternating flip\nThe result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in\nFigure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.\nAdding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final\ntraining method airbench94.py , the entire contents of which can be found in Section E. It reaches\n94% accuracy in 3.83 seconds on an NVIDIA A100.\n3.7 Compilation\nThe final step we take to speed up training is a non-algorithmic one: we compile our training\nmethod using torch.compile in order to more efficiently utilize the GPU. This results in a training\nscript which is mathematically equivalent (up to small differences in floating point arithmetic) to\nthe non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29\nA100-seconds. The downside is that the one-time compilation process takes up to several minutes to\ncomplete before training runs can begin, so that it is only beneficial when we plan to execute many\nruns of training at once. We release this version as airbench94_compiled.py .\n4 95% and 96% targets\nTo address scenarios where somewhat higher performance is desired, we additionally develop methods\ntargeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .\nTo attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel\ncount of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce\nthe learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%\naccuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.\n2Assuming none of the training inputs are already mirrors of each other.\n5\nFigure 4: Training speedups accumulate additively. Removing individual features from airbench94\nincreases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces\nthe epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-\n94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.\nFigure 3: FLOPs vs. error rate tradeoff. Our\nthree training methods apparently follow a linear\nlog-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-\nVries & Taylor, 2017) augmentation and raise the\ntraining epochs to 40. We add a third convolution to\neach block, and scale the first block to 128 channels\nand the second two to 512. We also add a residual\nconnection across the later two convolutions of each\nblock, which we find is still beneficial despite the fact\nthat we are already using identity initialization (Sec-\ntion 3.3) to ease gradient flow. Finally, we reduce\nthe learning rate by a factor of 0.78. These changes\nyield airbench96 which attains 96.05% accuracy in\n46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.\nFigure 3 shows the FLOPs and error rate of each of\nour three training methods.\n5 Experiments\n5.1 Interaction between features\nTo gain a better sense of the impact of each feature on training speed, we compare two quantities.\nFirst, we measure the number of epochs that can be saved by adding the feature to the whitened\nbaseline (Section 3.2). Second, we measure the number of epochs that must be added when the\nfeature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-\ntion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it\nfrom the final airbench94 increases epochs-to-94% from 9.9 to 12.8.\nFigure 4 shows both quantities for each feature. Surprisingly, we find that for all features except\nmulti-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even\nthough the whitened baseline requires more than twice as many epochs as the final configuration.\nThis indicates that the interaction between most features is additive rather than multiplicative.\n5.2 Does alternating flip generalize?\nIn this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of\ntraining configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all\ncases except those where neither alternating nor random flip improve over using no flipping at all.\nFor CIFAR-10 we consider the performance boost given by alternating flip across the follow-\ning 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and\nairbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For\n6\nFigure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,\nthe use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%\ntraining speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.\n95% confidence intervals are shown around each point.\neach configuration we compare the performance of alternating and random flip in terms of their mean\naccuracy across n= 400 runs of training.\nFigure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip\nimproves performance in every setting. To get a sense for how big the improvement is, we estimate\nthe effective speedup for each case, i.e., the fraction of epochs that could be saved by switching\nfrom random to alternating flip while maintaining the level of accuracy of random flip. We begin\nby fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of\neach random flip-based training configuration. We use these curves to calculate the effective speedup\nafforded by switching from random to alternating flip. For example, airbench94 with random flip\nand without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The\nsame configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law\nfit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.\nNote that using a power-law yields a more conservative estimate relative to using linear interpolation\nbetween the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.\nTable 2 shows the result. We observe the following patterns. First, the addition of extra augmentation\n(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that\nthe main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the\nsame way for many epochs in a row; we speculate that adding extra augmentation reduces the negative\nimpact of these cases because it increases data diversity. Next, TTA reduces the gap between random\nand alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),\nindicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training\nfor longer consistently increases the effective speedup given by alternating flip.\nWe next study ImageNet trainings with the following experiment. We train a ResNet-18 with a\nvariety of train and test crops, comparing three flipping options: alternating flip, random flip, and no\nflipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192\ncenter crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)\nto denote the latter. We also consider two training crops: 192x192 inception-style random resized\ncrop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area\nranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the\nshorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write\n7\nBaseline Cutout Epochs Speedup Speedup (w/ TTA)\nairbench94 No 10 15.0% 5.30%\nairbench94 No 20 27.1% 21.3%\nairbench94 No 40 38.3% 36.4%\nairbench94 No 80 102% 31.8%\nairbench94 Yes 10 3.84% 1.13%\nairbench94 Yes 20 7.42% 2.00%\nairbench94 Yes 40 18.6% 9.28%\nairbench94 Yes 80 29.2% 14.25%\nairbench96 Yes 10 4.94% 1.11%\nairbench96 Yes 20 8.99% 3.58%\nairbench96 Yes 40 17.2% 6.48%\nairbench96 Yes 80 18.8% Not measured\nTable 2: Effective speedups given by switching from random flip to alternating flip. The two configurations\nmost closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw\naccuracy values of the airbench94 experiments.\nFlipping augmentation option\nTrain crop Test crop Epochs TTA None Random Alternating\nHeavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28\nHeavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28\nLight RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26\nLight RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26\nHeavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28\nHeavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28\nLight RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26\nLight RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26\nLight RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16\nHeavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4\nTable 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for\nthose trainings where random flip improves significantly over not flipping at all. The single best flipping option\nin each row is bolded when the difference is statistically significant.\nHeavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided\nin Section C.\nTable 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC\nis better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly\nbetter when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test\nresolution discrepancy (Touvron et al., 2019).\nFor trainings which use Light RRC, we find that switching from random flip to alternating flip\nprovides a substantial boost to performance, amounting to a training speedup of more than 25%.\nIn Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to\nalternating flip improves performance by more than increasing the training duration from 16 to 20\nepochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our\nresults on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit\nfrom alternating flip. Indeed, even turning flipping off completely does not significantly reduce the\nperformance of these trainings. We conclude that alternating flip improves over random flip for every\ntraining scenario where the latter improves over no flipping at all.\n8\nEpochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE\n1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312\n2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233\n1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269\n1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533\n2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433\n1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444\nTable 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).\n5.3 Variance and class-wise calibration\nPrevious sections have focused on understanding what factors affect the first moment of accuracy\n(the mean). In this section we investigate the second moment, finding that TTA reduces variance at\nthe cost of calibration.\nOur experiment is to execute 10,000 runs of airbench94 training with several hyperparameter\nsettings. For each setting we report both the variance in test-set accuracy as well as an estimate of the\ndistribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.\nTable 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set\nvariance, replicating the main finding of Jordan (2023). This is a surprising result because these\ntrainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7\nas much distribution-wise variance when run for a similar duration, and reached a low variance only\nwhen run for 64 epochs. We conclude from this comparison that distribution-wise variance is more\nstrongly connected to the rate of convergence of a training rather than its duration as such. We also\nnote that the low distribution-wise variance of airbench94 indicates it has high training stability.\nUsing TTA significantly reduces the test-set variance, such that all three settings with TTA have lower\ntest-set variance than any setting without TTA. However, test-set variance is implied by the class-wise\ncalibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this\nreduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,\nwe compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which\nmeasures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA\nhas a higher CACE than every setting without TTA, confirming the hypothesis.\n6 Discussion\nIn this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7\nfaster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the\nairbench Python package.\nWe developed airbench solely with the goal of maximizing training speed on CIFAR-10. In\nSection B we find that it also generalizes well to other tasks. For example, without any extra tuning,\nairbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.\nOne factor contributing to the training speed of airbench was our finding that training can be accel-\nerated by partially derandomizing the standard random horizontal flipping augmentation, resulting in\nthe variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating\nflip improves the performance of every training we considered (Section 5.2), with the exception of\nthose trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to\nus, the standard ImageNet trainings that we considered do not significantly benefit from horizontal\nflipping. Future work might investigate whether it is possible to obtain derandomized improvements\nto other augmentations besides horizontal flip.\nThe methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,\nwith fixed performance and hardware constraints. These constraints mean that we cannot improve\nperformance by simply scaling up the amount of computational resources used; instead we are forced\nto develop new methods like the alternating flip. We look forward to seeing what other new methods\nfuture work discovers to push training speed further.\n9\nReferences\nDimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.\nDan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-\nperformance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,\n2011.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\nmark and competition. 2017.\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet\nor cifar-10. arXiv preprint arXiv:1810.03505 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\npp. 248\u2013255. IEEE, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552 , 2017.\nMert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic\ngradient descent. Mathematical Programming , 186:49\u201384, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 , 2016.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.\npmlr, 2015.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\nsgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.\nKeller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and\ninevitable. arXiv preprint arXiv:2304.01910 , 2023.\nAndrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.\nio/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for\nadvanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT\nLicense.\nGuillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander\nM \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,\nBenjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.\nBenchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural\nInformation Processing Systems , 35:25404\u201325421, 2022.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.\nDavid Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/\nhow-to-train-your-resnet-8-bag-of-tricks/ .\n10\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics , 22(3):400\u2013407, 1951.\nSantiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:\n//github.com/apple/ml-cifar-10-faster , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.\ntysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/\nhlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. Advances in neural information processing systems , 32, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,\n2016.\nMichael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps\nforward, 1 step back. Advances in neural information processing systems , 32, 2019.\n11\nA Network architecture\n1from torch import nn\n2\n3class Flatten (nn. Module ):\n4 def forward (self , x):\n5 return x. view (x. size (0) , -1)\n6\n7class Mul (nn. Module ):\n8 def __init__ (self , scale ):\n9 super (). __init__ ()\n10 self . scale = scale\n11 def forward (self , x):\n12 return x * self . scale\n13\n14def conv (ch_in , ch_out ):\n15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,\n16 padding =\u2019same \u2019, bias = False )\n17\n18def make_net ():\n19 act = lambda : nn. GELU ()\n20 bn = lambda ch: nn. BatchNorm2d (ch)\n21 return nn. Sequential (\n22 nn. Sequential (\n23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),\n24 act () ,\n25 ),\n26 nn. Sequential (\n27 conv (24 , 64) ,\n28 nn. MaxPool2d (2) ,\n29 bn (64) , act () ,\n30 conv (64 , 64) ,\n31 bn (64) , act () ,\n32 ),\n33 nn. Sequential (\n34 conv (64 , 256) ,\n35 nn. MaxPool2d (2) ,\n36 bn (256) , act () ,\n37 conv (256 , 256) ,\n38 bn (256) , act () ,\n39 ),\n40 nn. Sequential (\n41 conv (256 , 256) ,\n42 nn. MaxPool2d (2) ,\n43 bn (256) , act () ,\n44 conv (256 , 256) ,\n45 bn (256) , act () ,\n46 ),\n47 nn. MaxPool2d (3) ,\n48 Flatten () ,\n49 nn. Linear (256 , 10, bias = False ),\n50 Mul (1/9) ,\n51 )\nListing 3: Minimal PyTorch code for the network architecture used by airbench94 .\nWe note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock\ntime. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel\nsize of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this\ndoes not improve the wallclock training time on an A100. Reducing the batch size is another easy\nway to save FLOPs but not wallclock time.\n12\nDataset Flipping? Cutout? ResNet-18 airbench96\nCIFAR-10 Yes No 95.55% 95.61%\nCIFAR-10 Yes Yes 96.01% 96.05%\nCIFAR-100 Yes No 77.54% 79.27%\nCIFAR-100 Yes Yes 78.04% 79.76%\nCINIC-10 Yes No 87.58% 87.78%\nCINIC-10 Yes Yes not measured 88.22%\nSVHN No No 97.35% 97.38%\nSVHN No Yes not measured 97.64%\nTable 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We\ndirectly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off\nflipping for SVHN).\nB Extra dataset experiments\nWe developed airbench with the singular goal of maximizing training speed on CIFAR-10. To\nfind out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its\nperformance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-\n10 (Darlow et al., 2018).\nOn CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both\nthe case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not\n(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than\nResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say\nthat it generalizes.\nWe compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.\nWe do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal\non CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better\nperformance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where\nairbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is\nused and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows\nstrong generalization to other tasks.\nWe note that this comparison between airbench96 and ResNet-18 training is fair in the sense that\nit does demonstrate that the former has good generalization, but unfair in the sense that it does not\nindicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time\naugmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training\nwould outperform airbench96 if it were run using test-time augmentation. However, it also takes\n5-10 times longer to complete. The decision of which to use may be situational.\nThe accuracy values we report for ResNet-18 training are from the following sources. We tried\nto select the highest values we could find for each setting. Moreau et al. (2022) reports attaining\n95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports\nattaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on\nCIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.\nC ImageNet training details\nOur ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/\nlibffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and\nmomentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution\n160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of\ntraining. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.\n13\nHyperparameters Flipping augmentation option\nEpochs Cutout TTA None Random Alternating\n10 No No 92.3053 93.0988 93.2798\n20 No No 92.8166 93.7446 93.8652\n40 No No 93.0143 94.0133 94.0729\n80 No No 93.0612 94.1169 94.1628\n10 No Yes 93.4071 93.9488 94.0186\n20 No Yes 93.8528 94.5565 94.6530\n40 No Yes 94.0381 94.7803 94.8203\n80 No Yes 94.0638 94.8506 94.8676\n10 Yes No 91.8487 92.0402 92.1374\n20 Yes No 92.8474 93.3825 93.4876\n40 Yes No 93.2675 94.1014 94.1952\n80 Yes No 93.4193 94.4311 94.5204\n10 Yes Yes 92.6455 92.7780 92.8103\n20 Yes Yes 93.7862 94.1306 94.1670\n40 Yes Yes 94.3090 94.8511 94.8960\n80 Yes Yes 94.5253 95.1839 95.2362\nTable 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a\nmean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise\ndifference in means is statistically significant.\nD Extra tables & figures\nFigure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in\nSection 5.3.\nE Complete training code\n1\"\"\"\n2airbench94 .py\n33.83 s runtime on an A100 ; 0.36 PFLOPs .\n4Evidence for validity : 94.01 average accuracy in n =1000 runs .\n5\n6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB\nwith the following nvidia -smi:\n7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :\n11.7\n14\n8torch . __version__ == \u20192.1.2+ cu118 \u2019\n9\"\"\"\n10\n11# ############################################\n12# Setup / Hyperparameters #\n13# ############################################\n14\n15import os\n16import sys\n17import uuid\n18from math import ceil\n19\n20import torch\n21from torch import nn\n22import torch .nn. functional as F\n23import torchvision\n24import torchvision . transforms as T\n25\n26torch . backends . cudnn . benchmark = True\n27\n28\"\"\"\n29We express the main training hyperparameters ( batch size , learning\nrate , momentum , and weight decay ) in decoupled form , so that each\none can be tuned independently . This accomplishes the following :\n30* Assuming time - constant gradients , the average step size is decoupled\nfrom everything but the lr.\n31* The size of the weight decay update is decoupled from everything but\nthe wd.\n32In constrast , normally when we increase the ( Nesterov ) momentum , this\nalso scales up the step size proportionally to 1 + 1 / (1 -\nmomentum ), meaning we cannot change momentum without having to re -\ntune the learning rate . Similarly , normally when we increase the\nlearning rate this also increases the size of the weight decay ,\nrequiring a proportional decrease in the wd to maintain the same\ndecay strength .\n33\n34The practical impact is that hyperparameter tuning is faster , since\nthis parametrization allows each one to be tuned independently .\nSee https :// myrtle .ai/ learn /how -to -train -your - resnet -5-\nhyperparameters /.\n35\"\"\"\n36\n37hyp = {\n38 \u2019opt \u2019: {\n39 \u2019 train_epochs \u2019: 9.9 ,\n40 \u2019batch_size \u2019: 1024 ,\n41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples\n42 \u2019momentum \u2019: 0.85 ,\n43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (\ndecoupled from learning rate )\n44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not\nweight decay ) for BatchNorm biases\n45 \u2019 label_smoothing \u2019: 0.2 ,\n46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the\nwhitening layer bias before freezing\n47 },\n48 \u2019aug \u2019: {\n49 \u2019flip \u2019: True ,\n50 \u2019translate \u2019: 2,\n51 },\n52 \u2019net \u2019: {\n53 \u2019widths \u2019: {\n54 \u2019block1 \u2019: 64,\n55 \u2019block2 \u2019: 256 ,\n56 \u2019block3 \u2019: 256 ,\n15\n57 },\n58 \u2019 batchnorm_momentum \u2019: 0.6 ,\n59 \u2019 scaling_factor \u2019: 1/9 ,\n60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :\n0= none , 1= mirror , 2= mirror + translate\n61 },\n62}\n63\n64# ############################################\n65# DataLoader #\n66# ############################################\n67\n68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )\n69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )\n70\n71def batch_flip_lr ( inputs ):\n72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .\nview (-1, 1, 1, 1)\n73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\n74\n75def batch_crop (images , crop_size ):\n76 r = ( images . size ( -1) - crop_size )//2\n77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =\nimages . device )\n78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),\ndevice = images .device , dtype = images . dtype )\n79 # The two cropping methods in this if - else produce equivalent\nresults , but the second is faster for r > 2.\n80 if r <= 2:\n81 for sy in range (-r, r +1):\n82 for sx in range (-r, r +1):\n83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)\n84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size\n, r+sx:r+sx+ crop_size ]\n85 else :\n86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size\n+2* r), device = images .device , dtype = images . dtype )\n87 for s in range (-r, r +1):\n88 mask = ( shifts [:, 0] == s)\n89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]\n90 for s in range (-r, r +1):\n91 mask = ( shifts [:, 1] == s)\n92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+\ncrop_size ]\n93 return images_out\n94\n95class CifarLoader :\n96 \"\"\"\n97 GPU - accelerated dataloader for CIFAR -10 which implements\nalternating flip augmentation .\n98 \"\"\"\n99\n100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,\ndrop_last =None , shuffle =None , gpu =0):\n101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .\npt \u2019)\n102 if not os. path . exists ( data_path ):\n103 dset = torchvision . datasets . CIFAR10 (path , download =True ,\ntrain = train )\n104 images = torch . tensor ( dset . data )\n105 labels = torch . tensor ( dset . targets )\n106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:\ndset . classes }, data_path )\n107\n108 data = torch . load ( data_path , map_location = torch . device (gpu))\n16\n109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [\n\u2019labels \u2019], data [\u2019classes \u2019]\n110 # It \u2019s faster to load + process uint8 data than to load\npreprocessed fp16 data\n111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).\nto( memory_format = torch . channels_last )\n112\n113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )\n114 self . proc_images = {} # Saved results of image processing to\nbe done on the first epoch\n115 self . epoch = 0\n116\n117 self . aug = aug or {}\n118 for k in self .aug . keys ():\n119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019\n% k\n120\n121 self . batch_size = batch_size\n122 self . drop_last = train if drop_last is None else drop_last\n123 self . shuffle = train if shuffle is None else shuffle\n124\n125 def __len__ ( self ):\n126 return len( self . images )// self . batch_size if self . drop_last\nelse ceil (len( self . images )/ self . batch_size )\n127\n128 def __iter__ ( self ):\n129\n130 if self . epoch == 0:\n131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .\nimages )\n132 # Randomly flip all images on the first epoch as according\nto definition of alternating flip\n133 if self .aug.get(\u2019flip \u2019, False ):\n134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (\nimages )\n135 # Pre -pad images to save time when doing random\ntranslation\n136 pad = self .aug.get (\u2019translate \u2019, 0)\n137 if pad > 0:\n138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019\nreflect \u2019)\n139\n140 if self .aug.get(\u2019translate \u2019, 0) > 0:\n141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .\nshape [ -2])\n142 elif self .aug.get (\u2019flip \u2019, False ):\n143 images = self . proc_images [\u2019flip \u2019]\n144 else :\n145 images = self . proc_images [\u2019norm \u2019]\n146 if self .aug.get(\u2019flip \u2019, False ):\n147 if self . epoch % 2 == 1:\n148 images = images . flip ( -1)\n149\n150 self . epoch += 1\n151\n152 indices = ( torch . randperm if self . shuffle else torch . arange )(\nlen ( images ), device = images . device )\n153 for i in range (len ( self )):\n154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]\n155 yield ( images [ idxs ], self . labels [ idxs ])\n156\n157# ############################################\n158# Network Components #\n159# ############################################\n160\n17\n161class Flatten (nn. Module ):\n162 def forward (self , x):\n163 return x. view (x. size (0) , -1)\n164\n165class Mul (nn. Module ):\n166 def __init__ (self , scale ):\n167 super (). __init__ ()\n168 self . scale = scale\n169 def forward (self , x):\n170 return x * self . scale\n171\n172class BatchNorm (nn. BatchNorm2d ):\n173 def __init__ (self , num_features , momentum , eps =1e -12 ,\n174 weight =False , bias = True ):\n175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )\n176 self . weight . requires_grad = weight\n177 self . bias . requires_grad = bias\n178 # Note that PyTorch already initializes the weights to one and\nbiases to zero\n179\n180class Conv (nn. Conv2d ):\n181 def __init__ (self , in_channels , out_channels , kernel_size =3,\npadding =\u2019same \u2019, bias = False ):\n182 super (). __init__ ( in_channels , out_channels , kernel_size =\nkernel_size , padding = padding , bias = bias )\n183\n184 def reset_parameters ( self ):\n185 super (). reset_parameters ()\n186 if self . bias is not None :\n187 self . bias . data . zero_ ()\n188 w = self . weight . data\n189 torch .nn. init . dirac_ (w[:w. size (1) ])\n190\n191class ConvGroup (nn. Module ):\n192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):\n193 super (). __init__ ()\n194 self . conv1 = Conv ( channels_in , channels_out )\n195 self . pool = nn. MaxPool2d (2)\n196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )\n197 self . conv2 = Conv ( channels_out , channels_out )\n198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )\n199 self . activ = nn. GELU ()\n200\n201 def forward (self , x):\n202 x = self . conv1 (x)\n203 x = self . pool (x)\n204 x = self . norm1 (x)\n205 x = self . activ (x)\n206 x = self . conv2 (x)\n207 x = self . norm2 (x)\n208 x = self . activ (x)\n209 return x\n210\n211# ############################################\n212# Network Definition #\n213# ############################################\n214\n215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019\n][ \u2019 batchnorm_momentum \u2019]):\n216 whiten_kernel_size = 2\n217 whiten_width = 2 * 3 * whiten_kernel_size **2\n218 net = nn. Sequential (\n219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True\n),\n220 nn. GELU () ,\n18\n221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],\nbatchnorm_momentum ),\n222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],\nbatchnorm_momentum ),\n223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],\nbatchnorm_momentum ),\n224 nn. MaxPool2d (3) ,\n225 Flatten () ,\n226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),\n227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,\n228 )\n229 net [0]. weight . requires_grad = False\n230 net = net. half (). cuda ()\n231 net = net.to( memory_format = torch . channels_last )\n232 for mod in net. modules ():\n233 if isinstance (mod , BatchNorm ):\n234 mod . float ()\n235 return net\n236\n237# ############################################\n238# Whitening Conv Initialization #\n239# ############################################\n240\n241def get_patches (x, patch_shape ):\n242 c, (h, w) = x. shape [1] , patch_shape\n243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h\n,w). float ()\n244\n245def get_whitening_parameters ( patches ):\n246 n,c,h,w = patches . shape\n247 patches_flat = patches . view (n, -1)\n248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n\n249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance\n, UPLO =\u2019U\u2019)\n250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.\nreshape (c*h*w,c,h,w). flip (0)\n251\n252def init_whitening_conv (layer , train_set , eps =5e -4):\n253 patches = get_patches ( train_set , patch_shape = layer . weight . data .\nshape [2:])\n254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )\n255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)\n256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -\neigenvectors_scaled ))\n257\n258# ###########################################\n259# Lookahead #\n260# ###########################################\n261\n262class LookaheadState :\n263 def __init__ (self , net):\n264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().\nitems ()}\n265\n266 def update (self , net , decay ):\n267 for ema_param , net_param in zip( self . net_ema . values () , net.\nstate_dict (). values ()):\n268 if net_param . dtype in ( torch .half , torch . float ):\n269 ema_param . lerp_ ( net_param , 1- decay )\n270 net_param . copy_ ( ema_param )\n271\n272# ###########################################\n273# Logging #\n274# ###########################################\n275\n19\n276def print_columns ( columns_list , is_head =False , is_final_entry = False ):\n277 print_string = \u2019\u2019\n278 for col in columns_list :\n279 print_string += \u2019| %s \u2019 % col\n280 print_string += \u2019|\u2019\n281 if is_head :\n282 print (\u2019-\u2019*len( print_string ))\n283 print ( print_string )\n284 if is_head or is_final_entry :\n285 print (\u2019-\u2019*len( print_string ))\n286\n287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,\n\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]\n288def print_training_details ( variables , is_final_entry ):\n289 formatted = []\n290 for col in logging_columns_list :\n291 var = variables .get (col. strip () , None )\n292 if type (var) in (int , str):\n293 res = str(var)\n294 elif type (var) is float :\n295 res = \u2019{:0.4 f}\u2019. format ( var)\n296 else :\n297 assert var is None\n298 res = \u2019\u2019\n299 formatted . append (res. rjust (len (col)))\n300 print_columns ( formatted , is_final_entry = is_final_entry )\n301\n302# ###########################################\n303# Evaluation #\n304# ###########################################\n305\n306def infer (model , loader , tta_level =0) :\n307 \"\"\"\n308 Test - time augmentation strategy (for tta_level =2):\n309 1. Flip / mirror the image left -to - right (50% of the time ).\n310 2. Translate the image by one pixel either up -and - left or down -and\n- right (50% of the time , i.e. both happen 25% of the time ).\n311\n312 This creates 6 views per image ( left / right times the two\ntranslations and no - translation ), which we evaluate and then\nweight according to the given probabilities .\n313 \"\"\"\n314\n315 def infer_basic (inputs , net):\n316 return net( inputs ). clone ()\n317\n318 def infer_mirror (inputs , net):\n319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))\n320\n321 def infer_mirror_translate ( inputs , net ):\n322 logits = infer_mirror (inputs , net )\n323 pad = 1\n324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)\n325 inputs_translate_list = [\n326 padded_inputs [:, :, 0:32 , 0:32] ,\n327 padded_inputs [:, :, 2:34 , 2:34] ,\n328 ]\n329 logits_translate_list = [ infer_mirror ( inputs_translate , net)\n330 for inputs_translate in\ninputs_translate_list ]\n331 logits_translate = torch . stack ( logits_translate_list ). mean (0)\n332 return 0.5 * logits + 0.5 * logits_translate\n333\n334 model . eval ()\n335 test_images = loader . normalize ( loader . images )\n20\n336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][\ntta_level ]\n337 with torch . no_grad ():\n338 return torch .cat ([ infer_fn (inputs , model ) for inputs in\ntest_images . split (2000) ])\n339\n340def evaluate (model , loader , tta_level =0) :\n341 logits = infer (model , loader , tta_level )\n342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()\n343\n344# ###########################################\n345# Training #\n346# ###########################################\n347\n348def main (run):\n349\n350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]\n351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]\n352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]\n353 # Assuming gradients are constant in time , for Nesterov momentum ,\nthe below ratio is how much larger the default steps will be than\nthe underlying per - example gradients . We divide the learning rate\nby this ratio in order to ensure steps are the same scale as\ngradients , regardless of the choice of momentum .\n354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))\n355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning\nrate for PyTorch SGD\n356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale\n357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]\n358\n359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019\nlabel_smoothing \u2019], reduction =\u2019none \u2019)\n360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)\n361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =\nbatch_size , aug=hyp[\u2019aug \u2019])\n362 if run == \u2019warmup \u2019:\n363 # The only purpose of the first run is to warmup , so we can\nuse dummy data\n364 train_loader . labels = torch . randint (0, 10, size =( len (\ntrain_loader . labels ) ,), device = train_loader . labels . device )\n365 total_train_steps = ceil (len( train_loader ) * epochs )\n366\n367 model = make_net ()\n368 current_steps = 0\n369\n370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in\nk and p. requires_grad ]\n371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019\nnot in k and p. requires_grad ]\n372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,\nweight_decay =wd/ lr_biases ),\n373 dict ( params = other_params , lr=lr , weight_decay =wd/\nlr)]\n374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,\nnesterov = True )\n375\n376 def triangle (steps , start =0, end =0, peak =0.5) :\n377 xp = torch . tensor ([0 , int( peak * steps ), steps ])\n378 fp = torch . tensor ([ start , 1, end ])\n379 x = torch . arange (1+ steps )\n380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])\n381 b = fp [: -1] - (m * xp [: -1])\n382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1\n383 indices = torch . clamp ( indices , 0, len(m) - 1)\n384 return m[ indices ] * x + b[ indices ]\n21\n385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,\npeak =0.23)\n386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:\nlr_schedule [i])\n387\n388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /\ntotal_train_steps )**3\n389 lookahead_state = LookaheadState ( model )\n390\n391 # For accurately timing GPU code\n392 starter = torch . cuda . Event ( enable_timing = True )\n393 ender = torch . cuda . Event ( enable_timing = True )\n394 total_time_seconds = 0.0\n395\n396 # Initialize the first layer using statistics of training images\n397 starter . record ()\n398 train_images = train_loader . normalize ( train_loader . images [:5000])\n399 init_whitening_conv ( model [0] , train_images )\n400 ender . record ()\n401 torch . cuda . synchronize ()\n402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n403\n404 for epoch in range ( ceil ( epochs )):\n405\n406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019\nwhiten_bias_epochs \u2019])\n407\n408 # ###################\n409 # Training #\n410 # ###################\n411\n412 starter . record ()\n413\n414 model . train ()\n415 for inputs , labels in train_loader :\n416\n417 outputs = model ( inputs )\n418 loss = loss_fn ( outputs , labels ).sum ()\n419 optimizer . zero_grad ( set_to_none = True )\n420 loss . backward ()\n421 optimizer . step ()\n422 scheduler . step ()\n423\n424 current_steps += 1\n425\n426 if current_steps % 5 == 0:\n427 lookahead_state . update (model , decay = alpha_schedule [\ncurrent_steps ]. item ())\n428\n429 if current_steps >= total_train_steps :\n430 if lookahead_state is not None :\n431 lookahead_state . update (model , decay =1.0)\n432 break\n433\n434 ender . record ()\n435 torch . cuda . synchronize ()\n436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n437\n438 # ###################\n439 # Evaluation #\n440 # ###################\n441\n442 # Print the accuracy and loss from the last training batch of\nthe epoch\n22\n443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().\nmean (). item ()\n444 train_loss = loss . item () / batch_size\n445 val_acc = evaluate (model , test_loader , tta_level =0)\n446 print_training_details ( locals () , is_final_entry = False )\n447 run = None # Only print the run number once\n448\n449 # ###################\n450 # TTA Evaluation #\n451 # ###################\n452\n453 starter . record ()\n454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019\ntta_level \u2019])\n455 ender . record ()\n456 torch . cuda . synchronize ()\n457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n458\n459 epoch = \u2019eval \u2019\n460 print_training_details ( locals () , is_final_entry = True )\n461\n462 return tta_val_acc\n463\n464if __name__ == \" __main__ \":\n465 with open (sys. argv [0]) as f:\n466 code = f. read ()\n467\n468 print_columns ( logging_columns_list , is_head = True )\n469 main (\u2019warmup \u2019)\n470 accs = torch . tensor ([ main (run) for run in range (25) ])\n471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))\n472\n473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }\n474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))\n475 os. makedirs ( log_dir , exist_ok = True )\n476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)\n477 print (os. path . abspath ( log_path ))\n478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))\nListing 4: airbench94.py\n23\n\nCandidate Output: tasks=[TaskCandidate(name='Implement Alternating Flip Data Augmentation', description='Implement the alternating flip data augmentation method described in the paper. This method deterministically flips images in alternating epochs after the first epoch, avoiding redundancy and speeding up training.', relevant_paper_text='To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.', scoring_feasibility=9, llm_tractability=0.8, expert_tractability=0.95, layman_tractability=0.3), TaskCandidate(name='Implement Patch-Whitening Initialization', description=\"Implement the patch-whitening initialization for the first convolutional layer as described in the paper. This involves initializing the layer's weights based on the eigenvectors of the covariance matrix of 2x2 patches across the training distribution.\", relevant_paper_text='Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.', scoring_feasibility=8, llm_tractability=0.6, expert_tractability=0.9, layman_tractability=0.1), TaskCandidate(name='Implement Multi-Crop Test-Time Augmentation', description='Implement the multi-crop test-time augmentation (TTA) method described in the paper. This involves running the trained network on six augmented views of each test image and combining the outputs using a weighted average.', relevant_paper_text='To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.', scoring_feasibility=9, llm_tractability=0.7, expert_tractability=0.95, layman_tractability=0.4), TaskCandidate(name='Implement Identity Initialization for Convolutions', description='Implement the identity initialization method for convolutional layers after the first layer, as described in the paper. This involves initializing the first M filters of each convolution with N>=M outputs as an identity transform of the input.', relevant_paper_text='We initialize all convolutions after the first as partial identity transforms. That is, for a convolution with M input channels and N>=M outputs, we initialize its first M filters to an identity transform of the input, and leave the remaining N-M to their default initialization.', scoring_feasibility=8, llm_tractability=0.7, expert_tractability=0.9, layman_tractability=0.2), TaskCandidate(name='Implement Lookahead Optimization', description='Implement the Lookahead optimization method as described in the paper. This involves maintaining a slow-moving copy of the model parameters and periodically updating the fast-moving parameters towards the slow-moving ones.', relevant_paper_text='Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization. We note that Lookahead has also been found effective in prior work on training speed for ResNet-18 (Moreau et al., 2022).', scoring_feasibility=7, llm_tractability=0.6, expert_tractability=0.85, layman_tractability=0.1)]\n\nRequirements:\nExtract all promising task candidates from paper. Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\n\n    A good task candidate:\n    - describes a task that an agent can implement\n    - typically a programming task, e.g. modifying a 'baseline' implementation\n    - which reflects or mirrors actual tasks undertaken to produce the research paper\n    - and requires research and/or engineering expertise to implement correctly\n    - for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\n    - ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\n    - the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\n    \n\nAttempt Num: 2\n\nPrevious Critiques:\nSelf Critique:\n\nI'll evaluate the task candidates I extracted based on the given requirements:\n\n1. Implement alternating flip data augmentation:\nStrengths:\n- Describes a concrete programming task to modify a baseline implementation\n- Reflects an actual task undertaken in the research\n- Requires engineering expertise to implement correctly\n- Relatively straightforward to write an objective scoring function (e.g. check if images are flipped in the correct pattern across epochs)\n- Scoring function would be much easier to produce than implementing the task itself\n\nWeaknesses:\n- May not require significant research capabilities, mostly engineering\n\n2. Implement decoupled hyperparameters:\nStrengths:\n- Describes a programming task to modify hyperparameter handling\n- Reflects work done in the research\n- Requires some engineering expertise\n\nWeaknesses:\n- May be challenging to write an objective scoring function beyond checking if the hyperparameters are decoupled\n- Implementing the task may not be significantly harder than writing the scoring function\n- Does not require much research capability\n\n3. Implement patch-whitening initialization:\nStrengths:\n- Describes a concrete programming task\n- Reflects actual work from the research\n- Requires both research understanding and engineering skill\n- Possible to write an objective scoring function (e.g. check if filters are initialized correctly)\n- Scoring function would be easier than implementation\n\nWeaknesses:\n- May be very challenging for an agent without strong math/ML background\n\nOverall, the alternating flip and patch-whitening tasks seem to best meet the criteria. The decoupled hyperparameters task is less suitable. I could improve by extracting more task candidates from other parts of the paper, especially those requiring more research capabilities.\nSelf Critique:\n\nUpon reviewing my task candidate extraction and evaluation, I can identify several areas for improvement:\n\n1. Quantity of task candidates: I only extracted three task candidates, which is likely insufficient to cover the full range of research and engineering tasks described in the paper. I should have extracted more candidates to provide a more comprehensive set of options.\n\n2. Research vs Engineering balance: The tasks I extracted lean more heavily towards engineering implementation rather than research. I should have sought out more tasks that require deeper understanding and application of the research concepts presented in the paper.\n\n3. Missed important tasks: I overlooked several significant tasks described in the paper, such as implementing the identity initialization for convolutions, the Lookahead optimization, and the overall network architecture. These are important components of the research that should have been included as task candidates.\n\n4. Depth of analysis: While I provided some evaluation of the tasks against the criteria, my analysis could have been more thorough and systematic, addressing each point in the requirements more explicitly for each task.\n\n5. Scoring function consideration: I didn't give enough attention to the requirement of having easily implementable scoring functions that are much simpler than the tasks themselves. This is a crucial aspect that I should have emphasized more in my evaluation.\n\n6. Relevance to paper's core contributions: I could have focused more on tasks that directly relate to the paper's main contributions and novel aspects, rather than including more general ML implementation tasks.\n\nTo improve, I should re-examine the paper more carefully, extract a wider range of task candidates that better balance research and engineering aspects, and provide a more thorough evaluation of each candidate against all the specified criteria.\n\nPast Error in Should Retry: ValueError('Respond with true or false')\n\nSelf Critique:"
  },
  {
    "Response": "Self Critique: Upon reviewing my previous response, I realize I made several significant improvements but also some key mistakes:\n\nImprovements:\n1. I extracted more task candidates (5 total), providing a better coverage of the paper's key contributions.\n2. The tasks now have a better balance between research and engineering aspects.\n3. I included important tasks that were previously overlooked, such as identity initialization and Lookahead optimization.\n4. Each task candidate now has a more structured description, including relevant paper text and scoring metrics.\n\nMistakes:\n1. I failed to provide an explicit evaluation of each task against all the criteria specified in the requirements. My analysis was implicit rather than systematic.\n2. I did not directly address the ease of creating scoring functions compared to implementing the tasks for each candidate.\n3. I didn't explicitly discuss how each task reflects actual work undertaken to produce the research paper.\n4. I didn't provide a clear conclusion or recommendation on which tasks best meet the criteria.\n5. Most critically, I completely failed to respond to the \"Should Retry\" prompt with a boolean value as requested.\n\nTo improve further, I should provide a more systematic evaluation of each task against all criteria, explicitly discuss scoring function creation vs task implementation difficulty, and ensure I directly answer all parts of the prompt, including the boolean \"Should Retry\" response.\n\nShould Retry: true"
  },
  {
    "Request": "Given the fields `task_candidate`, produce the fields `task_rubric`.\n\n---\n\nFollow the following format.\n\nTask Candidate: ${task_candidate}\nTask Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"properties\": {\"criterion\": {\"description\": \"Criterion for evaluating the task\", \"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"List of criteria for evaluating task implementation; importance should sum to 100\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\n\n---\n\nTask Candidate:\n\nTask: Implement Alternating Flip Data Augmentation\nDescription: Implement the alternating flip data augmentation method described in the paper. This method deterministically flips images in alternating epochs after the first epoch, avoiding redundancy and speeding up training.\nRelevant Paper Text: To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\nEstimated Scoring Feasibility (1-10): 9\nEstimated LLM Tractability (0-1): 0.8\nEstimated Expert Tractability (0-1): 0.95\nEstimated Layman Tractability (0-1): 0.3\n\nTask Rubric:"
  },
  {
    "Response": "{\n  \"rubric\": [\n    {\n      \"criterion\": \"Correct implementation of alternating flip pattern across epochs\",\n      \"importance\": 30\n    },\n    {\n      \"criterion\": \"Proper handling of the first epoch with 50% random flipping\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"Consistent flipping of images in even and odd epochs based on first epoch\",\n      \"importance\": 25\n    },\n    {\n      \"criterion\": \"Efficient implementation to avoid redundancy and speed up training\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"Proper integration with existing data augmentation pipeline\",\n      \"importance\": 10\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.\n\n---\n\nFollow the following format.\n\nTask: ${task}\n\nRubric: ${rubric}\n\nReasoning: Let's think step by step in order to ${produce the justification}. We ...\n\nScorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified?\n\nJustification: Justification for scorable judgement\n\n---\n\nTask: Task: Implement Alternating Flip Data Augmentation Description: Implement the alternating flip data augmentation method described in the paper. This method deterministically flips images in alternating epochs after the first epoch, avoiding redundancy and speeding up training. Relevant Paper Text: To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch. Estimated Scoring Feasibility (1-10): 9 Estimated LLM Tractability (0-1): 0.8 Estimated Expert Tractability (0-1): 0.95 Estimated Layman Tractability (0-1): 0.3\n\nRubric: Criterion: Correct implementation of alternating flip pattern across epochs Importance: 30.0 Criterion: Proper handling of the first epoch with 50% random flipping Importance: 20.0 Criterion: Consistent flipping of images in even and odd epochs based on first epoch Importance: 25.0 Criterion: Efficient implementation to avoid redundancy and speed up training Importance: 15.0 Criterion: Proper integration with existing data augmentation pipeline Importance: 10.0\n\nReasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification:\n\n1. The task is clearly defined with specific implementation details from a research paper.\n2. The rubric provides clear, objective criteria for evaluating the implementation.\n3. Each criterion in the rubric can be objectively verified through code inspection and testing.\n4. The alternating flip pattern is deterministic and can be programmatically checked.\n5. The 50% random flipping in the first epoch can be verified statistically.\n6. Consistency of flipping in even and odd epochs can be tested systematically.\n7. Efficiency improvements can be measured through performance benchmarks.\n8. Integration with existing pipelines can be verified through functional testing.\n9. The task involves concrete programming concepts that can be objectively evaluated.\n10. The high estimated scoring feasibility (9/10) suggests that experts believe this task can be objectively scored.\n\nScorable[True/False]: True\n\nJustification: This task is highly scorable through an objective Python function. The implementation details are precisely defined, making it possible to create test cases that verify each aspect of the alternating flip data augmentation method. The rubric provides clear criteria that can be translated into programmatic checks. For example, the correct alternating pattern can be verified by tracking image flips across epochs, the first epoch's random flipping can be statistically validated, and the consistency of flipping in subsequent epochs can be deterministically checked. Performance improvements can be measured to ensure efficiency. The task's concrete nature, combined with the detailed rubric and high estimated scoring feasibility, strongly indicates that an objective scoring function can be implemented to verify all key aspects of the task."
  },
  {
    "Request": "Given the fields `task_candidate`, produce the fields `task_rubric`.\n\n---\n\nFollow the following format.\n\nTask Candidate: ${task_candidate}\nTask Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"properties\": {\"criterion\": {\"description\": \"Criterion for evaluating the task\", \"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"List of criteria for evaluating task implementation; importance should sum to 100\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\n\n---\n\nTask Candidate:\n\nTask: Implement Patch-Whitening Initialization\nDescription: Implement the patch-whitening initialization for the first convolutional layer as described in the paper. This involves initializing the layer's weights based on the eigenvectors of the covariance matrix of 2x2 patches across the training distribution.\nRelevant Paper Text: Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\nEstimated Scoring Feasibility (1-10): 8\nEstimated LLM Tractability (0-1): 0.6\nEstimated Expert Tractability (0-1): 0.9\nEstimated Layman Tractability (0-1): 0.1\n\nTask Rubric:"
  },
  {
    "Response": "{\n  \"rubric\": [\n    {\n      \"criterion\": \"Correct implementation of 2x2 convolution with 24 channels\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"Accurate calculation of the covariance matrix of 2x2 patches across the training distribution\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"Correct computation of eigenvectors from the covariance matrix\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"Proper initialization of the first 12 filters with the computed eigenvectors\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"Correct initialization of the second 12 filters as negations of the first 12\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"Verification that the outputs of the first 12 filters have identity covariance matrix\",\n      \"importance\": 10\n    },\n    {\n      \"criterion\": \"Efficient implementation of the patch-whitening initialization process\",\n      \"importance\": 5\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.\n\n---\n\nFollow the following format.\n\nTask: ${task}\n\nRubric: ${rubric}\n\nReasoning: Let's think step by step in order to ${produce the justification}. We ...\n\nScorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified?\n\nJustification: Justification for scorable judgement\n\n---\n\nTask: Task: Implement Patch-Whitening Initialization Description: Implement the patch-whitening initialization for the first convolutional layer as described in the paper. This involves initializing the layer's weights based on the eigenvectors of the covariance matrix of 2x2 patches across the training distribution. Relevant Paper Text: Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows. Estimated Scoring Feasibility (1-10): 8 Estimated LLM Tractability (0-1): 0.6 Estimated Expert Tractability (0-1): 0.9 Estimated Layman Tractability (0-1): 0.1\n\nRubric: Criterion: Correct implementation of 2x2 convolution with 24 channels Importance: 15.0 Criterion: Accurate calculation of the covariance matrix of 2x2 patches across the training distribution Importance: 20.0 Criterion: Correct computation of eigenvectors from the covariance matrix Importance: 20.0 Criterion: Proper initialization of the first 12 filters with the computed eigenvectors Importance: 15.0 Criterion: Correct initialization of the second 12 filters as negations of the first 12 Importance: 15.0 Criterion: Verification that the outputs of the first 12 filters have identity covariance matrix Importance: 10.0 Criterion: Efficient implementation of the patch-whitening initialization process Importance: 5.0\n\nReasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification:\n\n1. The task involves implementing a specific initialization method for a convolutional layer.\n2. The implementation requires several well-defined mathematical operations:\n   - Creating a 2x2 convolution with 24 channels\n   - Calculating the covariance matrix of 2x2 patches from the training data\n   - Computing eigenvectors of this covariance matrix\n   - Initializing filters based on these eigenvectors\n3. These operations can be objectively verified:\n   - The convolution dimensions and channel count can be checked\n   - The covariance matrix calculation can be validated\n   - Eigenvector computation can be verified\n   - Filter initialization can be inspected\n4. The rubric provides clear criteria that align with these verifiable aspects\n5. Most criteria are quantitative and can be programmatically checked\n6. The efficiency criterion, while somewhat subjective, can be assessed through benchmarking\n\nGiven these points, it appears that a Python function could be written to objectively score an implementation of this task.\n\nScorable[True/False]: True\n\nJustification: This task is scorable because it involves implementing a specific mathematical procedure with well-defined steps that can be objectively verified. The rubric provides clear criteria that align with these verifiable aspects of the implementation. Key points that make this task scorable include:\n\n1. The task specifies exact dimensions and parameters (2x2 convolution, 24 channels) that can be programmatically checked.\n2. It requires mathematical operations (covariance matrix calculation, eigenvector computation) whose correctness can be verified through numerical comparisons.\n3. The initialization process for the filters is precisely defined and can be inspected programmatically.\n4. The rubric criteria closely match the key components of the implementation, making it straightforward to assess each aspect.\n5. Most of the criteria are quantitative in nature, allowing for objective scoring.\n6. Even the efficiency criterion, while somewhat subjective, can be evaluated through benchmarking or complexity analysis.\n\nGiven these factors, it would be feasible to create a Python function that could automatically verify the correctness of an implementation of this patch-whitening initialization, covering all the key aspects specified in the task description and rubric."
  },
  {
    "Request": "Given the fields `task_candidate`, produce the fields `task_rubric`.\n\n---\n\nFollow the following format.\n\nTask Candidate: ${task_candidate}\nTask Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"properties\": {\"criterion\": {\"description\": \"Criterion for evaluating the task\", \"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"List of criteria for evaluating task implementation; importance should sum to 100\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\n\n---\n\nTask Candidate:\n\nTask: Implement Multi-Crop Test-Time Augmentation\nDescription: Implement the multi-crop test-time augmentation (TTA) method described in the paper. This involves running the trained network on six augmented views of each test image and combining the outputs using a weighted average.\nRelevant Paper Text: To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\nEstimated Scoring Feasibility (1-10): 9\nEstimated LLM Tractability (0-1): 0.7\nEstimated Expert Tractability (0-1): 0.95\nEstimated Layman Tractability (0-1): 0.4\n\nTask Rubric:"
  },
  {
    "Response": "{\n  \"rubric\": [\n    {\n      \"criterion\": \"Correct implementation of six augmented views (unmodified, translated up-left, translated down-right, and mirrored versions)\",\n      \"importance\": 30\n    },\n    {\n      \"criterion\": \"Accurate application of weighted average for combining outputs (0.25 for untranslated, 0.125 for others)\",\n      \"importance\": 25\n    },\n    {\n      \"criterion\": \"Proper integration with the existing trained network\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"Efficient implementation to handle multiple test images\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"Correct handling of edge cases (e.g., image boundaries for translations)\",\n      \"importance\": 10\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.\n\n---\n\nFollow the following format.\n\nTask: ${task}\n\nRubric: ${rubric}\n\nReasoning: Let's think step by step in order to ${produce the justification}. We ...\n\nScorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified?\n\nJustification: Justification for scorable judgement\n\n---\n\nTask: Task: Implement Multi-Crop Test-Time Augmentation Description: Implement the multi-crop test-time augmentation (TTA) method described in the paper. This involves running the trained network on six augmented views of each test image and combining the outputs using a weighted average. Relevant Paper Text: To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each. Estimated Scoring Feasibility (1-10): 9 Estimated LLM Tractability (0-1): 0.7 Estimated Expert Tractability (0-1): 0.95 Estimated Layman Tractability (0-1): 0.4\n\nRubric: Criterion: Correct implementation of six augmented views (unmodified, translated up-left, translated down-right, and mirrored versions) Importance: 30.0 Criterion: Accurate application of weighted average for combining outputs (0.25 for untranslated, 0.125 for others) Importance: 25.0 Criterion: Proper integration with the existing trained network Importance: 20.0 Criterion: Efficient implementation to handle multiple test images Importance: 15.0 Criterion: Correct handling of edge cases (e.g., image boundaries for translations) Importance: 10.0\n\nReasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification. We need to consider:\n\n1. The clarity and specificity of the task description\n2. The availability of objective criteria in the rubric\n3. The technical feasibility of implementing and testing the required functionality\n4. The potential for automating the scoring process\n\nAnalyzing these points:\n\n1. The task description is clear and specific, detailing the exact augmentations required and the weighting for each view.\n2. The rubric provides objective criteria with specific weightings, covering all major aspects of the implementation.\n3. The technical requirements (image augmentations, weighted averaging) can be implemented and verified programmatically.\n4. Automated scoring is feasible due to the well-defined expected outputs and weightings.\n\nGiven these considerations, it appears that creating an objective scoring function is highly feasible.\n\nScorable[True/False]: True\n\nJustification: This task is scorable for several reasons:\n\n1. The task description provides clear, specific requirements for the multi-crop test-time augmentation method, including exact augmentations and weightings.\n\n2. The rubric offers objective criteria that can be directly translated into testable components of a scoring function. Each criterion is quantifiable and can be verified programmatically.\n\n3. The implementation involves well-defined image processing operations (translations, mirroring) and mathematical calculations (weighted averaging) that can be objectively assessed.\n\n4. The expected output (combined predictions from six augmented views) can be precisely calculated and compared against a reference implementation.\n\n5. Edge cases, such as handling image boundaries during translations, are explicitly mentioned in the rubric and can be tested systematically.\n\n6. The high estimated scoring feasibility (9/10) suggests that experts consider this task to be highly amenable to objective scoring.\n\nGiven these factors, it would be possible to write a Python function that objectively scores an implementation of this task, verifying that all key functionalities are implemented as specified. The function could test each augmentation, verify the correct application of weights, and assess the overall integration and efficiency of the implementation."
  },
  {
    "Request": "Given the fields `task_candidate`, produce the fields `task_rubric`.\n\n---\n\nFollow the following format.\n\nTask Candidate: ${task_candidate}\nTask Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"properties\": {\"criterion\": {\"description\": \"Criterion for evaluating the task\", \"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"List of criteria for evaluating task implementation; importance should sum to 100\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\n\n---\n\nTask Candidate:\n\nTask: Implement Identity Initialization for Convolutions\nDescription: Implement the identity initialization method for convolutional layers after the first layer, as described in the paper. This involves initializing the first M filters of each convolution with N>=M outputs as an identity transform of the input.\nRelevant Paper Text: We initialize all convolutions after the first as partial identity transforms. That is, for a convolution with M input channels and N>=M outputs, we initialize its first M filters to an identity transform of the input, and leave the remaining N-M to their default initialization.\nEstimated Scoring Feasibility (1-10): 8\nEstimated LLM Tractability (0-1): 0.7\nEstimated Expert Tractability (0-1): 0.9\nEstimated Layman Tractability (0-1): 0.2\n\nTask Rubric:"
  },
  {
    "Response": "{\n  \"rubric\": [\n    {\n      \"criterion\": \"Correct implementation of identity initialization for convolutional layers after the first layer\",\n      \"importance\": 35\n    },\n    {\n      \"criterion\": \"Proper handling of cases where N >= M (number of outputs >= number of inputs)\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"Accurate initialization of the first M filters as identity transforms\",\n      \"importance\": 25\n    },\n    {\n      \"criterion\": \"Correct default initialization for the remaining N-M filters\",\n      \"importance\": 10\n    },\n    {\n      \"criterion\": \"Efficiency of the implementation\",\n      \"importance\": 5\n    },\n    {\n      \"criterion\": \"Code readability and documentation\",\n      \"importance\": 5\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.\n\n---\n\nFollow the following format.\n\nTask: ${task}\n\nRubric: ${rubric}\n\nReasoning: Let's think step by step in order to ${produce the justification}. We ...\n\nScorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified?\n\nJustification: Justification for scorable judgement\n\n---\n\nTask: Task: Implement Identity Initialization for Convolutions Description: Implement the identity initialization method for convolutional layers after the first layer, as described in the paper. This involves initializing the first M filters of each convolution with N>=M outputs as an identity transform of the input. Relevant Paper Text: We initialize all convolutions after the first as partial identity transforms. That is, for a convolution with M input channels and N>=M outputs, we initialize its first M filters to an identity transform of the input, and leave the remaining N-M to their default initialization. Estimated Scoring Feasibility (1-10): 8 Estimated LLM Tractability (0-1): 0.7 Estimated Expert Tractability (0-1): 0.9 Estimated Layman Tractability (0-1): 0.2\n\nRubric: Criterion: Correct implementation of identity initialization for convolutional layers after the first layer Importance: 35.0 Criterion: Proper handling of cases where N >= M (number of outputs >= number of inputs) Importance: 20.0 Criterion: Accurate initialization of the first M filters as identity transforms Importance: 25.0 Criterion: Correct default initialization for the remaining N-M filters Importance: 10.0 Criterion: Efficiency of the implementation Importance: 5.0 Criterion: Code readability and documentation Importance: 5.0\n\nReasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification:\n\n1. We need to determine if the task can be objectively scored using a Python function.\n\n2. The task involves implementing a specific initialization method for convolutional layers.\n\n3. The requirements are clearly defined:\n   - Apply to convolutional layers after the first layer\n   - Initialize first M filters as identity transforms for N>=M outputs\n   - Leave remaining N-M filters with default initialization\n\n4. These requirements can be verified programmatically:\n   - Check if the initialization is applied to correct layers\n   - Verify the identity transform for the first M filters\n   - Ensure remaining filters have default initialization\n\n5. The rubric provides specific criteria that can be checked:\n   - Correct implementation of identity initialization\n   - Proper handling of N >= M cases\n   - Accurate initialization of first M filters\n   - Correct default initialization for remaining filters\n   - Efficiency and readability can also be assessed\n\n6. While some criteria (like efficiency and readability) may require more complex evaluation, the core functionality can be objectively tested.\n\n7. Given the clear specifications and the nature of the task, it should be possible to write a Python function to verify the implementation.\n\nScorable[True/False]: True\n\nJustification: This task is scorable because it involves implementing a specific initialization method for convolutional layers with clear, well-defined requirements. A Python function can be written to objectively verify the key aspects of the implementation, including:\n\n1. Checking if the initialization is applied to the correct layers (after the first layer).\n2. Verifying that the first M filters are initialized as identity transforms for N>=M outputs.\n3. Ensuring that the remaining N-M filters retain their default initialization.\n4. Testing the implementation with various input sizes to confirm proper handling of N >= M cases.\n\nThe rubric provides specific criteria that can be programmatically checked, such as correct implementation, proper handling of cases, and accurate initialization. While some aspects like code efficiency and readability might require more sophisticated evaluation, the core functionality can be objectively tested. The high estimated scoring feasibility (8/10) also supports the conclusion that this task is scorable using a Python function."
  },
  {
    "Request": "Given the fields `task_candidate`, produce the fields `task_rubric`.\n\n---\n\nFollow the following format.\n\nTask Candidate: ${task_candidate}\nTask Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"properties\": {\"criterion\": {\"description\": \"Criterion for evaluating the task\", \"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"List of criteria for evaluating task implementation; importance should sum to 100\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\n\n---\n\nTask Candidate:\n\nTask: Implement Lookahead Optimization\nDescription: Implement the Lookahead optimization method as described in the paper. This involves maintaining a slow-moving copy of the model parameters and periodically updating the fast-moving parameters towards the slow-moving ones.\nRelevant Paper Text: Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization. We note that Lookahead has also been found effective in prior work on training speed for ResNet-18 (Moreau et al., 2022).\nEstimated Scoring Feasibility (1-10): 7\nEstimated LLM Tractability (0-1): 0.6\nEstimated Expert Tractability (0-1): 0.85\nEstimated Layman Tractability (0-1): 0.1\n\nTask Rubric:"
  },
  {
    "Response": "{\n  \"rubric\": [\n    {\n      \"criterion\": \"Correct implementation of the Lookahead algorithm\",\n      \"importance\": 35\n    },\n    {\n      \"criterion\": \"Proper maintenance of slow-moving and fast-moving parameter copies\",\n      \"importance\": 25\n    },\n    {\n      \"criterion\": \"Accurate periodic update mechanism for fast-moving parameters\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"Integration with existing optimization framework\",\n      \"importance\": 10\n    },\n    {\n      \"criterion\": \"Performance improvement in model training\",\n      \"importance\": 5\n    },\n    {\n      \"criterion\": \"Code readability and documentation\",\n      \"importance\": 5\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.\n\n---\n\nFollow the following format.\n\nTask: ${task}\n\nRubric: ${rubric}\n\nReasoning: Let's think step by step in order to ${produce the justification}. We ...\n\nScorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified?\n\nJustification: Justification for scorable judgement\n\n---\n\nTask: Task: Implement Lookahead Optimization Description: Implement the Lookahead optimization method as described in the paper. This involves maintaining a slow-moving copy of the model parameters and periodically updating the fast-moving parameters towards the slow-moving ones. Relevant Paper Text: Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization. We note that Lookahead has also been found effective in prior work on training speed for ResNet-18 (Moreau et al., 2022). Estimated Scoring Feasibility (1-10): 7 Estimated LLM Tractability (0-1): 0.6 Estimated Expert Tractability (0-1): 0.85 Estimated Layman Tractability (0-1): 0.1\n\nRubric: Criterion: Correct implementation of the Lookahead algorithm Importance: 35.0 Criterion: Proper maintenance of slow-moving and fast-moving parameter copies Importance: 25.0 Criterion: Accurate periodic update mechanism for fast-moving parameters Importance: 20.0 Criterion: Integration with existing optimization framework Importance: 10.0 Criterion: Performance improvement in model training Importance: 5.0 Criterion: Code readability and documentation Importance: 5.0\n\nReasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification. We need to consider:\n\n1. The nature of the task: Implementing Lookahead Optimization\n2. The specific requirements outlined in the task description\n3. The criteria provided in the rubric\n4. The possibility of objectively verifying these criteria through code\n\n1. The task involves implementing a specific optimization algorithm (Lookahead) with clear, defined steps.\n\n2. The task description specifies maintaining two sets of parameters (slow-moving and fast-moving) and periodically updating them.\n\n3. The rubric provides clear, measurable criteria:\n   - Correct implementation of the algorithm\n   - Proper maintenance of parameter copies\n   - Accurate update mechanism\n   - Integration with existing frameworks\n   - Performance improvement\n   - Code quality\n\n4. Most of these criteria can be objectively verified through code inspection and automated testing:\n   - The algorithm's implementation can be checked against the paper's description\n   - The maintenance of parameter copies can be verified programmatically\n   - The update mechanism can be tested with predefined inputs and expected outputs\n   - Integration can be checked by running the optimization within a standard framework\n   - Performance improvement can be measured through benchmarks\n   - Code quality can be assessed using standard linting tools\n\nGiven these considerations, it appears that a Python function could be written to objectively score most aspects of this implementation.\n\nScorable[True/False]: True\n\nJustification: This task is scorable because:\n\n1. The Lookahead optimization algorithm has a well-defined structure that can be verified programmatically.\n2. The main components of the implementation (parameter maintenance, update mechanism) can be objectively checked through code inspection and unit tests.\n3. The rubric provides clear, measurable criteria that align well with automated testing capabilities.\n4. Performance improvements can be quantitatively measured through benchmarks.\n5. Code quality and integration aspects can be assessed using standard development tools.\n\nWhile some subjective judgment might be needed for assessing code readability, the majority of the implementation can be objectively scored. Therefore, it is feasible to create a Python function that can verify the key functionality and score the implementation based on the given rubric."
  }
]