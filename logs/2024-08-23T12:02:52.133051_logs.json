[
  {
    "Request": "Given the fields `paper_text`, `guidance`, produce the fields `task_candidates`.\n\n---\n\nFollow the following format.\n\nPaper Text: Full text from research paper PDF\nGuidance: Guidance for extracting task candidates\nTask Candidates: JSON list of task candidate dicts with keys 'name', 'description', and 'relevant_paper_text'. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCandidate\": {\"description\": \"Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\\n\\nA good task candidate:\\n- describes a task that an agent can implement\\n- typically a programming task, e.g. modifying a 'baseline' implementation\\n- which reflects or mirrors actual tasks undertaken to produce the research paper\\n- and requires research and/or engineering expertise to implement correctly\\n- for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\\n- ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\\n- the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\", \"properties\": {\"name\": {\"description\": \"Name of the task\", \"title\": \"Name\", \"type\": \"string\"}, \"description\": {\"description\": \"Description of the task\", \"title\": \"Description\", \"type\": \"string\"}, \"baseline\": {\"description\": \"Short description of baseline implementation, if any, that will be given to implementers\", \"title\": \"Baseline\", \"type\": \"string\"}, \"skills\": {\"description\": \"Comma-separate list of skills required to implement the task\", \"items\": {\"type\": \"string\"}, \"title\": \"Skills\", \"type\": \"array\"}, \"relevant_paper_text\": {\"description\": \"Text from the paper that is relevant to the task\", \"title\": \"Relevant Paper Text\", \"type\": \"string\"}, \"scoring_feasibility\": {\"description\": \"\\nOn a scale of 1-10, how feasible is it to write a python function to objectively score an \\nimplementation of this task, verifying that key functionality is implemented as specified?\\n\", \"title\": \"Scoring Feasibility\", \"type\": \"integer\"}, \"llm_tractability\": {\"description\": \"Probability in (0, 1) that a frontier LLM can generate a correct implementation of this task\", \"title\": \"Llm Tractability\", \"type\": \"number\"}, \"expert_tractability\": {\"description\": \"Probability in (0, 1) that a human subject matter expert can generate a correct implementation of this task\", \"title\": \"Expert Tractability\", \"type\": \"number\"}, \"layman_tractability\": {\"description\": \"Probability in (0, 1) that a layman can generate a correct implementation of this task\", \"title\": \"Layman Tractability\", \"type\": \"number\"}, \"scoring_llm_tractability\": {\"description\": \"Probability in (0, 1) that a frontier LLM can generate a correct scoring function for this task\", \"title\": \"Scoring Llm Tractability\", \"type\": \"number\"}, \"asset_prerequisites\": {\"description\": \"List of assets which should be made available to implementers or which are required to implement scoring\", \"items\": {\"type\": \"string\"}, \"title\": \"Asset Prerequisites\", \"type\": \"array\"}}, \"required\": [\"name\", \"description\", \"baseline\", \"skills\", \"relevant_paper_text\", \"scoring_feasibility\", \"llm_tractability\", \"expert_tractability\", \"layman_tractability\", \"scoring_llm_tractability\", \"asset_prerequisites\"], \"title\": \"TaskCandidate\", \"type\": \"object\"}}, \"properties\": {\"tasks\": {\"description\": \"Tasks extracted from the paper\", \"items\": {\"$ref\": \"#/$defs/TaskCandidate\"}, \"title\": \"Tasks\", \"type\": \"array\"}}, \"required\": [\"tasks\"], \"title\": \"TaskCandidates\", \"type\": \"object\"}\n\n---\n\nPaper Text:\n94% on CIFAR-10 in 3.29 Seconds on a Single GPU\nKeller Jordan\nkjordan4077@gmail.com\nAbstract\nCIFAR-10 is among the most widely used datasets in machine learning, facilitating\nthousands of research projects per year. To accelerate research and reduce the cost\nof experiments, we introduce training methods for CIFAR-10 which reach 94%\naccuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when\nrun on a single NVIDIA A100 GPU. As one factor contributing to these training\nspeeds, we propose a derandomized variant of horizontal flipping augmentation,\nwhich we show improves over the standard method in every case where flipping is\nbeneficial over no flipping at all. Our code is released at https://github.com/\nKellerJordan/cifar10-airbench .\n1 Introduction\nCIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-\ning thousands of research projects per year1. Research can be accelerated and the cost of experiments\nreduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In\nthis paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single\nNVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,\n2023). To support scenarios where higher performance is needed, we additionally develop methods\ntargeting 95% and 96% accuracy. We release the following methods in total.\n1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).\n2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).\n3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).\n4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).\nAll runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-\nmatically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the\nfirst uses torch.compile to improve GPU utilization. It is intended for experiments where many\nnetworks are trained at once in order to amortize the one-time compilation cost. The non-compiled\nairbench94 variant can be easily installed and run using the following command.\n1pip install airbench\n2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"\nOne motivation for the development of these training methods is that they can accelerate the experi-\nmental iteration time of researchers working on compatible projects involving CIFAR-10. Another\nmotivation is that they can decrease the cost of projects involving a massive number of trained\nnetworks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used\n3 million trained networks to demonstrate that the outputs of a trained neural network on a given test\ninput follow an approximately linear function of the vector of binary choices of which examples the\nmodel was trained on. Another example is Jordan (2023), a study on training variance which used\n180 thousand trained networks to show that standard trainings have little variance in performance on\n1https://paperswithcode.com/datasets\nPreprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024\nFigure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal\nflipping augmentation, which flips each image with 50% probability per epoch. This results in some images\nbeing redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a\ndeterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\ntheir test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds\nand 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make\nit possible to replicate these studies, or conduct similar ones, with fewer computational resources.\nFast training also enables the rapid accumulation of statistical significance for subtle hyperparameter\ncomparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10\naccuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation\nbetween runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the\nimprovement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training\nthis will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.\nOur work builds on prior training speed projects. We utilize a modified version of the network,\ninitialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen\npatch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over\nprior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-\nure 1, Section 3.6, Section 5.2).\n2 Background\nOur objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set\nin the shortest possible amount of time. Timing begins when the method is first given access to\ntraining data, and ends when it produces test-set predictions. The method is considered valid if its\nmean accuracy over repeated runs is at least 94%.\nWe chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the\n2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more\nrecent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10\nseconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page\n(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is\ntysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,\n94% is the level of human accuracy reported by Karpathy (2011).\nWe note the following consequences of how the method is timed. First, it is permitted for the program\nto begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins\nwhen the training data is first accessed. This is helpful because otherwise the first run of training\nis typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA\nimproves the performance of a trained network by running it on multiple augmented views of each\ntest input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping\nTTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods\nattain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\nThe CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.\nIt is divided into a training set of 50,000 images and a validation set of 10,000 images. As a\nmatter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was\n2\n80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than\nairbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94\ncan be attributed entirely to algorithmic progress rather than compute scaling.\n3 Methods\n3.1 Network architecture and baseline training\nWe train a convolutional network with a total of 1.97 million parameters, following tysam-code\n(2023) with a few small changes. It contains seven convolutions with the latter six being divided into\nthree blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this\nsection we offer some comments on the main design choices.\nThe network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed\nentirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,\n2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with\nno padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather\nthan the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff\nbetween throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\nFollowing Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,\nand disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is\nscaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture\ndiffers only in that we decrease the number of output channels in the third block from 512 to 256,\nand we add learnable biases to the first convolution.\nAs our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of\n0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the\nmaximum at 20% of the way through training, and then decreases to zero. For data augmentation we\nuse random horizontal flipping alongside 2-pixel random translation. For translation we use reflection\npadding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note\nthat what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a\nrandom 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the\nnetwork is run on both a given test image and its mirror and inferences are made based on the average\nof the two outputs. With optimized choices of learning rate, momentum, and weight decay, this\nbaseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.\n3.2 Frozen patch-whitening initialization\nFollowing Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-\nwhitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code\n(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches\nacross the training distribution, so that their outputs have identity covariance matrix. The second 12\nFigure 2: The first layer\u2019s weights after whitening\ninitialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,\nso that input information is preserved through the\nactivation which follows. Figure 2 shows the result.\nWe do not update this layer\u2019s weights during training.\nDeparting from tysam-code (2023), we add learn-\nable biases to this layer, yielding a small performance\nboost. The biases are trained for 3 epochs, after which\nwe disable their gradient to increase backward-pass\nthroughput, which improves training speed without\nreducing accuracy. We also obtain a slight perfor-\nmance boost relative to tysam-code (2023) by re-\nducing the constant added to the eigenvalues during\ncalculation of the patch-whitening initialization for\nthe purpose of preventing numerical issues in the case\nof a singular patch-covariance matrix.\nPatch-whitening initialization is the single most impactful feature. Adding it to the baseline more\nthan doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.\n3\nRandom reshuffling Alternating flip Mean accuracy\nNo No 93.40%\nNo Yes 93.48%\nYes No 93.92%\nYes Yes 94.01%\nTable 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating\nflip (which we propose) reduce training data redundancy and improve performance.\n3.3 Identity initialization\ndirac : We initialize all convolutions after the first as partial identity transforms. That is, for a\nconvolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an\nidentity transform of the input, and leave the remaining N\u2212Mto their default initialization. In\nPyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight\nwof each convolutional layer. This method partially follows tysam-code (2023), which used a more\ncomplicated scheme where the identity weights are mixed in with the original initialization, which\nwe did not find to be more performant. With this feature added, training attains 94% accuracy in 18\nepochs taking 6.8 A100-seconds.\n3.4 Optimization tricks\nscalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor\nof64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%\nin 13.5 epochs taking 5.1 A100-seconds.\nlookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.\nWe note that Lookahead has also been found effective in prior work on training speed for ResNet-\n18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6\nA100-seconds.\n3.5 Multi-crop evaluation\nmulticrop : To generate predictions, we run the trained network on six augmented views of each\ntest image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a\nversion which is translated down-and-to-the-right by one pixel, and the mirrored versions of all\nthree. Predictions are made using a weighted average of all six outputs, where the two views of the\nuntranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125\neach. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.\nWe note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-\nmonyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of\nevaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using\nmore crops does improve performance, but the increase to inference time outweighs the potential\ntraining speedup.\n3.6 Alternating flip\nTo speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,\nwhich we motivate as follows. When training neural networks, it is standard practice to organize\ntraining into a set of epochs during which every training example is seen exactly once. This\ndiffers from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),\nwhich calls for data to be repeatedly sampled with-replacement from the training set, resulting in\nexamples being potentially seen multiple redundant times within a short window of training. The\nuse of randomly ordered epochs of data for training has a different name, being called the random\nreshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our\ntraining dataset consists of Nunique examples, then sampling data with replacement causes every\n\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N\nunique examples on average. On the other hand, random reshuffling leads to all Nunique examples\nbeing seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason\nthat it is beneficial to maximize the number of unique inputs seen per window of training time.\n4\nWe extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We\nfirst note that standard random horizontal flipping augmentation can be defined as follows.\n1import torch\n2def random_flip ( inputs ):\n3 # Applies random flipping to a batch of images\n4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)\n5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]\nListing 1: Random flip\nIf horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2\nwhich may be seen during training. Potentially, every pair of consecutive epochs could contain every\nunique input. But our main observation is that with standard random horizontal flipping, half of the\nimages will be redundantly flipped the same way during both epochs, so that on average only 1.5N\nunique inputs will be seen.\naltflip : To address this, we propose to modify standard random horizontal flipping augmentation as\nfollows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},\nwe flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we\nflip only those inputs which were flipped in the first epoch. We provide the following implementation\nwhich avoids the need for extra memory by using a pseudorandom function to decide the flips.\n1import torch\n2import hashlib\n3def hash_fn (n, seed =42) :\n4 k = n * seed\n5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,\n16)\n6def alternating_flip (inputs , indices , epoch ):\n7 # Applies alternating flipping to a batch of images\n8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist\n() ])\n9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)\n10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\nListing 2: Alternating flip\nThe result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in\nFigure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.\nAdding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final\ntraining method airbench94.py , the entire contents of which can be found in Section E. It reaches\n94% accuracy in 3.83 seconds on an NVIDIA A100.\n3.7 Compilation\nThe final step we take to speed up training is a non-algorithmic one: we compile our training\nmethod using torch.compile in order to more efficiently utilize the GPU. This results in a training\nscript which is mathematically equivalent (up to small differences in floating point arithmetic) to\nthe non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29\nA100-seconds. The downside is that the one-time compilation process takes up to several minutes to\ncomplete before training runs can begin, so that it is only beneficial when we plan to execute many\nruns of training at once. We release this version as airbench94_compiled.py .\n4 95% and 96% targets\nTo address scenarios where somewhat higher performance is desired, we additionally develop methods\ntargeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .\nTo attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel\ncount of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce\nthe learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%\naccuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.\n2Assuming none of the training inputs are already mirrors of each other.\n5\nFigure 4: Training speedups accumulate additively. Removing individual features from airbench94\nincreases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces\nthe epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-\n94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.\nFigure 3: FLOPs vs. error rate tradeoff. Our\nthree training methods apparently follow a linear\nlog-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-\nVries & Taylor, 2017) augmentation and raise the\ntraining epochs to 40. We add a third convolution to\neach block, and scale the first block to 128 channels\nand the second two to 512. We also add a residual\nconnection across the later two convolutions of each\nblock, which we find is still beneficial despite the fact\nthat we are already using identity initialization (Sec-\ntion 3.3) to ease gradient flow. Finally, we reduce\nthe learning rate by a factor of 0.78. These changes\nyield airbench96 which attains 96.05% accuracy in\n46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.\nFigure 3 shows the FLOPs and error rate of each of\nour three training methods.\n5 Experiments\n5.1 Interaction between features\nTo gain a better sense of the impact of each feature on training speed, we compare two quantities.\nFirst, we measure the number of epochs that can be saved by adding the feature to the whitened\nbaseline (Section 3.2). Second, we measure the number of epochs that must be added when the\nfeature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-\ntion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it\nfrom the final airbench94 increases epochs-to-94% from 9.9 to 12.8.\nFigure 4 shows both quantities for each feature. Surprisingly, we find that for all features except\nmulti-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even\nthough the whitened baseline requires more than twice as many epochs as the final configuration.\nThis indicates that the interaction between most features is additive rather than multiplicative.\n5.2 Does alternating flip generalize?\nIn this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of\ntraining configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all\ncases except those where neither alternating nor random flip improve over using no flipping at all.\nFor CIFAR-10 we consider the performance boost given by alternating flip across the follow-\ning 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and\nairbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For\n6\nFigure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,\nthe use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%\ntraining speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.\n95% confidence intervals are shown around each point.\neach configuration we compare the performance of alternating and random flip in terms of their mean\naccuracy across n= 400 runs of training.\nFigure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip\nimproves performance in every setting. To get a sense for how big the improvement is, we estimate\nthe effective speedup for each case, i.e., the fraction of epochs that could be saved by switching\nfrom random to alternating flip while maintaining the level of accuracy of random flip. We begin\nby fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of\neach random flip-based training configuration. We use these curves to calculate the effective speedup\nafforded by switching from random to alternating flip. For example, airbench94 with random flip\nand without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The\nsame configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law\nfit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.\nNote that using a power-law yields a more conservative estimate relative to using linear interpolation\nbetween the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.\nTable 2 shows the result. We observe the following patterns. First, the addition of extra augmentation\n(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that\nthe main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the\nsame way for many epochs in a row; we speculate that adding extra augmentation reduces the negative\nimpact of these cases because it increases data diversity. Next, TTA reduces the gap between random\nand alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),\nindicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training\nfor longer consistently increases the effective speedup given by alternating flip.\nWe next study ImageNet trainings with the following experiment. We train a ResNet-18 with a\nvariety of train and test crops, comparing three flipping options: alternating flip, random flip, and no\nflipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192\ncenter crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)\nto denote the latter. We also consider two training crops: 192x192 inception-style random resized\ncrop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area\nranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the\nshorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write\n7\nBaseline Cutout Epochs Speedup Speedup (w/ TTA)\nairbench94 No 10 15.0% 5.30%\nairbench94 No 20 27.1% 21.3%\nairbench94 No 40 38.3% 36.4%\nairbench94 No 80 102% 31.8%\nairbench94 Yes 10 3.84% 1.13%\nairbench94 Yes 20 7.42% 2.00%\nairbench94 Yes 40 18.6% 9.28%\nairbench94 Yes 80 29.2% 14.25%\nairbench96 Yes 10 4.94% 1.11%\nairbench96 Yes 20 8.99% 3.58%\nairbench96 Yes 40 17.2% 6.48%\nairbench96 Yes 80 18.8% Not measured\nTable 2: Effective speedups given by switching from random flip to alternating flip. The two configurations\nmost closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw\naccuracy values of the airbench94 experiments.\nFlipping augmentation option\nTrain crop Test crop Epochs TTA None Random Alternating\nHeavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28\nHeavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28\nLight RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26\nLight RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26\nHeavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28\nHeavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28\nLight RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26\nLight RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26\nLight RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16\nHeavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4\nTable 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for\nthose trainings where random flip improves significantly over not flipping at all. The single best flipping option\nin each row is bolded when the difference is statistically significant.\nHeavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided\nin Section C.\nTable 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC\nis better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly\nbetter when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test\nresolution discrepancy (Touvron et al., 2019).\nFor trainings which use Light RRC, we find that switching from random flip to alternating flip\nprovides a substantial boost to performance, amounting to a training speedup of more than 25%.\nIn Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to\nalternating flip improves performance by more than increasing the training duration from 16 to 20\nepochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our\nresults on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit\nfrom alternating flip. Indeed, even turning flipping off completely does not significantly reduce the\nperformance of these trainings. We conclude that alternating flip improves over random flip for every\ntraining scenario where the latter improves over no flipping at all.\n8\nEpochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE\n1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312\n2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233\n1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269\n1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533\n2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433\n1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444\nTable 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).\n5.3 Variance and class-wise calibration\nPrevious sections have focused on understanding what factors affect the first moment of accuracy\n(the mean). In this section we investigate the second moment, finding that TTA reduces variance at\nthe cost of calibration.\nOur experiment is to execute 10,000 runs of airbench94 training with several hyperparameter\nsettings. For each setting we report both the variance in test-set accuracy as well as an estimate of the\ndistribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.\nTable 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set\nvariance, replicating the main finding of Jordan (2023). This is a surprising result because these\ntrainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7\nas much distribution-wise variance when run for a similar duration, and reached a low variance only\nwhen run for 64 epochs. We conclude from this comparison that distribution-wise variance is more\nstrongly connected to the rate of convergence of a training rather than its duration as such. We also\nnote that the low distribution-wise variance of airbench94 indicates it has high training stability.\nUsing TTA significantly reduces the test-set variance, such that all three settings with TTA have lower\ntest-set variance than any setting without TTA. However, test-set variance is implied by the class-wise\ncalibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this\nreduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,\nwe compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which\nmeasures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA\nhas a higher CACE than every setting without TTA, confirming the hypothesis.\n6 Discussion\nIn this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7\nfaster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the\nairbench Python package.\nWe developed airbench solely with the goal of maximizing training speed on CIFAR-10. In\nSection B we find that it also generalizes well to other tasks. For example, without any extra tuning,\nairbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.\nOne factor contributing to the training speed of airbench was our finding that training can be accel-\nerated by partially derandomizing the standard random horizontal flipping augmentation, resulting in\nthe variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating\nflip improves the performance of every training we considered (Section 5.2), with the exception of\nthose trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to\nus, the standard ImageNet trainings that we considered do not significantly benefit from horizontal\nflipping. Future work might investigate whether it is possible to obtain derandomized improvements\nto other augmentations besides horizontal flip.\nThe methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,\nwith fixed performance and hardware constraints. These constraints mean that we cannot improve\nperformance by simply scaling up the amount of computational resources used; instead we are forced\nto develop new methods like the alternating flip. We look forward to seeing what other new methods\nfuture work discovers to push training speed further.\n9\nReferences\nDimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.\nDan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-\nperformance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,\n2011.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\nmark and competition. 2017.\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet\nor cifar-10. arXiv preprint arXiv:1810.03505 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\npp. 248\u2013255. IEEE, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552 , 2017.\nMert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic\ngradient descent. Mathematical Programming , 186:49\u201384, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 , 2016.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.\npmlr, 2015.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\nsgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.\nKeller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and\ninevitable. arXiv preprint arXiv:2304.01910 , 2023.\nAndrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.\nio/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for\nadvanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT\nLicense.\nGuillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander\nM \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,\nBenjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.\nBenchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural\nInformation Processing Systems , 35:25404\u201325421, 2022.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.\nDavid Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/\nhow-to-train-your-resnet-8-bag-of-tricks/ .\n10\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics , 22(3):400\u2013407, 1951.\nSantiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:\n//github.com/apple/ml-cifar-10-faster , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.\ntysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/\nhlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. Advances in neural information processing systems , 32, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,\n2016.\nMichael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps\nforward, 1 step back. Advances in neural information processing systems , 32, 2019.\n11\nA Network architecture\n1from torch import nn\n2\n3class Flatten (nn. Module ):\n4 def forward (self , x):\n5 return x. view (x. size (0) , -1)\n6\n7class Mul (nn. Module ):\n8 def __init__ (self , scale ):\n9 super (). __init__ ()\n10 self . scale = scale\n11 def forward (self , x):\n12 return x * self . scale\n13\n14def conv (ch_in , ch_out ):\n15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,\n16 padding =\u2019same \u2019, bias = False )\n17\n18def make_net ():\n19 act = lambda : nn. GELU ()\n20 bn = lambda ch: nn. BatchNorm2d (ch)\n21 return nn. Sequential (\n22 nn. Sequential (\n23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),\n24 act () ,\n25 ),\n26 nn. Sequential (\n27 conv (24 , 64) ,\n28 nn. MaxPool2d (2) ,\n29 bn (64) , act () ,\n30 conv (64 , 64) ,\n31 bn (64) , act () ,\n32 ),\n33 nn. Sequential (\n34 conv (64 , 256) ,\n35 nn. MaxPool2d (2) ,\n36 bn (256) , act () ,\n37 conv (256 , 256) ,\n38 bn (256) , act () ,\n39 ),\n40 nn. Sequential (\n41 conv (256 , 256) ,\n42 nn. MaxPool2d (2) ,\n43 bn (256) , act () ,\n44 conv (256 , 256) ,\n45 bn (256) , act () ,\n46 ),\n47 nn. MaxPool2d (3) ,\n48 Flatten () ,\n49 nn. Linear (256 , 10, bias = False ),\n50 Mul (1/9) ,\n51 )\nListing 3: Minimal PyTorch code for the network architecture used by airbench94 .\nWe note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock\ntime. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel\nsize of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this\ndoes not improve the wallclock training time on an A100. Reducing the batch size is another easy\nway to save FLOPs but not wallclock time.\n12\nDataset Flipping? Cutout? ResNet-18 airbench96\nCIFAR-10 Yes No 95.55% 95.61%\nCIFAR-10 Yes Yes 96.01% 96.05%\nCIFAR-100 Yes No 77.54% 79.27%\nCIFAR-100 Yes Yes 78.04% 79.76%\nCINIC-10 Yes No 87.58% 87.78%\nCINIC-10 Yes Yes not measured 88.22%\nSVHN No No 97.35% 97.38%\nSVHN No Yes not measured 97.64%\nTable 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We\ndirectly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off\nflipping for SVHN).\nB Extra dataset experiments\nWe developed airbench with the singular goal of maximizing training speed on CIFAR-10. To\nfind out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its\nperformance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-\n10 (Darlow et al., 2018).\nOn CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both\nthe case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not\n(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than\nResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say\nthat it generalizes.\nWe compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.\nWe do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal\non CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better\nperformance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where\nairbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is\nused and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows\nstrong generalization to other tasks.\nWe note that this comparison between airbench96 and ResNet-18 training is fair in the sense that\nit does demonstrate that the former has good generalization, but unfair in the sense that it does not\nindicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time\naugmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training\nwould outperform airbench96 if it were run using test-time augmentation. However, it also takes\n5-10 times longer to complete. The decision of which to use may be situational.\nThe accuracy values we report for ResNet-18 training are from the following sources. We tried\nto select the highest values we could find for each setting. Moreau et al. (2022) reports attaining\n95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports\nattaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on\nCIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.\nC ImageNet training details\nOur ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/\nlibffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and\nmomentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution\n160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of\ntraining. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.\n13\nHyperparameters Flipping augmentation option\nEpochs Cutout TTA None Random Alternating\n10 No No 92.3053 93.0988 93.2798\n20 No No 92.8166 93.7446 93.8652\n40 No No 93.0143 94.0133 94.0729\n80 No No 93.0612 94.1169 94.1628\n10 No Yes 93.4071 93.9488 94.0186\n20 No Yes 93.8528 94.5565 94.6530\n40 No Yes 94.0381 94.7803 94.8203\n80 No Yes 94.0638 94.8506 94.8676\n10 Yes No 91.8487 92.0402 92.1374\n20 Yes No 92.8474 93.3825 93.4876\n40 Yes No 93.2675 94.1014 94.1952\n80 Yes No 93.4193 94.4311 94.5204\n10 Yes Yes 92.6455 92.7780 92.8103\n20 Yes Yes 93.7862 94.1306 94.1670\n40 Yes Yes 94.3090 94.8511 94.8960\n80 Yes Yes 94.5253 95.1839 95.2362\nTable 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a\nmean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise\ndifference in means is statistically significant.\nD Extra tables & figures\nFigure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in\nSection 5.3.\nE Complete training code\n1\"\"\"\n2airbench94 .py\n33.83 s runtime on an A100 ; 0.36 PFLOPs .\n4Evidence for validity : 94.01 average accuracy in n =1000 runs .\n5\n6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB\nwith the following nvidia -smi:\n7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :\n11.7\n14\n8torch . __version__ == \u20192.1.2+ cu118 \u2019\n9\"\"\"\n10\n11# ############################################\n12# Setup / Hyperparameters #\n13# ############################################\n14\n15import os\n16import sys\n17import uuid\n18from math import ceil\n19\n20import torch\n21from torch import nn\n22import torch .nn. functional as F\n23import torchvision\n24import torchvision . transforms as T\n25\n26torch . backends . cudnn . benchmark = True\n27\n28\"\"\"\n29We express the main training hyperparameters ( batch size , learning\nrate , momentum , and weight decay ) in decoupled form , so that each\none can be tuned independently . This accomplishes the following :\n30* Assuming time - constant gradients , the average step size is decoupled\nfrom everything but the lr.\n31* The size of the weight decay update is decoupled from everything but\nthe wd.\n32In constrast , normally when we increase the ( Nesterov ) momentum , this\nalso scales up the step size proportionally to 1 + 1 / (1 -\nmomentum ), meaning we cannot change momentum without having to re -\ntune the learning rate . Similarly , normally when we increase the\nlearning rate this also increases the size of the weight decay ,\nrequiring a proportional decrease in the wd to maintain the same\ndecay strength .\n33\n34The practical impact is that hyperparameter tuning is faster , since\nthis parametrization allows each one to be tuned independently .\nSee https :// myrtle .ai/ learn /how -to -train -your - resnet -5-\nhyperparameters /.\n35\"\"\"\n36\n37hyp = {\n38 \u2019opt \u2019: {\n39 \u2019 train_epochs \u2019: 9.9 ,\n40 \u2019batch_size \u2019: 1024 ,\n41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples\n42 \u2019momentum \u2019: 0.85 ,\n43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (\ndecoupled from learning rate )\n44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not\nweight decay ) for BatchNorm biases\n45 \u2019 label_smoothing \u2019: 0.2 ,\n46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the\nwhitening layer bias before freezing\n47 },\n48 \u2019aug \u2019: {\n49 \u2019flip \u2019: True ,\n50 \u2019translate \u2019: 2,\n51 },\n52 \u2019net \u2019: {\n53 \u2019widths \u2019: {\n54 \u2019block1 \u2019: 64,\n55 \u2019block2 \u2019: 256 ,\n56 \u2019block3 \u2019: 256 ,\n15\n57 },\n58 \u2019 batchnorm_momentum \u2019: 0.6 ,\n59 \u2019 scaling_factor \u2019: 1/9 ,\n60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :\n0= none , 1= mirror , 2= mirror + translate\n61 },\n62}\n63\n64# ############################################\n65# DataLoader #\n66# ############################################\n67\n68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )\n69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )\n70\n71def batch_flip_lr ( inputs ):\n72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .\nview (-1, 1, 1, 1)\n73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\n74\n75def batch_crop (images , crop_size ):\n76 r = ( images . size ( -1) - crop_size )//2\n77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =\nimages . device )\n78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),\ndevice = images .device , dtype = images . dtype )\n79 # The two cropping methods in this if - else produce equivalent\nresults , but the second is faster for r > 2.\n80 if r <= 2:\n81 for sy in range (-r, r +1):\n82 for sx in range (-r, r +1):\n83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)\n84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size\n, r+sx:r+sx+ crop_size ]\n85 else :\n86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size\n+2* r), device = images .device , dtype = images . dtype )\n87 for s in range (-r, r +1):\n88 mask = ( shifts [:, 0] == s)\n89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]\n90 for s in range (-r, r +1):\n91 mask = ( shifts [:, 1] == s)\n92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+\ncrop_size ]\n93 return images_out\n94\n95class CifarLoader :\n96 \"\"\"\n97 GPU - accelerated dataloader for CIFAR -10 which implements\nalternating flip augmentation .\n98 \"\"\"\n99\n100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,\ndrop_last =None , shuffle =None , gpu =0):\n101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .\npt \u2019)\n102 if not os. path . exists ( data_path ):\n103 dset = torchvision . datasets . CIFAR10 (path , download =True ,\ntrain = train )\n104 images = torch . tensor ( dset . data )\n105 labels = torch . tensor ( dset . targets )\n106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:\ndset . classes }, data_path )\n107\n108 data = torch . load ( data_path , map_location = torch . device (gpu))\n16\n109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [\n\u2019labels \u2019], data [\u2019classes \u2019]\n110 # It \u2019s faster to load + process uint8 data than to load\npreprocessed fp16 data\n111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).\nto( memory_format = torch . channels_last )\n112\n113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )\n114 self . proc_images = {} # Saved results of image processing to\nbe done on the first epoch\n115 self . epoch = 0\n116\n117 self . aug = aug or {}\n118 for k in self .aug . keys ():\n119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019\n% k\n120\n121 self . batch_size = batch_size\n122 self . drop_last = train if drop_last is None else drop_last\n123 self . shuffle = train if shuffle is None else shuffle\n124\n125 def __len__ ( self ):\n126 return len( self . images )// self . batch_size if self . drop_last\nelse ceil (len( self . images )/ self . batch_size )\n127\n128 def __iter__ ( self ):\n129\n130 if self . epoch == 0:\n131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .\nimages )\n132 # Randomly flip all images on the first epoch as according\nto definition of alternating flip\n133 if self .aug.get(\u2019flip \u2019, False ):\n134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (\nimages )\n135 # Pre -pad images to save time when doing random\ntranslation\n136 pad = self .aug.get (\u2019translate \u2019, 0)\n137 if pad > 0:\n138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019\nreflect \u2019)\n139\n140 if self .aug.get(\u2019translate \u2019, 0) > 0:\n141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .\nshape [ -2])\n142 elif self .aug.get (\u2019flip \u2019, False ):\n143 images = self . proc_images [\u2019flip \u2019]\n144 else :\n145 images = self . proc_images [\u2019norm \u2019]\n146 if self .aug.get(\u2019flip \u2019, False ):\n147 if self . epoch % 2 == 1:\n148 images = images . flip ( -1)\n149\n150 self . epoch += 1\n151\n152 indices = ( torch . randperm if self . shuffle else torch . arange )(\nlen ( images ), device = images . device )\n153 for i in range (len ( self )):\n154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]\n155 yield ( images [ idxs ], self . labels [ idxs ])\n156\n157# ############################################\n158# Network Components #\n159# ############################################\n160\n17\n161class Flatten (nn. Module ):\n162 def forward (self , x):\n163 return x. view (x. size (0) , -1)\n164\n165class Mul (nn. Module ):\n166 def __init__ (self , scale ):\n167 super (). __init__ ()\n168 self . scale = scale\n169 def forward (self , x):\n170 return x * self . scale\n171\n172class BatchNorm (nn. BatchNorm2d ):\n173 def __init__ (self , num_features , momentum , eps =1e -12 ,\n174 weight =False , bias = True ):\n175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )\n176 self . weight . requires_grad = weight\n177 self . bias . requires_grad = bias\n178 # Note that PyTorch already initializes the weights to one and\nbiases to zero\n179\n180class Conv (nn. Conv2d ):\n181 def __init__ (self , in_channels , out_channels , kernel_size =3,\npadding =\u2019same \u2019, bias = False ):\n182 super (). __init__ ( in_channels , out_channels , kernel_size =\nkernel_size , padding = padding , bias = bias )\n183\n184 def reset_parameters ( self ):\n185 super (). reset_parameters ()\n186 if self . bias is not None :\n187 self . bias . data . zero_ ()\n188 w = self . weight . data\n189 torch .nn. init . dirac_ (w[:w. size (1) ])\n190\n191class ConvGroup (nn. Module ):\n192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):\n193 super (). __init__ ()\n194 self . conv1 = Conv ( channels_in , channels_out )\n195 self . pool = nn. MaxPool2d (2)\n196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )\n197 self . conv2 = Conv ( channels_out , channels_out )\n198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )\n199 self . activ = nn. GELU ()\n200\n201 def forward (self , x):\n202 x = self . conv1 (x)\n203 x = self . pool (x)\n204 x = self . norm1 (x)\n205 x = self . activ (x)\n206 x = self . conv2 (x)\n207 x = self . norm2 (x)\n208 x = self . activ (x)\n209 return x\n210\n211# ############################################\n212# Network Definition #\n213# ############################################\n214\n215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019\n][ \u2019 batchnorm_momentum \u2019]):\n216 whiten_kernel_size = 2\n217 whiten_width = 2 * 3 * whiten_kernel_size **2\n218 net = nn. Sequential (\n219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True\n),\n220 nn. GELU () ,\n18\n221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],\nbatchnorm_momentum ),\n222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],\nbatchnorm_momentum ),\n223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],\nbatchnorm_momentum ),\n224 nn. MaxPool2d (3) ,\n225 Flatten () ,\n226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),\n227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,\n228 )\n229 net [0]. weight . requires_grad = False\n230 net = net. half (). cuda ()\n231 net = net.to( memory_format = torch . channels_last )\n232 for mod in net. modules ():\n233 if isinstance (mod , BatchNorm ):\n234 mod . float ()\n235 return net\n236\n237# ############################################\n238# Whitening Conv Initialization #\n239# ############################################\n240\n241def get_patches (x, patch_shape ):\n242 c, (h, w) = x. shape [1] , patch_shape\n243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h\n,w). float ()\n244\n245def get_whitening_parameters ( patches ):\n246 n,c,h,w = patches . shape\n247 patches_flat = patches . view (n, -1)\n248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n\n249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance\n, UPLO =\u2019U\u2019)\n250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.\nreshape (c*h*w,c,h,w). flip (0)\n251\n252def init_whitening_conv (layer , train_set , eps =5e -4):\n253 patches = get_patches ( train_set , patch_shape = layer . weight . data .\nshape [2:])\n254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )\n255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)\n256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -\neigenvectors_scaled ))\n257\n258# ###########################################\n259# Lookahead #\n260# ###########################################\n261\n262class LookaheadState :\n263 def __init__ (self , net):\n264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().\nitems ()}\n265\n266 def update (self , net , decay ):\n267 for ema_param , net_param in zip( self . net_ema . values () , net.\nstate_dict (). values ()):\n268 if net_param . dtype in ( torch .half , torch . float ):\n269 ema_param . lerp_ ( net_param , 1- decay )\n270 net_param . copy_ ( ema_param )\n271\n272# ###########################################\n273# Logging #\n274# ###########################################\n275\n19\n276def print_columns ( columns_list , is_head =False , is_final_entry = False ):\n277 print_string = \u2019\u2019\n278 for col in columns_list :\n279 print_string += \u2019| %s \u2019 % col\n280 print_string += \u2019|\u2019\n281 if is_head :\n282 print (\u2019-\u2019*len( print_string ))\n283 print ( print_string )\n284 if is_head or is_final_entry :\n285 print (\u2019-\u2019*len( print_string ))\n286\n287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,\n\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]\n288def print_training_details ( variables , is_final_entry ):\n289 formatted = []\n290 for col in logging_columns_list :\n291 var = variables .get (col. strip () , None )\n292 if type (var) in (int , str):\n293 res = str(var)\n294 elif type (var) is float :\n295 res = \u2019{:0.4 f}\u2019. format ( var)\n296 else :\n297 assert var is None\n298 res = \u2019\u2019\n299 formatted . append (res. rjust (len (col)))\n300 print_columns ( formatted , is_final_entry = is_final_entry )\n301\n302# ###########################################\n303# Evaluation #\n304# ###########################################\n305\n306def infer (model , loader , tta_level =0) :\n307 \"\"\"\n308 Test - time augmentation strategy (for tta_level =2):\n309 1. Flip / mirror the image left -to - right (50% of the time ).\n310 2. Translate the image by one pixel either up -and - left or down -and\n- right (50% of the time , i.e. both happen 25% of the time ).\n311\n312 This creates 6 views per image ( left / right times the two\ntranslations and no - translation ), which we evaluate and then\nweight according to the given probabilities .\n313 \"\"\"\n314\n315 def infer_basic (inputs , net):\n316 return net( inputs ). clone ()\n317\n318 def infer_mirror (inputs , net):\n319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))\n320\n321 def infer_mirror_translate ( inputs , net ):\n322 logits = infer_mirror (inputs , net )\n323 pad = 1\n324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)\n325 inputs_translate_list = [\n326 padded_inputs [:, :, 0:32 , 0:32] ,\n327 padded_inputs [:, :, 2:34 , 2:34] ,\n328 ]\n329 logits_translate_list = [ infer_mirror ( inputs_translate , net)\n330 for inputs_translate in\ninputs_translate_list ]\n331 logits_translate = torch . stack ( logits_translate_list ). mean (0)\n332 return 0.5 * logits + 0.5 * logits_translate\n333\n334 model . eval ()\n335 test_images = loader . normalize ( loader . images )\n20\n336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][\ntta_level ]\n337 with torch . no_grad ():\n338 return torch .cat ([ infer_fn (inputs , model ) for inputs in\ntest_images . split (2000) ])\n339\n340def evaluate (model , loader , tta_level =0) :\n341 logits = infer (model , loader , tta_level )\n342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()\n343\n344# ###########################################\n345# Training #\n346# ###########################################\n347\n348def main (run):\n349\n350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]\n351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]\n352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]\n353 # Assuming gradients are constant in time , for Nesterov momentum ,\nthe below ratio is how much larger the default steps will be than\nthe underlying per - example gradients . We divide the learning rate\nby this ratio in order to ensure steps are the same scale as\ngradients , regardless of the choice of momentum .\n354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))\n355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning\nrate for PyTorch SGD\n356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale\n357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]\n358\n359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019\nlabel_smoothing \u2019], reduction =\u2019none \u2019)\n360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)\n361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =\nbatch_size , aug=hyp[\u2019aug \u2019])\n362 if run == \u2019warmup \u2019:\n363 # The only purpose of the first run is to warmup , so we can\nuse dummy data\n364 train_loader . labels = torch . randint (0, 10, size =( len (\ntrain_loader . labels ) ,), device = train_loader . labels . device )\n365 total_train_steps = ceil (len( train_loader ) * epochs )\n366\n367 model = make_net ()\n368 current_steps = 0\n369\n370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in\nk and p. requires_grad ]\n371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019\nnot in k and p. requires_grad ]\n372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,\nweight_decay =wd/ lr_biases ),\n373 dict ( params = other_params , lr=lr , weight_decay =wd/\nlr)]\n374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,\nnesterov = True )\n375\n376 def triangle (steps , start =0, end =0, peak =0.5) :\n377 xp = torch . tensor ([0 , int( peak * steps ), steps ])\n378 fp = torch . tensor ([ start , 1, end ])\n379 x = torch . arange (1+ steps )\n380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])\n381 b = fp [: -1] - (m * xp [: -1])\n382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1\n383 indices = torch . clamp ( indices , 0, len(m) - 1)\n384 return m[ indices ] * x + b[ indices ]\n21\n385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,\npeak =0.23)\n386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:\nlr_schedule [i])\n387\n388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /\ntotal_train_steps )**3\n389 lookahead_state = LookaheadState ( model )\n390\n391 # For accurately timing GPU code\n392 starter = torch . cuda . Event ( enable_timing = True )\n393 ender = torch . cuda . Event ( enable_timing = True )\n394 total_time_seconds = 0.0\n395\n396 # Initialize the first layer using statistics of training images\n397 starter . record ()\n398 train_images = train_loader . normalize ( train_loader . images [:5000])\n399 init_whitening_conv ( model [0] , train_images )\n400 ender . record ()\n401 torch . cuda . synchronize ()\n402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n403\n404 for epoch in range ( ceil ( epochs )):\n405\n406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019\nwhiten_bias_epochs \u2019])\n407\n408 # ###################\n409 # Training #\n410 # ###################\n411\n412 starter . record ()\n413\n414 model . train ()\n415 for inputs , labels in train_loader :\n416\n417 outputs = model ( inputs )\n418 loss = loss_fn ( outputs , labels ).sum ()\n419 optimizer . zero_grad ( set_to_none = True )\n420 loss . backward ()\n421 optimizer . step ()\n422 scheduler . step ()\n423\n424 current_steps += 1\n425\n426 if current_steps % 5 == 0:\n427 lookahead_state . update (model , decay = alpha_schedule [\ncurrent_steps ]. item ())\n428\n429 if current_steps >= total_train_steps :\n430 if lookahead_state is not None :\n431 lookahead_state . update (model , decay =1.0)\n432 break\n433\n434 ender . record ()\n435 torch . cuda . synchronize ()\n436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n437\n438 # ###################\n439 # Evaluation #\n440 # ###################\n441\n442 # Print the accuracy and loss from the last training batch of\nthe epoch\n22\n443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().\nmean (). item ()\n444 train_loss = loss . item () / batch_size\n445 val_acc = evaluate (model , test_loader , tta_level =0)\n446 print_training_details ( locals () , is_final_entry = False )\n447 run = None # Only print the run number once\n448\n449 # ###################\n450 # TTA Evaluation #\n451 # ###################\n452\n453 starter . record ()\n454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019\ntta_level \u2019])\n455 ender . record ()\n456 torch . cuda . synchronize ()\n457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n458\n459 epoch = \u2019eval \u2019\n460 print_training_details ( locals () , is_final_entry = True )\n461\n462 return tta_val_acc\n463\n464if __name__ == \" __main__ \":\n465 with open (sys. argv [0]) as f:\n466 code = f. read ()\n467\n468 print_columns ( logging_columns_list , is_head = True )\n469 main (\u2019warmup \u2019)\n470 accs = torch . tensor ([ main (run) for run in range (25) ])\n471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))\n472\n473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }\n474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))\n475 os. makedirs ( log_dir , exist_ok = True )\n476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)\n477 print (os. path . abspath ( log_path ))\n478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))\nListing 4: airbench94.py\n23\nGuidance:"
  },
  {
    "Response": "Here are some potential task candidates extracted from the paper:\n\n{\n  \"tasks\": [\n    {\n      \"name\": \"Implement alternating flip data augmentation\",\n      \"description\": \"Implement the proposed alternating flip data augmentation method, which deterministically flips images in alternating epochs after the first epoch to avoid redundancy.\",\n      \"baseline\": \"Standard random horizontal flipping augmentation\",\n      \"skills\": [\"PyTorch\", \"Computer vision\", \"Data augmentation\"],\n      \"relevant_paper_text\": \"We propose (Section 3.6) to flip images in a deterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\",\n      \"scoring_feasibility\": 9,\n      \"llm_tractability\": 0.8,\n      \"expert_tractability\": 0.95,\n      \"layman_tractability\": 0.3,\n      \"scoring_llm_tractability\": 0.9,\n      \"asset_prerequisites\": [\"CIFAR-10 dataset\"]\n    },\n    {\n      \"name\": \"Implement frozen patch-whitening initialization\",\n      \"description\": \"Initialize the first convolutional layer as a patch-whitening transformation and freeze its weights during training, with learnable biases trained for 3 epochs.\",\n      \"baseline\": \"Standard random initialization of first layer\",\n      \"skills\": [\"PyTorch\", \"Computer vision\", \"Neural network initialization\"],\n      \"relevant_paper_text\": \"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows. Figure 2 shows the result. We do not update this layer's weights during training.\",\n      \"scoring_feasibility\": 8,\n      \"llm_tractability\": 0.6,\n      \"expert_tractability\": 0.9,\n      \"layman_tractability\": 0.2,\n      \"scoring_llm_tractability\": 0.8,\n      \"asset_prerequisites\": [\"CIFAR-10 dataset\"]\n    },\n    {\n      \"name\": \"Implement multi-crop test-time augmentation\",\n      \"description\": \"Implement the proposed multi-crop test-time augmentation method, which evaluates 6 augmented views of each test image and averages the outputs with specific weights.\",\n      \"baseline\": \"Standard single-crop evaluation or horizontal flip TTA\",\n      \"skills\": [\"PyTorch\", \"Computer vision\", \"Test-time augmentation\"],\n      \"relevant_paper_text\": \"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\",\n      \"scoring_feasibility\": 9,\n      \"llm_tractability\": 0.7,\n      \"expert_tractability\": 0.9,\n      \"layman_tractability\": 0.3,\n      \"scoring_llm_tractability\": 0.9,\n      \"asset_prerequisites\": [\"CIFAR-10 dataset\", \"Trained model checkpoint\"]\n    }\n  ]\n}"
  },
  {
    "Request": "Make a very succinct json object that validates with the following schema\n\n---\n\nFollow the following format.\n\nJson Schema: ${json_schema}\nJson Object: ${json_object}\n\n---\n\nJson Schema: {\"$defs\": {\"TaskCandidate\": {\"description\": \"Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\\n\\nA good task candidate:\\n- describes a task that an agent can implement\\n- typically a programming task, e.g. modifying a 'baseline' implementation\\n- which reflects or mirrors actual tasks undertaken to produce the research paper\\n- and requires research and/or engineering expertise to implement correctly\\n- for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\\n- ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\\n- the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\", \"properties\": {\"name\": {\"description\": \"Name of the task\", \"title\": \"Name\", \"type\": \"string\"}, \"description\": {\"description\": \"Description of the task\", \"title\": \"Description\", \"type\": \"string\"}, \"baseline\": {\"description\": \"Short description of baseline implementation, if any, that will be given to implementers\", \"title\": \"Baseline\", \"type\": \"string\"}, \"skills\": {\"description\": \"Comma-separate list of skills required to implement the task\", \"items\": {\"type\": \"string\"}, \"title\": \"Skills\", \"type\": \"array\"}, \"relevant_paper_text\": {\"description\": \"Text from the paper that is relevant to the task\", \"title\": \"Relevant Paper Text\", \"type\": \"string\"}, \"scoring_feasibility\": {\"description\": \"\\nOn a scale of 1-10, how feasible is it to write a python function to objectively score an \\nimplementation of this task, verifying that key functionality is implemented as specified?\\n\", \"title\": \"Scoring Feasibility\", \"type\": \"integer\"}, \"llm_tractability\": {\"description\": \"Probability in (0, 1) that a frontier LLM can generate a correct implementation of this task\", \"title\": \"Llm Tractability\", \"type\": \"number\"}, \"expert_tractability\": {\"description\": \"Probability in (0, 1) that a human subject matter expert can generate a correct implementation of this task\", \"title\": \"Expert Tractability\", \"type\": \"number\"}, \"layman_tractability\": {\"description\": \"Probability in (0, 1) that a layman can generate a correct implementation of this task\", \"title\": \"Layman Tractability\", \"type\": \"number\"}, \"scoring_llm_tractability\": {\"description\": \"Probability in (0, 1) that a frontier LLM can generate a correct scoring function for this task\", \"title\": \"Scoring Llm Tractability\", \"type\": \"number\"}, \"asset_prerequisites\": {\"description\": \"List of assets which should be made available to implementers or which are required to implement scoring\", \"items\": {\"type\": \"string\"}, \"title\": \"Asset Prerequisites\", \"type\": \"array\"}}, \"required\": [\"name\", \"description\", \"baseline\", \"skills\", \"relevant_paper_text\", \"scoring_feasibility\", \"llm_tractability\", \"expert_tractability\", \"layman_tractability\", \"scoring_llm_tractability\", \"asset_prerequisites\"], \"title\": \"TaskCandidate\", \"type\": \"object\"}}, \"properties\": {\"tasks\": {\"description\": \"Tasks extracted from the paper\", \"items\": {\"$ref\": \"#/$defs/TaskCandidate\"}, \"title\": \"Tasks\", \"type\": \"array\"}}, \"required\": [\"tasks\"], \"title\": \"TaskCandidates\", \"type\": \"object\"}\nJson Object:"
  },
  {
    "Response": "Json Object: {\n  \"tasks\": [\n    {\n      \"name\": \"Implement BERT-based Text Classification\",\n      \"description\": \"Develop a BERT-based text classifier for sentiment analysis\",\n      \"baseline\": \"Basic LSTM classifier\",\n      \"skills\": [\"NLP\", \"Deep Learning\", \"PyTorch\"],\n      \"relevant_paper_text\": \"We fine-tuned BERT for sentiment classification.\",\n      \"scoring_feasibility\": 8,\n      \"llm_tractability\": 0.8,\n      \"expert_tractability\": 0.9,\n      \"layman_tractability\": 0.1,\n      \"scoring_llm_tractability\": 0.7,\n      \"asset_prerequisites\": [\"BERT model\", \"Sentiment dataset\"]\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `paper_text`, `guidance`, produce the fields `task_candidates`.\n\n---\n\nFollow the following format.\n\nPaper Text: Full text from research paper PDF\n\nGuidance: Guidance for extracting task candidates\n\nPast Error in Task Candidates: An error to avoid in the future\n\nTask Candidates: JSON list of task candidate dicts with keys 'name', 'description', and 'relevant_paper_text'. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCandidate\": {\"description\": \"Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\\n\\nA good task candidate:\\n- describes a task that an agent can implement\\n- typically a programming task, e.g. modifying a 'baseline' implementation\\n- which reflects or mirrors actual tasks undertaken to produce the research paper\\n- and requires research and/or engineering expertise to implement correctly\\n- for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\\n- ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\\n- the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\", \"properties\": {\"name\": {\"description\": \"Name of the task\", \"title\": \"Name\", \"type\": \"string\"}, \"description\": {\"description\": \"Description of the task\", \"title\": \"Description\", \"type\": \"string\"}, \"baseline\": {\"description\": \"Short description of baseline implementation, if any, that will be given to implementers\", \"title\": \"Baseline\", \"type\": \"string\"}, \"skills\": {\"description\": \"Comma-separate list of skills required to implement the task\", \"items\": {\"type\": \"string\"}, \"title\": \"Skills\", \"type\": \"array\"}, \"relevant_paper_text\": {\"description\": \"Text from the paper that is relevant to the task\", \"title\": \"Relevant Paper Text\", \"type\": \"string\"}, \"scoring_feasibility\": {\"description\": \"\\nOn a scale of 1-10, how feasible is it to write a python function to objectively score an \\nimplementation of this task, verifying that key functionality is implemented as specified?\\n\", \"title\": \"Scoring Feasibility\", \"type\": \"integer\"}, \"llm_tractability\": {\"description\": \"Probability in (0, 1) that a frontier LLM can generate a correct implementation of this task\", \"title\": \"Llm Tractability\", \"type\": \"number\"}, \"expert_tractability\": {\"description\": \"Probability in (0, 1) that a human subject matter expert can generate a correct implementation of this task\", \"title\": \"Expert Tractability\", \"type\": \"number\"}, \"layman_tractability\": {\"description\": \"Probability in (0, 1) that a layman can generate a correct implementation of this task\", \"title\": \"Layman Tractability\", \"type\": \"number\"}, \"scoring_llm_tractability\": {\"description\": \"Probability in (0, 1) that a frontier LLM can generate a correct scoring function for this task\", \"title\": \"Scoring Llm Tractability\", \"type\": \"number\"}, \"asset_prerequisites\": {\"description\": \"List of assets which should be made available to implementers or which are required to implement scoring\", \"items\": {\"type\": \"string\"}, \"title\": \"Asset Prerequisites\", \"type\": \"array\"}}, \"required\": [\"name\", \"description\", \"baseline\", \"skills\", \"relevant_paper_text\", \"scoring_feasibility\", \"llm_tractability\", \"expert_tractability\", \"layman_tractability\", \"scoring_llm_tractability\", \"asset_prerequisites\"], \"title\": \"TaskCandidate\", \"type\": \"object\"}}, \"properties\": {\"tasks\": {\"description\": \"Tasks extracted from the paper\", \"items\": {\"$ref\": \"#/$defs/TaskCandidate\"}, \"title\": \"Tasks\", \"type\": \"array\"}}, \"required\": [\"tasks\"], \"title\": \"TaskCandidates\", \"type\": \"object\"}\n\n---\n\nPaper Text:\n94% on CIFAR-10 in 3.29 Seconds on a Single GPU\nKeller Jordan\nkjordan4077@gmail.com\nAbstract\nCIFAR-10 is among the most widely used datasets in machine learning, facilitating\nthousands of research projects per year. To accelerate research and reduce the cost\nof experiments, we introduce training methods for CIFAR-10 which reach 94%\naccuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when\nrun on a single NVIDIA A100 GPU. As one factor contributing to these training\nspeeds, we propose a derandomized variant of horizontal flipping augmentation,\nwhich we show improves over the standard method in every case where flipping is\nbeneficial over no flipping at all. Our code is released at https://github.com/\nKellerJordan/cifar10-airbench .\n1 Introduction\nCIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-\ning thousands of research projects per year1. Research can be accelerated and the cost of experiments\nreduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In\nthis paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single\nNVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,\n2023). To support scenarios where higher performance is needed, we additionally develop methods\ntargeting 95% and 96% accuracy. We release the following methods in total.\n1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).\n2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).\n3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).\n4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).\nAll runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-\nmatically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the\nfirst uses torch.compile to improve GPU utilization. It is intended for experiments where many\nnetworks are trained at once in order to amortize the one-time compilation cost. The non-compiled\nairbench94 variant can be easily installed and run using the following command.\n1pip install airbench\n2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"\nOne motivation for the development of these training methods is that they can accelerate the experi-\nmental iteration time of researchers working on compatible projects involving CIFAR-10. Another\nmotivation is that they can decrease the cost of projects involving a massive number of trained\nnetworks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used\n3 million trained networks to demonstrate that the outputs of a trained neural network on a given test\ninput follow an approximately linear function of the vector of binary choices of which examples the\nmodel was trained on. Another example is Jordan (2023), a study on training variance which used\n180 thousand trained networks to show that standard trainings have little variance in performance on\n1https://paperswithcode.com/datasets\nPreprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024\nFigure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal\nflipping augmentation, which flips each image with 50% probability per epoch. This results in some images\nbeing redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a\ndeterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\ntheir test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds\nand 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make\nit possible to replicate these studies, or conduct similar ones, with fewer computational resources.\nFast training also enables the rapid accumulation of statistical significance for subtle hyperparameter\ncomparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10\naccuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation\nbetween runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the\nimprovement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training\nthis will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.\nOur work builds on prior training speed projects. We utilize a modified version of the network,\ninitialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen\npatch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over\nprior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-\nure 1, Section 3.6, Section 5.2).\n2 Background\nOur objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set\nin the shortest possible amount of time. Timing begins when the method is first given access to\ntraining data, and ends when it produces test-set predictions. The method is considered valid if its\nmean accuracy over repeated runs is at least 94%.\nWe chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the\n2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more\nrecent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10\nseconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page\n(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is\ntysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,\n94% is the level of human accuracy reported by Karpathy (2011).\nWe note the following consequences of how the method is timed. First, it is permitted for the program\nto begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins\nwhen the training data is first accessed. This is helpful because otherwise the first run of training\nis typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA\nimproves the performance of a trained network by running it on multiple augmented views of each\ntest input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping\nTTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods\nattain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\nThe CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.\nIt is divided into a training set of 50,000 images and a validation set of 10,000 images. As a\nmatter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was\n2\n80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than\nairbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94\ncan be attributed entirely to algorithmic progress rather than compute scaling.\n3 Methods\n3.1 Network architecture and baseline training\nWe train a convolutional network with a total of 1.97 million parameters, following tysam-code\n(2023) with a few small changes. It contains seven convolutions with the latter six being divided into\nthree blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this\nsection we offer some comments on the main design choices.\nThe network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed\nentirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,\n2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with\nno padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather\nthan the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff\nbetween throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\nFollowing Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,\nand disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is\nscaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture\ndiffers only in that we decrease the number of output channels in the third block from 512 to 256,\nand we add learnable biases to the first convolution.\nAs our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of\n0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the\nmaximum at 20% of the way through training, and then decreases to zero. For data augmentation we\nuse random horizontal flipping alongside 2-pixel random translation. For translation we use reflection\npadding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note\nthat what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a\nrandom 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the\nnetwork is run on both a given test image and its mirror and inferences are made based on the average\nof the two outputs. With optimized choices of learning rate, momentum, and weight decay, this\nbaseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.\n3.2 Frozen patch-whitening initialization\nFollowing Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-\nwhitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code\n(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches\nacross the training distribution, so that their outputs have identity covariance matrix. The second 12\nFigure 2: The first layer\u2019s weights after whitening\ninitialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,\nso that input information is preserved through the\nactivation which follows. Figure 2 shows the result.\nWe do not update this layer\u2019s weights during training.\nDeparting from tysam-code (2023), we add learn-\nable biases to this layer, yielding a small performance\nboost. The biases are trained for 3 epochs, after which\nwe disable their gradient to increase backward-pass\nthroughput, which improves training speed without\nreducing accuracy. We also obtain a slight perfor-\nmance boost relative to tysam-code (2023) by re-\nducing the constant added to the eigenvalues during\ncalculation of the patch-whitening initialization for\nthe purpose of preventing numerical issues in the case\nof a singular patch-covariance matrix.\nPatch-whitening initialization is the single most impactful feature. Adding it to the baseline more\nthan doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.\n3\nRandom reshuffling Alternating flip Mean accuracy\nNo No 93.40%\nNo Yes 93.48%\nYes No 93.92%\nYes Yes 94.01%\nTable 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating\nflip (which we propose) reduce training data redundancy and improve performance.\n3.3 Identity initialization\ndirac : We initialize all convolutions after the first as partial identity transforms. That is, for a\nconvolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an\nidentity transform of the input, and leave the remaining N\u2212Mto their default initialization. In\nPyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight\nwof each convolutional layer. This method partially follows tysam-code (2023), which used a more\ncomplicated scheme where the identity weights are mixed in with the original initialization, which\nwe did not find to be more performant. With this feature added, training attains 94% accuracy in 18\nepochs taking 6.8 A100-seconds.\n3.4 Optimization tricks\nscalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor\nof64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%\nin 13.5 epochs taking 5.1 A100-seconds.\nlookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.\nWe note that Lookahead has also been found effective in prior work on training speed for ResNet-\n18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6\nA100-seconds.\n3.5 Multi-crop evaluation\nmulticrop : To generate predictions, we run the trained network on six augmented views of each\ntest image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a\nversion which is translated down-and-to-the-right by one pixel, and the mirrored versions of all\nthree. Predictions are made using a weighted average of all six outputs, where the two views of the\nuntranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125\neach. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.\nWe note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-\nmonyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of\nevaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using\nmore crops does improve performance, but the increase to inference time outweighs the potential\ntraining speedup.\n3.6 Alternating flip\nTo speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,\nwhich we motivate as follows. When training neural networks, it is standard practice to organize\ntraining into a set of epochs during which every training example is seen exactly once. This\ndiffers from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),\nwhich calls for data to be repeatedly sampled with-replacement from the training set, resulting in\nexamples being potentially seen multiple redundant times within a short window of training. The\nuse of randomly ordered epochs of data for training has a different name, being called the random\nreshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our\ntraining dataset consists of Nunique examples, then sampling data with replacement causes every\n\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N\nunique examples on average. On the other hand, random reshuffling leads to all Nunique examples\nbeing seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason\nthat it is beneficial to maximize the number of unique inputs seen per window of training time.\n4\nWe extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We\nfirst note that standard random horizontal flipping augmentation can be defined as follows.\n1import torch\n2def random_flip ( inputs ):\n3 # Applies random flipping to a batch of images\n4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)\n5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]\nListing 1: Random flip\nIf horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2\nwhich may be seen during training. Potentially, every pair of consecutive epochs could contain every\nunique input. But our main observation is that with standard random horizontal flipping, half of the\nimages will be redundantly flipped the same way during both epochs, so that on average only 1.5N\nunique inputs will be seen.\naltflip : To address this, we propose to modify standard random horizontal flipping augmentation as\nfollows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},\nwe flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we\nflip only those inputs which were flipped in the first epoch. We provide the following implementation\nwhich avoids the need for extra memory by using a pseudorandom function to decide the flips.\n1import torch\n2import hashlib\n3def hash_fn (n, seed =42) :\n4 k = n * seed\n5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,\n16)\n6def alternating_flip (inputs , indices , epoch ):\n7 # Applies alternating flipping to a batch of images\n8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist\n() ])\n9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)\n10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\nListing 2: Alternating flip\nThe result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in\nFigure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.\nAdding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final\ntraining method airbench94.py , the entire contents of which can be found in Section E. It reaches\n94% accuracy in 3.83 seconds on an NVIDIA A100.\n3.7 Compilation\nThe final step we take to speed up training is a non-algorithmic one: we compile our training\nmethod using torch.compile in order to more efficiently utilize the GPU. This results in a training\nscript which is mathematically equivalent (up to small differences in floating point arithmetic) to\nthe non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29\nA100-seconds. The downside is that the one-time compilation process takes up to several minutes to\ncomplete before training runs can begin, so that it is only beneficial when we plan to execute many\nruns of training at once. We release this version as airbench94_compiled.py .\n4 95% and 96% targets\nTo address scenarios where somewhat higher performance is desired, we additionally develop methods\ntargeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .\nTo attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel\ncount of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce\nthe learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%\naccuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.\n2Assuming none of the training inputs are already mirrors of each other.\n5\nFigure 4: Training speedups accumulate additively. Removing individual features from airbench94\nincreases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces\nthe epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-\n94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.\nFigure 3: FLOPs vs. error rate tradeoff. Our\nthree training methods apparently follow a linear\nlog-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-\nVries & Taylor, 2017) augmentation and raise the\ntraining epochs to 40. We add a third convolution to\neach block, and scale the first block to 128 channels\nand the second two to 512. We also add a residual\nconnection across the later two convolutions of each\nblock, which we find is still beneficial despite the fact\nthat we are already using identity initialization (Sec-\ntion 3.3) to ease gradient flow. Finally, we reduce\nthe learning rate by a factor of 0.78. These changes\nyield airbench96 which attains 96.05% accuracy in\n46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.\nFigure 3 shows the FLOPs and error rate of each of\nour three training methods.\n5 Experiments\n5.1 Interaction between features\nTo gain a better sense of the impact of each feature on training speed, we compare two quantities.\nFirst, we measure the number of epochs that can be saved by adding the feature to the whitened\nbaseline (Section 3.2). Second, we measure the number of epochs that must be added when the\nfeature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-\ntion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it\nfrom the final airbench94 increases epochs-to-94% from 9.9 to 12.8.\nFigure 4 shows both quantities for each feature. Surprisingly, we find that for all features except\nmulti-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even\nthough the whitened baseline requires more than twice as many epochs as the final configuration.\nThis indicates that the interaction between most features is additive rather than multiplicative.\n5.2 Does alternating flip generalize?\nIn this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of\ntraining configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all\ncases except those where neither alternating nor random flip improve over using no flipping at all.\nFor CIFAR-10 we consider the performance boost given by alternating flip across the follow-\ning 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and\nairbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For\n6\nFigure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,\nthe use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%\ntraining speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.\n95% confidence intervals are shown around each point.\neach configuration we compare the performance of alternating and random flip in terms of their mean\naccuracy across n= 400 runs of training.\nFigure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip\nimproves performance in every setting. To get a sense for how big the improvement is, we estimate\nthe effective speedup for each case, i.e., the fraction of epochs that could be saved by switching\nfrom random to alternating flip while maintaining the level of accuracy of random flip. We begin\nby fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of\neach random flip-based training configuration. We use these curves to calculate the effective speedup\nafforded by switching from random to alternating flip. For example, airbench94 with random flip\nand without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The\nsame configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law\nfit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.\nNote that using a power-law yields a more conservative estimate relative to using linear interpolation\nbetween the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.\nTable 2 shows the result. We observe the following patterns. First, the addition of extra augmentation\n(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that\nthe main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the\nsame way for many epochs in a row; we speculate that adding extra augmentation reduces the negative\nimpact of these cases because it increases data diversity. Next, TTA reduces the gap between random\nand alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),\nindicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training\nfor longer consistently increases the effective speedup given by alternating flip.\nWe next study ImageNet trainings with the following experiment. We train a ResNet-18 with a\nvariety of train and test crops, comparing three flipping options: alternating flip, random flip, and no\nflipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192\ncenter crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)\nto denote the latter. We also consider two training crops: 192x192 inception-style random resized\ncrop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area\nranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the\nshorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write\n7\nBaseline Cutout Epochs Speedup Speedup (w/ TTA)\nairbench94 No 10 15.0% 5.30%\nairbench94 No 20 27.1% 21.3%\nairbench94 No 40 38.3% 36.4%\nairbench94 No 80 102% 31.8%\nairbench94 Yes 10 3.84% 1.13%\nairbench94 Yes 20 7.42% 2.00%\nairbench94 Yes 40 18.6% 9.28%\nairbench94 Yes 80 29.2% 14.25%\nairbench96 Yes 10 4.94% 1.11%\nairbench96 Yes 20 8.99% 3.58%\nairbench96 Yes 40 17.2% 6.48%\nairbench96 Yes 80 18.8% Not measured\nTable 2: Effective speedups given by switching from random flip to alternating flip. The two configurations\nmost closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw\naccuracy values of the airbench94 experiments.\nFlipping augmentation option\nTrain crop Test crop Epochs TTA None Random Alternating\nHeavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28\nHeavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28\nLight RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26\nLight RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26\nHeavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28\nHeavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28\nLight RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26\nLight RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26\nLight RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16\nHeavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4\nTable 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for\nthose trainings where random flip improves significantly over not flipping at all. The single best flipping option\nin each row is bolded when the difference is statistically significant.\nHeavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided\nin Section C.\nTable 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC\nis better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly\nbetter when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test\nresolution discrepancy (Touvron et al., 2019).\nFor trainings which use Light RRC, we find that switching from random flip to alternating flip\nprovides a substantial boost to performance, amounting to a training speedup of more than 25%.\nIn Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to\nalternating flip improves performance by more than increasing the training duration from 16 to 20\nepochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our\nresults on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit\nfrom alternating flip. Indeed, even turning flipping off completely does not significantly reduce the\nperformance of these trainings. We conclude that alternating flip improves over random flip for every\ntraining scenario where the latter improves over no flipping at all.\n8\nEpochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE\n1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312\n2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233\n1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269\n1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533\n2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433\n1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444\nTable 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).\n5.3 Variance and class-wise calibration\nPrevious sections have focused on understanding what factors affect the first moment of accuracy\n(the mean). In this section we investigate the second moment, finding that TTA reduces variance at\nthe cost of calibration.\nOur experiment is to execute 10,000 runs of airbench94 training with several hyperparameter\nsettings. For each setting we report both the variance in test-set accuracy as well as an estimate of the\ndistribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.\nTable 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set\nvariance, replicating the main finding of Jordan (2023). This is a surprising result because these\ntrainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7\nas much distribution-wise variance when run for a similar duration, and reached a low variance only\nwhen run for 64 epochs. We conclude from this comparison that distribution-wise variance is more\nstrongly connected to the rate of convergence of a training rather than its duration as such. We also\nnote that the low distribution-wise variance of airbench94 indicates it has high training stability.\nUsing TTA significantly reduces the test-set variance, such that all three settings with TTA have lower\ntest-set variance than any setting without TTA. However, test-set variance is implied by the class-wise\ncalibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this\nreduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,\nwe compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which\nmeasures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA\nhas a higher CACE than every setting without TTA, confirming the hypothesis.\n6 Discussion\nIn this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7\nfaster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the\nairbench Python package.\nWe developed airbench solely with the goal of maximizing training speed on CIFAR-10. In\nSection B we find that it also generalizes well to other tasks. For example, without any extra tuning,\nairbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.\nOne factor contributing to the training speed of airbench was our finding that training can be accel-\nerated by partially derandomizing the standard random horizontal flipping augmentation, resulting in\nthe variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating\nflip improves the performance of every training we considered (Section 5.2), with the exception of\nthose trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to\nus, the standard ImageNet trainings that we considered do not significantly benefit from horizontal\nflipping. Future work might investigate whether it is possible to obtain derandomized improvements\nto other augmentations besides horizontal flip.\nThe methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,\nwith fixed performance and hardware constraints. These constraints mean that we cannot improve\nperformance by simply scaling up the amount of computational resources used; instead we are forced\nto develop new methods like the alternating flip. We look forward to seeing what other new methods\nfuture work discovers to push training speed further.\n9\nReferences\nDimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.\nDan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-\nperformance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,\n2011.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\nmark and competition. 2017.\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet\nor cifar-10. arXiv preprint arXiv:1810.03505 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\npp. 248\u2013255. IEEE, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552 , 2017.\nMert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic\ngradient descent. Mathematical Programming , 186:49\u201384, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 , 2016.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.\npmlr, 2015.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\nsgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.\nKeller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and\ninevitable. arXiv preprint arXiv:2304.01910 , 2023.\nAndrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.\nio/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for\nadvanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT\nLicense.\nGuillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander\nM \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,\nBenjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.\nBenchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural\nInformation Processing Systems , 35:25404\u201325421, 2022.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.\nDavid Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/\nhow-to-train-your-resnet-8-bag-of-tricks/ .\n10\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics , 22(3):400\u2013407, 1951.\nSantiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:\n//github.com/apple/ml-cifar-10-faster , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.\ntysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/\nhlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. Advances in neural information processing systems , 32, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,\n2016.\nMichael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps\nforward, 1 step back. Advances in neural information processing systems , 32, 2019.\n11\nA Network architecture\n1from torch import nn\n2\n3class Flatten (nn. Module ):\n4 def forward (self , x):\n5 return x. view (x. size (0) , -1)\n6\n7class Mul (nn. Module ):\n8 def __init__ (self , scale ):\n9 super (). __init__ ()\n10 self . scale = scale\n11 def forward (self , x):\n12 return x * self . scale\n13\n14def conv (ch_in , ch_out ):\n15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,\n16 padding =\u2019same \u2019, bias = False )\n17\n18def make_net ():\n19 act = lambda : nn. GELU ()\n20 bn = lambda ch: nn. BatchNorm2d (ch)\n21 return nn. Sequential (\n22 nn. Sequential (\n23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),\n24 act () ,\n25 ),\n26 nn. Sequential (\n27 conv (24 , 64) ,\n28 nn. MaxPool2d (2) ,\n29 bn (64) , act () ,\n30 conv (64 , 64) ,\n31 bn (64) , act () ,\n32 ),\n33 nn. Sequential (\n34 conv (64 , 256) ,\n35 nn. MaxPool2d (2) ,\n36 bn (256) , act () ,\n37 conv (256 , 256) ,\n38 bn (256) , act () ,\n39 ),\n40 nn. Sequential (\n41 conv (256 , 256) ,\n42 nn. MaxPool2d (2) ,\n43 bn (256) , act () ,\n44 conv (256 , 256) ,\n45 bn (256) , act () ,\n46 ),\n47 nn. MaxPool2d (3) ,\n48 Flatten () ,\n49 nn. Linear (256 , 10, bias = False ),\n50 Mul (1/9) ,\n51 )\nListing 3: Minimal PyTorch code for the network architecture used by airbench94 .\nWe note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock\ntime. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel\nsize of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this\ndoes not improve the wallclock training time on an A100. Reducing the batch size is another easy\nway to save FLOPs but not wallclock time.\n12\nDataset Flipping? Cutout? ResNet-18 airbench96\nCIFAR-10 Yes No 95.55% 95.61%\nCIFAR-10 Yes Yes 96.01% 96.05%\nCIFAR-100 Yes No 77.54% 79.27%\nCIFAR-100 Yes Yes 78.04% 79.76%\nCINIC-10 Yes No 87.58% 87.78%\nCINIC-10 Yes Yes not measured 88.22%\nSVHN No No 97.35% 97.38%\nSVHN No Yes not measured 97.64%\nTable 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We\ndirectly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off\nflipping for SVHN).\nB Extra dataset experiments\nWe developed airbench with the singular goal of maximizing training speed on CIFAR-10. To\nfind out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its\nperformance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-\n10 (Darlow et al., 2018).\nOn CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both\nthe case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not\n(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than\nResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say\nthat it generalizes.\nWe compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.\nWe do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal\non CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better\nperformance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where\nairbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is\nused and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows\nstrong generalization to other tasks.\nWe note that this comparison between airbench96 and ResNet-18 training is fair in the sense that\nit does demonstrate that the former has good generalization, but unfair in the sense that it does not\nindicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time\naugmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training\nwould outperform airbench96 if it were run using test-time augmentation. However, it also takes\n5-10 times longer to complete. The decision of which to use may be situational.\nThe accuracy values we report for ResNet-18 training are from the following sources. We tried\nto select the highest values we could find for each setting. Moreau et al. (2022) reports attaining\n95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports\nattaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on\nCIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.\nC ImageNet training details\nOur ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/\nlibffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and\nmomentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution\n160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of\ntraining. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.\n13\nHyperparameters Flipping augmentation option\nEpochs Cutout TTA None Random Alternating\n10 No No 92.3053 93.0988 93.2798\n20 No No 92.8166 93.7446 93.8652\n40 No No 93.0143 94.0133 94.0729\n80 No No 93.0612 94.1169 94.1628\n10 No Yes 93.4071 93.9488 94.0186\n20 No Yes 93.8528 94.5565 94.6530\n40 No Yes 94.0381 94.7803 94.8203\n80 No Yes 94.0638 94.8506 94.8676\n10 Yes No 91.8487 92.0402 92.1374\n20 Yes No 92.8474 93.3825 93.4876\n40 Yes No 93.2675 94.1014 94.1952\n80 Yes No 93.4193 94.4311 94.5204\n10 Yes Yes 92.6455 92.7780 92.8103\n20 Yes Yes 93.7862 94.1306 94.1670\n40 Yes Yes 94.3090 94.8511 94.8960\n80 Yes Yes 94.5253 95.1839 95.2362\nTable 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a\nmean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise\ndifference in means is statistically significant.\nD Extra tables & figures\nFigure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in\nSection 5.3.\nE Complete training code\n1\"\"\"\n2airbench94 .py\n33.83 s runtime on an A100 ; 0.36 PFLOPs .\n4Evidence for validity : 94.01 average accuracy in n =1000 runs .\n5\n6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB\nwith the following nvidia -smi:\n7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :\n11.7\n14\n8torch . __version__ == \u20192.1.2+ cu118 \u2019\n9\"\"\"\n10\n11# ############################################\n12# Setup / Hyperparameters #\n13# ############################################\n14\n15import os\n16import sys\n17import uuid\n18from math import ceil\n19\n20import torch\n21from torch import nn\n22import torch .nn. functional as F\n23import torchvision\n24import torchvision . transforms as T\n25\n26torch . backends . cudnn . benchmark = True\n27\n28\"\"\"\n29We express the main training hyperparameters ( batch size , learning\nrate , momentum , and weight decay ) in decoupled form , so that each\none can be tuned independently . This accomplishes the following :\n30* Assuming time - constant gradients , the average step size is decoupled\nfrom everything but the lr.\n31* The size of the weight decay update is decoupled from everything but\nthe wd.\n32In constrast , normally when we increase the ( Nesterov ) momentum , this\nalso scales up the step size proportionally to 1 + 1 / (1 -\nmomentum ), meaning we cannot change momentum without having to re -\ntune the learning rate . Similarly , normally when we increase the\nlearning rate this also increases the size of the weight decay ,\nrequiring a proportional decrease in the wd to maintain the same\ndecay strength .\n33\n34The practical impact is that hyperparameter tuning is faster , since\nthis parametrization allows each one to be tuned independently .\nSee https :// myrtle .ai/ learn /how -to -train -your - resnet -5-\nhyperparameters /.\n35\"\"\"\n36\n37hyp = {\n38 \u2019opt \u2019: {\n39 \u2019 train_epochs \u2019: 9.9 ,\n40 \u2019batch_size \u2019: 1024 ,\n41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples\n42 \u2019momentum \u2019: 0.85 ,\n43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (\ndecoupled from learning rate )\n44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not\nweight decay ) for BatchNorm biases\n45 \u2019 label_smoothing \u2019: 0.2 ,\n46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the\nwhitening layer bias before freezing\n47 },\n48 \u2019aug \u2019: {\n49 \u2019flip \u2019: True ,\n50 \u2019translate \u2019: 2,\n51 },\n52 \u2019net \u2019: {\n53 \u2019widths \u2019: {\n54 \u2019block1 \u2019: 64,\n55 \u2019block2 \u2019: 256 ,\n56 \u2019block3 \u2019: 256 ,\n15\n57 },\n58 \u2019 batchnorm_momentum \u2019: 0.6 ,\n59 \u2019 scaling_factor \u2019: 1/9 ,\n60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :\n0= none , 1= mirror , 2= mirror + translate\n61 },\n62}\n63\n64# ############################################\n65# DataLoader #\n66# ############################################\n67\n68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )\n69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )\n70\n71def batch_flip_lr ( inputs ):\n72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .\nview (-1, 1, 1, 1)\n73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\n74\n75def batch_crop (images , crop_size ):\n76 r = ( images . size ( -1) - crop_size )//2\n77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =\nimages . device )\n78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),\ndevice = images .device , dtype = images . dtype )\n79 # The two cropping methods in this if - else produce equivalent\nresults , but the second is faster for r > 2.\n80 if r <= 2:\n81 for sy in range (-r, r +1):\n82 for sx in range (-r, r +1):\n83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)\n84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size\n, r+sx:r+sx+ crop_size ]\n85 else :\n86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size\n+2* r), device = images .device , dtype = images . dtype )\n87 for s in range (-r, r +1):\n88 mask = ( shifts [:, 0] == s)\n89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]\n90 for s in range (-r, r +1):\n91 mask = ( shifts [:, 1] == s)\n92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+\ncrop_size ]\n93 return images_out\n94\n95class CifarLoader :\n96 \"\"\"\n97 GPU - accelerated dataloader for CIFAR -10 which implements\nalternating flip augmentation .\n98 \"\"\"\n99\n100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,\ndrop_last =None , shuffle =None , gpu =0):\n101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .\npt \u2019)\n102 if not os. path . exists ( data_path ):\n103 dset = torchvision . datasets . CIFAR10 (path , download =True ,\ntrain = train )\n104 images = torch . tensor ( dset . data )\n105 labels = torch . tensor ( dset . targets )\n106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:\ndset . classes }, data_path )\n107\n108 data = torch . load ( data_path , map_location = torch . device (gpu))\n16\n109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [\n\u2019labels \u2019], data [\u2019classes \u2019]\n110 # It \u2019s faster to load + process uint8 data than to load\npreprocessed fp16 data\n111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).\nto( memory_format = torch . channels_last )\n112\n113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )\n114 self . proc_images = {} # Saved results of image processing to\nbe done on the first epoch\n115 self . epoch = 0\n116\n117 self . aug = aug or {}\n118 for k in self .aug . keys ():\n119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019\n% k\n120\n121 self . batch_size = batch_size\n122 self . drop_last = train if drop_last is None else drop_last\n123 self . shuffle = train if shuffle is None else shuffle\n124\n125 def __len__ ( self ):\n126 return len( self . images )// self . batch_size if self . drop_last\nelse ceil (len( self . images )/ self . batch_size )\n127\n128 def __iter__ ( self ):\n129\n130 if self . epoch == 0:\n131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .\nimages )\n132 # Randomly flip all images on the first epoch as according\nto definition of alternating flip\n133 if self .aug.get(\u2019flip \u2019, False ):\n134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (\nimages )\n135 # Pre -pad images to save time when doing random\ntranslation\n136 pad = self .aug.get (\u2019translate \u2019, 0)\n137 if pad > 0:\n138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019\nreflect \u2019)\n139\n140 if self .aug.get(\u2019translate \u2019, 0) > 0:\n141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .\nshape [ -2])\n142 elif self .aug.get (\u2019flip \u2019, False ):\n143 images = self . proc_images [\u2019flip \u2019]\n144 else :\n145 images = self . proc_images [\u2019norm \u2019]\n146 if self .aug.get(\u2019flip \u2019, False ):\n147 if self . epoch % 2 == 1:\n148 images = images . flip ( -1)\n149\n150 self . epoch += 1\n151\n152 indices = ( torch . randperm if self . shuffle else torch . arange )(\nlen ( images ), device = images . device )\n153 for i in range (len ( self )):\n154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]\n155 yield ( images [ idxs ], self . labels [ idxs ])\n156\n157# ############################################\n158# Network Components #\n159# ############################################\n160\n17\n161class Flatten (nn. Module ):\n162 def forward (self , x):\n163 return x. view (x. size (0) , -1)\n164\n165class Mul (nn. Module ):\n166 def __init__ (self , scale ):\n167 super (). __init__ ()\n168 self . scale = scale\n169 def forward (self , x):\n170 return x * self . scale\n171\n172class BatchNorm (nn. BatchNorm2d ):\n173 def __init__ (self , num_features , momentum , eps =1e -12 ,\n174 weight =False , bias = True ):\n175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )\n176 self . weight . requires_grad = weight\n177 self . bias . requires_grad = bias\n178 # Note that PyTorch already initializes the weights to one and\nbiases to zero\n179\n180class Conv (nn. Conv2d ):\n181 def __init__ (self , in_channels , out_channels , kernel_size =3,\npadding =\u2019same \u2019, bias = False ):\n182 super (). __init__ ( in_channels , out_channels , kernel_size =\nkernel_size , padding = padding , bias = bias )\n183\n184 def reset_parameters ( self ):\n185 super (). reset_parameters ()\n186 if self . bias is not None :\n187 self . bias . data . zero_ ()\n188 w = self . weight . data\n189 torch .nn. init . dirac_ (w[:w. size (1) ])\n190\n191class ConvGroup (nn. Module ):\n192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):\n193 super (). __init__ ()\n194 self . conv1 = Conv ( channels_in , channels_out )\n195 self . pool = nn. MaxPool2d (2)\n196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )\n197 self . conv2 = Conv ( channels_out , channels_out )\n198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )\n199 self . activ = nn. GELU ()\n200\n201 def forward (self , x):\n202 x = self . conv1 (x)\n203 x = self . pool (x)\n204 x = self . norm1 (x)\n205 x = self . activ (x)\n206 x = self . conv2 (x)\n207 x = self . norm2 (x)\n208 x = self . activ (x)\n209 return x\n210\n211# ############################################\n212# Network Definition #\n213# ############################################\n214\n215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019\n][ \u2019 batchnorm_momentum \u2019]):\n216 whiten_kernel_size = 2\n217 whiten_width = 2 * 3 * whiten_kernel_size **2\n218 net = nn. Sequential (\n219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True\n),\n220 nn. GELU () ,\n18\n221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],\nbatchnorm_momentum ),\n222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],\nbatchnorm_momentum ),\n223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],\nbatchnorm_momentum ),\n224 nn. MaxPool2d (3) ,\n225 Flatten () ,\n226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),\n227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,\n228 )\n229 net [0]. weight . requires_grad = False\n230 net = net. half (). cuda ()\n231 net = net.to( memory_format = torch . channels_last )\n232 for mod in net. modules ():\n233 if isinstance (mod , BatchNorm ):\n234 mod . float ()\n235 return net\n236\n237# ############################################\n238# Whitening Conv Initialization #\n239# ############################################\n240\n241def get_patches (x, patch_shape ):\n242 c, (h, w) = x. shape [1] , patch_shape\n243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h\n,w). float ()\n244\n245def get_whitening_parameters ( patches ):\n246 n,c,h,w = patches . shape\n247 patches_flat = patches . view (n, -1)\n248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n\n249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance\n, UPLO =\u2019U\u2019)\n250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.\nreshape (c*h*w,c,h,w). flip (0)\n251\n252def init_whitening_conv (layer , train_set , eps =5e -4):\n253 patches = get_patches ( train_set , patch_shape = layer . weight . data .\nshape [2:])\n254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )\n255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)\n256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -\neigenvectors_scaled ))\n257\n258# ###########################################\n259# Lookahead #\n260# ###########################################\n261\n262class LookaheadState :\n263 def __init__ (self , net):\n264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().\nitems ()}\n265\n266 def update (self , net , decay ):\n267 for ema_param , net_param in zip( self . net_ema . values () , net.\nstate_dict (). values ()):\n268 if net_param . dtype in ( torch .half , torch . float ):\n269 ema_param . lerp_ ( net_param , 1- decay )\n270 net_param . copy_ ( ema_param )\n271\n272# ###########################################\n273# Logging #\n274# ###########################################\n275\n19\n276def print_columns ( columns_list , is_head =False , is_final_entry = False ):\n277 print_string = \u2019\u2019\n278 for col in columns_list :\n279 print_string += \u2019| %s \u2019 % col\n280 print_string += \u2019|\u2019\n281 if is_head :\n282 print (\u2019-\u2019*len( print_string ))\n283 print ( print_string )\n284 if is_head or is_final_entry :\n285 print (\u2019-\u2019*len( print_string ))\n286\n287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,\n\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]\n288def print_training_details ( variables , is_final_entry ):\n289 formatted = []\n290 for col in logging_columns_list :\n291 var = variables .get (col. strip () , None )\n292 if type (var) in (int , str):\n293 res = str(var)\n294 elif type (var) is float :\n295 res = \u2019{:0.4 f}\u2019. format ( var)\n296 else :\n297 assert var is None\n298 res = \u2019\u2019\n299 formatted . append (res. rjust (len (col)))\n300 print_columns ( formatted , is_final_entry = is_final_entry )\n301\n302# ###########################################\n303# Evaluation #\n304# ###########################################\n305\n306def infer (model , loader , tta_level =0) :\n307 \"\"\"\n308 Test - time augmentation strategy (for tta_level =2):\n309 1. Flip / mirror the image left -to - right (50% of the time ).\n310 2. Translate the image by one pixel either up -and - left or down -and\n- right (50% of the time , i.e. both happen 25% of the time ).\n311\n312 This creates 6 views per image ( left / right times the two\ntranslations and no - translation ), which we evaluate and then\nweight according to the given probabilities .\n313 \"\"\"\n314\n315 def infer_basic (inputs , net):\n316 return net( inputs ). clone ()\n317\n318 def infer_mirror (inputs , net):\n319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))\n320\n321 def infer_mirror_translate ( inputs , net ):\n322 logits = infer_mirror (inputs , net )\n323 pad = 1\n324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)\n325 inputs_translate_list = [\n326 padded_inputs [:, :, 0:32 , 0:32] ,\n327 padded_inputs [:, :, 2:34 , 2:34] ,\n328 ]\n329 logits_translate_list = [ infer_mirror ( inputs_translate , net)\n330 for inputs_translate in\ninputs_translate_list ]\n331 logits_translate = torch . stack ( logits_translate_list ). mean (0)\n332 return 0.5 * logits + 0.5 * logits_translate\n333\n334 model . eval ()\n335 test_images = loader . normalize ( loader . images )\n20\n336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][\ntta_level ]\n337 with torch . no_grad ():\n338 return torch .cat ([ infer_fn (inputs , model ) for inputs in\ntest_images . split (2000) ])\n339\n340def evaluate (model , loader , tta_level =0) :\n341 logits = infer (model , loader , tta_level )\n342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()\n343\n344# ###########################################\n345# Training #\n346# ###########################################\n347\n348def main (run):\n349\n350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]\n351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]\n352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]\n353 # Assuming gradients are constant in time , for Nesterov momentum ,\nthe below ratio is how much larger the default steps will be than\nthe underlying per - example gradients . We divide the learning rate\nby this ratio in order to ensure steps are the same scale as\ngradients , regardless of the choice of momentum .\n354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))\n355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning\nrate for PyTorch SGD\n356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale\n357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]\n358\n359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019\nlabel_smoothing \u2019], reduction =\u2019none \u2019)\n360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)\n361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =\nbatch_size , aug=hyp[\u2019aug \u2019])\n362 if run == \u2019warmup \u2019:\n363 # The only purpose of the first run is to warmup , so we can\nuse dummy data\n364 train_loader . labels = torch . randint (0, 10, size =( len (\ntrain_loader . labels ) ,), device = train_loader . labels . device )\n365 total_train_steps = ceil (len( train_loader ) * epochs )\n366\n367 model = make_net ()\n368 current_steps = 0\n369\n370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in\nk and p. requires_grad ]\n371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019\nnot in k and p. requires_grad ]\n372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,\nweight_decay =wd/ lr_biases ),\n373 dict ( params = other_params , lr=lr , weight_decay =wd/\nlr)]\n374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,\nnesterov = True )\n375\n376 def triangle (steps , start =0, end =0, peak =0.5) :\n377 xp = torch . tensor ([0 , int( peak * steps ), steps ])\n378 fp = torch . tensor ([ start , 1, end ])\n379 x = torch . arange (1+ steps )\n380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])\n381 b = fp [: -1] - (m * xp [: -1])\n382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1\n383 indices = torch . clamp ( indices , 0, len(m) - 1)\n384 return m[ indices ] * x + b[ indices ]\n21\n385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,\npeak =0.23)\n386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:\nlr_schedule [i])\n387\n388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /\ntotal_train_steps )**3\n389 lookahead_state = LookaheadState ( model )\n390\n391 # For accurately timing GPU code\n392 starter = torch . cuda . Event ( enable_timing = True )\n393 ender = torch . cuda . Event ( enable_timing = True )\n394 total_time_seconds = 0.0\n395\n396 # Initialize the first layer using statistics of training images\n397 starter . record ()\n398 train_images = train_loader . normalize ( train_loader . images [:5000])\n399 init_whitening_conv ( model [0] , train_images )\n400 ender . record ()\n401 torch . cuda . synchronize ()\n402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n403\n404 for epoch in range ( ceil ( epochs )):\n405\n406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019\nwhiten_bias_epochs \u2019])\n407\n408 # ###################\n409 # Training #\n410 # ###################\n411\n412 starter . record ()\n413\n414 model . train ()\n415 for inputs , labels in train_loader :\n416\n417 outputs = model ( inputs )\n418 loss = loss_fn ( outputs , labels ).sum ()\n419 optimizer . zero_grad ( set_to_none = True )\n420 loss . backward ()\n421 optimizer . step ()\n422 scheduler . step ()\n423\n424 current_steps += 1\n425\n426 if current_steps % 5 == 0:\n427 lookahead_state . update (model , decay = alpha_schedule [\ncurrent_steps ]. item ())\n428\n429 if current_steps >= total_train_steps :\n430 if lookahead_state is not None :\n431 lookahead_state . update (model , decay =1.0)\n432 break\n433\n434 ender . record ()\n435 torch . cuda . synchronize ()\n436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n437\n438 # ###################\n439 # Evaluation #\n440 # ###################\n441\n442 # Print the accuracy and loss from the last training batch of\nthe epoch\n22\n443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().\nmean (). item ()\n444 train_loss = loss . item () / batch_size\n445 val_acc = evaluate (model , test_loader , tta_level =0)\n446 print_training_details ( locals () , is_final_entry = False )\n447 run = None # Only print the run number once\n448\n449 # ###################\n450 # TTA Evaluation #\n451 # ###################\n452\n453 starter . record ()\n454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019\ntta_level \u2019])\n455 ender . record ()\n456 torch . cuda . synchronize ()\n457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n458\n459 epoch = \u2019eval \u2019\n460 print_training_details ( locals () , is_final_entry = True )\n461\n462 return tta_val_acc\n463\n464if __name__ == \" __main__ \":\n465 with open (sys. argv [0]) as f:\n466 code = f. read ()\n467\n468 print_columns ( logging_columns_list , is_head = True )\n469 main (\u2019warmup \u2019)\n470 accs = torch . tensor ([ main (run) for run in range (25) ])\n471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))\n472\n473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }\n474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))\n475 os. makedirs ( log_dir , exist_ok = True )\n476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)\n477 print (os. path . abspath ( log_path ))\n478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))\nListing 4: airbench94.py\n23\n\nGuidance: \n\nPast Error in Task Candidates: ValueError('json output should start and end with { and }')\n\nTask Candidates:"
  },
  {
    "Response": "{\n  \"tasks\": [\n    {\n      \"name\": \"Implement alternating flip data augmentation\",\n      \"description\": \"Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in a deterministic alternating pattern across epochs, rather than randomly.\",\n      \"baseline\": \"Standard random horizontal flipping augmentation\",\n      \"skills\": [\"PyTorch\", \"Computer Vision\", \"Data Augmentation\"],\n      \"relevant_paper_text\": \"We propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\",\n      \"scoring_feasibility\": 8,\n      \"llm_tractability\": 0.7,\n      \"expert_tractability\": 0.9,\n      \"layman_tractability\": 0.3,\n      \"scoring_llm_tractability\": 0.8,\n      \"asset_prerequisites\": [\"CIFAR-10 dataset\"]\n    },\n    {\n      \"name\": \"Implement patch-whitening initialization\",\n      \"description\": \"Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper.\",\n      \"baseline\": \"Standard random initialization\",\n      \"skills\": [\"PyTorch\", \"Linear Algebra\", \"Neural Network Initialization\"],\n      \"relevant_paper_text\": \"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\",\n      \"scoring_feasibility\": 7,\n      \"llm_tractability\": 0.5,\n      \"expert_tractability\": 0.8,\n      \"layman_tractability\": 0.1,\n      \"scoring_llm_tractability\": 0.6,\n      \"asset_prerequisites\": [\"CIFAR-10 dataset\"]\n    },\n    {\n      \"name\": \"Implement multi-crop test-time augmentation\",\n      \"description\": \"Implement the multi-crop test-time augmentation technique described in the paper, which involves evaluating the model on multiple augmented versions of each test image.\",\n      \"baseline\": \"No test-time augmentation\",\n      \"skills\": [\"PyTorch\", \"Computer Vision\", \"Test-Time Augmentation\"],\n      \"relevant_paper_text\": \"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\",\n      \"scoring_feasibility\": 8,\n      \"llm_tractability\": 0.8,\n      \"expert_tractability\": 0.9,\n      \"layman_tractability\": 0.4,\n      \"scoring_llm_tractability\": 0.9,\n      \"asset_prerequisites\": [\"CIFAR-10 dataset\", \"Trained model\"]\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `candidate_input`, `candidate_output`, `requirements`, `attempt_num`, `previous_critiques`, produce the fields `self_critique`, `should_retry`.\n\n---\n\nFollow the following format.\n\nCandidate Input: ${candidate_input}\n\nCandidate Output: ${candidate_output}\n\nRequirements: ${requirements}\n\nAttempt Num: ${attempt_num}\n\nPrevious Critiques: ${previous_critiques}\n\nSelf Critique: Self-critique of how well the candidate output satisfies the requirements given the input\n\nShould Retry: Should we attempt to generate the output again taking this critique into account? True/False (Respond with true or false)\n\n---\n\nCandidate Input:\n94% on CIFAR-10 in 3.29 Seconds on a Single GPU\nKeller Jordan\nkjordan4077@gmail.com\nAbstract\nCIFAR-10 is among the most widely used datasets in machine learning, facilitating\nthousands of research projects per year. To accelerate research and reduce the cost\nof experiments, we introduce training methods for CIFAR-10 which reach 94%\naccuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when\nrun on a single NVIDIA A100 GPU. As one factor contributing to these training\nspeeds, we propose a derandomized variant of horizontal flipping augmentation,\nwhich we show improves over the standard method in every case where flipping is\nbeneficial over no flipping at all. Our code is released at https://github.com/\nKellerJordan/cifar10-airbench .\n1 Introduction\nCIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-\ning thousands of research projects per year1. Research can be accelerated and the cost of experiments\nreduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In\nthis paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single\nNVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,\n2023). To support scenarios where higher performance is needed, we additionally develop methods\ntargeting 95% and 96% accuracy. We release the following methods in total.\n1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).\n2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).\n3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).\n4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).\nAll runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-\nmatically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the\nfirst uses torch.compile to improve GPU utilization. It is intended for experiments where many\nnetworks are trained at once in order to amortize the one-time compilation cost. The non-compiled\nairbench94 variant can be easily installed and run using the following command.\n1pip install airbench\n2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"\nOne motivation for the development of these training methods is that they can accelerate the experi-\nmental iteration time of researchers working on compatible projects involving CIFAR-10. Another\nmotivation is that they can decrease the cost of projects involving a massive number of trained\nnetworks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used\n3 million trained networks to demonstrate that the outputs of a trained neural network on a given test\ninput follow an approximately linear function of the vector of binary choices of which examples the\nmodel was trained on. Another example is Jordan (2023), a study on training variance which used\n180 thousand trained networks to show that standard trainings have little variance in performance on\n1https://paperswithcode.com/datasets\nPreprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024\nFigure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal\nflipping augmentation, which flips each image with 50% probability per epoch. This results in some images\nbeing redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a\ndeterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\ntheir test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds\nand 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make\nit possible to replicate these studies, or conduct similar ones, with fewer computational resources.\nFast training also enables the rapid accumulation of statistical significance for subtle hyperparameter\ncomparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10\naccuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation\nbetween runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the\nimprovement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training\nthis will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.\nOur work builds on prior training speed projects. We utilize a modified version of the network,\ninitialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen\npatch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over\nprior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-\nure 1, Section 3.6, Section 5.2).\n2 Background\nOur objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set\nin the shortest possible amount of time. Timing begins when the method is first given access to\ntraining data, and ends when it produces test-set predictions. The method is considered valid if its\nmean accuracy over repeated runs is at least 94%.\nWe chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the\n2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more\nrecent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10\nseconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page\n(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is\ntysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,\n94% is the level of human accuracy reported by Karpathy (2011).\nWe note the following consequences of how the method is timed. First, it is permitted for the program\nto begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins\nwhen the training data is first accessed. This is helpful because otherwise the first run of training\nis typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA\nimproves the performance of a trained network by running it on multiple augmented views of each\ntest input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping\nTTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods\nattain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\nThe CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.\nIt is divided into a training set of 50,000 images and a validation set of 10,000 images. As a\nmatter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was\n2\n80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than\nairbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94\ncan be attributed entirely to algorithmic progress rather than compute scaling.\n3 Methods\n3.1 Network architecture and baseline training\nWe train a convolutional network with a total of 1.97 million parameters, following tysam-code\n(2023) with a few small changes. It contains seven convolutions with the latter six being divided into\nthree blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this\nsection we offer some comments on the main design choices.\nThe network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed\nentirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,\n2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with\nno padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather\nthan the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff\nbetween throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\nFollowing Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,\nand disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is\nscaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture\ndiffers only in that we decrease the number of output channels in the third block from 512 to 256,\nand we add learnable biases to the first convolution.\nAs our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of\n0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the\nmaximum at 20% of the way through training, and then decreases to zero. For data augmentation we\nuse random horizontal flipping alongside 2-pixel random translation. For translation we use reflection\npadding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note\nthat what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a\nrandom 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the\nnetwork is run on both a given test image and its mirror and inferences are made based on the average\nof the two outputs. With optimized choices of learning rate, momentum, and weight decay, this\nbaseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.\n3.2 Frozen patch-whitening initialization\nFollowing Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-\nwhitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code\n(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches\nacross the training distribution, so that their outputs have identity covariance matrix. The second 12\nFigure 2: The first layer\u2019s weights after whitening\ninitialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,\nso that input information is preserved through the\nactivation which follows. Figure 2 shows the result.\nWe do not update this layer\u2019s weights during training.\nDeparting from tysam-code (2023), we add learn-\nable biases to this layer, yielding a small performance\nboost. The biases are trained for 3 epochs, after which\nwe disable their gradient to increase backward-pass\nthroughput, which improves training speed without\nreducing accuracy. We also obtain a slight perfor-\nmance boost relative to tysam-code (2023) by re-\nducing the constant added to the eigenvalues during\ncalculation of the patch-whitening initialization for\nthe purpose of preventing numerical issues in the case\nof a singular patch-covariance matrix.\nPatch-whitening initialization is the single most impactful feature. Adding it to the baseline more\nthan doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.\n3\nRandom reshuffling Alternating flip Mean accuracy\nNo No 93.40%\nNo Yes 93.48%\nYes No 93.92%\nYes Yes 94.01%\nTable 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating\nflip (which we propose) reduce training data redundancy and improve performance.\n3.3 Identity initialization\ndirac : We initialize all convolutions after the first as partial identity transforms. That is, for a\nconvolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an\nidentity transform of the input, and leave the remaining N\u2212Mto their default initialization. In\nPyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight\nwof each convolutional layer. This method partially follows tysam-code (2023), which used a more\ncomplicated scheme where the identity weights are mixed in with the original initialization, which\nwe did not find to be more performant. With this feature added, training attains 94% accuracy in 18\nepochs taking 6.8 A100-seconds.\n3.4 Optimization tricks\nscalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor\nof64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%\nin 13.5 epochs taking 5.1 A100-seconds.\nlookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.\nWe note that Lookahead has also been found effective in prior work on training speed for ResNet-\n18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6\nA100-seconds.\n3.5 Multi-crop evaluation\nmulticrop : To generate predictions, we run the trained network on six augmented views of each\ntest image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a\nversion which is translated down-and-to-the-right by one pixel, and the mirrored versions of all\nthree. Predictions are made using a weighted average of all six outputs, where the two views of the\nuntranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125\neach. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.\nWe note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-\nmonyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of\nevaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using\nmore crops does improve performance, but the increase to inference time outweighs the potential\ntraining speedup.\n3.6 Alternating flip\nTo speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,\nwhich we motivate as follows. When training neural networks, it is standard practice to organize\ntraining into a set of epochs during which every training example is seen exactly once. This\ndiffers from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),\nwhich calls for data to be repeatedly sampled with-replacement from the training set, resulting in\nexamples being potentially seen multiple redundant times within a short window of training. The\nuse of randomly ordered epochs of data for training has a different name, being called the random\nreshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our\ntraining dataset consists of Nunique examples, then sampling data with replacement causes every\n\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N\nunique examples on average. On the other hand, random reshuffling leads to all Nunique examples\nbeing seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason\nthat it is beneficial to maximize the number of unique inputs seen per window of training time.\n4\nWe extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We\nfirst note that standard random horizontal flipping augmentation can be defined as follows.\n1import torch\n2def random_flip ( inputs ):\n3 # Applies random flipping to a batch of images\n4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)\n5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]\nListing 1: Random flip\nIf horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2\nwhich may be seen during training. Potentially, every pair of consecutive epochs could contain every\nunique input. But our main observation is that with standard random horizontal flipping, half of the\nimages will be redundantly flipped the same way during both epochs, so that on average only 1.5N\nunique inputs will be seen.\naltflip : To address this, we propose to modify standard random horizontal flipping augmentation as\nfollows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},\nwe flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we\nflip only those inputs which were flipped in the first epoch. We provide the following implementation\nwhich avoids the need for extra memory by using a pseudorandom function to decide the flips.\n1import torch\n2import hashlib\n3def hash_fn (n, seed =42) :\n4 k = n * seed\n5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,\n16)\n6def alternating_flip (inputs , indices , epoch ):\n7 # Applies alternating flipping to a batch of images\n8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist\n() ])\n9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)\n10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\nListing 2: Alternating flip\nThe result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in\nFigure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.\nAdding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final\ntraining method airbench94.py , the entire contents of which can be found in Section E. It reaches\n94% accuracy in 3.83 seconds on an NVIDIA A100.\n3.7 Compilation\nThe final step we take to speed up training is a non-algorithmic one: we compile our training\nmethod using torch.compile in order to more efficiently utilize the GPU. This results in a training\nscript which is mathematically equivalent (up to small differences in floating point arithmetic) to\nthe non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29\nA100-seconds. The downside is that the one-time compilation process takes up to several minutes to\ncomplete before training runs can begin, so that it is only beneficial when we plan to execute many\nruns of training at once. We release this version as airbench94_compiled.py .\n4 95% and 96% targets\nTo address scenarios where somewhat higher performance is desired, we additionally develop methods\ntargeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .\nTo attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel\ncount of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce\nthe learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%\naccuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.\n2Assuming none of the training inputs are already mirrors of each other.\n5\nFigure 4: Training speedups accumulate additively. Removing individual features from airbench94\nincreases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces\nthe epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-\n94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.\nFigure 3: FLOPs vs. error rate tradeoff. Our\nthree training methods apparently follow a linear\nlog-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-\nVries & Taylor, 2017) augmentation and raise the\ntraining epochs to 40. We add a third convolution to\neach block, and scale the first block to 128 channels\nand the second two to 512. We also add a residual\nconnection across the later two convolutions of each\nblock, which we find is still beneficial despite the fact\nthat we are already using identity initialization (Sec-\ntion 3.3) to ease gradient flow. Finally, we reduce\nthe learning rate by a factor of 0.78. These changes\nyield airbench96 which attains 96.05% accuracy in\n46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.\nFigure 3 shows the FLOPs and error rate of each of\nour three training methods.\n5 Experiments\n5.1 Interaction between features\nTo gain a better sense of the impact of each feature on training speed, we compare two quantities.\nFirst, we measure the number of epochs that can be saved by adding the feature to the whitened\nbaseline (Section 3.2). Second, we measure the number of epochs that must be added when the\nfeature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-\ntion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it\nfrom the final airbench94 increases epochs-to-94% from 9.9 to 12.8.\nFigure 4 shows both quantities for each feature. Surprisingly, we find that for all features except\nmulti-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even\nthough the whitened baseline requires more than twice as many epochs as the final configuration.\nThis indicates that the interaction between most features is additive rather than multiplicative.\n5.2 Does alternating flip generalize?\nIn this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of\ntraining configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all\ncases except those where neither alternating nor random flip improve over using no flipping at all.\nFor CIFAR-10 we consider the performance boost given by alternating flip across the follow-\ning 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and\nairbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For\n6\nFigure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,\nthe use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%\ntraining speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.\n95% confidence intervals are shown around each point.\neach configuration we compare the performance of alternating and random flip in terms of their mean\naccuracy across n= 400 runs of training.\nFigure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip\nimproves performance in every setting. To get a sense for how big the improvement is, we estimate\nthe effective speedup for each case, i.e., the fraction of epochs that could be saved by switching\nfrom random to alternating flip while maintaining the level of accuracy of random flip. We begin\nby fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of\neach random flip-based training configuration. We use these curves to calculate the effective speedup\nafforded by switching from random to alternating flip. For example, airbench94 with random flip\nand without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The\nsame configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law\nfit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.\nNote that using a power-law yields a more conservative estimate relative to using linear interpolation\nbetween the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.\nTable 2 shows the result. We observe the following patterns. First, the addition of extra augmentation\n(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that\nthe main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the\nsame way for many epochs in a row; we speculate that adding extra augmentation reduces the negative\nimpact of these cases because it increases data diversity. Next, TTA reduces the gap between random\nand alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),\nindicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training\nfor longer consistently increases the effective speedup given by alternating flip.\nWe next study ImageNet trainings with the following experiment. We train a ResNet-18 with a\nvariety of train and test crops, comparing three flipping options: alternating flip, random flip, and no\nflipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192\ncenter crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)\nto denote the latter. We also consider two training crops: 192x192 inception-style random resized\ncrop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area\nranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the\nshorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write\n7\nBaseline Cutout Epochs Speedup Speedup (w/ TTA)\nairbench94 No 10 15.0% 5.30%\nairbench94 No 20 27.1% 21.3%\nairbench94 No 40 38.3% 36.4%\nairbench94 No 80 102% 31.8%\nairbench94 Yes 10 3.84% 1.13%\nairbench94 Yes 20 7.42% 2.00%\nairbench94 Yes 40 18.6% 9.28%\nairbench94 Yes 80 29.2% 14.25%\nairbench96 Yes 10 4.94% 1.11%\nairbench96 Yes 20 8.99% 3.58%\nairbench96 Yes 40 17.2% 6.48%\nairbench96 Yes 80 18.8% Not measured\nTable 2: Effective speedups given by switching from random flip to alternating flip. The two configurations\nmost closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw\naccuracy values of the airbench94 experiments.\nFlipping augmentation option\nTrain crop Test crop Epochs TTA None Random Alternating\nHeavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28\nHeavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28\nLight RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26\nLight RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26\nHeavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28\nHeavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28\nLight RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26\nLight RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26\nLight RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16\nHeavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4\nTable 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for\nthose trainings where random flip improves significantly over not flipping at all. The single best flipping option\nin each row is bolded when the difference is statistically significant.\nHeavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided\nin Section C.\nTable 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC\nis better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly\nbetter when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test\nresolution discrepancy (Touvron et al., 2019).\nFor trainings which use Light RRC, we find that switching from random flip to alternating flip\nprovides a substantial boost to performance, amounting to a training speedup of more than 25%.\nIn Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to\nalternating flip improves performance by more than increasing the training duration from 16 to 20\nepochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our\nresults on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit\nfrom alternating flip. Indeed, even turning flipping off completely does not significantly reduce the\nperformance of these trainings. We conclude that alternating flip improves over random flip for every\ntraining scenario where the latter improves over no flipping at all.\n8\nEpochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE\n1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312\n2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233\n1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269\n1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533\n2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433\n1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444\nTable 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).\n5.3 Variance and class-wise calibration\nPrevious sections have focused on understanding what factors affect the first moment of accuracy\n(the mean). In this section we investigate the second moment, finding that TTA reduces variance at\nthe cost of calibration.\nOur experiment is to execute 10,000 runs of airbench94 training with several hyperparameter\nsettings. For each setting we report both the variance in test-set accuracy as well as an estimate of the\ndistribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.\nTable 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set\nvariance, replicating the main finding of Jordan (2023). This is a surprising result because these\ntrainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7\nas much distribution-wise variance when run for a similar duration, and reached a low variance only\nwhen run for 64 epochs. We conclude from this comparison that distribution-wise variance is more\nstrongly connected to the rate of convergence of a training rather than its duration as such. We also\nnote that the low distribution-wise variance of airbench94 indicates it has high training stability.\nUsing TTA significantly reduces the test-set variance, such that all three settings with TTA have lower\ntest-set variance than any setting without TTA. However, test-set variance is implied by the class-wise\ncalibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this\nreduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,\nwe compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which\nmeasures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA\nhas a higher CACE than every setting without TTA, confirming the hypothesis.\n6 Discussion\nIn this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7\nfaster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the\nairbench Python package.\nWe developed airbench solely with the goal of maximizing training speed on CIFAR-10. In\nSection B we find that it also generalizes well to other tasks. For example, without any extra tuning,\nairbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.\nOne factor contributing to the training speed of airbench was our finding that training can be accel-\nerated by partially derandomizing the standard random horizontal flipping augmentation, resulting in\nthe variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating\nflip improves the performance of every training we considered (Section 5.2), with the exception of\nthose trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to\nus, the standard ImageNet trainings that we considered do not significantly benefit from horizontal\nflipping. Future work might investigate whether it is possible to obtain derandomized improvements\nto other augmentations besides horizontal flip.\nThe methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,\nwith fixed performance and hardware constraints. These constraints mean that we cannot improve\nperformance by simply scaling up the amount of computational resources used; instead we are forced\nto develop new methods like the alternating flip. We look forward to seeing what other new methods\nfuture work discovers to push training speed further.\n9\nReferences\nDimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.\nDan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-\nperformance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,\n2011.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\nmark and competition. 2017.\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet\nor cifar-10. arXiv preprint arXiv:1810.03505 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\npp. 248\u2013255. IEEE, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552 , 2017.\nMert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic\ngradient descent. Mathematical Programming , 186:49\u201384, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 , 2016.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.\npmlr, 2015.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\nsgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.\nKeller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and\ninevitable. arXiv preprint arXiv:2304.01910 , 2023.\nAndrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.\nio/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for\nadvanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT\nLicense.\nGuillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander\nM \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,\nBenjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.\nBenchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural\nInformation Processing Systems , 35:25404\u201325421, 2022.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.\nDavid Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/\nhow-to-train-your-resnet-8-bag-of-tricks/ .\n10\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics , 22(3):400\u2013407, 1951.\nSantiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:\n//github.com/apple/ml-cifar-10-faster , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.\ntysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/\nhlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. Advances in neural information processing systems , 32, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,\n2016.\nMichael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps\nforward, 1 step back. Advances in neural information processing systems , 32, 2019.\n11\nA Network architecture\n1from torch import nn\n2\n3class Flatten (nn. Module ):\n4 def forward (self , x):\n5 return x. view (x. size (0) , -1)\n6\n7class Mul (nn. Module ):\n8 def __init__ (self , scale ):\n9 super (). __init__ ()\n10 self . scale = scale\n11 def forward (self , x):\n12 return x * self . scale\n13\n14def conv (ch_in , ch_out ):\n15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,\n16 padding =\u2019same \u2019, bias = False )\n17\n18def make_net ():\n19 act = lambda : nn. GELU ()\n20 bn = lambda ch: nn. BatchNorm2d (ch)\n21 return nn. Sequential (\n22 nn. Sequential (\n23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),\n24 act () ,\n25 ),\n26 nn. Sequential (\n27 conv (24 , 64) ,\n28 nn. MaxPool2d (2) ,\n29 bn (64) , act () ,\n30 conv (64 , 64) ,\n31 bn (64) , act () ,\n32 ),\n33 nn. Sequential (\n34 conv (64 , 256) ,\n35 nn. MaxPool2d (2) ,\n36 bn (256) , act () ,\n37 conv (256 , 256) ,\n38 bn (256) , act () ,\n39 ),\n40 nn. Sequential (\n41 conv (256 , 256) ,\n42 nn. MaxPool2d (2) ,\n43 bn (256) , act () ,\n44 conv (256 , 256) ,\n45 bn (256) , act () ,\n46 ),\n47 nn. MaxPool2d (3) ,\n48 Flatten () ,\n49 nn. Linear (256 , 10, bias = False ),\n50 Mul (1/9) ,\n51 )\nListing 3: Minimal PyTorch code for the network architecture used by airbench94 .\nWe note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock\ntime. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel\nsize of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this\ndoes not improve the wallclock training time on an A100. Reducing the batch size is another easy\nway to save FLOPs but not wallclock time.\n12\nDataset Flipping? Cutout? ResNet-18 airbench96\nCIFAR-10 Yes No 95.55% 95.61%\nCIFAR-10 Yes Yes 96.01% 96.05%\nCIFAR-100 Yes No 77.54% 79.27%\nCIFAR-100 Yes Yes 78.04% 79.76%\nCINIC-10 Yes No 87.58% 87.78%\nCINIC-10 Yes Yes not measured 88.22%\nSVHN No No 97.35% 97.38%\nSVHN No Yes not measured 97.64%\nTable 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We\ndirectly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off\nflipping for SVHN).\nB Extra dataset experiments\nWe developed airbench with the singular goal of maximizing training speed on CIFAR-10. To\nfind out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its\nperformance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-\n10 (Darlow et al., 2018).\nOn CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both\nthe case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not\n(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than\nResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say\nthat it generalizes.\nWe compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.\nWe do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal\non CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better\nperformance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where\nairbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is\nused and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows\nstrong generalization to other tasks.\nWe note that this comparison between airbench96 and ResNet-18 training is fair in the sense that\nit does demonstrate that the former has good generalization, but unfair in the sense that it does not\nindicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time\naugmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training\nwould outperform airbench96 if it were run using test-time augmentation. However, it also takes\n5-10 times longer to complete. The decision of which to use may be situational.\nThe accuracy values we report for ResNet-18 training are from the following sources. We tried\nto select the highest values we could find for each setting. Moreau et al. (2022) reports attaining\n95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports\nattaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on\nCIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.\nC ImageNet training details\nOur ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/\nlibffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and\nmomentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution\n160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of\ntraining. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.\n13\nHyperparameters Flipping augmentation option\nEpochs Cutout TTA None Random Alternating\n10 No No 92.3053 93.0988 93.2798\n20 No No 92.8166 93.7446 93.8652\n40 No No 93.0143 94.0133 94.0729\n80 No No 93.0612 94.1169 94.1628\n10 No Yes 93.4071 93.9488 94.0186\n20 No Yes 93.8528 94.5565 94.6530\n40 No Yes 94.0381 94.7803 94.8203\n80 No Yes 94.0638 94.8506 94.8676\n10 Yes No 91.8487 92.0402 92.1374\n20 Yes No 92.8474 93.3825 93.4876\n40 Yes No 93.2675 94.1014 94.1952\n80 Yes No 93.4193 94.4311 94.5204\n10 Yes Yes 92.6455 92.7780 92.8103\n20 Yes Yes 93.7862 94.1306 94.1670\n40 Yes Yes 94.3090 94.8511 94.8960\n80 Yes Yes 94.5253 95.1839 95.2362\nTable 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a\nmean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise\ndifference in means is statistically significant.\nD Extra tables & figures\nFigure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in\nSection 5.3.\nE Complete training code\n1\"\"\"\n2airbench94 .py\n33.83 s runtime on an A100 ; 0.36 PFLOPs .\n4Evidence for validity : 94.01 average accuracy in n =1000 runs .\n5\n6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB\nwith the following nvidia -smi:\n7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :\n11.7\n14\n8torch . __version__ == \u20192.1.2+ cu118 \u2019\n9\"\"\"\n10\n11# ############################################\n12# Setup / Hyperparameters #\n13# ############################################\n14\n15import os\n16import sys\n17import uuid\n18from math import ceil\n19\n20import torch\n21from torch import nn\n22import torch .nn. functional as F\n23import torchvision\n24import torchvision . transforms as T\n25\n26torch . backends . cudnn . benchmark = True\n27\n28\"\"\"\n29We express the main training hyperparameters ( batch size , learning\nrate , momentum , and weight decay ) in decoupled form , so that each\none can be tuned independently . This accomplishes the following :\n30* Assuming time - constant gradients , the average step size is decoupled\nfrom everything but the lr.\n31* The size of the weight decay update is decoupled from everything but\nthe wd.\n32In constrast , normally when we increase the ( Nesterov ) momentum , this\nalso scales up the step size proportionally to 1 + 1 / (1 -\nmomentum ), meaning we cannot change momentum without having to re -\ntune the learning rate . Similarly , normally when we increase the\nlearning rate this also increases the size of the weight decay ,\nrequiring a proportional decrease in the wd to maintain the same\ndecay strength .\n33\n34The practical impact is that hyperparameter tuning is faster , since\nthis parametrization allows each one to be tuned independently .\nSee https :// myrtle .ai/ learn /how -to -train -your - resnet -5-\nhyperparameters /.\n35\"\"\"\n36\n37hyp = {\n38 \u2019opt \u2019: {\n39 \u2019 train_epochs \u2019: 9.9 ,\n40 \u2019batch_size \u2019: 1024 ,\n41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples\n42 \u2019momentum \u2019: 0.85 ,\n43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (\ndecoupled from learning rate )\n44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not\nweight decay ) for BatchNorm biases\n45 \u2019 label_smoothing \u2019: 0.2 ,\n46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the\nwhitening layer bias before freezing\n47 },\n48 \u2019aug \u2019: {\n49 \u2019flip \u2019: True ,\n50 \u2019translate \u2019: 2,\n51 },\n52 \u2019net \u2019: {\n53 \u2019widths \u2019: {\n54 \u2019block1 \u2019: 64,\n55 \u2019block2 \u2019: 256 ,\n56 \u2019block3 \u2019: 256 ,\n15\n57 },\n58 \u2019 batchnorm_momentum \u2019: 0.6 ,\n59 \u2019 scaling_factor \u2019: 1/9 ,\n60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :\n0= none , 1= mirror , 2= mirror + translate\n61 },\n62}\n63\n64# ############################################\n65# DataLoader #\n66# ############################################\n67\n68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )\n69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )\n70\n71def batch_flip_lr ( inputs ):\n72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .\nview (-1, 1, 1, 1)\n73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\n74\n75def batch_crop (images , crop_size ):\n76 r = ( images . size ( -1) - crop_size )//2\n77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =\nimages . device )\n78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),\ndevice = images .device , dtype = images . dtype )\n79 # The two cropping methods in this if - else produce equivalent\nresults , but the second is faster for r > 2.\n80 if r <= 2:\n81 for sy in range (-r, r +1):\n82 for sx in range (-r, r +1):\n83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)\n84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size\n, r+sx:r+sx+ crop_size ]\n85 else :\n86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size\n+2* r), device = images .device , dtype = images . dtype )\n87 for s in range (-r, r +1):\n88 mask = ( shifts [:, 0] == s)\n89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]\n90 for s in range (-r, r +1):\n91 mask = ( shifts [:, 1] == s)\n92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+\ncrop_size ]\n93 return images_out\n94\n95class CifarLoader :\n96 \"\"\"\n97 GPU - accelerated dataloader for CIFAR -10 which implements\nalternating flip augmentation .\n98 \"\"\"\n99\n100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,\ndrop_last =None , shuffle =None , gpu =0):\n101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .\npt \u2019)\n102 if not os. path . exists ( data_path ):\n103 dset = torchvision . datasets . CIFAR10 (path , download =True ,\ntrain = train )\n104 images = torch . tensor ( dset . data )\n105 labels = torch . tensor ( dset . targets )\n106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:\ndset . classes }, data_path )\n107\n108 data = torch . load ( data_path , map_location = torch . device (gpu))\n16\n109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [\n\u2019labels \u2019], data [\u2019classes \u2019]\n110 # It \u2019s faster to load + process uint8 data than to load\npreprocessed fp16 data\n111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).\nto( memory_format = torch . channels_last )\n112\n113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )\n114 self . proc_images = {} # Saved results of image processing to\nbe done on the first epoch\n115 self . epoch = 0\n116\n117 self . aug = aug or {}\n118 for k in self .aug . keys ():\n119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019\n% k\n120\n121 self . batch_size = batch_size\n122 self . drop_last = train if drop_last is None else drop_last\n123 self . shuffle = train if shuffle is None else shuffle\n124\n125 def __len__ ( self ):\n126 return len( self . images )// self . batch_size if self . drop_last\nelse ceil (len( self . images )/ self . batch_size )\n127\n128 def __iter__ ( self ):\n129\n130 if self . epoch == 0:\n131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .\nimages )\n132 # Randomly flip all images on the first epoch as according\nto definition of alternating flip\n133 if self .aug.get(\u2019flip \u2019, False ):\n134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (\nimages )\n135 # Pre -pad images to save time when doing random\ntranslation\n136 pad = self .aug.get (\u2019translate \u2019, 0)\n137 if pad > 0:\n138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019\nreflect \u2019)\n139\n140 if self .aug.get(\u2019translate \u2019, 0) > 0:\n141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .\nshape [ -2])\n142 elif self .aug.get (\u2019flip \u2019, False ):\n143 images = self . proc_images [\u2019flip \u2019]\n144 else :\n145 images = self . proc_images [\u2019norm \u2019]\n146 if self .aug.get(\u2019flip \u2019, False ):\n147 if self . epoch % 2 == 1:\n148 images = images . flip ( -1)\n149\n150 self . epoch += 1\n151\n152 indices = ( torch . randperm if self . shuffle else torch . arange )(\nlen ( images ), device = images . device )\n153 for i in range (len ( self )):\n154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]\n155 yield ( images [ idxs ], self . labels [ idxs ])\n156\n157# ############################################\n158# Network Components #\n159# ############################################\n160\n17\n161class Flatten (nn. Module ):\n162 def forward (self , x):\n163 return x. view (x. size (0) , -1)\n164\n165class Mul (nn. Module ):\n166 def __init__ (self , scale ):\n167 super (). __init__ ()\n168 self . scale = scale\n169 def forward (self , x):\n170 return x * self . scale\n171\n172class BatchNorm (nn. BatchNorm2d ):\n173 def __init__ (self , num_features , momentum , eps =1e -12 ,\n174 weight =False , bias = True ):\n175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )\n176 self . weight . requires_grad = weight\n177 self . bias . requires_grad = bias\n178 # Note that PyTorch already initializes the weights to one and\nbiases to zero\n179\n180class Conv (nn. Conv2d ):\n181 def __init__ (self , in_channels , out_channels , kernel_size =3,\npadding =\u2019same \u2019, bias = False ):\n182 super (). __init__ ( in_channels , out_channels , kernel_size =\nkernel_size , padding = padding , bias = bias )\n183\n184 def reset_parameters ( self ):\n185 super (). reset_parameters ()\n186 if self . bias is not None :\n187 self . bias . data . zero_ ()\n188 w = self . weight . data\n189 torch .nn. init . dirac_ (w[:w. size (1) ])\n190\n191class ConvGroup (nn. Module ):\n192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):\n193 super (). __init__ ()\n194 self . conv1 = Conv ( channels_in , channels_out )\n195 self . pool = nn. MaxPool2d (2)\n196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )\n197 self . conv2 = Conv ( channels_out , channels_out )\n198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )\n199 self . activ = nn. GELU ()\n200\n201 def forward (self , x):\n202 x = self . conv1 (x)\n203 x = self . pool (x)\n204 x = self . norm1 (x)\n205 x = self . activ (x)\n206 x = self . conv2 (x)\n207 x = self . norm2 (x)\n208 x = self . activ (x)\n209 return x\n210\n211# ############################################\n212# Network Definition #\n213# ############################################\n214\n215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019\n][ \u2019 batchnorm_momentum \u2019]):\n216 whiten_kernel_size = 2\n217 whiten_width = 2 * 3 * whiten_kernel_size **2\n218 net = nn. Sequential (\n219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True\n),\n220 nn. GELU () ,\n18\n221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],\nbatchnorm_momentum ),\n222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],\nbatchnorm_momentum ),\n223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],\nbatchnorm_momentum ),\n224 nn. MaxPool2d (3) ,\n225 Flatten () ,\n226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),\n227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,\n228 )\n229 net [0]. weight . requires_grad = False\n230 net = net. half (). cuda ()\n231 net = net.to( memory_format = torch . channels_last )\n232 for mod in net. modules ():\n233 if isinstance (mod , BatchNorm ):\n234 mod . float ()\n235 return net\n236\n237# ############################################\n238# Whitening Conv Initialization #\n239# ############################################\n240\n241def get_patches (x, patch_shape ):\n242 c, (h, w) = x. shape [1] , patch_shape\n243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h\n,w). float ()\n244\n245def get_whitening_parameters ( patches ):\n246 n,c,h,w = patches . shape\n247 patches_flat = patches . view (n, -1)\n248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n\n249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance\n, UPLO =\u2019U\u2019)\n250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.\nreshape (c*h*w,c,h,w). flip (0)\n251\n252def init_whitening_conv (layer , train_set , eps =5e -4):\n253 patches = get_patches ( train_set , patch_shape = layer . weight . data .\nshape [2:])\n254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )\n255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)\n256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -\neigenvectors_scaled ))\n257\n258# ###########################################\n259# Lookahead #\n260# ###########################################\n261\n262class LookaheadState :\n263 def __init__ (self , net):\n264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().\nitems ()}\n265\n266 def update (self , net , decay ):\n267 for ema_param , net_param in zip( self . net_ema . values () , net.\nstate_dict (). values ()):\n268 if net_param . dtype in ( torch .half , torch . float ):\n269 ema_param . lerp_ ( net_param , 1- decay )\n270 net_param . copy_ ( ema_param )\n271\n272# ###########################################\n273# Logging #\n274# ###########################################\n275\n19\n276def print_columns ( columns_list , is_head =False , is_final_entry = False ):\n277 print_string = \u2019\u2019\n278 for col in columns_list :\n279 print_string += \u2019| %s \u2019 % col\n280 print_string += \u2019|\u2019\n281 if is_head :\n282 print (\u2019-\u2019*len( print_string ))\n283 print ( print_string )\n284 if is_head or is_final_entry :\n285 print (\u2019-\u2019*len( print_string ))\n286\n287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,\n\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]\n288def print_training_details ( variables , is_final_entry ):\n289 formatted = []\n290 for col in logging_columns_list :\n291 var = variables .get (col. strip () , None )\n292 if type (var) in (int , str):\n293 res = str(var)\n294 elif type (var) is float :\n295 res = \u2019{:0.4 f}\u2019. format ( var)\n296 else :\n297 assert var is None\n298 res = \u2019\u2019\n299 formatted . append (res. rjust (len (col)))\n300 print_columns ( formatted , is_final_entry = is_final_entry )\n301\n302# ###########################################\n303# Evaluation #\n304# ###########################################\n305\n306def infer (model , loader , tta_level =0) :\n307 \"\"\"\n308 Test - time augmentation strategy (for tta_level =2):\n309 1. Flip / mirror the image left -to - right (50% of the time ).\n310 2. Translate the image by one pixel either up -and - left or down -and\n- right (50% of the time , i.e. both happen 25% of the time ).\n311\n312 This creates 6 views per image ( left / right times the two\ntranslations and no - translation ), which we evaluate and then\nweight according to the given probabilities .\n313 \"\"\"\n314\n315 def infer_basic (inputs , net):\n316 return net( inputs ). clone ()\n317\n318 def infer_mirror (inputs , net):\n319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))\n320\n321 def infer_mirror_translate ( inputs , net ):\n322 logits = infer_mirror (inputs , net )\n323 pad = 1\n324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)\n325 inputs_translate_list = [\n326 padded_inputs [:, :, 0:32 , 0:32] ,\n327 padded_inputs [:, :, 2:34 , 2:34] ,\n328 ]\n329 logits_translate_list = [ infer_mirror ( inputs_translate , net)\n330 for inputs_translate in\ninputs_translate_list ]\n331 logits_translate = torch . stack ( logits_translate_list ). mean (0)\n332 return 0.5 * logits + 0.5 * logits_translate\n333\n334 model . eval ()\n335 test_images = loader . normalize ( loader . images )\n20\n336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][\ntta_level ]\n337 with torch . no_grad ():\n338 return torch .cat ([ infer_fn (inputs , model ) for inputs in\ntest_images . split (2000) ])\n339\n340def evaluate (model , loader , tta_level =0) :\n341 logits = infer (model , loader , tta_level )\n342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()\n343\n344# ###########################################\n345# Training #\n346# ###########################################\n347\n348def main (run):\n349\n350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]\n351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]\n352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]\n353 # Assuming gradients are constant in time , for Nesterov momentum ,\nthe below ratio is how much larger the default steps will be than\nthe underlying per - example gradients . We divide the learning rate\nby this ratio in order to ensure steps are the same scale as\ngradients , regardless of the choice of momentum .\n354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))\n355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning\nrate for PyTorch SGD\n356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale\n357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]\n358\n359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019\nlabel_smoothing \u2019], reduction =\u2019none \u2019)\n360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)\n361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =\nbatch_size , aug=hyp[\u2019aug \u2019])\n362 if run == \u2019warmup \u2019:\n363 # The only purpose of the first run is to warmup , so we can\nuse dummy data\n364 train_loader . labels = torch . randint (0, 10, size =( len (\ntrain_loader . labels ) ,), device = train_loader . labels . device )\n365 total_train_steps = ceil (len( train_loader ) * epochs )\n366\n367 model = make_net ()\n368 current_steps = 0\n369\n370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in\nk and p. requires_grad ]\n371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019\nnot in k and p. requires_grad ]\n372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,\nweight_decay =wd/ lr_biases ),\n373 dict ( params = other_params , lr=lr , weight_decay =wd/\nlr)]\n374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,\nnesterov = True )\n375\n376 def triangle (steps , start =0, end =0, peak =0.5) :\n377 xp = torch . tensor ([0 , int( peak * steps ), steps ])\n378 fp = torch . tensor ([ start , 1, end ])\n379 x = torch . arange (1+ steps )\n380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])\n381 b = fp [: -1] - (m * xp [: -1])\n382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1\n383 indices = torch . clamp ( indices , 0, len(m) - 1)\n384 return m[ indices ] * x + b[ indices ]\n21\n385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,\npeak =0.23)\n386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:\nlr_schedule [i])\n387\n388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /\ntotal_train_steps )**3\n389 lookahead_state = LookaheadState ( model )\n390\n391 # For accurately timing GPU code\n392 starter = torch . cuda . Event ( enable_timing = True )\n393 ender = torch . cuda . Event ( enable_timing = True )\n394 total_time_seconds = 0.0\n395\n396 # Initialize the first layer using statistics of training images\n397 starter . record ()\n398 train_images = train_loader . normalize ( train_loader . images [:5000])\n399 init_whitening_conv ( model [0] , train_images )\n400 ender . record ()\n401 torch . cuda . synchronize ()\n402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n403\n404 for epoch in range ( ceil ( epochs )):\n405\n406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019\nwhiten_bias_epochs \u2019])\n407\n408 # ###################\n409 # Training #\n410 # ###################\n411\n412 starter . record ()\n413\n414 model . train ()\n415 for inputs , labels in train_loader :\n416\n417 outputs = model ( inputs )\n418 loss = loss_fn ( outputs , labels ).sum ()\n419 optimizer . zero_grad ( set_to_none = True )\n420 loss . backward ()\n421 optimizer . step ()\n422 scheduler . step ()\n423\n424 current_steps += 1\n425\n426 if current_steps % 5 == 0:\n427 lookahead_state . update (model , decay = alpha_schedule [\ncurrent_steps ]. item ())\n428\n429 if current_steps >= total_train_steps :\n430 if lookahead_state is not None :\n431 lookahead_state . update (model , decay =1.0)\n432 break\n433\n434 ender . record ()\n435 torch . cuda . synchronize ()\n436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n437\n438 # ###################\n439 # Evaluation #\n440 # ###################\n441\n442 # Print the accuracy and loss from the last training batch of\nthe epoch\n22\n443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().\nmean (). item ()\n444 train_loss = loss . item () / batch_size\n445 val_acc = evaluate (model , test_loader , tta_level =0)\n446 print_training_details ( locals () , is_final_entry = False )\n447 run = None # Only print the run number once\n448\n449 # ###################\n450 # TTA Evaluation #\n451 # ###################\n452\n453 starter . record ()\n454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019\ntta_level \u2019])\n455 ender . record ()\n456 torch . cuda . synchronize ()\n457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n458\n459 epoch = \u2019eval \u2019\n460 print_training_details ( locals () , is_final_entry = True )\n461\n462 return tta_val_acc\n463\n464if __name__ == \" __main__ \":\n465 with open (sys. argv [0]) as f:\n466 code = f. read ()\n467\n468 print_columns ( logging_columns_list , is_head = True )\n469 main (\u2019warmup \u2019)\n470 accs = torch . tensor ([ main (run) for run in range (25) ])\n471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))\n472\n473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }\n474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))\n475 os. makedirs ( log_dir , exist_ok = True )\n476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)\n477 print (os. path . abspath ( log_path ))\n478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))\nListing 4: airbench94.py\n23\n\nCandidate Output: tasks=[TaskCandidate(name='Implement alternating flip data augmentation', description='Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in a deterministic alternating pattern across epochs, rather than randomly.', baseline='Standard random horizontal flipping augmentation', skills=['PyTorch', 'Computer Vision', 'Data Augmentation'], relevant_paper_text='We propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.', scoring_feasibility=8, llm_tractability=0.7, expert_tractability=0.9, layman_tractability=0.3, scoring_llm_tractability=0.8, asset_prerequisites=['CIFAR-10 dataset']), TaskCandidate(name='Implement patch-whitening initialization', description='Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper.', baseline='Standard random initialization', skills=['PyTorch', 'Linear Algebra', 'Neural Network Initialization'], relevant_paper_text='Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.', scoring_feasibility=7, llm_tractability=0.5, expert_tractability=0.8, layman_tractability=0.1, scoring_llm_tractability=0.6, asset_prerequisites=['CIFAR-10 dataset']), TaskCandidate(name='Implement multi-crop test-time augmentation', description='Implement the multi-crop test-time augmentation technique described in the paper, which involves evaluating the model on multiple augmented versions of each test image.', baseline='No test-time augmentation', skills=['PyTorch', 'Computer Vision', 'Test-Time Augmentation'], relevant_paper_text='To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.', scoring_feasibility=8, llm_tractability=0.8, expert_tractability=0.9, layman_tractability=0.4, scoring_llm_tractability=0.9, asset_prerequisites=['CIFAR-10 dataset', 'Trained model'])]\n\nRequirements:\nExtract all promising task candidates from paper. Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\n\n    A good task candidate:\n    - describes a task that an agent can implement\n    - typically a programming task, e.g. modifying a 'baseline' implementation\n    - which reflects or mirrors actual tasks undertaken to produce the research paper\n    - and requires research and/or engineering expertise to implement correctly\n    - for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\n    - ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\n    - the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\n    \n\nAttempt Num: 0\n\nPrevious Critiques:"
  },
  {
    "Response": "Self Critique:\n\nOverall, I believe I have extracted some promising task candidates from the paper that satisfy many of the key requirements. However, there are a few areas where the task candidates could be improved:\n\nStrengths:\n1. The tasks describe concrete programming tasks that an agent could implement, reflecting actual work done in the paper.\n2. They require research and engineering expertise to implement correctly.\n3. The tasks modify baseline implementations, which aligns well with the requirements.\n4. The tasks seem feasible to score objectively based on the paper's descriptions.\n\nAreas for improvement:\n1. I could have provided more detail on potential scoring functions for each task. While they seem scorable, I didn't explicitly describe how this could be done.\n2. The tasks may not fully capture the difficulty gap between implementation and scoring. While scoring seems easier than implementation, this could be emphasized more.\n3. I could have extracted more diverse tasks from the paper. The current set focuses heavily on data augmentation techniques, while the paper likely contains other interesting algorithmic or architectural innovations.\n4. The \"expert tractability\" scores may be too high across the board. More variation here could better differentiate the tasks.\n5. I could have included more quantitative metrics from the paper to aid in potential scoring functions.\n\nShould Retry: While the current task candidates are reasonably good, I believe we could benefit from another attempt to address these shortcomings and potentially extract additional diverse tasks from the paper. Therefore, I suggest we should retry."
  },
  {
    "Request": "Given the fields `candidate_input`, `candidate_output`, `requirements`, `attempt_num`, `previous_critiques`, produce the fields `self_critique`, `should_retry`.\n\n---\n\nFollow the following format.\n\nCandidate Input: ${candidate_input}\n\nCandidate Output: ${candidate_output}\n\nRequirements: ${requirements}\n\nAttempt Num: ${attempt_num}\n\nPrevious Critiques: ${previous_critiques}\n\nPast Error in Should Retry: An error to avoid in the future\n\nSelf Critique: Self-critique of how well the candidate output satisfies the requirements given the input\n\nShould Retry: Should we attempt to generate the output again taking this critique into account? True/False (Respond with true or false)\n\n---\n\nCandidate Input:\n94% on CIFAR-10 in 3.29 Seconds on a Single GPU\nKeller Jordan\nkjordan4077@gmail.com\nAbstract\nCIFAR-10 is among the most widely used datasets in machine learning, facilitating\nthousands of research projects per year. To accelerate research and reduce the cost\nof experiments, we introduce training methods for CIFAR-10 which reach 94%\naccuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when\nrun on a single NVIDIA A100 GPU. As one factor contributing to these training\nspeeds, we propose a derandomized variant of horizontal flipping augmentation,\nwhich we show improves over the standard method in every case where flipping is\nbeneficial over no flipping at all. Our code is released at https://github.com/\nKellerJordan/cifar10-airbench .\n1 Introduction\nCIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-\ning thousands of research projects per year1. Research can be accelerated and the cost of experiments\nreduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In\nthis paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single\nNVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,\n2023). To support scenarios where higher performance is needed, we additionally develop methods\ntargeting 95% and 96% accuracy. We release the following methods in total.\n1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).\n2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).\n3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).\n4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).\nAll runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-\nmatically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the\nfirst uses torch.compile to improve GPU utilization. It is intended for experiments where many\nnetworks are trained at once in order to amortize the one-time compilation cost. The non-compiled\nairbench94 variant can be easily installed and run using the following command.\n1pip install airbench\n2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"\nOne motivation for the development of these training methods is that they can accelerate the experi-\nmental iteration time of researchers working on compatible projects involving CIFAR-10. Another\nmotivation is that they can decrease the cost of projects involving a massive number of trained\nnetworks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used\n3 million trained networks to demonstrate that the outputs of a trained neural network on a given test\ninput follow an approximately linear function of the vector of binary choices of which examples the\nmodel was trained on. Another example is Jordan (2023), a study on training variance which used\n180 thousand trained networks to show that standard trainings have little variance in performance on\n1https://paperswithcode.com/datasets\nPreprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024\nFigure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal\nflipping augmentation, which flips each image with 50% probability per epoch. This results in some images\nbeing redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a\ndeterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\ntheir test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds\nand 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make\nit possible to replicate these studies, or conduct similar ones, with fewer computational resources.\nFast training also enables the rapid accumulation of statistical significance for subtle hyperparameter\ncomparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10\naccuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation\nbetween runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the\nimprovement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training\nthis will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.\nOur work builds on prior training speed projects. We utilize a modified version of the network,\ninitialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen\npatch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over\nprior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-\nure 1, Section 3.6, Section 5.2).\n2 Background\nOur objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set\nin the shortest possible amount of time. Timing begins when the method is first given access to\ntraining data, and ends when it produces test-set predictions. The method is considered valid if its\nmean accuracy over repeated runs is at least 94%.\nWe chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the\n2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more\nrecent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10\nseconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page\n(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is\ntysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,\n94% is the level of human accuracy reported by Karpathy (2011).\nWe note the following consequences of how the method is timed. First, it is permitted for the program\nto begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins\nwhen the training data is first accessed. This is helpful because otherwise the first run of training\nis typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA\nimproves the performance of a trained network by running it on multiple augmented views of each\ntest input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping\nTTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods\nattain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\nThe CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.\nIt is divided into a training set of 50,000 images and a validation set of 10,000 images. As a\nmatter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was\n2\n80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than\nairbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94\ncan be attributed entirely to algorithmic progress rather than compute scaling.\n3 Methods\n3.1 Network architecture and baseline training\nWe train a convolutional network with a total of 1.97 million parameters, following tysam-code\n(2023) with a few small changes. It contains seven convolutions with the latter six being divided into\nthree blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this\nsection we offer some comments on the main design choices.\nThe network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed\nentirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,\n2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with\nno padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather\nthan the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff\nbetween throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\nFollowing Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,\nand disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is\nscaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture\ndiffers only in that we decrease the number of output channels in the third block from 512 to 256,\nand we add learnable biases to the first convolution.\nAs our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of\n0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the\nmaximum at 20% of the way through training, and then decreases to zero. For data augmentation we\nuse random horizontal flipping alongside 2-pixel random translation. For translation we use reflection\npadding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note\nthat what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a\nrandom 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the\nnetwork is run on both a given test image and its mirror and inferences are made based on the average\nof the two outputs. With optimized choices of learning rate, momentum, and weight decay, this\nbaseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.\n3.2 Frozen patch-whitening initialization\nFollowing Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-\nwhitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code\n(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches\nacross the training distribution, so that their outputs have identity covariance matrix. The second 12\nFigure 2: The first layer\u2019s weights after whitening\ninitialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,\nso that input information is preserved through the\nactivation which follows. Figure 2 shows the result.\nWe do not update this layer\u2019s weights during training.\nDeparting from tysam-code (2023), we add learn-\nable biases to this layer, yielding a small performance\nboost. The biases are trained for 3 epochs, after which\nwe disable their gradient to increase backward-pass\nthroughput, which improves training speed without\nreducing accuracy. We also obtain a slight perfor-\nmance boost relative to tysam-code (2023) by re-\nducing the constant added to the eigenvalues during\ncalculation of the patch-whitening initialization for\nthe purpose of preventing numerical issues in the case\nof a singular patch-covariance matrix.\nPatch-whitening initialization is the single most impactful feature. Adding it to the baseline more\nthan doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.\n3\nRandom reshuffling Alternating flip Mean accuracy\nNo No 93.40%\nNo Yes 93.48%\nYes No 93.92%\nYes Yes 94.01%\nTable 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating\nflip (which we propose) reduce training data redundancy and improve performance.\n3.3 Identity initialization\ndirac : We initialize all convolutions after the first as partial identity transforms. That is, for a\nconvolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an\nidentity transform of the input, and leave the remaining N\u2212Mto their default initialization. In\nPyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight\nwof each convolutional layer. This method partially follows tysam-code (2023), which used a more\ncomplicated scheme where the identity weights are mixed in with the original initialization, which\nwe did not find to be more performant. With this feature added, training attains 94% accuracy in 18\nepochs taking 6.8 A100-seconds.\n3.4 Optimization tricks\nscalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor\nof64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%\nin 13.5 epochs taking 5.1 A100-seconds.\nlookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.\nWe note that Lookahead has also been found effective in prior work on training speed for ResNet-\n18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6\nA100-seconds.\n3.5 Multi-crop evaluation\nmulticrop : To generate predictions, we run the trained network on six augmented views of each\ntest image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a\nversion which is translated down-and-to-the-right by one pixel, and the mirrored versions of all\nthree. Predictions are made using a weighted average of all six outputs, where the two views of the\nuntranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125\neach. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.\nWe note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-\nmonyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of\nevaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using\nmore crops does improve performance, but the increase to inference time outweighs the potential\ntraining speedup.\n3.6 Alternating flip\nTo speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,\nwhich we motivate as follows. When training neural networks, it is standard practice to organize\ntraining into a set of epochs during which every training example is seen exactly once. This\ndiffers from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),\nwhich calls for data to be repeatedly sampled with-replacement from the training set, resulting in\nexamples being potentially seen multiple redundant times within a short window of training. The\nuse of randomly ordered epochs of data for training has a different name, being called the random\nreshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our\ntraining dataset consists of Nunique examples, then sampling data with replacement causes every\n\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N\nunique examples on average. On the other hand, random reshuffling leads to all Nunique examples\nbeing seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason\nthat it is beneficial to maximize the number of unique inputs seen per window of training time.\n4\nWe extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We\nfirst note that standard random horizontal flipping augmentation can be defined as follows.\n1import torch\n2def random_flip ( inputs ):\n3 # Applies random flipping to a batch of images\n4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)\n5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]\nListing 1: Random flip\nIf horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2\nwhich may be seen during training. Potentially, every pair of consecutive epochs could contain every\nunique input. But our main observation is that with standard random horizontal flipping, half of the\nimages will be redundantly flipped the same way during both epochs, so that on average only 1.5N\nunique inputs will be seen.\naltflip : To address this, we propose to modify standard random horizontal flipping augmentation as\nfollows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},\nwe flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we\nflip only those inputs which were flipped in the first epoch. We provide the following implementation\nwhich avoids the need for extra memory by using a pseudorandom function to decide the flips.\n1import torch\n2import hashlib\n3def hash_fn (n, seed =42) :\n4 k = n * seed\n5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,\n16)\n6def alternating_flip (inputs , indices , epoch ):\n7 # Applies alternating flipping to a batch of images\n8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist\n() ])\n9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)\n10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\nListing 2: Alternating flip\nThe result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in\nFigure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.\nAdding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final\ntraining method airbench94.py , the entire contents of which can be found in Section E. It reaches\n94% accuracy in 3.83 seconds on an NVIDIA A100.\n3.7 Compilation\nThe final step we take to speed up training is a non-algorithmic one: we compile our training\nmethod using torch.compile in order to more efficiently utilize the GPU. This results in a training\nscript which is mathematically equivalent (up to small differences in floating point arithmetic) to\nthe non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29\nA100-seconds. The downside is that the one-time compilation process takes up to several minutes to\ncomplete before training runs can begin, so that it is only beneficial when we plan to execute many\nruns of training at once. We release this version as airbench94_compiled.py .\n4 95% and 96% targets\nTo address scenarios where somewhat higher performance is desired, we additionally develop methods\ntargeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .\nTo attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel\ncount of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce\nthe learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%\naccuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.\n2Assuming none of the training inputs are already mirrors of each other.\n5\nFigure 4: Training speedups accumulate additively. Removing individual features from airbench94\nincreases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces\nthe epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-\n94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.\nFigure 3: FLOPs vs. error rate tradeoff. Our\nthree training methods apparently follow a linear\nlog-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-\nVries & Taylor, 2017) augmentation and raise the\ntraining epochs to 40. We add a third convolution to\neach block, and scale the first block to 128 channels\nand the second two to 512. We also add a residual\nconnection across the later two convolutions of each\nblock, which we find is still beneficial despite the fact\nthat we are already using identity initialization (Sec-\ntion 3.3) to ease gradient flow. Finally, we reduce\nthe learning rate by a factor of 0.78. These changes\nyield airbench96 which attains 96.05% accuracy in\n46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.\nFigure 3 shows the FLOPs and error rate of each of\nour three training methods.\n5 Experiments\n5.1 Interaction between features\nTo gain a better sense of the impact of each feature on training speed, we compare two quantities.\nFirst, we measure the number of epochs that can be saved by adding the feature to the whitened\nbaseline (Section 3.2). Second, we measure the number of epochs that must be added when the\nfeature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-\ntion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it\nfrom the final airbench94 increases epochs-to-94% from 9.9 to 12.8.\nFigure 4 shows both quantities for each feature. Surprisingly, we find that for all features except\nmulti-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even\nthough the whitened baseline requires more than twice as many epochs as the final configuration.\nThis indicates that the interaction between most features is additive rather than multiplicative.\n5.2 Does alternating flip generalize?\nIn this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of\ntraining configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all\ncases except those where neither alternating nor random flip improve over using no flipping at all.\nFor CIFAR-10 we consider the performance boost given by alternating flip across the follow-\ning 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and\nairbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For\n6\nFigure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,\nthe use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%\ntraining speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.\n95% confidence intervals are shown around each point.\neach configuration we compare the performance of alternating and random flip in terms of their mean\naccuracy across n= 400 runs of training.\nFigure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip\nimproves performance in every setting. To get a sense for how big the improvement is, we estimate\nthe effective speedup for each case, i.e., the fraction of epochs that could be saved by switching\nfrom random to alternating flip while maintaining the level of accuracy of random flip. We begin\nby fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of\neach random flip-based training configuration. We use these curves to calculate the effective speedup\nafforded by switching from random to alternating flip. For example, airbench94 with random flip\nand without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The\nsame configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law\nfit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.\nNote that using a power-law yields a more conservative estimate relative to using linear interpolation\nbetween the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.\nTable 2 shows the result. We observe the following patterns. First, the addition of extra augmentation\n(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that\nthe main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the\nsame way for many epochs in a row; we speculate that adding extra augmentation reduces the negative\nimpact of these cases because it increases data diversity. Next, TTA reduces the gap between random\nand alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),\nindicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training\nfor longer consistently increases the effective speedup given by alternating flip.\nWe next study ImageNet trainings with the following experiment. We train a ResNet-18 with a\nvariety of train and test crops, comparing three flipping options: alternating flip, random flip, and no\nflipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192\ncenter crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)\nto denote the latter. We also consider two training crops: 192x192 inception-style random resized\ncrop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area\nranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the\nshorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write\n7\nBaseline Cutout Epochs Speedup Speedup (w/ TTA)\nairbench94 No 10 15.0% 5.30%\nairbench94 No 20 27.1% 21.3%\nairbench94 No 40 38.3% 36.4%\nairbench94 No 80 102% 31.8%\nairbench94 Yes 10 3.84% 1.13%\nairbench94 Yes 20 7.42% 2.00%\nairbench94 Yes 40 18.6% 9.28%\nairbench94 Yes 80 29.2% 14.25%\nairbench96 Yes 10 4.94% 1.11%\nairbench96 Yes 20 8.99% 3.58%\nairbench96 Yes 40 17.2% 6.48%\nairbench96 Yes 80 18.8% Not measured\nTable 2: Effective speedups given by switching from random flip to alternating flip. The two configurations\nmost closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw\naccuracy values of the airbench94 experiments.\nFlipping augmentation option\nTrain crop Test crop Epochs TTA None Random Alternating\nHeavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28\nHeavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28\nLight RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26\nLight RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26\nHeavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28\nHeavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28\nLight RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26\nLight RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26\nLight RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16\nHeavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4\nTable 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for\nthose trainings where random flip improves significantly over not flipping at all. The single best flipping option\nin each row is bolded when the difference is statistically significant.\nHeavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided\nin Section C.\nTable 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC\nis better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly\nbetter when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test\nresolution discrepancy (Touvron et al., 2019).\nFor trainings which use Light RRC, we find that switching from random flip to alternating flip\nprovides a substantial boost to performance, amounting to a training speedup of more than 25%.\nIn Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to\nalternating flip improves performance by more than increasing the training duration from 16 to 20\nepochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our\nresults on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit\nfrom alternating flip. Indeed, even turning flipping off completely does not significantly reduce the\nperformance of these trainings. We conclude that alternating flip improves over random flip for every\ntraining scenario where the latter improves over no flipping at all.\n8\nEpochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE\n1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312\n2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233\n1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269\n1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533\n2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433\n1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444\nTable 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).\n5.3 Variance and class-wise calibration\nPrevious sections have focused on understanding what factors affect the first moment of accuracy\n(the mean). In this section we investigate the second moment, finding that TTA reduces variance at\nthe cost of calibration.\nOur experiment is to execute 10,000 runs of airbench94 training with several hyperparameter\nsettings. For each setting we report both the variance in test-set accuracy as well as an estimate of the\ndistribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.\nTable 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set\nvariance, replicating the main finding of Jordan (2023). This is a surprising result because these\ntrainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7\nas much distribution-wise variance when run for a similar duration, and reached a low variance only\nwhen run for 64 epochs. We conclude from this comparison that distribution-wise variance is more\nstrongly connected to the rate of convergence of a training rather than its duration as such. We also\nnote that the low distribution-wise variance of airbench94 indicates it has high training stability.\nUsing TTA significantly reduces the test-set variance, such that all three settings with TTA have lower\ntest-set variance than any setting without TTA. However, test-set variance is implied by the class-wise\ncalibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this\nreduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,\nwe compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which\nmeasures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA\nhas a higher CACE than every setting without TTA, confirming the hypothesis.\n6 Discussion\nIn this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7\nfaster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the\nairbench Python package.\nWe developed airbench solely with the goal of maximizing training speed on CIFAR-10. In\nSection B we find that it also generalizes well to other tasks. For example, without any extra tuning,\nairbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.\nOne factor contributing to the training speed of airbench was our finding that training can be accel-\nerated by partially derandomizing the standard random horizontal flipping augmentation, resulting in\nthe variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating\nflip improves the performance of every training we considered (Section 5.2), with the exception of\nthose trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to\nus, the standard ImageNet trainings that we considered do not significantly benefit from horizontal\nflipping. Future work might investigate whether it is possible to obtain derandomized improvements\nto other augmentations besides horizontal flip.\nThe methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,\nwith fixed performance and hardware constraints. These constraints mean that we cannot improve\nperformance by simply scaling up the amount of computational resources used; instead we are forced\nto develop new methods like the alternating flip. We look forward to seeing what other new methods\nfuture work discovers to push training speed further.\n9\nReferences\nDimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.\nDan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-\nperformance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,\n2011.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\nmark and competition. 2017.\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet\nor cifar-10. arXiv preprint arXiv:1810.03505 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\npp. 248\u2013255. IEEE, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552 , 2017.\nMert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic\ngradient descent. Mathematical Programming , 186:49\u201384, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 , 2016.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.\npmlr, 2015.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\nsgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.\nKeller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and\ninevitable. arXiv preprint arXiv:2304.01910 , 2023.\nAndrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.\nio/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for\nadvanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT\nLicense.\nGuillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander\nM \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,\nBenjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.\nBenchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural\nInformation Processing Systems , 35:25404\u201325421, 2022.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.\nDavid Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/\nhow-to-train-your-resnet-8-bag-of-tricks/ .\n10\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics , 22(3):400\u2013407, 1951.\nSantiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:\n//github.com/apple/ml-cifar-10-faster , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.\ntysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/\nhlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. Advances in neural information processing systems , 32, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,\n2016.\nMichael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps\nforward, 1 step back. Advances in neural information processing systems , 32, 2019.\n11\nA Network architecture\n1from torch import nn\n2\n3class Flatten (nn. Module ):\n4 def forward (self , x):\n5 return x. view (x. size (0) , -1)\n6\n7class Mul (nn. Module ):\n8 def __init__ (self , scale ):\n9 super (). __init__ ()\n10 self . scale = scale\n11 def forward (self , x):\n12 return x * self . scale\n13\n14def conv (ch_in , ch_out ):\n15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,\n16 padding =\u2019same \u2019, bias = False )\n17\n18def make_net ():\n19 act = lambda : nn. GELU ()\n20 bn = lambda ch: nn. BatchNorm2d (ch)\n21 return nn. Sequential (\n22 nn. Sequential (\n23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),\n24 act () ,\n25 ),\n26 nn. Sequential (\n27 conv (24 , 64) ,\n28 nn. MaxPool2d (2) ,\n29 bn (64) , act () ,\n30 conv (64 , 64) ,\n31 bn (64) , act () ,\n32 ),\n33 nn. Sequential (\n34 conv (64 , 256) ,\n35 nn. MaxPool2d (2) ,\n36 bn (256) , act () ,\n37 conv (256 , 256) ,\n38 bn (256) , act () ,\n39 ),\n40 nn. Sequential (\n41 conv (256 , 256) ,\n42 nn. MaxPool2d (2) ,\n43 bn (256) , act () ,\n44 conv (256 , 256) ,\n45 bn (256) , act () ,\n46 ),\n47 nn. MaxPool2d (3) ,\n48 Flatten () ,\n49 nn. Linear (256 , 10, bias = False ),\n50 Mul (1/9) ,\n51 )\nListing 3: Minimal PyTorch code for the network architecture used by airbench94 .\nWe note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock\ntime. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel\nsize of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this\ndoes not improve the wallclock training time on an A100. Reducing the batch size is another easy\nway to save FLOPs but not wallclock time.\n12\nDataset Flipping? Cutout? ResNet-18 airbench96\nCIFAR-10 Yes No 95.55% 95.61%\nCIFAR-10 Yes Yes 96.01% 96.05%\nCIFAR-100 Yes No 77.54% 79.27%\nCIFAR-100 Yes Yes 78.04% 79.76%\nCINIC-10 Yes No 87.58% 87.78%\nCINIC-10 Yes Yes not measured 88.22%\nSVHN No No 97.35% 97.38%\nSVHN No Yes not measured 97.64%\nTable 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We\ndirectly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off\nflipping for SVHN).\nB Extra dataset experiments\nWe developed airbench with the singular goal of maximizing training speed on CIFAR-10. To\nfind out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its\nperformance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-\n10 (Darlow et al., 2018).\nOn CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both\nthe case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not\n(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than\nResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say\nthat it generalizes.\nWe compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.\nWe do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal\non CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better\nperformance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where\nairbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is\nused and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows\nstrong generalization to other tasks.\nWe note that this comparison between airbench96 and ResNet-18 training is fair in the sense that\nit does demonstrate that the former has good generalization, but unfair in the sense that it does not\nindicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time\naugmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training\nwould outperform airbench96 if it were run using test-time augmentation. However, it also takes\n5-10 times longer to complete. The decision of which to use may be situational.\nThe accuracy values we report for ResNet-18 training are from the following sources. We tried\nto select the highest values we could find for each setting. Moreau et al. (2022) reports attaining\n95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports\nattaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on\nCIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.\nC ImageNet training details\nOur ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/\nlibffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and\nmomentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution\n160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of\ntraining. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.\n13\nHyperparameters Flipping augmentation option\nEpochs Cutout TTA None Random Alternating\n10 No No 92.3053 93.0988 93.2798\n20 No No 92.8166 93.7446 93.8652\n40 No No 93.0143 94.0133 94.0729\n80 No No 93.0612 94.1169 94.1628\n10 No Yes 93.4071 93.9488 94.0186\n20 No Yes 93.8528 94.5565 94.6530\n40 No Yes 94.0381 94.7803 94.8203\n80 No Yes 94.0638 94.8506 94.8676\n10 Yes No 91.8487 92.0402 92.1374\n20 Yes No 92.8474 93.3825 93.4876\n40 Yes No 93.2675 94.1014 94.1952\n80 Yes No 93.4193 94.4311 94.5204\n10 Yes Yes 92.6455 92.7780 92.8103\n20 Yes Yes 93.7862 94.1306 94.1670\n40 Yes Yes 94.3090 94.8511 94.8960\n80 Yes Yes 94.5253 95.1839 95.2362\nTable 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a\nmean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise\ndifference in means is statistically significant.\nD Extra tables & figures\nFigure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in\nSection 5.3.\nE Complete training code\n1\"\"\"\n2airbench94 .py\n33.83 s runtime on an A100 ; 0.36 PFLOPs .\n4Evidence for validity : 94.01 average accuracy in n =1000 runs .\n5\n6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB\nwith the following nvidia -smi:\n7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :\n11.7\n14\n8torch . __version__ == \u20192.1.2+ cu118 \u2019\n9\"\"\"\n10\n11# ############################################\n12# Setup / Hyperparameters #\n13# ############################################\n14\n15import os\n16import sys\n17import uuid\n18from math import ceil\n19\n20import torch\n21from torch import nn\n22import torch .nn. functional as F\n23import torchvision\n24import torchvision . transforms as T\n25\n26torch . backends . cudnn . benchmark = True\n27\n28\"\"\"\n29We express the main training hyperparameters ( batch size , learning\nrate , momentum , and weight decay ) in decoupled form , so that each\none can be tuned independently . This accomplishes the following :\n30* Assuming time - constant gradients , the average step size is decoupled\nfrom everything but the lr.\n31* The size of the weight decay update is decoupled from everything but\nthe wd.\n32In constrast , normally when we increase the ( Nesterov ) momentum , this\nalso scales up the step size proportionally to 1 + 1 / (1 -\nmomentum ), meaning we cannot change momentum without having to re -\ntune the learning rate . Similarly , normally when we increase the\nlearning rate this also increases the size of the weight decay ,\nrequiring a proportional decrease in the wd to maintain the same\ndecay strength .\n33\n34The practical impact is that hyperparameter tuning is faster , since\nthis parametrization allows each one to be tuned independently .\nSee https :// myrtle .ai/ learn /how -to -train -your - resnet -5-\nhyperparameters /.\n35\"\"\"\n36\n37hyp = {\n38 \u2019opt \u2019: {\n39 \u2019 train_epochs \u2019: 9.9 ,\n40 \u2019batch_size \u2019: 1024 ,\n41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples\n42 \u2019momentum \u2019: 0.85 ,\n43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (\ndecoupled from learning rate )\n44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not\nweight decay ) for BatchNorm biases\n45 \u2019 label_smoothing \u2019: 0.2 ,\n46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the\nwhitening layer bias before freezing\n47 },\n48 \u2019aug \u2019: {\n49 \u2019flip \u2019: True ,\n50 \u2019translate \u2019: 2,\n51 },\n52 \u2019net \u2019: {\n53 \u2019widths \u2019: {\n54 \u2019block1 \u2019: 64,\n55 \u2019block2 \u2019: 256 ,\n56 \u2019block3 \u2019: 256 ,\n15\n57 },\n58 \u2019 batchnorm_momentum \u2019: 0.6 ,\n59 \u2019 scaling_factor \u2019: 1/9 ,\n60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :\n0= none , 1= mirror , 2= mirror + translate\n61 },\n62}\n63\n64# ############################################\n65# DataLoader #\n66# ############################################\n67\n68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )\n69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )\n70\n71def batch_flip_lr ( inputs ):\n72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .\nview (-1, 1, 1, 1)\n73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\n74\n75def batch_crop (images , crop_size ):\n76 r = ( images . size ( -1) - crop_size )//2\n77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =\nimages . device )\n78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),\ndevice = images .device , dtype = images . dtype )\n79 # The two cropping methods in this if - else produce equivalent\nresults , but the second is faster for r > 2.\n80 if r <= 2:\n81 for sy in range (-r, r +1):\n82 for sx in range (-r, r +1):\n83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)\n84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size\n, r+sx:r+sx+ crop_size ]\n85 else :\n86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size\n+2* r), device = images .device , dtype = images . dtype )\n87 for s in range (-r, r +1):\n88 mask = ( shifts [:, 0] == s)\n89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]\n90 for s in range (-r, r +1):\n91 mask = ( shifts [:, 1] == s)\n92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+\ncrop_size ]\n93 return images_out\n94\n95class CifarLoader :\n96 \"\"\"\n97 GPU - accelerated dataloader for CIFAR -10 which implements\nalternating flip augmentation .\n98 \"\"\"\n99\n100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,\ndrop_last =None , shuffle =None , gpu =0):\n101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .\npt \u2019)\n102 if not os. path . exists ( data_path ):\n103 dset = torchvision . datasets . CIFAR10 (path , download =True ,\ntrain = train )\n104 images = torch . tensor ( dset . data )\n105 labels = torch . tensor ( dset . targets )\n106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:\ndset . classes }, data_path )\n107\n108 data = torch . load ( data_path , map_location = torch . device (gpu))\n16\n109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [\n\u2019labels \u2019], data [\u2019classes \u2019]\n110 # It \u2019s faster to load + process uint8 data than to load\npreprocessed fp16 data\n111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).\nto( memory_format = torch . channels_last )\n112\n113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )\n114 self . proc_images = {} # Saved results of image processing to\nbe done on the first epoch\n115 self . epoch = 0\n116\n117 self . aug = aug or {}\n118 for k in self .aug . keys ():\n119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019\n% k\n120\n121 self . batch_size = batch_size\n122 self . drop_last = train if drop_last is None else drop_last\n123 self . shuffle = train if shuffle is None else shuffle\n124\n125 def __len__ ( self ):\n126 return len( self . images )// self . batch_size if self . drop_last\nelse ceil (len( self . images )/ self . batch_size )\n127\n128 def __iter__ ( self ):\n129\n130 if self . epoch == 0:\n131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .\nimages )\n132 # Randomly flip all images on the first epoch as according\nto definition of alternating flip\n133 if self .aug.get(\u2019flip \u2019, False ):\n134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (\nimages )\n135 # Pre -pad images to save time when doing random\ntranslation\n136 pad = self .aug.get (\u2019translate \u2019, 0)\n137 if pad > 0:\n138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019\nreflect \u2019)\n139\n140 if self .aug.get(\u2019translate \u2019, 0) > 0:\n141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .\nshape [ -2])\n142 elif self .aug.get (\u2019flip \u2019, False ):\n143 images = self . proc_images [\u2019flip \u2019]\n144 else :\n145 images = self . proc_images [\u2019norm \u2019]\n146 if self .aug.get(\u2019flip \u2019, False ):\n147 if self . epoch % 2 == 1:\n148 images = images . flip ( -1)\n149\n150 self . epoch += 1\n151\n152 indices = ( torch . randperm if self . shuffle else torch . arange )(\nlen ( images ), device = images . device )\n153 for i in range (len ( self )):\n154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]\n155 yield ( images [ idxs ], self . labels [ idxs ])\n156\n157# ############################################\n158# Network Components #\n159# ############################################\n160\n17\n161class Flatten (nn. Module ):\n162 def forward (self , x):\n163 return x. view (x. size (0) , -1)\n164\n165class Mul (nn. Module ):\n166 def __init__ (self , scale ):\n167 super (). __init__ ()\n168 self . scale = scale\n169 def forward (self , x):\n170 return x * self . scale\n171\n172class BatchNorm (nn. BatchNorm2d ):\n173 def __init__ (self , num_features , momentum , eps =1e -12 ,\n174 weight =False , bias = True ):\n175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )\n176 self . weight . requires_grad = weight\n177 self . bias . requires_grad = bias\n178 # Note that PyTorch already initializes the weights to one and\nbiases to zero\n179\n180class Conv (nn. Conv2d ):\n181 def __init__ (self , in_channels , out_channels , kernel_size =3,\npadding =\u2019same \u2019, bias = False ):\n182 super (). __init__ ( in_channels , out_channels , kernel_size =\nkernel_size , padding = padding , bias = bias )\n183\n184 def reset_parameters ( self ):\n185 super (). reset_parameters ()\n186 if self . bias is not None :\n187 self . bias . data . zero_ ()\n188 w = self . weight . data\n189 torch .nn. init . dirac_ (w[:w. size (1) ])\n190\n191class ConvGroup (nn. Module ):\n192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):\n193 super (). __init__ ()\n194 self . conv1 = Conv ( channels_in , channels_out )\n195 self . pool = nn. MaxPool2d (2)\n196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )\n197 self . conv2 = Conv ( channels_out , channels_out )\n198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )\n199 self . activ = nn. GELU ()\n200\n201 def forward (self , x):\n202 x = self . conv1 (x)\n203 x = self . pool (x)\n204 x = self . norm1 (x)\n205 x = self . activ (x)\n206 x = self . conv2 (x)\n207 x = self . norm2 (x)\n208 x = self . activ (x)\n209 return x\n210\n211# ############################################\n212# Network Definition #\n213# ############################################\n214\n215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019\n][ \u2019 batchnorm_momentum \u2019]):\n216 whiten_kernel_size = 2\n217 whiten_width = 2 * 3 * whiten_kernel_size **2\n218 net = nn. Sequential (\n219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True\n),\n220 nn. GELU () ,\n18\n221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],\nbatchnorm_momentum ),\n222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],\nbatchnorm_momentum ),\n223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],\nbatchnorm_momentum ),\n224 nn. MaxPool2d (3) ,\n225 Flatten () ,\n226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),\n227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,\n228 )\n229 net [0]. weight . requires_grad = False\n230 net = net. half (). cuda ()\n231 net = net.to( memory_format = torch . channels_last )\n232 for mod in net. modules ():\n233 if isinstance (mod , BatchNorm ):\n234 mod . float ()\n235 return net\n236\n237# ############################################\n238# Whitening Conv Initialization #\n239# ############################################\n240\n241def get_patches (x, patch_shape ):\n242 c, (h, w) = x. shape [1] , patch_shape\n243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h\n,w). float ()\n244\n245def get_whitening_parameters ( patches ):\n246 n,c,h,w = patches . shape\n247 patches_flat = patches . view (n, -1)\n248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n\n249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance\n, UPLO =\u2019U\u2019)\n250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.\nreshape (c*h*w,c,h,w). flip (0)\n251\n252def init_whitening_conv (layer , train_set , eps =5e -4):\n253 patches = get_patches ( train_set , patch_shape = layer . weight . data .\nshape [2:])\n254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )\n255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)\n256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -\neigenvectors_scaled ))\n257\n258# ###########################################\n259# Lookahead #\n260# ###########################################\n261\n262class LookaheadState :\n263 def __init__ (self , net):\n264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().\nitems ()}\n265\n266 def update (self , net , decay ):\n267 for ema_param , net_param in zip( self . net_ema . values () , net.\nstate_dict (). values ()):\n268 if net_param . dtype in ( torch .half , torch . float ):\n269 ema_param . lerp_ ( net_param , 1- decay )\n270 net_param . copy_ ( ema_param )\n271\n272# ###########################################\n273# Logging #\n274# ###########################################\n275\n19\n276def print_columns ( columns_list , is_head =False , is_final_entry = False ):\n277 print_string = \u2019\u2019\n278 for col in columns_list :\n279 print_string += \u2019| %s \u2019 % col\n280 print_string += \u2019|\u2019\n281 if is_head :\n282 print (\u2019-\u2019*len( print_string ))\n283 print ( print_string )\n284 if is_head or is_final_entry :\n285 print (\u2019-\u2019*len( print_string ))\n286\n287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,\n\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]\n288def print_training_details ( variables , is_final_entry ):\n289 formatted = []\n290 for col in logging_columns_list :\n291 var = variables .get (col. strip () , None )\n292 if type (var) in (int , str):\n293 res = str(var)\n294 elif type (var) is float :\n295 res = \u2019{:0.4 f}\u2019. format ( var)\n296 else :\n297 assert var is None\n298 res = \u2019\u2019\n299 formatted . append (res. rjust (len (col)))\n300 print_columns ( formatted , is_final_entry = is_final_entry )\n301\n302# ###########################################\n303# Evaluation #\n304# ###########################################\n305\n306def infer (model , loader , tta_level =0) :\n307 \"\"\"\n308 Test - time augmentation strategy (for tta_level =2):\n309 1. Flip / mirror the image left -to - right (50% of the time ).\n310 2. Translate the image by one pixel either up -and - left or down -and\n- right (50% of the time , i.e. both happen 25% of the time ).\n311\n312 This creates 6 views per image ( left / right times the two\ntranslations and no - translation ), which we evaluate and then\nweight according to the given probabilities .\n313 \"\"\"\n314\n315 def infer_basic (inputs , net):\n316 return net( inputs ). clone ()\n317\n318 def infer_mirror (inputs , net):\n319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))\n320\n321 def infer_mirror_translate ( inputs , net ):\n322 logits = infer_mirror (inputs , net )\n323 pad = 1\n324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)\n325 inputs_translate_list = [\n326 padded_inputs [:, :, 0:32 , 0:32] ,\n327 padded_inputs [:, :, 2:34 , 2:34] ,\n328 ]\n329 logits_translate_list = [ infer_mirror ( inputs_translate , net)\n330 for inputs_translate in\ninputs_translate_list ]\n331 logits_translate = torch . stack ( logits_translate_list ). mean (0)\n332 return 0.5 * logits + 0.5 * logits_translate\n333\n334 model . eval ()\n335 test_images = loader . normalize ( loader . images )\n20\n336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][\ntta_level ]\n337 with torch . no_grad ():\n338 return torch .cat ([ infer_fn (inputs , model ) for inputs in\ntest_images . split (2000) ])\n339\n340def evaluate (model , loader , tta_level =0) :\n341 logits = infer (model , loader , tta_level )\n342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()\n343\n344# ###########################################\n345# Training #\n346# ###########################################\n347\n348def main (run):\n349\n350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]\n351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]\n352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]\n353 # Assuming gradients are constant in time , for Nesterov momentum ,\nthe below ratio is how much larger the default steps will be than\nthe underlying per - example gradients . We divide the learning rate\nby this ratio in order to ensure steps are the same scale as\ngradients , regardless of the choice of momentum .\n354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))\n355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning\nrate for PyTorch SGD\n356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale\n357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]\n358\n359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019\nlabel_smoothing \u2019], reduction =\u2019none \u2019)\n360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)\n361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =\nbatch_size , aug=hyp[\u2019aug \u2019])\n362 if run == \u2019warmup \u2019:\n363 # The only purpose of the first run is to warmup , so we can\nuse dummy data\n364 train_loader . labels = torch . randint (0, 10, size =( len (\ntrain_loader . labels ) ,), device = train_loader . labels . device )\n365 total_train_steps = ceil (len( train_loader ) * epochs )\n366\n367 model = make_net ()\n368 current_steps = 0\n369\n370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in\nk and p. requires_grad ]\n371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019\nnot in k and p. requires_grad ]\n372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,\nweight_decay =wd/ lr_biases ),\n373 dict ( params = other_params , lr=lr , weight_decay =wd/\nlr)]\n374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,\nnesterov = True )\n375\n376 def triangle (steps , start =0, end =0, peak =0.5) :\n377 xp = torch . tensor ([0 , int( peak * steps ), steps ])\n378 fp = torch . tensor ([ start , 1, end ])\n379 x = torch . arange (1+ steps )\n380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])\n381 b = fp [: -1] - (m * xp [: -1])\n382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1\n383 indices = torch . clamp ( indices , 0, len(m) - 1)\n384 return m[ indices ] * x + b[ indices ]\n21\n385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,\npeak =0.23)\n386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:\nlr_schedule [i])\n387\n388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /\ntotal_train_steps )**3\n389 lookahead_state = LookaheadState ( model )\n390\n391 # For accurately timing GPU code\n392 starter = torch . cuda . Event ( enable_timing = True )\n393 ender = torch . cuda . Event ( enable_timing = True )\n394 total_time_seconds = 0.0\n395\n396 # Initialize the first layer using statistics of training images\n397 starter . record ()\n398 train_images = train_loader . normalize ( train_loader . images [:5000])\n399 init_whitening_conv ( model [0] , train_images )\n400 ender . record ()\n401 torch . cuda . synchronize ()\n402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n403\n404 for epoch in range ( ceil ( epochs )):\n405\n406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019\nwhiten_bias_epochs \u2019])\n407\n408 # ###################\n409 # Training #\n410 # ###################\n411\n412 starter . record ()\n413\n414 model . train ()\n415 for inputs , labels in train_loader :\n416\n417 outputs = model ( inputs )\n418 loss = loss_fn ( outputs , labels ).sum ()\n419 optimizer . zero_grad ( set_to_none = True )\n420 loss . backward ()\n421 optimizer . step ()\n422 scheduler . step ()\n423\n424 current_steps += 1\n425\n426 if current_steps % 5 == 0:\n427 lookahead_state . update (model , decay = alpha_schedule [\ncurrent_steps ]. item ())\n428\n429 if current_steps >= total_train_steps :\n430 if lookahead_state is not None :\n431 lookahead_state . update (model , decay =1.0)\n432 break\n433\n434 ender . record ()\n435 torch . cuda . synchronize ()\n436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n437\n438 # ###################\n439 # Evaluation #\n440 # ###################\n441\n442 # Print the accuracy and loss from the last training batch of\nthe epoch\n22\n443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().\nmean (). item ()\n444 train_loss = loss . item () / batch_size\n445 val_acc = evaluate (model , test_loader , tta_level =0)\n446 print_training_details ( locals () , is_final_entry = False )\n447 run = None # Only print the run number once\n448\n449 # ###################\n450 # TTA Evaluation #\n451 # ###################\n452\n453 starter . record ()\n454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019\ntta_level \u2019])\n455 ender . record ()\n456 torch . cuda . synchronize ()\n457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n458\n459 epoch = \u2019eval \u2019\n460 print_training_details ( locals () , is_final_entry = True )\n461\n462 return tta_val_acc\n463\n464if __name__ == \" __main__ \":\n465 with open (sys. argv [0]) as f:\n466 code = f. read ()\n467\n468 print_columns ( logging_columns_list , is_head = True )\n469 main (\u2019warmup \u2019)\n470 accs = torch . tensor ([ main (run) for run in range (25) ])\n471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))\n472\n473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }\n474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))\n475 os. makedirs ( log_dir , exist_ok = True )\n476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)\n477 print (os. path . abspath ( log_path ))\n478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))\nListing 4: airbench94.py\n23\n\nCandidate Output: tasks=[TaskCandidate(name='Implement alternating flip data augmentation', description='Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in a deterministic alternating pattern across epochs, rather than randomly.', baseline='Standard random horizontal flipping augmentation', skills=['PyTorch', 'Computer Vision', 'Data Augmentation'], relevant_paper_text='We propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.', scoring_feasibility=8, llm_tractability=0.7, expert_tractability=0.9, layman_tractability=0.3, scoring_llm_tractability=0.8, asset_prerequisites=['CIFAR-10 dataset']), TaskCandidate(name='Implement patch-whitening initialization', description='Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper.', baseline='Standard random initialization', skills=['PyTorch', 'Linear Algebra', 'Neural Network Initialization'], relevant_paper_text='Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.', scoring_feasibility=7, llm_tractability=0.5, expert_tractability=0.8, layman_tractability=0.1, scoring_llm_tractability=0.6, asset_prerequisites=['CIFAR-10 dataset']), TaskCandidate(name='Implement multi-crop test-time augmentation', description='Implement the multi-crop test-time augmentation technique described in the paper, which involves evaluating the model on multiple augmented versions of each test image.', baseline='No test-time augmentation', skills=['PyTorch', 'Computer Vision', 'Test-Time Augmentation'], relevant_paper_text='To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.', scoring_feasibility=8, llm_tractability=0.8, expert_tractability=0.9, layman_tractability=0.4, scoring_llm_tractability=0.9, asset_prerequisites=['CIFAR-10 dataset', 'Trained model'])]\n\nRequirements:\nExtract all promising task candidates from paper. Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\n\n    A good task candidate:\n    - describes a task that an agent can implement\n    - typically a programming task, e.g. modifying a 'baseline' implementation\n    - which reflects or mirrors actual tasks undertaken to produce the research paper\n    - and requires research and/or engineering expertise to implement correctly\n    - for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\n    - ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\n    - the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\n    \n\nAttempt Num: 0\n\nPrevious Critiques: \n\nPast Error in Should Retry: ValueError('Respond with true or false')\n\nSelf Critique:"
  },
  {
    "Response": "Self Critique:\n\n1. Relevance and Scope: The task candidates extracted are highly relevant to the paper and reflect actual research tasks undertaken. They cover key innovations described in the paper: alternating flip data augmentation, patch-whitening initialization, and multi-crop test-time augmentation.\n\n2. Implementability: All three tasks are implementable programming tasks that modify baseline implementations. They require research and engineering expertise to implement correctly.\n\n3. Scoring Feasibility: The tasks seem feasible to score objectively. For example, the alternating flip augmentation could be scored based on whether the implementation correctly flips images in the described deterministic pattern across epochs. The patch-whitening initialization could be scored based on whether the first layer's weights match the described eigenvector initialization. The multi-crop TTA could be scored based on whether it correctly generates and weights the six augmented views.\n\n4. Difficulty Balance: The tasks appear to strike a good balance between being challenging to implement but relatively straightforward to score. Implementing these techniques correctly requires understanding the concepts and translating them into code, while scoring can be done by checking if the implementation matches the paper's description.\n\n5. Completeness: The tasks cover three major innovations from the paper. However, there may be other potential task candidates that could have been extracted, such as implementing the decoupled learning rate and weight decay optimization or the Lookahead optimizer.\n\n6. Detail Level: The task descriptions provide a good overview, but could potentially benefit from more specific implementation details or evaluation criteria.\n\n7. Baseline Specification: Each task includes a baseline implementation, which is helpful for understanding the starting point and the required modifications.\n\nOverall, the extracted task candidates appear to be well-suited for evaluating research and engineering capabilities related to the paper's innovations. They require expertise to implement correctly while being feasible to score objectively.\n\nShould Retry: false"
  },
  {
    "Request": "Given the fields `task_candidate`, `guidance`, produce the fields `task_rubric`.\n\n---\n\nFollow the following format.\n\nTask Candidate: ${task_candidate}\nGuidance: ${guidance}\nTask Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"description\": \"Objective, programatically verifiable criterion for evaluating a task\\nimplementation. Criteria may call a function and use outputs and observed\\nside effects to determine if the criterion is satisfied, but will\\notherwise not be able to make any assumptions about how the implementation\\nworks - it is a black box.\\nCriteria should never be subjective or require examining the actual code in \\nthe implementation. It must be strictly empirical.\", \"properties\": {\"criterion\": {\"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"\\nList of criteria for evaluating task implementation; importance should sum to 100.\\n\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\n\n---\n\nTask Candidate: {\"name\":\"Implement alternating flip data augmentation\",\"description\":\"Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in a deterministic alternating pattern across epochs, rather than randomly.\",\"baseline\":\"Standard random horizontal flipping augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Data Augmentation\"],\"relevant_paper_text\":\"We propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\",\"scoring_feasibility\":8,\"llm_tractability\":0.7,\"expert_tractability\":0.9,\"layman_tractability\":0.3,\"scoring_llm_tractability\":0.8,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\nGuidance:"
  },
  {
    "Response": "Here is a proposed task rubric based on the given task candidate and guidance:\n\nTask Rubric: {\n  \"rubric\": [\n    {\n      \"criterion\": \"Correctly implements alternating flip pattern across epochs as described in the paper\",\n      \"importance\": 40\n    },\n    {\n      \"criterion\": \"Maintains consistent flipping for each image across odd and even epochs after the first epoch\",\n      \"importance\": 25\n    },\n    {\n      \"criterion\": \"Correctly handles the first epoch by randomly flipping 50% of inputs\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"Works with the CIFAR-10 dataset\",\n      \"importance\": 10\n    },\n    {\n      \"criterion\": \"Integrates smoothly with existing PyTorch data augmentation pipelines\",\n      \"importance\": 10\n    }\n  ]\n}"
  },
  {
    "Request": "Make a very succinct json object that validates with the following schema\n\n---\n\nFollow the following format.\n\nJson Schema: ${json_schema}\nJson Object: ${json_object}\n\n---\n\nJson Schema: {\"$defs\": {\"TaskCriterion\": {\"description\": \"Objective, programatically verifiable criterion for evaluating a task\\nimplementation. Criteria may call a function and use outputs and observed\\nside effects to determine if the criterion is satisfied, but will\\notherwise not be able to make any assumptions about how the implementation\\nworks - it is a black box.\\nCriteria should never be subjective or require examining the actual code in \\nthe implementation. It must be strictly empirical.\", \"properties\": {\"criterion\": {\"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"\\nList of criteria for evaluating task implementation; importance should sum to 100.\\n\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\nJson Object:"
  },
  {
    "Response": "Json Object: {\n  \"rubric\": [\n    {\n      \"criterion\": \"Function returns correct output\",\n      \"importance\": 60\n    },\n    {\n      \"criterion\": \"Handles edge cases properly\",\n      \"importance\": 40\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `task_candidate`, `guidance`, produce the fields `task_rubric`.\n\n---\n\nFollow the following format.\n\nTask Candidate: ${task_candidate}\n\nGuidance: ${guidance}\n\nPast Error in Task Rubric: An error to avoid in the future\n\nTask Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"description\": \"Objective, programatically verifiable criterion for evaluating a task\\nimplementation. Criteria may call a function and use outputs and observed\\nside effects to determine if the criterion is satisfied, but will\\notherwise not be able to make any assumptions about how the implementation\\nworks - it is a black box.\\nCriteria should never be subjective or require examining the actual code in \\nthe implementation. It must be strictly empirical.\", \"properties\": {\"criterion\": {\"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"\\nList of criteria for evaluating task implementation; importance should sum to 100.\\n\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\n\n---\n\nTask Candidate: {\"name\":\"Implement alternating flip data augmentation\",\"description\":\"Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in a deterministic alternating pattern across epochs, rather than randomly.\",\"baseline\":\"Standard random horizontal flipping augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Data Augmentation\"],\"relevant_paper_text\":\"We propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\",\"scoring_feasibility\":8,\"llm_tractability\":0.7,\"expert_tractability\":0.9,\"layman_tractability\":0.3,\"scoring_llm_tractability\":0.8,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\n\nGuidance: \n\nPast Error in Task Rubric: ValueError('json output should start and end with { and }')\n\nTask Rubric:"
  },
  {
    "Response": "{\n  \"rubric\": [\n    {\n      \"criterion\": \"The implementation correctly flips 50% of inputs randomly in the first epoch\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"On even-numbered epochs (2,4,6,...), the implementation flips only those inputs which were not flipped in the first epoch\",\n      \"importance\": 25\n    },\n    {\n      \"criterion\": \"On odd-numbered epochs (3,5,7,...), the implementation flips only those inputs which were flipped in the first epoch\",\n      \"importance\": 25\n    },\n    {\n      \"criterion\": \"The implementation maintains consistency in flipping patterns across multiple runs with the same random seed\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"The implementation works correctly with the CIFAR-10 dataset\",\n      \"importance\": 10\n    },\n    {\n      \"criterion\": \"The implementation is compatible with PyTorch's data augmentation pipeline\",\n      \"importance\": 5\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.\n\n---\n\nFollow the following format.\n\nTask: ${task}\n\nRubric: ${rubric}\n\nReasoning: Let's think step by step in order to ${produce the justification}. We ...\n\nScorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified? The scoring function will only have access to the function implementing the task, and we cannot make any assumptions about how the task is implemented by the agent. We only want to validate correctness.\n\nJustification: Justification for scorable judgement\n\n---\n\nTask: {\"name\":\"Implement alternating flip data augmentation\",\"description\":\"Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in a deterministic alternating pattern across epochs, rather than randomly.\",\"baseline\":\"Standard random horizontal flipping augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Data Augmentation\"],\"relevant_paper_text\":\"We propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\",\"scoring_feasibility\":8,\"llm_tractability\":0.7,\"expert_tractability\":0.9,\"layman_tractability\":0.3,\"scoring_llm_tractability\":0.8,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\n\nRubric: criterion='The implementation correctly flips 50% of inputs randomly in the first epoch' importance=20.0 criterion='On even-numbered epochs (2,4,6,...), the implementation flips only those inputs which were not flipped in the first epoch' importance=25.0 criterion='On odd-numbered epochs (3,5,7,...), the implementation flips only those inputs which were flipped in the first epoch' importance=25.0 criterion='The implementation maintains consistency in flipping patterns across multiple runs with the same random seed' importance=15.0 criterion='The implementation works correctly with the CIFAR-10 dataset' importance=10.0 criterion=\"The implementation is compatible with PyTorch's data augmentation pipeline\" importance=5.0\n\nReasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification:\n\n1. We need to determine if it's possible to write a Python function that can objectively score an implementation of this task.\n\n2. The task involves implementing a specific data augmentation technique for image flipping across epochs.\n\n3. Key aspects that need to be verified:\n   a. Random flipping of 50% of inputs in the first epoch\n   b. Flipping pattern for even-numbered epochs\n   c. Flipping pattern for odd-numbered epochs\n   d. Consistency across runs with the same random seed\n   e. Compatibility with CIFAR-10 dataset and PyTorch\n\n4. To verify these aspects, we would need:\n   - Access to the implementation function\n   - Ability to run the function multiple times with controlled inputs\n   - Ability to track the flipping pattern across multiple epochs\n   - A way to set and control the random seed\n\n5. Challenges:\n   - We can't make assumptions about how the task is implemented\n   - We only have access to the function implementing the task\n\n6. Despite these challenges, it seems possible to create a scoring function that can:\n   - Generate a set of test images\n   - Run the implementation for multiple epochs\n   - Check the flipping patterns against expected behavior\n   - Verify consistency with repeated runs using the same seed\n\n7. The scoring function can be designed to be objective, based on the criteria specified in the rubric.\n\nBased on this reasoning, we can conclude that it is possible to write a Python function to objectively score an implementation of this task.\n\nScorable[True/False]: True\n\nJustification: It is possible to write a Python function to objectively score an implementation of this task. The scoring function can generate a set of test images and run the implementation over multiple epochs to verify the correct flipping patterns. It can check if 50% of inputs are randomly flipped in the first epoch, verify the correct flipping pattern for even and odd-numbered epochs, and ensure consistency across runs with the same random seed. The function can also test compatibility with the CIFAR-10 dataset and PyTorch's data augmentation pipeline. While we cannot make assumptions about the implementation details, we can design tests that focus on the expected input-output behavior and flipping patterns, which are objectively verifiable. The rubric provides clear, measurable criteria that can be translated into automated checks within the scoring function."
  },
  {
    "Request": "Given the fields `candidate_input`, `candidate_output`, `requirements`, `attempt_num`, `previous_critiques`, produce the fields `self_critique`, `should_retry`.\n\n---\n\nFollow the following format.\n\nCandidate Input: ${candidate_input}\n\nCandidate Output: ${candidate_output}\n\nRequirements: ${requirements}\n\nAttempt Num: ${attempt_num}\n\nPrevious Critiques: ${previous_critiques}\n\nSelf Critique: Self-critique of how well the candidate output satisfies the requirements given the input\n\nShould Retry: Should we attempt to generate the output again taking this critique into account? True/False (Respond with true or false)\n\n---\n\nCandidate Input: {\"name\":\"Implement alternating flip data augmentation\",\"description\":\"Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in a deterministic alternating pattern across epochs, rather than randomly.\",\"baseline\":\"Standard random horizontal flipping augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Data Augmentation\"],\"relevant_paper_text\":\"We propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\",\"scoring_feasibility\":8,\"llm_tractability\":0.7,\"expert_tractability\":0.9,\"layman_tractability\":0.3,\"scoring_llm_tractability\":0.8,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\n\nCandidate Output:\nPrediction(\n    task_rubric=TaskRubric(rubric=[TaskCriterion(criterion='The implementation correctly flips 50% of inputs randomly in the first epoch', importance=20.0), TaskCriterion(criterion='On even-numbered epochs (2,4,6,...), the implementation flips only those inputs which were not flipped in the first epoch', importance=25.0), TaskCriterion(criterion='On odd-numbered epochs (3,5,7,...), the implementation flips only those inputs which were flipped in the first epoch', importance=25.0), TaskCriterion(criterion='The implementation maintains consistency in flipping patterns across multiple runs with the same random seed', importance=15.0), TaskCriterion(criterion='The implementation works correctly with the CIFAR-10 dataset', importance=10.0), TaskCriterion(criterion=\"The implementation is compatible with PyTorch's data augmentation pipeline\", importance=5.0)])\n)\n\nRequirements:\nDefine a rubric for programming task evaluation, where criteria satisfy \n    Objective, programatically verifiable criterion for evaluating a task\n    implementation. Criteria may call a function and use outputs and observed\n    side effects to determine if the criterion is satisfied, but will\n    otherwise not be able to make any assumptions about how the implementation\n    works - it is a black box.\n    Criteria should never be subjective or require examining the actual code in \n    the implementation. It must be strictly empirical.\n    \n\nAttempt Num: 1\n\nPrevious Critiques:\nSelf Critique:\n\n1. Relevance and Scope: The task candidates extracted are highly relevant to the paper and reflect actual research tasks undertaken. They cover key innovations described in the paper: alternating flip data augmentation, patch-whitening initialization, and multi-crop test-time augmentation.\n\n2. Implementability: All three tasks are implementable programming tasks that modify baseline implementations. They require research and engineering expertise to implement correctly.\n\n3. Scoring Feasibility: The tasks seem feasible to score objectively. For example, the alternating flip augmentation could be scored based on whether the implementation correctly flips images in the described deterministic pattern across epochs. The patch-whitening initialization could be scored based on whether the first layer's weights match the described eigenvector initialization. The multi-crop TTA could be scored based on whether it correctly generates and weights the six augmented views.\n\n4. Difficulty Balance: The tasks appear to strike a good balance between being challenging to implement but relatively straightforward to score. Implementing these techniques correctly requires understanding the concepts and translating them into code, while scoring can be done by checking if the implementation matches the paper's description.\n\n5. Completeness: The tasks cover three major innovations from the paper. However, there may be other potential task candidates that could have been extracted, such as implementing the decoupled learning rate and weight decay optimization or the Lookahead optimizer.\n\n6. Detail Level: The task descriptions provide a good overview, but could potentially benefit from more specific implementation details or evaluation criteria.\n\n7. Baseline Specification: Each task includes a baseline implementation, which is helpful for understanding the starting point and the required modifications.\n\nOverall, the extracted task candidates appear to be well-suited for evaluating research and engineering capabilities related to the paper's innovations. They require expertise to implement correctly while being feasible to score objectively.\n\nSelf Critique:"
  },
  {
    "Response": "Self Critique: The candidate output provides a good starting point for a task rubric, but it has some areas for improvement:\n\n1. Objectivity: The criteria are generally objective and verifiable, which aligns well with the requirements.\n\n2. Programmatic Verifiability: While the criteria are stated in a way that could be programmatically verified, they don't explicitly mention how this verification would be done (e.g., through function calls or specific tests).\n\n3. Completeness: The rubric covers the main aspects of the alternating flip data augmentation technique as described in the input. However, it doesn't include a criterion for verifying that the implementation works with PyTorch's data augmentation pipeline, which was mentioned in the skills required.\n\n4. Specificity: Some criteria could be more specific. For example, the criterion about working with the CIFAR-10 dataset could specify what aspect of compatibility needs to be verified.\n\n5. Quantifiability: The importance values assigned to each criterion provide a good way to weight different aspects of the implementation, which is helpful for scoring.\n\n6. Black Box Evaluation: The criteria are generally formulated in a way that allows for black box evaluation, which aligns with the requirements.\n\n7. Empirical Focus: The criteria focus on observable behaviors rather than implementation details, which is in line with the requirement for empirical evaluation.\n\n8. Comprehensiveness: The rubric covers the main aspects of the task, but it could potentially include additional criteria to make it more comprehensive, such as performance benchmarks or compatibility with different batch sizes.\n\nShould Retry: true"
  },
  {
    "Request": "Given the fields `task_candidate`, `guidance`, produce the fields `task_rubric`.\n\n---\n\nFollow the following format.\n\nTask Candidate: ${task_candidate}\nGuidance: ${guidance}\nTask Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"description\": \"Objective, programatically verifiable criterion for evaluating a task\\nimplementation. Criteria may call a function and use outputs and observed\\nside effects to determine if the criterion is satisfied, but will\\notherwise not be able to make any assumptions about how the implementation\\nworks - it is a black box.\\nCriteria should never be subjective or require examining the actual code in \\nthe implementation. It must be strictly empirical.\", \"properties\": {\"criterion\": {\"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"\\nList of criteria for evaluating task implementation; importance should sum to 100.\\n\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\n\n---\n\nTask Candidate: {\"name\":\"Implement alternating flip data augmentation\",\"description\":\"Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in a deterministic alternating pattern across epochs, rather than randomly.\",\"baseline\":\"Standard random horizontal flipping augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Data Augmentation\"],\"relevant_paper_text\":\"We propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\",\"scoring_feasibility\":8,\"llm_tractability\":0.7,\"expert_tractability\":0.9,\"layman_tractability\":0.3,\"scoring_llm_tractability\":0.8,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\nGuidance:\nSelf Critique: The candidate output provides a good starting point for a task rubric, but it has some areas for improvement:\n\n1. Objectivity: The criteria are generally objective and verifiable, which aligns well with the requirements.\n\n2. Programmatic Verifiability: While the criteria are stated in a way that could be programmatically verified, they don't explicitly mention how this verification would be done (e.g., through function calls or specific tests).\n\n3. Completeness: The rubric covers the main aspects of the alternating flip data augmentation technique as described in the input. However, it doesn't include a criterion for verifying that the implementation works with PyTorch's data augmentation pipeline, which was mentioned in the skills required.\n\n4. Specificity: Some criteria could be more specific. For example, the criterion about working with the CIFAR-10 dataset could specify what aspect of compatibility needs to be verified.\n\n5. Quantifiability: The importance values assigned to each criterion provide a good way to weight different aspects of the implementation, which is helpful for scoring.\n\n6. Black Box Evaluation: The criteria are generally formulated in a way that allows for black box evaluation, which aligns with the requirements.\n\n7. Empirical Focus: The criteria focus on observable behaviors rather than implementation details, which is in line with the requirement for empirical evaluation.\n\n8. Comprehensiveness: The rubric covers the main aspects of the task, but it could potentially include additional criteria to make it more comprehensive, such as performance benchmarks or compatibility with different batch sizes.\nTask Rubric:"
  },
  {
    "Response": "{\n  \"rubric\": [\n    {\n      \"criterion\": \"The implementation correctly flips 50% of inputs randomly in the first epoch, as verified by comparing input and output tensors.\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"For even-numbered epochs (2,4,6,...), the implementation flips only those inputs that were not flipped in the first epoch, as verified by comparing input and output tensors across epochs.\",\n      \"importance\": 25\n    },\n    {\n      \"criterion\": \"For odd-numbered epochs (3,5,7,...), the implementation flips only those inputs that were flipped in the first epoch, as verified by comparing input and output tensors across epochs.\",\n      \"importance\": 25\n    },\n    {\n      \"criterion\": \"The implementation works correctly with the CIFAR-10 dataset, as verified by successfully processing a batch of CIFAR-10 images without errors.\",\n      \"importance\": 10\n    },\n    {\n      \"criterion\": \"The implementation is compatible with PyTorch's data augmentation pipeline, as verified by successfully integrating it into a PyTorch DataLoader without errors.\",\n      \"importance\": 10\n    },\n    {\n      \"criterion\": \"The implementation maintains consistent flipping patterns across multiple runs with the same random seed, as verified by comparing outputs from multiple executions.\",\n      \"importance\": 10\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.\n\n---\n\nFollow the following format.\n\nTask: ${task}\n\nRubric: ${rubric}\n\nReasoning: Let's think step by step in order to ${produce the justification}. We ...\n\nScorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified? The scoring function will only have access to the function implementing the task, and we cannot make any assumptions about how the task is implemented by the agent. We only want to validate correctness.\n\nJustification: Justification for scorable judgement\n\n---\n\nTask: {\"name\":\"Implement alternating flip data augmentation\",\"description\":\"Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in a deterministic alternating pattern across epochs, rather than randomly.\",\"baseline\":\"Standard random horizontal flipping augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Data Augmentation\"],\"relevant_paper_text\":\"We propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\",\"scoring_feasibility\":8,\"llm_tractability\":0.7,\"expert_tractability\":0.9,\"layman_tractability\":0.3,\"scoring_llm_tractability\":0.8,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\n\nRubric: criterion='The implementation correctly flips 50% of inputs randomly in the first epoch, as verified by comparing input and output tensors.' importance=20.0 criterion='For even-numbered epochs (2,4,6,...), the implementation flips only those inputs that were not flipped in the first epoch, as verified by comparing input and output tensors across epochs.' importance=25.0 criterion='For odd-numbered epochs (3,5,7,...), the implementation flips only those inputs that were flipped in the first epoch, as verified by comparing input and output tensors across epochs.' importance=25.0 criterion='The implementation works correctly with the CIFAR-10 dataset, as verified by successfully processing a batch of CIFAR-10 images without errors.' importance=10.0 criterion=\"The implementation is compatible with PyTorch's data augmentation pipeline, as verified by successfully integrating it into a PyTorch DataLoader without errors.\" importance=10.0 criterion='The implementation maintains consistent flipping patterns across multiple runs with the same random seed, as verified by comparing outputs from multiple executions.' importance=10.0\n\nReasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification:\n\n1. We need to consider if it's possible to objectively verify the key functionality of this task through a Python function.\n\n2. The task involves implementing a specific data augmentation technique for image flipping across epochs.\n\n3. We can verify the following key aspects:\n   a. Random flipping of 50% of inputs in the first epoch\n   b. Correct flipping pattern for even-numbered epochs\n   c. Correct flipping pattern for odd-numbered epochs\n   d. Compatibility with CIFAR-10 dataset\n   e. Integration with PyTorch's data augmentation pipeline\n   f. Consistency across multiple runs with the same random seed\n\n4. To verify these aspects, we would need:\n   - Access to the implementation function\n   - Ability to run the function over multiple epochs\n   - Access to input and output tensors\n   - Ability to set and control random seeds\n\n5. A scoring function could:\n   - Generate a set of test inputs\n   - Run the implementation over multiple epochs\n   - Compare input and output tensors to verify correct flipping patterns\n   - Check compatibility with CIFAR-10 and PyTorch\n   - Verify consistency with fixed random seeds\n\n6. The scoring function doesn't need to know the internal implementation details, only the expected behavior.\n\n7. All key aspects of the task can be objectively verified through input-output comparisons and behavior checks.\n\nScorable[True/False]: True\n\nJustification: This task is scorable because all key aspects of the alternating flip data augmentation technique can be objectively verified through a Python function. The scoring function can generate test inputs, run the implementation over multiple epochs, and compare input and output tensors to verify the correct flipping patterns for different epochs. It can also check compatibility with the CIFAR-10 dataset and PyTorch's data augmentation pipeline. The consistency across multiple runs with the same random seed can be verified by comparing outputs. Importantly, these verifications can be done without knowledge of the internal implementation details, focusing solely on the expected behavior of the function. The rubric provides clear, measurable criteria that can be programmatically checked, making it feasible to create an objective scoring function for this task."
  },
  {
    "Request": "Given the fields `candidate_input`, `candidate_output`, `requirements`, `attempt_num`, `previous_critiques`, produce the fields `self_critique`, `should_retry`.\n\n---\n\nFollow the following format.\n\nCandidate Input: ${candidate_input}\n\nCandidate Output: ${candidate_output}\n\nRequirements: ${requirements}\n\nAttempt Num: ${attempt_num}\n\nPrevious Critiques: ${previous_critiques}\n\nSelf Critique: Self-critique of how well the candidate output satisfies the requirements given the input\n\nShould Retry: Should we attempt to generate the output again taking this critique into account? True/False (Respond with true or false)\n\n---\n\nCandidate Input: {\"name\":\"Implement alternating flip data augmentation\",\"description\":\"Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in a deterministic alternating pattern across epochs, rather than randomly.\",\"baseline\":\"Standard random horizontal flipping augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Data Augmentation\"],\"relevant_paper_text\":\"We propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\",\"scoring_feasibility\":8,\"llm_tractability\":0.7,\"expert_tractability\":0.9,\"layman_tractability\":0.3,\"scoring_llm_tractability\":0.8,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\n\nCandidate Output:\nPrediction(\n    task_rubric=TaskRubric(rubric=[TaskCriterion(criterion='The implementation correctly flips 50% of inputs randomly in the first epoch, as verified by comparing input and output tensors.', importance=20.0), TaskCriterion(criterion='For even-numbered epochs (2,4,6,...), the implementation flips only those inputs that were not flipped in the first epoch, as verified by comparing input and output tensors across epochs.', importance=25.0), TaskCriterion(criterion='For odd-numbered epochs (3,5,7,...), the implementation flips only those inputs that were flipped in the first epoch, as verified by comparing input and output tensors across epochs.', importance=25.0), TaskCriterion(criterion='The implementation works correctly with the CIFAR-10 dataset, as verified by successfully processing a batch of CIFAR-10 images without errors.', importance=10.0), TaskCriterion(criterion=\"The implementation is compatible with PyTorch's data augmentation pipeline, as verified by successfully integrating it into a PyTorch DataLoader without errors.\", importance=10.0), TaskCriterion(criterion='The implementation maintains consistent flipping patterns across multiple runs with the same random seed, as verified by comparing outputs from multiple executions.', importance=10.0)])\n)\n\nRequirements:\nDefine a rubric for programming task evaluation, where criteria satisfy \n    Objective, programatically verifiable criterion for evaluating a task\n    implementation. Criteria may call a function and use outputs and observed\n    side effects to determine if the criterion is satisfied, but will\n    otherwise not be able to make any assumptions about how the implementation\n    works - it is a black box.\n    Criteria should never be subjective or require examining the actual code in \n    the implementation. It must be strictly empirical.\n    \n\nAttempt Num: 2\n\nPrevious Critiques:\nSelf Critique:\n\n1. Relevance and Scope: The task candidates extracted are highly relevant to the paper and reflect actual research tasks undertaken. They cover key innovations described in the paper: alternating flip data augmentation, patch-whitening initialization, and multi-crop test-time augmentation.\n\n2. Implementability: All three tasks are implementable programming tasks that modify baseline implementations. They require research and engineering expertise to implement correctly.\n\n3. Scoring Feasibility: The tasks seem feasible to score objectively. For example, the alternating flip augmentation could be scored based on whether the implementation correctly flips images in the described deterministic pattern across epochs. The patch-whitening initialization could be scored based on whether the first layer's weights match the described eigenvector initialization. The multi-crop TTA could be scored based on whether it correctly generates and weights the six augmented views.\n\n4. Difficulty Balance: The tasks appear to strike a good balance between being challenging to implement but relatively straightforward to score. Implementing these techniques correctly requires understanding the concepts and translating them into code, while scoring can be done by checking if the implementation matches the paper's description.\n\n5. Completeness: The tasks cover three major innovations from the paper. However, there may be other potential task candidates that could have been extracted, such as implementing the decoupled learning rate and weight decay optimization or the Lookahead optimizer.\n\n6. Detail Level: The task descriptions provide a good overview, but could potentially benefit from more specific implementation details or evaluation criteria.\n\n7. Baseline Specification: Each task includes a baseline implementation, which is helpful for understanding the starting point and the required modifications.\n\nOverall, the extracted task candidates appear to be well-suited for evaluating research and engineering capabilities related to the paper's innovations. They require expertise to implement correctly while being feasible to score objectively.\n\nSelf Critique:"
  },
  {
    "Response": "Self Critique: The candidate output provides a good starting point for a task rubric, but there are some areas for improvement:\n\n1. Objectivity: The criteria are generally objective and verifiable, which is good. They focus on observable outcomes rather than implementation details.\n\n2. Specificity: Some criteria could be more specific. For example, the criterion about working with CIFAR-10 could specify the exact size of the batch to be processed.\n\n3. Completeness: The rubric covers the main aspects of the alternating flip augmentation technique, including the behavior for the first epoch and subsequent even and odd epochs.\n\n4. Measurability: While the criteria are verifiable, they don't specify exact methods for verification. For instance, how exactly should one compare input and output tensors?\n\n5. Importance Weighting: The importance values sum to 100, which is good for creating a percentage-based score. The weights seem reasonably distributed based on the complexity and importance of each aspect.\n\n6. PyTorch Integration: The criterion about PyTorch compatibility is good, but could be more specific about what constitutes successful integration.\n\n7. Consistency: The criterion about maintaining consistent patterns across runs with the same seed is valuable for ensuring deterministic behavior.\n\n8. Missing Aspects: The rubric doesn't include criteria for error handling or edge cases, which could be important for a robust implementation.\n\nShould Retry: true"
  },
  {
    "Request": "Given the fields `task_candidate`, `guidance`, produce the fields `task_rubric`.\n\n---\n\nFollow the following format.\n\nTask Candidate: ${task_candidate}\nGuidance: ${guidance}\nTask Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"description\": \"Objective, programatically verifiable criterion for evaluating a task\\nimplementation. Criteria may call a function and use outputs and observed\\nside effects to determine if the criterion is satisfied, but will\\notherwise not be able to make any assumptions about how the implementation\\nworks - it is a black box.\\nCriteria should never be subjective or require examining the actual code in \\nthe implementation. It must be strictly empirical.\", \"properties\": {\"criterion\": {\"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"\\nList of criteria for evaluating task implementation; importance should sum to 100.\\n\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\n\n---\n\nTask Candidate: {\"name\":\"Implement alternating flip data augmentation\",\"description\":\"Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in a deterministic alternating pattern across epochs, rather than randomly.\",\"baseline\":\"Standard random horizontal flipping augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Data Augmentation\"],\"relevant_paper_text\":\"We propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\",\"scoring_feasibility\":8,\"llm_tractability\":0.7,\"expert_tractability\":0.9,\"layman_tractability\":0.3,\"scoring_llm_tractability\":0.8,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\nGuidance:\nSelf Critique: The candidate output provides a good starting point for a task rubric, but there are some areas for improvement:\n\n1. Objectivity: The criteria are generally objective and verifiable, which is good. They focus on observable outcomes rather than implementation details.\n\n2. Specificity: Some criteria could be more specific. For example, the criterion about working with CIFAR-10 could specify the exact size of the batch to be processed.\n\n3. Completeness: The rubric covers the main aspects of the alternating flip augmentation technique, including the behavior for the first epoch and subsequent even and odd epochs.\n\n4. Measurability: While the criteria are verifiable, they don't specify exact methods for verification. For instance, how exactly should one compare input and output tensors?\n\n5. Importance Weighting: The importance values sum to 100, which is good for creating a percentage-based score. The weights seem reasonably distributed based on the complexity and importance of each aspect.\n\n6. PyTorch Integration: The criterion about PyTorch compatibility is good, but could be more specific about what constitutes successful integration.\n\n7. Consistency: The criterion about maintaining consistent patterns across runs with the same seed is valuable for ensuring deterministic behavior.\n\n8. Missing Aspects: The rubric doesn't include criteria for error handling or edge cases, which could be important for a robust implementation.\nTask Rubric:"
  },
  {
    "Response": "{\n  \"rubric\": [\n    {\n      \"criterion\": \"The implementation correctly processes a batch of 32 CIFAR-10 images (3x32x32 tensors) without raising errors.\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"For the first epoch, exactly 50% of the images in a batch of 32 are flipped horizontally.\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"For even-numbered epochs (2,4,6,...), the implementation flips only those images that were not flipped in the first epoch.\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"For odd-numbered epochs (3,5,7,...), the implementation flips only those images that were flipped in the first epoch.\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"The implementation maintains consistent flipping patterns across multiple runs when the same random seed is set.\",\n      \"importance\": 10\n    },\n    {\n      \"criterion\": \"The implementation can be used as a PyTorch transform and is compatible with PyTorch's DataLoader.\",\n      \"importance\": 10\n    },\n    {\n      \"criterion\": \"The implementation handles edge cases gracefully (e.g., empty batch, non-image tensor) without crashing.\",\n      \"importance\": 5\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.\n\n---\n\nFollow the following format.\n\nTask: ${task}\n\nRubric: ${rubric}\n\nReasoning: Let's think step by step in order to ${produce the justification}. We ...\n\nScorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified? The scoring function will only have access to the function implementing the task, and we cannot make any assumptions about how the task is implemented by the agent. We only want to validate correctness.\n\nJustification: Justification for scorable judgement\n\n---\n\nTask: {\"name\":\"Implement alternating flip data augmentation\",\"description\":\"Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in a deterministic alternating pattern across epochs, rather than randomly.\",\"baseline\":\"Standard random horizontal flipping augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Data Augmentation\"],\"relevant_paper_text\":\"We propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\",\"scoring_feasibility\":8,\"llm_tractability\":0.7,\"expert_tractability\":0.9,\"layman_tractability\":0.3,\"scoring_llm_tractability\":0.8,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\n\nRubric: criterion='The implementation correctly processes a batch of 32 CIFAR-10 images (3x32x32 tensors) without raising errors.' importance=15.0 criterion='For the first epoch, exactly 50% of the images in a batch of 32 are flipped horizontally.' importance=20.0 criterion='For even-numbered epochs (2,4,6,...), the implementation flips only those images that were not flipped in the first epoch.' importance=20.0 criterion='For odd-numbered epochs (3,5,7,...), the implementation flips only those images that were flipped in the first epoch.' importance=20.0 criterion='The implementation maintains consistent flipping patterns across multiple runs when the same random seed is set.' importance=10.0 criterion=\"The implementation can be used as a PyTorch transform and is compatible with PyTorch's DataLoader.\" importance=10.0 criterion='The implementation handles edge cases gracefully (e.g., empty batch, non-image tensor) without crashing.' importance=5.0\n\nReasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification:\n\n1. The task involves implementing a specific data augmentation technique for image processing.\n2. The technique requires deterministic flipping of images across epochs.\n3. The rubric provides clear, objective criteria for evaluating the implementation.\n4. Each criterion can be tested programmatically:\n   - Processing a batch of 32 CIFAR-10 images without errors\n   - Verifying the correct percentage of flipped images in the first epoch\n   - Checking the correct flipping pattern for even and odd epochs\n   - Testing consistency across multiple runs with the same seed\n   - Verifying compatibility with PyTorch's DataLoader\n   - Testing edge cases\n\n5. The implementation can be treated as a black box function, with inputs (images, epoch number) and outputs (augmented images).\n6. A scoring function can be written to:\n   - Generate test inputs\n   - Call the implementation function\n   - Verify the outputs against the expected results\n   - Assign scores based on the rubric criteria\n\n7. All necessary information for scoring is available from the function's inputs and outputs, without needing to know the internal implementation details.\n\nGiven these considerations, it appears possible to write an objective scoring function for this task.\n\nScorable[True/False]: True\n\nJustification: This task is scorable because it involves implementing a specific data augmentation technique with clear, objective criteria that can be programmatically verified. The rubric provides detailed requirements that can be translated into test cases, including checking the correct flipping patterns across epochs, verifying the percentage of flipped images, and testing compatibility with PyTorch components. A scoring function can generate appropriate test inputs, call the implementation, and verify the outputs against expected results without needing to know the internal details of the implementation. The deterministic nature of the technique also allows for consistency checks across multiple runs. Additionally, the rubric covers edge cases and error handling, which can be objectively tested. All of these factors make it feasible to create a comprehensive and objective scoring function for this task."
  },
  {
    "Request": "Given the fields `candidate_input`, `candidate_output`, `requirements`, `attempt_num`, `previous_critiques`, produce the fields `self_critique`, `should_retry`.\n\n---\n\nFollow the following format.\n\nCandidate Input: ${candidate_input}\n\nCandidate Output: ${candidate_output}\n\nRequirements: ${requirements}\n\nAttempt Num: ${attempt_num}\n\nPrevious Critiques: ${previous_critiques}\n\nSelf Critique: Self-critique of how well the candidate output satisfies the requirements given the input\n\nShould Retry: Should we attempt to generate the output again taking this critique into account? True/False (Respond with true or false)\n\n---\n\nCandidate Input: {\"name\":\"Implement alternating flip data augmentation\",\"description\":\"Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in a deterministic alternating pattern across epochs, rather than randomly.\",\"baseline\":\"Standard random horizontal flipping augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Data Augmentation\"],\"relevant_paper_text\":\"We propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\",\"scoring_feasibility\":8,\"llm_tractability\":0.7,\"expert_tractability\":0.9,\"layman_tractability\":0.3,\"scoring_llm_tractability\":0.8,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\n\nCandidate Output:\nPrediction(\n    task_rubric=TaskRubric(rubric=[TaskCriterion(criterion='The implementation correctly processes a batch of 32 CIFAR-10 images (3x32x32 tensors) without raising errors.', importance=15.0), TaskCriterion(criterion='For the first epoch, exactly 50% of the images in a batch of 32 are flipped horizontally.', importance=20.0), TaskCriterion(criterion='For even-numbered epochs (2,4,6,...), the implementation flips only those images that were not flipped in the first epoch.', importance=20.0), TaskCriterion(criterion='For odd-numbered epochs (3,5,7,...), the implementation flips only those images that were flipped in the first epoch.', importance=20.0), TaskCriterion(criterion='The implementation maintains consistent flipping patterns across multiple runs when the same random seed is set.', importance=10.0), TaskCriterion(criterion=\"The implementation can be used as a PyTorch transform and is compatible with PyTorch's DataLoader.\", importance=10.0), TaskCriterion(criterion='The implementation handles edge cases gracefully (e.g., empty batch, non-image tensor) without crashing.', importance=5.0)])\n)\n\nRequirements:\nDefine a rubric for programming task evaluation, where criteria satisfy \n    Objective, programatically verifiable criterion for evaluating a task\n    implementation. Criteria may call a function and use outputs and observed\n    side effects to determine if the criterion is satisfied, but will\n    otherwise not be able to make any assumptions about how the implementation\n    works - it is a black box.\n    Criteria should never be subjective or require examining the actual code in \n    the implementation. It must be strictly empirical.\n    \n\nAttempt Num: 3\n\nPrevious Critiques:\nSelf Critique:\n\n1. Relevance and Scope: The task candidates extracted are highly relevant to the paper and reflect actual research tasks undertaken. They cover key innovations described in the paper: alternating flip data augmentation, patch-whitening initialization, and multi-crop test-time augmentation.\n\n2. Implementability: All three tasks are implementable programming tasks that modify baseline implementations. They require research and engineering expertise to implement correctly.\n\n3. Scoring Feasibility: The tasks seem feasible to score objectively. For example, the alternating flip augmentation could be scored based on whether the implementation correctly flips images in the described deterministic pattern across epochs. The patch-whitening initialization could be scored based on whether the first layer's weights match the described eigenvector initialization. The multi-crop TTA could be scored based on whether it correctly generates and weights the six augmented views.\n\n4. Difficulty Balance: The tasks appear to strike a good balance between being challenging to implement but relatively straightforward to score. Implementing these techniques correctly requires understanding the concepts and translating them into code, while scoring can be done by checking if the implementation matches the paper's description.\n\n5. Completeness: The tasks cover three major innovations from the paper. However, there may be other potential task candidates that could have been extracted, such as implementing the decoupled learning rate and weight decay optimization or the Lookahead optimizer.\n\n6. Detail Level: The task descriptions provide a good overview, but could potentially benefit from more specific implementation details or evaluation criteria.\n\n7. Baseline Specification: Each task includes a baseline implementation, which is helpful for understanding the starting point and the required modifications.\n\nOverall, the extracted task candidates appear to be well-suited for evaluating research and engineering capabilities related to the paper's innovations. They require expertise to implement correctly while being feasible to score objectively.\n\nSelf Critique:"
  },
  {
    "Response": "Self Critique: The candidate output provides a good start for a task rubric, but it has some areas for improvement:\n\n1. Comprehensiveness: The rubric covers the main aspects of the alternating flip data augmentation technique, including the behavior for the first epoch and subsequent even and odd epochs. It also includes criteria for basic functionality and compatibility with PyTorch.\n\n2. Objectivity: Most criteria are objective and can be programmatically verified, which aligns well with the requirements.\n\n3. Specificity: Some criteria could be more specific. For example, the criterion for the first epoch could specify that it should be exactly 16 out of 32 images that are flipped, rather than just saying \"50%\".\n\n4. Measurability: The importance values assigned to each criterion provide a way to measure the implementation's performance quantitatively.\n\n5. Edge Cases: The rubric includes a criterion for handling edge cases, which is good, but it could be more specific about which edge cases to test.\n\n6. Consistency: The rubric includes a criterion for consistency across runs with the same random seed, which is important for reproducibility.\n\n7. Missing Elements: The rubric doesn't include any criteria for verifying the performance impact of the augmentation technique, which could be an important aspect to consider.\n\n8. Clarity: Some criteria could be clearer. For example, the criterion about compatibility with PyTorch's DataLoader could specify what exactly constitutes compatibility.\n\nOverall, while the rubric covers the main points of the task, there's room for improvement in terms of specificity and comprehensiveness.\n\nShould Retry: true"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.\n\n---\n\nFollow the following format.\n\nTask: ${task}\n\nRubric: ${rubric}\n\nReasoning: Let's think step by step in order to ${produce the justification}. We ...\n\nScorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified? The scoring function will only have access to the function implementing the task, and we cannot make any assumptions about how the task is implemented by the agent. We only want to validate correctness.\n\nJustification: Justification for scorable judgement\n\n---\n\nTask: {\"name\":\"Implement alternating flip data augmentation\",\"description\":\"Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in a deterministic alternating pattern across epochs, rather than randomly.\",\"baseline\":\"Standard random horizontal flipping augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Data Augmentation\"],\"relevant_paper_text\":\"We propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\",\"scoring_feasibility\":8,\"llm_tractability\":0.7,\"expert_tractability\":0.9,\"layman_tractability\":0.3,\"scoring_llm_tractability\":0.8,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\n\nRubric: criterion='The implementation correctly processes a batch of 32 CIFAR-10 images (3x32x32 tensors) without raising errors.' importance=15.0 criterion='For the first epoch, exactly 50% of the images in a batch of 32 are flipped horizontally.' importance=20.0 criterion='For even-numbered epochs (2,4,6,...), the implementation flips only those images that were not flipped in the first epoch.' importance=20.0 criterion='For odd-numbered epochs (3,5,7,...), the implementation flips only those images that were flipped in the first epoch.' importance=20.0 criterion='The implementation maintains consistent flipping patterns across multiple runs when the same random seed is set.' importance=10.0 criterion=\"The implementation can be used as a PyTorch transform and is compatible with PyTorch's DataLoader.\" importance=10.0 criterion='The implementation handles edge cases gracefully (e.g., empty batch, non-image tensor) without crashing.' importance=5.0\n\nReasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification:\n\n1. The task involves implementing a specific data augmentation technique for image processing.\n2. The technique requires deterministic flipping of images across epochs.\n3. The rubric provides clear, objective criteria for evaluating the implementation.\n4. Each criterion can be tested programmatically:\n   - Processing a batch of 32 CIFAR-10 images without errors\n   - Verifying the correct percentage of flipped images in the first epoch\n   - Checking the correct flipping pattern for even and odd epochs\n   - Testing consistency across multiple runs with the same seed\n   - Verifying compatibility with PyTorch's DataLoader\n   - Testing edge cases\n\n5. The implementation can be treated as a black box function, with inputs (images, epoch number) and outputs (augmented images).\n6. A scoring function can be written to:\n   - Generate test inputs\n   - Call the implementation function\n   - Verify the outputs against the expected results\n   - Assign scores based on the rubric criteria\n\n7. All necessary information for scoring is available from the function's inputs and outputs, without needing to know the internal implementation details.\n\nGiven these considerations, it appears possible to write an objective scoring function for this task.\n\nScorable[True/False]: True\n\nJustification: This task is scorable because it involves implementing a specific data augmentation technique with clear, objective criteria that can be programmatically verified. The rubric provides detailed requirements that can be translated into test cases, including checking the correct flipping patterns across epochs, verifying the percentage of flipped images, and testing compatibility with PyTorch components. A scoring function can generate appropriate test inputs, call the implementation, and verify the outputs against expected results without needing to know the internal details of the implementation. The deterministic nature of the technique also allows for consistency checks across multiple runs. Additionally, the rubric covers edge cases and error handling, which can be objectively tested. All of these factors make it feasible to create a comprehensive and objective scoring function for this task."
  },
  {
    "Request": "Given the fields `task_candidate`, `guidance`, produce the fields `task_rubric`.\n\n---\n\nFollow the following format.\n\nTask Candidate: ${task_candidate}\nGuidance: ${guidance}\nTask Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"description\": \"Objective, programatically verifiable criterion for evaluating a task\\nimplementation. Criteria may call a function and use outputs and observed\\nside effects to determine if the criterion is satisfied, but will\\notherwise not be able to make any assumptions about how the implementation\\nworks - it is a black box.\\nCriteria should never be subjective or require examining the actual code in \\nthe implementation. It must be strictly empirical.\", \"properties\": {\"criterion\": {\"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"\\nList of criteria for evaluating task implementation; importance should sum to 100.\\n\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\n\n---\n\nTask Candidate: {\"name\":\"Implement patch-whitening initialization\",\"description\":\"Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper.\",\"baseline\":\"Standard random initialization\",\"skills\":[\"PyTorch\",\"Linear Algebra\",\"Neural Network Initialization\"],\"relevant_paper_text\":\"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\",\"scoring_feasibility\":7,\"llm_tractability\":0.5,\"expert_tractability\":0.8,\"layman_tractability\":0.1,\"scoring_llm_tractability\":0.6,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\nGuidance:"
  },
  {
    "Response": "Here's the task rubric based on the given task candidate and guidance:\n\nTask Rubric: {\n  \"rubric\": [\n    {\n      \"criterion\": \"The implemented initialization uses a 2x2 convolution with 24 channels for the first convolutional layer.\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"The first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution.\",\n      \"importance\": 25\n    },\n    {\n      \"criterion\": \"The outputs of the first 12 filters have an identity covariance matrix when applied to the training data.\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"The second 12 filters are initialized as the negation of the first 12 filters.\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"The initialization preserves input information through the activation which follows the first convolutional layer.\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"The implementation is done using PyTorch.\",\n      \"importance\": 10\n    }\n  ]\n}"
  },
  {
    "Request": "Make a very succinct json object that validates with the following schema\n\n---\n\nFollow the following format.\n\nJson Schema: ${json_schema}\nJson Object: ${json_object}\n\n---\n\nJson Schema: {\"$defs\": {\"TaskCriterion\": {\"description\": \"Objective, programatically verifiable criterion for evaluating a task\\nimplementation. Criteria may call a function and use outputs and observed\\nside effects to determine if the criterion is satisfied, but will\\notherwise not be able to make any assumptions about how the implementation\\nworks - it is a black box.\\nCriteria should never be subjective or require examining the actual code in \\nthe implementation. It must be strictly empirical.\", \"properties\": {\"criterion\": {\"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"\\nList of criteria for evaluating task implementation; importance should sum to 100.\\n\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\nJson Object:"
  },
  {
    "Response": "Json Object: {\n  \"rubric\": [\n    {\n      \"criterion\": \"Function returns correct output\",\n      \"importance\": 60\n    },\n    {\n      \"criterion\": \"Handles edge cases properly\",\n      \"importance\": 40\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `task_candidate`, `guidance`, produce the fields `task_rubric`.\n\n---\n\nFollow the following format.\n\nTask Candidate: ${task_candidate}\n\nGuidance: ${guidance}\n\nPast Error in Task Rubric: An error to avoid in the future\n\nTask Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"description\": \"Objective, programatically verifiable criterion for evaluating a task\\nimplementation. Criteria may call a function and use outputs and observed\\nside effects to determine if the criterion is satisfied, but will\\notherwise not be able to make any assumptions about how the implementation\\nworks - it is a black box.\\nCriteria should never be subjective or require examining the actual code in \\nthe implementation. It must be strictly empirical.\", \"properties\": {\"criterion\": {\"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"\\nList of criteria for evaluating task implementation; importance should sum to 100.\\n\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\n\n---\n\nTask Candidate: {\"name\":\"Implement patch-whitening initialization\",\"description\":\"Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper.\",\"baseline\":\"Standard random initialization\",\"skills\":[\"PyTorch\",\"Linear Algebra\",\"Neural Network Initialization\"],\"relevant_paper_text\":\"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\",\"scoring_feasibility\":7,\"llm_tractability\":0.5,\"expert_tractability\":0.8,\"layman_tractability\":0.1,\"scoring_llm_tractability\":0.6,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\n\nGuidance: \n\nPast Error in Task Rubric: ValueError('json output should start and end with { and }')\n\nTask Rubric:"
  },
  {
    "Response": "{\n  \"rubric\": [\n    {\n      \"criterion\": \"The first convolutional layer is initialized as a 2x2 convolution with 24 channels\",\n      \"importance\": 10\n    },\n    {\n      \"criterion\": \"The first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution\",\n      \"importance\": 25\n    },\n    {\n      \"criterion\": \"The outputs of the first 12 filters have an identity covariance matrix\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"The second 12 filters are initialized as the negation of the first 12 filters\",\n      \"importance\": 25\n    },\n    {\n      \"criterion\": \"The implementation preserves input information through the activation which follows the first convolutional layer\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"The implementation is done using PyTorch\",\n      \"importance\": 5\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.\n\n---\n\nFollow the following format.\n\nTask: ${task}\n\nRubric: ${rubric}\n\nReasoning: Let's think step by step in order to ${produce the justification}. We ...\n\nScorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified? The scoring function will only have access to the function implementing the task, and we cannot make any assumptions about how the task is implemented by the agent. We only want to validate correctness.\n\nJustification: Justification for scorable judgement\n\n---\n\nTask: {\"name\":\"Implement patch-whitening initialization\",\"description\":\"Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper.\",\"baseline\":\"Standard random initialization\",\"skills\":[\"PyTorch\",\"Linear Algebra\",\"Neural Network Initialization\"],\"relevant_paper_text\":\"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\",\"scoring_feasibility\":7,\"llm_tractability\":0.5,\"expert_tractability\":0.8,\"layman_tractability\":0.1,\"scoring_llm_tractability\":0.6,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\n\nRubric: criterion='The first convolutional layer is initialized as a 2x2 convolution with 24 channels' importance=10.0 criterion='The first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution' importance=25.0 criterion='The outputs of the first 12 filters have an identity covariance matrix' importance=20.0 criterion='The second 12 filters are initialized as the negation of the first 12 filters' importance=25.0 criterion='The implementation preserves input information through the activation which follows the first convolutional layer' importance=15.0 criterion='The implementation is done using PyTorch' importance=5.0\n\nReasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification:\n\n1. We need to determine if it's possible to write a Python function that can objectively score an implementation of this task.\n\n2. The task involves implementing a specific initialization technique for a convolutional layer in a neural network.\n\n3. We need to consider if we can verify the key aspects of the implementation without making assumptions about how it's done.\n\n4. Let's break down the main components we need to verify:\n   a. The structure of the convolutional layer (2x2, 24 channels)\n   b. The initialization of the first 12 filters\n   c. The covariance matrix of the outputs of the first 12 filters\n   d. The initialization of the second 12 filters\n   e. The preservation of input information\n\n5. For point (a), we can easily check the layer's structure using PyTorch's built-in functions.\n\n6. For points (b) and (d), we can access the weights of the convolutional layer and verify their properties.\n\n7. For point (c), we can pass sample data through the layer and compute the covariance matrix of the outputs.\n\n8. Point (e) is more challenging to verify objectively, as it requires understanding the behavior of the layer in the context of the entire network.\n\n9. The implementation being in PyTorch (criterion 6) can be easily verified by checking the types of the objects used.\n\n10. Overall, most of the critical aspects of the implementation can be objectively verified through a scoring function.\n\nScorable[True/False]: True\n\nJustification: This task is scorable because we can write a Python function to objectively verify most of the key aspects of the implementation. We can check the structure of the convolutional layer, examine the weights to verify the initialization of the filters, and compute the covariance matrix of the outputs to ensure it matches the identity matrix for the first 12 filters. We can also verify that the second 12 filters are the negation of the first 12. The only aspect that might be challenging to verify objectively is the preservation of input information through the activation, but this is a minor part of the overall implementation. The use of PyTorch can be easily confirmed by checking object types. While we can't make assumptions about the specific implementation details, we can access the necessary components of the convolutional layer to verify its correctness according to the given criteria."
  },
  {
    "Request": "Given the fields `candidate_input`, `candidate_output`, `requirements`, `attempt_num`, `previous_critiques`, produce the fields `self_critique`, `should_retry`.\n\n---\n\nFollow the following format.\n\nCandidate Input: ${candidate_input}\n\nCandidate Output: ${candidate_output}\n\nRequirements: ${requirements}\n\nAttempt Num: ${attempt_num}\n\nPrevious Critiques: ${previous_critiques}\n\nSelf Critique: Self-critique of how well the candidate output satisfies the requirements given the input\n\nShould Retry: Should we attempt to generate the output again taking this critique into account? True/False (Respond with true or false)\n\n---\n\nCandidate Input: {\"name\":\"Implement patch-whitening initialization\",\"description\":\"Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper.\",\"baseline\":\"Standard random initialization\",\"skills\":[\"PyTorch\",\"Linear Algebra\",\"Neural Network Initialization\"],\"relevant_paper_text\":\"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\",\"scoring_feasibility\":7,\"llm_tractability\":0.5,\"expert_tractability\":0.8,\"layman_tractability\":0.1,\"scoring_llm_tractability\":0.6,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\n\nCandidate Output:\nPrediction(\n    task_rubric=TaskRubric(rubric=[TaskCriterion(criterion='The first convolutional layer is initialized as a 2x2 convolution with 24 channels', importance=10.0), TaskCriterion(criterion='The first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution', importance=25.0), TaskCriterion(criterion='The outputs of the first 12 filters have an identity covariance matrix', importance=20.0), TaskCriterion(criterion='The second 12 filters are initialized as the negation of the first 12 filters', importance=25.0), TaskCriterion(criterion='The implementation preserves input information through the activation which follows the first convolutional layer', importance=15.0), TaskCriterion(criterion='The implementation is done using PyTorch', importance=5.0)])\n)\n\nRequirements:\nDefine a rubric for programming task evaluation, where criteria satisfy \n    Objective, programatically verifiable criterion for evaluating a task\n    implementation. Criteria may call a function and use outputs and observed\n    side effects to determine if the criterion is satisfied, but will\n    otherwise not be able to make any assumptions about how the implementation\n    works - it is a black box.\n    Criteria should never be subjective or require examining the actual code in \n    the implementation. It must be strictly empirical.\n    \n\nAttempt Num: 1\n\nPrevious Critiques:\nSelf Critique:\n\n1. Relevance and Scope: The task candidates extracted are highly relevant to the paper and reflect actual research tasks undertaken. They cover key innovations described in the paper: alternating flip data augmentation, patch-whitening initialization, and multi-crop test-time augmentation.\n\n2. Implementability: All three tasks are implementable programming tasks that modify baseline implementations. They require research and engineering expertise to implement correctly.\n\n3. Scoring Feasibility: The tasks seem feasible to score objectively. For example, the alternating flip augmentation could be scored based on whether the implementation correctly flips images in the described deterministic pattern across epochs. The patch-whitening initialization could be scored based on whether the first layer's weights match the described eigenvector initialization. The multi-crop TTA could be scored based on whether it correctly generates and weights the six augmented views.\n\n4. Difficulty Balance: The tasks appear to strike a good balance between being challenging to implement but relatively straightforward to score. Implementing these techniques correctly requires understanding the concepts and translating them into code, while scoring can be done by checking if the implementation matches the paper's description.\n\n5. Completeness: The tasks cover three major innovations from the paper. However, there may be other potential task candidates that could have been extracted, such as implementing the decoupled learning rate and weight decay optimization or the Lookahead optimizer.\n\n6. Detail Level: The task descriptions provide a good overview, but could potentially benefit from more specific implementation details or evaluation criteria.\n\n7. Baseline Specification: Each task includes a baseline implementation, which is helpful for understanding the starting point and the required modifications.\n\nOverall, the extracted task candidates appear to be well-suited for evaluating research and engineering capabilities related to the paper's innovations. They require expertise to implement correctly while being feasible to score objectively.\n\nSelf Critique:"
  },
  {
    "Response": "Self Critique: The candidate output provides a task rubric with specific, objective criteria for evaluating the implementation of patch-whitening initialization. However, there are some areas for improvement:\n\n1. Completeness: The rubric covers the main aspects of the patch-whitening initialization technique as described in the input. It includes criteria for the layer structure, filter initialization, and preservation of input information.\n\n2. Objectivity: Most criteria are objective and can be programmatically verified. However, the criterion \"The implementation preserves input information through the activation which follows the first convolutional layer\" may be challenging to verify programmatically without additional specificity.\n\n3. Specificity: Some criteria could be more specific. For example, the criterion about the identity covariance matrix could specify a tolerance for numerical precision.\n\n4. Measurability: The rubric uses importance scores for each criterion, which is good for prioritization. However, it doesn't specify how to measure partial compliance with each criterion.\n\n5. PyTorch Implementation: While the rubric includes a criterion for using PyTorch, it doesn't specify how to verify this programmatically.\n\n6. Missing Details: The rubric doesn't include a criterion for verifying that the initialization is based on the covariance matrix of 2x2 patches across the training distribution, which is an important detail from the input.\n\n7. Scoring Mechanism: The rubric doesn't provide a clear mechanism for combining the individual criterion scores into an overall score for the implementation.\n\nShould Retry: true"
  },
  {
    "Request": "Given the fields `task_candidate`, `guidance`, produce the fields `task_rubric`.\n\n---\n\nFollow the following format.\n\nTask Candidate: ${task_candidate}\nGuidance: ${guidance}\nTask Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"description\": \"Objective, programatically verifiable criterion for evaluating a task\\nimplementation. Criteria may call a function and use outputs and observed\\nside effects to determine if the criterion is satisfied, but will\\notherwise not be able to make any assumptions about how the implementation\\nworks - it is a black box.\\nCriteria should never be subjective or require examining the actual code in \\nthe implementation. It must be strictly empirical.\", \"properties\": {\"criterion\": {\"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"\\nList of criteria for evaluating task implementation; importance should sum to 100.\\n\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\n\n---\n\nTask Candidate: {\"name\":\"Implement patch-whitening initialization\",\"description\":\"Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper.\",\"baseline\":\"Standard random initialization\",\"skills\":[\"PyTorch\",\"Linear Algebra\",\"Neural Network Initialization\"],\"relevant_paper_text\":\"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\",\"scoring_feasibility\":7,\"llm_tractability\":0.5,\"expert_tractability\":0.8,\"layman_tractability\":0.1,\"scoring_llm_tractability\":0.6,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\nGuidance:\nSelf Critique: The candidate output provides a task rubric with specific, objective criteria for evaluating the implementation of patch-whitening initialization. However, there are some areas for improvement:\n\n1. Completeness: The rubric covers the main aspects of the patch-whitening initialization technique as described in the input. It includes criteria for the layer structure, filter initialization, and preservation of input information.\n\n2. Objectivity: Most criteria are objective and can be programmatically verified. However, the criterion \"The implementation preserves input information through the activation which follows the first convolutional layer\" may be challenging to verify programmatically without additional specificity.\n\n3. Specificity: Some criteria could be more specific. For example, the criterion about the identity covariance matrix could specify a tolerance for numerical precision.\n\n4. Measurability: The rubric uses importance scores for each criterion, which is good for prioritization. However, it doesn't specify how to measure partial compliance with each criterion.\n\n5. PyTorch Implementation: While the rubric includes a criterion for using PyTorch, it doesn't specify how to verify this programmatically.\n\n6. Missing Details: The rubric doesn't include a criterion for verifying that the initialization is based on the covariance matrix of 2x2 patches across the training distribution, which is an important detail from the input.\n\n7. Scoring Mechanism: The rubric doesn't provide a clear mechanism for combining the individual criterion scores into an overall score for the implementation.\nTask Rubric:"
  },
  {
    "Response": "{\n  \"rubric\": [\n    {\n      \"criterion\": \"The implementation uses PyTorch for the convolutional layer\",\n      \"importance\": 10\n    },\n    {\n      \"criterion\": \"The first convolutional layer is a 2x2 convolution with 24 channels\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"The first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution\",\n      \"importance\": 25\n    },\n    {\n      \"criterion\": \"The outputs of the first 12 filters have a covariance matrix that is approximately identity (within a tolerance of 1e-6)\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"The second 12 filters are initialized as the negation of the first 12 filters\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"The implementation demonstrates preservation of input information through the activation following the first convolutional layer (e.g., by showing high mutual information between input and output)\",\n      \"importance\": 10\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.\n\n---\n\nFollow the following format.\n\nTask: ${task}\n\nRubric: ${rubric}\n\nReasoning: Let's think step by step in order to ${produce the justification}. We ...\n\nScorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified? The scoring function will only have access to the function implementing the task, and we cannot make any assumptions about how the task is implemented by the agent. We only want to validate correctness.\n\nJustification: Justification for scorable judgement\n\n---\n\nTask: {\"name\":\"Implement patch-whitening initialization\",\"description\":\"Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper.\",\"baseline\":\"Standard random initialization\",\"skills\":[\"PyTorch\",\"Linear Algebra\",\"Neural Network Initialization\"],\"relevant_paper_text\":\"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\",\"scoring_feasibility\":7,\"llm_tractability\":0.5,\"expert_tractability\":0.8,\"layman_tractability\":0.1,\"scoring_llm_tractability\":0.6,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\n\nRubric: criterion='The implementation uses PyTorch for the convolutional layer' importance=10.0 criterion='The first convolutional layer is a 2x2 convolution with 24 channels' importance=15.0 criterion='The first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution' importance=25.0 criterion='The outputs of the first 12 filters have a covariance matrix that is approximately identity (within a tolerance of 1e-6)' importance=20.0 criterion='The second 12 filters are initialized as the negation of the first 12 filters' importance=20.0 criterion='The implementation demonstrates preservation of input information through the activation following the first convolutional layer (e.g., by showing high mutual information between input and output)' importance=10.0\n\nReasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification:\n\n1. The task requires implementing a specific initialization technique for a convolutional layer in PyTorch.\n\n2. The rubric provides clear, objective criteria for evaluating the implementation:\n   - Use of PyTorch\n   - Correct layer dimensions (2x2 convolution, 24 channels)\n   - Specific initialization method for the first 12 filters\n   - Verification of the covariance matrix property\n   - Correct initialization of the second 12 filters\n   - Demonstration of information preservation\n\n3. Each of these criteria can be objectively verified through code inspection and numerical tests:\n   - PyTorch usage can be checked by examining import statements and layer definitions\n   - Layer dimensions can be verified by inspecting the layer's properties\n   - Filter initialization can be checked by examining the weights and comparing them to the expected eigenvectors\n   - Covariance matrix property can be tested numerically\n   - Negation of filters can be verified by comparing the first and second sets of 12 filters\n   - Information preservation can be tested by computing mutual information or through other statistical measures\n\n4. The task provides specific details about the initialization process, making it possible to write a scoring function that checks for these details.\n\n5. The scoring function would only need access to the implemented function and the CIFAR-10 dataset (mentioned as an asset prerequisite) to perform all necessary checks.\n\n6. While the implementation itself may be complex, verifying its correctness against the given criteria is straightforward and can be done programmatically.\n\nScorable[True/False]: True\n\nJustification: This task is scorable because it provides clear, objective criteria that can be verified programmatically. The rubric outlines specific requirements for the implementation, including the use of PyTorch, exact layer dimensions, and a detailed initialization process. Each of these criteria can be checked through code inspection and numerical tests. The scoring function can examine the structure of the implemented layer, verify the initialization process by comparing weights to expected values, and perform numerical tests to confirm properties like the covariance matrix condition and information preservation. While the implementation of the patch-whitening initialization may be complex, verifying its correctness against the given criteria is straightforward and can be done objectively using only the implemented function and the specified CIFAR-10 dataset. This makes it possible to write a Python function that can reliably and objectively score the implementation."
  },
  {
    "Request": "Given the fields `candidate_input`, `candidate_output`, `requirements`, `attempt_num`, `previous_critiques`, produce the fields `self_critique`, `should_retry`.\n\n---\n\nFollow the following format.\n\nCandidate Input: ${candidate_input}\n\nCandidate Output: ${candidate_output}\n\nRequirements: ${requirements}\n\nAttempt Num: ${attempt_num}\n\nPrevious Critiques: ${previous_critiques}\n\nSelf Critique: Self-critique of how well the candidate output satisfies the requirements given the input\n\nShould Retry: Should we attempt to generate the output again taking this critique into account? True/False (Respond with true or false)\n\n---\n\nCandidate Input: {\"name\":\"Implement patch-whitening initialization\",\"description\":\"Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper.\",\"baseline\":\"Standard random initialization\",\"skills\":[\"PyTorch\",\"Linear Algebra\",\"Neural Network Initialization\"],\"relevant_paper_text\":\"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\",\"scoring_feasibility\":7,\"llm_tractability\":0.5,\"expert_tractability\":0.8,\"layman_tractability\":0.1,\"scoring_llm_tractability\":0.6,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\n\nCandidate Output:\nPrediction(\n    task_rubric=TaskRubric(rubric=[TaskCriterion(criterion='The implementation uses PyTorch for the convolutional layer', importance=10.0), TaskCriterion(criterion='The first convolutional layer is a 2x2 convolution with 24 channels', importance=15.0), TaskCriterion(criterion='The first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution', importance=25.0), TaskCriterion(criterion='The outputs of the first 12 filters have a covariance matrix that is approximately identity (within a tolerance of 1e-6)', importance=20.0), TaskCriterion(criterion='The second 12 filters are initialized as the negation of the first 12 filters', importance=20.0), TaskCriterion(criterion='The implementation demonstrates preservation of input information through the activation following the first convolutional layer (e.g., by showing high mutual information between input and output)', importance=10.0)])\n)\n\nRequirements:\nDefine a rubric for programming task evaluation, where criteria satisfy \n    Objective, programatically verifiable criterion for evaluating a task\n    implementation. Criteria may call a function and use outputs and observed\n    side effects to determine if the criterion is satisfied, but will\n    otherwise not be able to make any assumptions about how the implementation\n    works - it is a black box.\n    Criteria should never be subjective or require examining the actual code in \n    the implementation. It must be strictly empirical.\n    \n\nAttempt Num: 2\n\nPrevious Critiques:\nSelf Critique:\n\n1. Relevance and Scope: The task candidates extracted are highly relevant to the paper and reflect actual research tasks undertaken. They cover key innovations described in the paper: alternating flip data augmentation, patch-whitening initialization, and multi-crop test-time augmentation.\n\n2. Implementability: All three tasks are implementable programming tasks that modify baseline implementations. They require research and engineering expertise to implement correctly.\n\n3. Scoring Feasibility: The tasks seem feasible to score objectively. For example, the alternating flip augmentation could be scored based on whether the implementation correctly flips images in the described deterministic pattern across epochs. The patch-whitening initialization could be scored based on whether the first layer's weights match the described eigenvector initialization. The multi-crop TTA could be scored based on whether it correctly generates and weights the six augmented views.\n\n4. Difficulty Balance: The tasks appear to strike a good balance between being challenging to implement but relatively straightforward to score. Implementing these techniques correctly requires understanding the concepts and translating them into code, while scoring can be done by checking if the implementation matches the paper's description.\n\n5. Completeness: The tasks cover three major innovations from the paper. However, there may be other potential task candidates that could have been extracted, such as implementing the decoupled learning rate and weight decay optimization or the Lookahead optimizer.\n\n6. Detail Level: The task descriptions provide a good overview, but could potentially benefit from more specific implementation details or evaluation criteria.\n\n7. Baseline Specification: Each task includes a baseline implementation, which is helpful for understanding the starting point and the required modifications.\n\nOverall, the extracted task candidates appear to be well-suited for evaluating research and engineering capabilities related to the paper's innovations. They require expertise to implement correctly while being feasible to score objectively.\n\nSelf Critique:"
  },
  {
    "Response": "Self Critique: The candidate output provides a good start for a task rubric, but it has some areas for improvement:\n\n1. Objectivity: The criteria are generally objective and verifiable, which aligns well with the requirements.\n\n2. Programmatic Verifiability: Most criteria can be programmatically verified, but some may need more specific metrics or thresholds for verification.\n\n3. Completeness: The rubric covers the main aspects of the patch-whitening initialization technique as described in the input.\n\n4. Specificity: Some criteria could be more specific. For example, the criterion about preserving input information could benefit from a more concrete, measurable definition.\n\n5. Quantification: The importance values are provided for each criterion, which is good for prioritization.\n\n6. Black Box Approach: The criteria focus on observable outputs and effects, adhering to the black box requirement.\n\n7. Missing Elements: The rubric doesn't include a criterion for verifying the use of the CIFAR-10 dataset, which is mentioned in the asset prerequisites.\n\n8. Format: The output is structured as a Prediction object with a TaskRubric containing TaskCriterion objects, which may not be the exact format required.\n\nShould Retry: true"
  },
  {
    "Request": "Given the fields `task_candidate`, `guidance`, produce the fields `task_rubric`.\n\n---\n\nFollow the following format.\n\nTask Candidate: ${task_candidate}\nGuidance: ${guidance}\nTask Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"description\": \"Objective, programatically verifiable criterion for evaluating a task\\nimplementation. Criteria may call a function and use outputs and observed\\nside effects to determine if the criterion is satisfied, but will\\notherwise not be able to make any assumptions about how the implementation\\nworks - it is a black box.\\nCriteria should never be subjective or require examining the actual code in \\nthe implementation. It must be strictly empirical.\", \"properties\": {\"criterion\": {\"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"\\nList of criteria for evaluating task implementation; importance should sum to 100.\\n\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\n\n---\n\nTask Candidate: {\"name\":\"Implement patch-whitening initialization\",\"description\":\"Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper.\",\"baseline\":\"Standard random initialization\",\"skills\":[\"PyTorch\",\"Linear Algebra\",\"Neural Network Initialization\"],\"relevant_paper_text\":\"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\",\"scoring_feasibility\":7,\"llm_tractability\":0.5,\"expert_tractability\":0.8,\"layman_tractability\":0.1,\"scoring_llm_tractability\":0.6,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\nGuidance:\nSelf Critique: The candidate output provides a good start for a task rubric, but it has some areas for improvement:\n\n1. Objectivity: The criteria are generally objective and verifiable, which aligns well with the requirements.\n\n2. Programmatic Verifiability: Most criteria can be programmatically verified, but some may need more specific metrics or thresholds for verification.\n\n3. Completeness: The rubric covers the main aspects of the patch-whitening initialization technique as described in the input.\n\n4. Specificity: Some criteria could be more specific. For example, the criterion about preserving input information could benefit from a more concrete, measurable definition.\n\n5. Quantification: The importance values are provided for each criterion, which is good for prioritization.\n\n6. Black Box Approach: The criteria focus on observable outputs and effects, adhering to the black box requirement.\n\n7. Missing Elements: The rubric doesn't include a criterion for verifying the use of the CIFAR-10 dataset, which is mentioned in the asset prerequisites.\n\n8. Format: The output is structured as a Prediction object with a TaskRubric containing TaskCriterion objects, which may not be the exact format required.\nTask Rubric:"
  },
  {
    "Response": "{\n  \"rubric\": [\n    {\n      \"criterion\": \"The first convolutional layer is a 2x2 convolution with 24 channels.\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"The first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the CIFAR-10 training distribution.\",\n      \"importance\": 25\n    },\n    {\n      \"criterion\": \"The outputs of the first 12 filters have an identity covariance matrix when applied to the CIFAR-10 training set.\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"The second 12 filters are initialized as the negation of the first 12 filters.\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"The initialization preserves input information through the activation function, as measured by mutual information between input and output.\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"The implementation uses PyTorch for tensor operations and network construction.\",\n      \"importance\": 10\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.\n\n---\n\nFollow the following format.\n\nTask: ${task}\n\nRubric: ${rubric}\n\nReasoning: Let's think step by step in order to ${produce the justification}. We ...\n\nScorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified? The scoring function will only have access to the function implementing the task, and we cannot make any assumptions about how the task is implemented by the agent. We only want to validate correctness.\n\nJustification: Justification for scorable judgement\n\n---\n\nTask: {\"name\":\"Implement patch-whitening initialization\",\"description\":\"Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper.\",\"baseline\":\"Standard random initialization\",\"skills\":[\"PyTorch\",\"Linear Algebra\",\"Neural Network Initialization\"],\"relevant_paper_text\":\"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\",\"scoring_feasibility\":7,\"llm_tractability\":0.5,\"expert_tractability\":0.8,\"layman_tractability\":0.1,\"scoring_llm_tractability\":0.6,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\n\nRubric: criterion='The first convolutional layer is a 2x2 convolution with 24 channels.' importance=15.0 criterion='The first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the CIFAR-10 training distribution.' importance=25.0 criterion='The outputs of the first 12 filters have an identity covariance matrix when applied to the CIFAR-10 training set.' importance=20.0 criterion='The second 12 filters are initialized as the negation of the first 12 filters.' importance=15.0 criterion='The initialization preserves input information through the activation function, as measured by mutual information between input and output.' importance=15.0 criterion='The implementation uses PyTorch for tensor operations and network construction.' importance=10.0\n\nReasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification:\n\n1. We need to determine if it's possible to write a Python function that can objectively score an implementation of this task.\n\n2. The task involves implementing a specific initialization technique for a convolutional layer.\n\n3. The rubric provides clear, measurable criteria that can be checked programmatically:\n   - Layer structure (2x2 convolution with 24 channels)\n   - Initialization method for the first 12 filters\n   - Covariance matrix property of the first 12 filters' outputs\n   - Initialization method for the second 12 filters\n   - Information preservation property\n   - Use of PyTorch\n\n4. Most of these criteria can be verified by examining the structure and weights of the implemented layer, and by running it on the CIFAR-10 dataset.\n\n5. The information preservation criterion might be more challenging to verify, but it's still possible to compute mutual information between input and output.\n\n6. All necessary data (CIFAR-10 dataset) is available as per the asset prerequisites.\n\n7. The scoring function would have access to the implemented function, which should be sufficient to perform these checks.\n\n8. No assumptions about the specific implementation details are required beyond what's specified in the task and rubric.\n\nGiven these considerations, it appears that an objective scoring function can be written.\n\nScorable[True/False]: True\n\nJustification: This task is scorable because:\n1. The rubric provides clear, quantifiable criteria that can be programmatically verified.\n2. The layer structure, initialization methods, and PyTorch usage can be directly checked by examining the implemented function.\n3. The covariance matrix property and information preservation can be verified by running the layer on the CIFAR-10 dataset, which is available.\n4. All checks can be performed using only the implemented function and the specified dataset, without making assumptions about implementation details.\n5. While some criteria (like information preservation) may be more complex to verify, they are still computationally feasible.\nTherefore, it is possible to write a Python function that objectively scores an implementation of this task, verifying all key aspects of the specified functionality."
  },
  {
    "Request": "Given the fields `candidate_input`, `candidate_output`, `requirements`, `attempt_num`, `previous_critiques`, produce the fields `self_critique`, `should_retry`.\n\n---\n\nFollow the following format.\n\nCandidate Input: ${candidate_input}\n\nCandidate Output: ${candidate_output}\n\nRequirements: ${requirements}\n\nAttempt Num: ${attempt_num}\n\nPrevious Critiques: ${previous_critiques}\n\nSelf Critique: Self-critique of how well the candidate output satisfies the requirements given the input\n\nShould Retry: Should we attempt to generate the output again taking this critique into account? True/False (Respond with true or false)\n\n---\n\nCandidate Input: {\"name\":\"Implement patch-whitening initialization\",\"description\":\"Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper.\",\"baseline\":\"Standard random initialization\",\"skills\":[\"PyTorch\",\"Linear Algebra\",\"Neural Network Initialization\"],\"relevant_paper_text\":\"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\",\"scoring_feasibility\":7,\"llm_tractability\":0.5,\"expert_tractability\":0.8,\"layman_tractability\":0.1,\"scoring_llm_tractability\":0.6,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\n\nCandidate Output:\nPrediction(\n    task_rubric=TaskRubric(rubric=[TaskCriterion(criterion='The first convolutional layer is a 2x2 convolution with 24 channels.', importance=15.0), TaskCriterion(criterion='The first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the CIFAR-10 training distribution.', importance=25.0), TaskCriterion(criterion='The outputs of the first 12 filters have an identity covariance matrix when applied to the CIFAR-10 training set.', importance=20.0), TaskCriterion(criterion='The second 12 filters are initialized as the negation of the first 12 filters.', importance=15.0), TaskCriterion(criterion='The initialization preserves input information through the activation function, as measured by mutual information between input and output.', importance=15.0), TaskCriterion(criterion='The implementation uses PyTorch for tensor operations and network construction.', importance=10.0)])\n)\n\nRequirements:\nDefine a rubric for programming task evaluation, where criteria satisfy \n    Objective, programatically verifiable criterion for evaluating a task\n    implementation. Criteria may call a function and use outputs and observed\n    side effects to determine if the criterion is satisfied, but will\n    otherwise not be able to make any assumptions about how the implementation\n    works - it is a black box.\n    Criteria should never be subjective or require examining the actual code in \n    the implementation. It must be strictly empirical.\n    \n\nAttempt Num: 3\n\nPrevious Critiques:\nSelf Critique:\n\n1. Relevance and Scope: The task candidates extracted are highly relevant to the paper and reflect actual research tasks undertaken. They cover key innovations described in the paper: alternating flip data augmentation, patch-whitening initialization, and multi-crop test-time augmentation.\n\n2. Implementability: All three tasks are implementable programming tasks that modify baseline implementations. They require research and engineering expertise to implement correctly.\n\n3. Scoring Feasibility: The tasks seem feasible to score objectively. For example, the alternating flip augmentation could be scored based on whether the implementation correctly flips images in the described deterministic pattern across epochs. The patch-whitening initialization could be scored based on whether the first layer's weights match the described eigenvector initialization. The multi-crop TTA could be scored based on whether it correctly generates and weights the six augmented views.\n\n4. Difficulty Balance: The tasks appear to strike a good balance between being challenging to implement but relatively straightforward to score. Implementing these techniques correctly requires understanding the concepts and translating them into code, while scoring can be done by checking if the implementation matches the paper's description.\n\n5. Completeness: The tasks cover three major innovations from the paper. However, there may be other potential task candidates that could have been extracted, such as implementing the decoupled learning rate and weight decay optimization or the Lookahead optimizer.\n\n6. Detail Level: The task descriptions provide a good overview, but could potentially benefit from more specific implementation details or evaluation criteria.\n\n7. Baseline Specification: Each task includes a baseline implementation, which is helpful for understanding the starting point and the required modifications.\n\nOverall, the extracted task candidates appear to be well-suited for evaluating research and engineering capabilities related to the paper's innovations. They require expertise to implement correctly while being feasible to score objectively.\n\nSelf Critique:"
  },
  {
    "Response": "Self Critique: The candidate output provides a good start for a task rubric, but it has some areas for improvement:\n\n1. Comprehensiveness: The rubric covers the main aspects of the patch-whitening initialization technique described in the input, including the layer structure, initialization method, and expected properties. This is positive.\n\n2. Objectivity: Most criteria are objective and verifiable, which aligns well with the requirements. However, the criterion about preserving input information through mutual information might be challenging to verify programmatically without additional specifics.\n\n3. Specificity: Some criteria could be more specific. For example, the criterion about the covariance matrix of the first 12 filters' outputs could specify a tolerance for how close to identity it should be, as perfect identity is unlikely in practice.\n\n4. Measurability: While most criteria are measurable, the rubric doesn't specify how to measure some aspects, such as the mutual information between input and output.\n\n5. Completeness: The rubric covers the main points from the input description but doesn't include a criterion for verifying the use of the CIFAR-10 dataset, which is mentioned in the asset prerequisites.\n\n6. Weighting: The importance values assigned to each criterion seem reasonable, giving more weight to the core aspects of the initialization technique.\n\n7. PyTorch Requirement: The rubric includes a criterion for using PyTorch, which aligns with the skills mentioned in the input.\n\n8. Missing Elements: The rubric doesn't include criteria for evaluating the implementation against the baseline or for comparing the performance of the initialized network to a standard randomly initialized one.\n\nShould Retry: true"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.\n\n---\n\nFollow the following format.\n\nTask: ${task}\n\nRubric: ${rubric}\n\nReasoning: Let's think step by step in order to ${produce the justification}. We ...\n\nScorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified? The scoring function will only have access to the function implementing the task, and we cannot make any assumptions about how the task is implemented by the agent. We only want to validate correctness.\n\nJustification: Justification for scorable judgement\n\n---\n\nTask: {\"name\":\"Implement patch-whitening initialization\",\"description\":\"Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper.\",\"baseline\":\"Standard random initialization\",\"skills\":[\"PyTorch\",\"Linear Algebra\",\"Neural Network Initialization\"],\"relevant_paper_text\":\"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\",\"scoring_feasibility\":7,\"llm_tractability\":0.5,\"expert_tractability\":0.8,\"layman_tractability\":0.1,\"scoring_llm_tractability\":0.6,\"asset_prerequisites\":[\"CIFAR-10 dataset\"]}\n\nRubric: criterion='The first convolutional layer is a 2x2 convolution with 24 channels.' importance=15.0 criterion='The first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the CIFAR-10 training distribution.' importance=25.0 criterion='The outputs of the first 12 filters have an identity covariance matrix when applied to the CIFAR-10 training set.' importance=20.0 criterion='The second 12 filters are initialized as the negation of the first 12 filters.' importance=15.0 criterion='The initialization preserves input information through the activation function, as measured by mutual information between input and output.' importance=15.0 criterion='The implementation uses PyTorch for tensor operations and network construction.' importance=10.0\n\nReasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification:\n\n1. We need to determine if it's possible to write a Python function that can objectively score an implementation of this task.\n\n2. The task involves implementing a specific initialization technique for a convolutional layer.\n\n3. The rubric provides clear, measurable criteria that can be checked programmatically:\n   - Layer structure (2x2 convolution with 24 channels)\n   - Initialization method for the first 12 filters\n   - Covariance matrix property of the first 12 filters' outputs\n   - Initialization method for the second 12 filters\n   - Information preservation property\n   - Use of PyTorch\n\n4. Most of these criteria can be verified by examining the structure and weights of the implemented layer, and by running it on the CIFAR-10 dataset.\n\n5. The information preservation criterion might be more challenging to verify, but it's still possible to compute mutual information between input and output.\n\n6. All necessary data (CIFAR-10 dataset) is available as per the asset prerequisites.\n\n7. The scoring function would have access to the implemented function, which should be sufficient to perform these checks.\n\n8. No assumptions about the specific implementation details are required beyond what's specified in the task and rubric.\n\nGiven these considerations, it appears that an objective scoring function can be written.\n\nScorable[True/False]: True\n\nJustification: This task is scorable because:\n1. The rubric provides clear, quantifiable criteria that can be programmatically verified.\n2. The layer structure, initialization methods, and PyTorch usage can be directly checked by examining the implemented function.\n3. The covariance matrix property and information preservation can be verified by running the layer on the CIFAR-10 dataset, which is available.\n4. All checks can be performed using only the implemented function and the specified dataset, without making assumptions about implementation details.\n5. While some criteria (like information preservation) may be more complex to verify, they are still computationally feasible.\nTherefore, it is possible to write a Python function that objectively scores an implementation of this task, verifying all key aspects of the specified functionality."
  },
  {
    "Request": "Given the fields `task_candidate`, `guidance`, produce the fields `task_rubric`.\n\n---\n\nFollow the following format.\n\nTask Candidate: ${task_candidate}\nGuidance: ${guidance}\nTask Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"description\": \"Objective, programatically verifiable criterion for evaluating a task\\nimplementation. Criteria may call a function and use outputs and observed\\nside effects to determine if the criterion is satisfied, but will\\notherwise not be able to make any assumptions about how the implementation\\nworks - it is a black box.\\nCriteria should never be subjective or require examining the actual code in \\nthe implementation. It must be strictly empirical.\", \"properties\": {\"criterion\": {\"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"\\nList of criteria for evaluating task implementation; importance should sum to 100.\\n\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\n\n---\n\nTask Candidate: {\"name\":\"Implement multi-crop test-time augmentation\",\"description\":\"Implement the multi-crop test-time augmentation technique described in the paper, which involves evaluating the model on multiple augmented versions of each test image.\",\"baseline\":\"No test-time augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Test-Time Augmentation\"],\"relevant_paper_text\":\"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\",\"scoring_feasibility\":8,\"llm_tractability\":0.8,\"expert_tractability\":0.9,\"layman_tractability\":0.4,\"scoring_llm_tractability\":0.9,\"asset_prerequisites\":[\"CIFAR-10 dataset\",\"Trained model\"]}\nGuidance:"
  },
  {
    "Response": "Here's the task rubric based on the given task candidate and guidance:\n\nTask Rubric: {\n  \"rubric\": [\n    {\n      \"criterion\": \"The implementation correctly applies the six specified augmentations to each test image: unmodified, translated up-and-left by one pixel, translated down-and-right by one pixel, and mirrored versions of all three.\",\n      \"importance\": 30\n    },\n    {\n      \"criterion\": \"The model's predictions are generated using a weighted average of all six outputs, with correct weights: 0.25 for each untranslated view and 0.125 for each of the four translated views.\",\n      \"importance\": 30\n    },\n    {\n      \"criterion\": \"The implementation works correctly with the provided CIFAR-10 dataset and trained model.\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"The implementation is done using PyTorch, as specified in the required skills.\",\n      \"importance\": 10\n    },\n    {\n      \"criterion\": \"The implementation shows improved performance compared to the baseline of no test-time augmentation.\",\n      \"importance\": 10\n    }\n  ]\n}"
  },
  {
    "Request": "Make a very succinct json object that validates with the following schema\n\n---\n\nFollow the following format.\n\nJson Schema: ${json_schema}\nJson Object: ${json_object}\n\n---\n\nJson Schema: {\"$defs\": {\"TaskCriterion\": {\"description\": \"Objective, programatically verifiable criterion for evaluating a task\\nimplementation. Criteria may call a function and use outputs and observed\\nside effects to determine if the criterion is satisfied, but will\\notherwise not be able to make any assumptions about how the implementation\\nworks - it is a black box.\\nCriteria should never be subjective or require examining the actual code in \\nthe implementation. It must be strictly empirical.\", \"properties\": {\"criterion\": {\"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"\\nList of criteria for evaluating task implementation; importance should sum to 100.\\n\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\nJson Object:"
  },
  {
    "Response": "Json Object: {\n  \"rubric\": [\n    {\n      \"criterion\": \"Function returns correct output\",\n      \"importance\": 60\n    },\n    {\n      \"criterion\": \"Handles edge cases properly\",\n      \"importance\": 40\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `task_candidate`, `guidance`, produce the fields `task_rubric`.\n\n---\n\nFollow the following format.\n\nTask Candidate: ${task_candidate}\n\nGuidance: ${guidance}\n\nPast Error in Task Rubric: An error to avoid in the future\n\nTask Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"description\": \"Objective, programatically verifiable criterion for evaluating a task\\nimplementation. Criteria may call a function and use outputs and observed\\nside effects to determine if the criterion is satisfied, but will\\notherwise not be able to make any assumptions about how the implementation\\nworks - it is a black box.\\nCriteria should never be subjective or require examining the actual code in \\nthe implementation. It must be strictly empirical.\", \"properties\": {\"criterion\": {\"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"\\nList of criteria for evaluating task implementation; importance should sum to 100.\\n\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\n\n---\n\nTask Candidate: {\"name\":\"Implement multi-crop test-time augmentation\",\"description\":\"Implement the multi-crop test-time augmentation technique described in the paper, which involves evaluating the model on multiple augmented versions of each test image.\",\"baseline\":\"No test-time augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Test-Time Augmentation\"],\"relevant_paper_text\":\"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\",\"scoring_feasibility\":8,\"llm_tractability\":0.8,\"expert_tractability\":0.9,\"layman_tractability\":0.4,\"scoring_llm_tractability\":0.9,\"asset_prerequisites\":[\"CIFAR-10 dataset\",\"Trained model\"]}\n\nGuidance: \n\nPast Error in Task Rubric: ValueError('json output should start and end with { and }')\n\nTask Rubric:"
  },
  {
    "Response": "{\n  \"rubric\": [\n    {\n      \"criterion\": \"The implementation correctly applies the six specified augmentations to each test image: original, up-left translation, down-right translation, and their mirrored versions.\",\n      \"importance\": 25\n    },\n    {\n      \"criterion\": \"The model is evaluated on all six augmented versions of each test image.\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"The final prediction is calculated using a weighted average of all six outputs, with correct weights: 0.25 for untranslated images and 0.125 for translated images.\",\n      \"importance\": 25\n    },\n    {\n      \"criterion\": \"The implementation works with the provided CIFAR-10 dataset and trained model.\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"The implementation is done using PyTorch, as specified in the required skills.\",\n      \"importance\": 10\n    },\n    {\n      \"criterion\": \"The implementation shows improved performance compared to the baseline of no test-time augmentation.\",\n      \"importance\": 5\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.\n\n---\n\nFollow the following format.\n\nTask: ${task}\n\nRubric: ${rubric}\n\nReasoning: Let's think step by step in order to ${produce the justification}. We ...\n\nScorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified? The scoring function will only have access to the function implementing the task, and we cannot make any assumptions about how the task is implemented by the agent. We only want to validate correctness.\n\nJustification: Justification for scorable judgement\n\n---\n\nTask: {\"name\":\"Implement multi-crop test-time augmentation\",\"description\":\"Implement the multi-crop test-time augmentation technique described in the paper, which involves evaluating the model on multiple augmented versions of each test image.\",\"baseline\":\"No test-time augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Test-Time Augmentation\"],\"relevant_paper_text\":\"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\",\"scoring_feasibility\":8,\"llm_tractability\":0.8,\"expert_tractability\":0.9,\"layman_tractability\":0.4,\"scoring_llm_tractability\":0.9,\"asset_prerequisites\":[\"CIFAR-10 dataset\",\"Trained model\"]}\n\nRubric: criterion='The implementation correctly applies the six specified augmentations to each test image: original, up-left translation, down-right translation, and their mirrored versions.' importance=25.0 criterion='The model is evaluated on all six augmented versions of each test image.' importance=20.0 criterion='The final prediction is calculated using a weighted average of all six outputs, with correct weights: 0.25 for untranslated images and 0.125 for translated images.' importance=25.0 criterion='The implementation works with the provided CIFAR-10 dataset and trained model.' importance=15.0 criterion='The implementation is done using PyTorch, as specified in the required skills.' importance=10.0 criterion='The implementation shows improved performance compared to the baseline of no test-time augmentation.' importance=5.0\n\nReasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification:\n\n1. We need to determine if it's possible to write a Python function that can objectively score an implementation of this task.\n\n2. The task involves implementing multi-crop test-time augmentation with specific requirements:\n   - Six augmentations: original, up-left translation, down-right translation, and their mirrored versions\n   - Evaluation on all six augmented versions\n   - Weighted average calculation with specific weights\n   - Use of PyTorch and compatibility with CIFAR-10 dataset and a trained model\n\n3. We can verify most of these requirements programmatically:\n   - Check if the function applies the correct number and types of augmentations\n   - Verify that the model is evaluated on all augmented versions\n   - Confirm the correct weighted average calculation\n   - Check if PyTorch is used and if the function works with CIFAR-10 and a given model\n\n4. However, there are some challenges:\n   - Verifying the exact pixel translations (up-left and down-right by one pixel) might be difficult without access to the implementation details\n   - Comparing performance to the baseline requires running both the baseline and the new implementation, which might be beyond the scope of a simple scoring function\n\n5. Despite these challenges, the core functionality and most important aspects of the implementation can be objectively verified.\n\nScorable[True/False]: True\n\nJustification: This task is scorable because most of the key requirements can be objectively verified through a Python function. We can check the number and types of augmentations, ensure all augmented versions are evaluated, verify the weighted average calculation, and confirm the use of PyTorch and compatibility with CIFAR-10 and a trained model. While some aspects like exact pixel translations and performance improvement might be challenging to verify automatically, the core functionality of the multi-crop test-time augmentation can be objectively scored. The scoring function can analyze the input, output, and behavior of the implementation without needing to know the specific implementation details, making it possible to validate correctness for various implementations of the task."
  },
  {
    "Request": "Given the fields `candidate_input`, `candidate_output`, `requirements`, `attempt_num`, `previous_critiques`, produce the fields `self_critique`, `should_retry`.\n\n---\n\nFollow the following format.\n\nCandidate Input: ${candidate_input}\n\nCandidate Output: ${candidate_output}\n\nRequirements: ${requirements}\n\nAttempt Num: ${attempt_num}\n\nPrevious Critiques: ${previous_critiques}\n\nSelf Critique: Self-critique of how well the candidate output satisfies the requirements given the input\n\nShould Retry: Should we attempt to generate the output again taking this critique into account? True/False (Respond with true or false)\n\n---\n\nCandidate Input: {\"name\":\"Implement multi-crop test-time augmentation\",\"description\":\"Implement the multi-crop test-time augmentation technique described in the paper, which involves evaluating the model on multiple augmented versions of each test image.\",\"baseline\":\"No test-time augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Test-Time Augmentation\"],\"relevant_paper_text\":\"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\",\"scoring_feasibility\":8,\"llm_tractability\":0.8,\"expert_tractability\":0.9,\"layman_tractability\":0.4,\"scoring_llm_tractability\":0.9,\"asset_prerequisites\":[\"CIFAR-10 dataset\",\"Trained model\"]}\n\nCandidate Output:\nPrediction(\n    task_rubric=TaskRubric(rubric=[TaskCriterion(criterion='The implementation correctly applies the six specified augmentations to each test image: original, up-left translation, down-right translation, and their mirrored versions.', importance=25.0), TaskCriterion(criterion='The model is evaluated on all six augmented versions of each test image.', importance=20.0), TaskCriterion(criterion='The final prediction is calculated using a weighted average of all six outputs, with correct weights: 0.25 for untranslated images and 0.125 for translated images.', importance=25.0), TaskCriterion(criterion='The implementation works with the provided CIFAR-10 dataset and trained model.', importance=15.0), TaskCriterion(criterion='The implementation is done using PyTorch, as specified in the required skills.', importance=10.0), TaskCriterion(criterion='The implementation shows improved performance compared to the baseline of no test-time augmentation.', importance=5.0)])\n)\n\nRequirements:\nDefine a rubric for programming task evaluation, where criteria satisfy \n    Objective, programatically verifiable criterion for evaluating a task\n    implementation. Criteria may call a function and use outputs and observed\n    side effects to determine if the criterion is satisfied, but will\n    otherwise not be able to make any assumptions about how the implementation\n    works - it is a black box.\n    Criteria should never be subjective or require examining the actual code in \n    the implementation. It must be strictly empirical.\n    \n\nAttempt Num: 1\n\nPrevious Critiques:\nSelf Critique:\n\n1. Relevance and Scope: The task candidates extracted are highly relevant to the paper and reflect actual research tasks undertaken. They cover key innovations described in the paper: alternating flip data augmentation, patch-whitening initialization, and multi-crop test-time augmentation.\n\n2. Implementability: All three tasks are implementable programming tasks that modify baseline implementations. They require research and engineering expertise to implement correctly.\n\n3. Scoring Feasibility: The tasks seem feasible to score objectively. For example, the alternating flip augmentation could be scored based on whether the implementation correctly flips images in the described deterministic pattern across epochs. The patch-whitening initialization could be scored based on whether the first layer's weights match the described eigenvector initialization. The multi-crop TTA could be scored based on whether it correctly generates and weights the six augmented views.\n\n4. Difficulty Balance: The tasks appear to strike a good balance between being challenging to implement but relatively straightforward to score. Implementing these techniques correctly requires understanding the concepts and translating them into code, while scoring can be done by checking if the implementation matches the paper's description.\n\n5. Completeness: The tasks cover three major innovations from the paper. However, there may be other potential task candidates that could have been extracted, such as implementing the decoupled learning rate and weight decay optimization or the Lookahead optimizer.\n\n6. Detail Level: The task descriptions provide a good overview, but could potentially benefit from more specific implementation details or evaluation criteria.\n\n7. Baseline Specification: Each task includes a baseline implementation, which is helpful for understanding the starting point and the required modifications.\n\nOverall, the extracted task candidates appear to be well-suited for evaluating research and engineering capabilities related to the paper's innovations. They require expertise to implement correctly while being feasible to score objectively.\n\nSelf Critique:"
  },
  {
    "Response": "Self Critique: The candidate output provides a good start for defining a rubric for evaluating the implementation of multi-crop test-time augmentation. However, there are some areas that could be improved:\n\n1. Objectivity: While most criteria are objective, the last criterion about showing improved performance is somewhat subjective and may not be easily verifiable programmatically.\n\n2. Verifiability: Some criteria, like correctly applying the six specified augmentations, may be challenging to verify programmatically without examining the code. We need to focus more on observable outputs and behaviors.\n\n3. Specificity: Some criteria could be more specific. For example, instead of just stating that the implementation works with CIFAR-10 and the trained model, we could specify what exactly should be observed when using these assets.\n\n4. Completeness: The rubric covers the main aspects of the task, but it could be more comprehensive. For instance, it doesn't address error handling or performance considerations.\n\n5. Weights: The importance weights are provided, which is good, but their distribution could be reconsidered to better reflect the core requirements of the task.\n\n6. PyTorch Requirement: While using PyTorch is mentioned, it might be challenging to verify this programmatically without examining the code.\n\nTo improve, we should focus on creating criteria that are more easily verifiable through outputs or observable behaviors, be more specific about what constitutes correct implementation, and ensure all criteria are truly objective and empirically testable.\n\nShould Retry: true"
  },
  {
    "Request": "Given the fields `task_candidate`, `guidance`, produce the fields `task_rubric`.\n\n---\n\nFollow the following format.\n\nTask Candidate: ${task_candidate}\nGuidance: ${guidance}\nTask Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"description\": \"Objective, programatically verifiable criterion for evaluating a task\\nimplementation. Criteria may call a function and use outputs and observed\\nside effects to determine if the criterion is satisfied, but will\\notherwise not be able to make any assumptions about how the implementation\\nworks - it is a black box.\\nCriteria should never be subjective or require examining the actual code in \\nthe implementation. It must be strictly empirical.\", \"properties\": {\"criterion\": {\"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"\\nList of criteria for evaluating task implementation; importance should sum to 100.\\n\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\n\n---\n\nTask Candidate: {\"name\":\"Implement multi-crop test-time augmentation\",\"description\":\"Implement the multi-crop test-time augmentation technique described in the paper, which involves evaluating the model on multiple augmented versions of each test image.\",\"baseline\":\"No test-time augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Test-Time Augmentation\"],\"relevant_paper_text\":\"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\",\"scoring_feasibility\":8,\"llm_tractability\":0.8,\"expert_tractability\":0.9,\"layman_tractability\":0.4,\"scoring_llm_tractability\":0.9,\"asset_prerequisites\":[\"CIFAR-10 dataset\",\"Trained model\"]}\nGuidance:\nSelf Critique: The candidate output provides a good start for defining a rubric for evaluating the implementation of multi-crop test-time augmentation. However, there are some areas that could be improved:\n\n1. Objectivity: While most criteria are objective, the last criterion about showing improved performance is somewhat subjective and may not be easily verifiable programmatically.\n\n2. Verifiability: Some criteria, like correctly applying the six specified augmentations, may be challenging to verify programmatically without examining the code. We need to focus more on observable outputs and behaviors.\n\n3. Specificity: Some criteria could be more specific. For example, instead of just stating that the implementation works with CIFAR-10 and the trained model, we could specify what exactly should be observed when using these assets.\n\n4. Completeness: The rubric covers the main aspects of the task, but it could be more comprehensive. For instance, it doesn't address error handling or performance considerations.\n\n5. Weights: The importance weights are provided, which is good, but their distribution could be reconsidered to better reflect the core requirements of the task.\n\n6. PyTorch Requirement: While using PyTorch is mentioned, it might be challenging to verify this programmatically without examining the code.\n\nTo improve, we should focus on creating criteria that are more easily verifiable through outputs or observable behaviors, be more specific about what constitutes correct implementation, and ensure all criteria are truly objective and empirically testable.\nTask Rubric:"
  },
  {
    "Response": "{\n  \"rubric\": [\n    {\n      \"criterion\": \"The implementation correctly processes the CIFAR-10 test dataset, producing output predictions for all 10,000 test images.\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"The output shape for each test image is consistent with the expected format of the trained model (e.g., a vector of 10 class probabilities for CIFAR-10).\",\n      \"importance\": 10\n    },\n    {\n      \"criterion\": \"The implementation produces 6 sets of predictions for each test image, as evidenced by intermediate outputs or logging.\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"The final prediction for each test image is a weighted average of the 6 sets of predictions, with weights of 0.25 for the original and mirrored unmodified images, and 0.125 for the other four augmented versions.\",\n      \"importance\": 25\n    },\n    {\n      \"criterion\": \"The implementation handles edge cases correctly, such as images at the boundaries of the dataset, without raising errors.\",\n      \"importance\": 10\n    },\n    {\n      \"criterion\": \"The implementation completes the evaluation of the entire CIFAR-10 test set within a reasonable time frame (e.g., less than 1 hour on a standard GPU).\",\n      \"importance\": 10\n    },\n    {\n      \"criterion\": \"The implementation provides a way to compare the performance (e.g., accuracy) of the multi-crop augmentation method against the baseline (no augmentation) using the same model and test set.\",\n      \"importance\": 10\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.\n\n---\n\nFollow the following format.\n\nTask: ${task}\n\nRubric: ${rubric}\n\nReasoning: Let's think step by step in order to ${produce the justification}. We ...\n\nScorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified? The scoring function will only have access to the function implementing the task, and we cannot make any assumptions about how the task is implemented by the agent. We only want to validate correctness.\n\nJustification: Justification for scorable judgement\n\n---\n\nTask: {\"name\":\"Implement multi-crop test-time augmentation\",\"description\":\"Implement the multi-crop test-time augmentation technique described in the paper, which involves evaluating the model on multiple augmented versions of each test image.\",\"baseline\":\"No test-time augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Test-Time Augmentation\"],\"relevant_paper_text\":\"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\",\"scoring_feasibility\":8,\"llm_tractability\":0.8,\"expert_tractability\":0.9,\"layman_tractability\":0.4,\"scoring_llm_tractability\":0.9,\"asset_prerequisites\":[\"CIFAR-10 dataset\",\"Trained model\"]}\n\nRubric: criterion='The implementation correctly processes the CIFAR-10 test dataset, producing output predictions for all 10,000 test images.' importance=15.0 criterion='The output shape for each test image is consistent with the expected format of the trained model (e.g., a vector of 10 class probabilities for CIFAR-10).' importance=10.0 criterion='The implementation produces 6 sets of predictions for each test image, as evidenced by intermediate outputs or logging.' importance=20.0 criterion='The final prediction for each test image is a weighted average of the 6 sets of predictions, with weights of 0.25 for the original and mirrored unmodified images, and 0.125 for the other four augmented versions.' importance=25.0 criterion='The implementation handles edge cases correctly, such as images at the boundaries of the dataset, without raising errors.' importance=10.0 criterion='The implementation completes the evaluation of the entire CIFAR-10 test set within a reasonable time frame (e.g., less than 1 hour on a standard GPU).' importance=10.0 criterion='The implementation provides a way to compare the performance (e.g., accuracy) of the multi-crop augmentation method against the baseline (no augmentation) using the same model and test set.' importance=10.0\n\nReasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification:\n\n1. The task involves implementing a specific test-time augmentation technique for image classification.\n\n2. The technique is well-defined with clear steps:\n   - Generate 6 augmented versions of each test image\n   - Run the model on each augmented version\n   - Combine predictions using specified weights\n\n3. The rubric provides clear, objective criteria for evaluating the implementation:\n   - Correct processing of all 10,000 CIFAR-10 test images\n   - Proper output shape\n   - Generation of 6 sets of predictions per image\n   - Correct weighted averaging of predictions\n   - Handling of edge cases\n   - Reasonable runtime\n   - Comparison with baseline (no augmentation)\n\n4. These criteria can be objectively verified by examining the implementation's output and behavior.\n\n5. The task uses a standard dataset (CIFAR-10) and assumes a pre-trained model, which allows for consistent testing.\n\n6. The implementation can be wrapped in a function that takes the necessary inputs (model, test dataset) and produces the required outputs (augmented predictions).\n\n7. A scoring function can be written to:\n   - Check the shape and content of the output\n   - Verify the presence of intermediate results (6 sets of predictions)\n   - Validate the weighted averaging calculation\n   - Measure runtime\n   - Compare results with a baseline implementation\n\n8. The scoring function can be designed to be independent of the specific implementation details, focusing only on the correctness of the output and adherence to the specified technique.\n\nScorable[True/False]: True\n\nJustification: This task is scorable because it involves implementing a well-defined algorithm with clear, objective criteria for success. A scoring function can be written to verify that the implementation correctly processes the CIFAR-10 test dataset, produces the expected number and format of predictions, applies the specified augmentations and weighting scheme, handles edge cases, runs within a reasonable time frame, and provides a way to compare performance against a baseline. The use of a standard dataset and pre-trained model allows for consistent testing across different implementations. The scoring function can focus on the correctness of the output and adherence to the specified technique without needing to understand the internal details of the implementation."
  },
  {
    "Request": "Given the fields `candidate_input`, `candidate_output`, `requirements`, `attempt_num`, `previous_critiques`, produce the fields `self_critique`, `should_retry`.\n\n---\n\nFollow the following format.\n\nCandidate Input: ${candidate_input}\n\nCandidate Output: ${candidate_output}\n\nRequirements: ${requirements}\n\nAttempt Num: ${attempt_num}\n\nPrevious Critiques: ${previous_critiques}\n\nSelf Critique: Self-critique of how well the candidate output satisfies the requirements given the input\n\nShould Retry: Should we attempt to generate the output again taking this critique into account? True/False (Respond with true or false)\n\n---\n\nCandidate Input: {\"name\":\"Implement multi-crop test-time augmentation\",\"description\":\"Implement the multi-crop test-time augmentation technique described in the paper, which involves evaluating the model on multiple augmented versions of each test image.\",\"baseline\":\"No test-time augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Test-Time Augmentation\"],\"relevant_paper_text\":\"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\",\"scoring_feasibility\":8,\"llm_tractability\":0.8,\"expert_tractability\":0.9,\"layman_tractability\":0.4,\"scoring_llm_tractability\":0.9,\"asset_prerequisites\":[\"CIFAR-10 dataset\",\"Trained model\"]}\n\nCandidate Output:\nPrediction(\n    task_rubric=TaskRubric(rubric=[TaskCriterion(criterion='The implementation correctly processes the CIFAR-10 test dataset, producing output predictions for all 10,000 test images.', importance=15.0), TaskCriterion(criterion='The output shape for each test image is consistent with the expected format of the trained model (e.g., a vector of 10 class probabilities for CIFAR-10).', importance=10.0), TaskCriterion(criterion='The implementation produces 6 sets of predictions for each test image, as evidenced by intermediate outputs or logging.', importance=20.0), TaskCriterion(criterion='The final prediction for each test image is a weighted average of the 6 sets of predictions, with weights of 0.25 for the original and mirrored unmodified images, and 0.125 for the other four augmented versions.', importance=25.0), TaskCriterion(criterion='The implementation handles edge cases correctly, such as images at the boundaries of the dataset, without raising errors.', importance=10.0), TaskCriterion(criterion='The implementation completes the evaluation of the entire CIFAR-10 test set within a reasonable time frame (e.g., less than 1 hour on a standard GPU).', importance=10.0), TaskCriterion(criterion='The implementation provides a way to compare the performance (e.g., accuracy) of the multi-crop augmentation method against the baseline (no augmentation) using the same model and test set.', importance=10.0)])\n)\n\nRequirements:\nDefine a rubric for programming task evaluation, where criteria satisfy \n    Objective, programatically verifiable criterion for evaluating a task\n    implementation. Criteria may call a function and use outputs and observed\n    side effects to determine if the criterion is satisfied, but will\n    otherwise not be able to make any assumptions about how the implementation\n    works - it is a black box.\n    Criteria should never be subjective or require examining the actual code in \n    the implementation. It must be strictly empirical.\n    \n\nAttempt Num: 2\n\nPrevious Critiques:\nSelf Critique:\n\n1. Relevance and Scope: The task candidates extracted are highly relevant to the paper and reflect actual research tasks undertaken. They cover key innovations described in the paper: alternating flip data augmentation, patch-whitening initialization, and multi-crop test-time augmentation.\n\n2. Implementability: All three tasks are implementable programming tasks that modify baseline implementations. They require research and engineering expertise to implement correctly.\n\n3. Scoring Feasibility: The tasks seem feasible to score objectively. For example, the alternating flip augmentation could be scored based on whether the implementation correctly flips images in the described deterministic pattern across epochs. The patch-whitening initialization could be scored based on whether the first layer's weights match the described eigenvector initialization. The multi-crop TTA could be scored based on whether it correctly generates and weights the six augmented views.\n\n4. Difficulty Balance: The tasks appear to strike a good balance between being challenging to implement but relatively straightforward to score. Implementing these techniques correctly requires understanding the concepts and translating them into code, while scoring can be done by checking if the implementation matches the paper's description.\n\n5. Completeness: The tasks cover three major innovations from the paper. However, there may be other potential task candidates that could have been extracted, such as implementing the decoupled learning rate and weight decay optimization or the Lookahead optimizer.\n\n6. Detail Level: The task descriptions provide a good overview, but could potentially benefit from more specific implementation details or evaluation criteria.\n\n7. Baseline Specification: Each task includes a baseline implementation, which is helpful for understanding the starting point and the required modifications.\n\nOverall, the extracted task candidates appear to be well-suited for evaluating research and engineering capabilities related to the paper's innovations. They require expertise to implement correctly while being feasible to score objectively.\n\nSelf Critique:"
  },
  {
    "Response": "Self Critique: The candidate output provides a well-structured TaskRubric with seven TaskCriterion objects, which is a good start. However, there are several areas where the rubric could be improved to better meet the requirements:\n\n1. Objectivity: Some criteria, such as \"reasonable time frame\" and \"handles edge cases correctly,\" are somewhat subjective and may not be easily verifiable programmatically.\n\n2. Verifiability: While many criteria are objectively verifiable, the rubric doesn't specify exact methods or functions to be called for verification. For example, it doesn't detail how to check if the implementation produces 6 sets of predictions or how to verify the weighted average calculation.\n\n3. Black Box Assumption: Some criteria, like checking for 6 sets of predictions, might require examining the implementation's internals, which violates the black box requirement.\n\n4. Completeness: The rubric covers most aspects of the task but doesn't explicitly address the specific augmentations mentioned in the paper (e.g., translations and mirroring).\n\n5. Precision: Some criteria could be more precise. For instance, instead of \"consistent with the expected format,\" it could specify the exact expected output shape.\n\n6. Baseline Comparison: While the rubric includes a criterion for comparing against the baseline, it doesn't specify how this comparison should be conducted or what metrics should be used.\n\nShould Retry: true"
  },
  {
    "Request": "Given the fields `task_candidate`, `guidance`, produce the fields `task_rubric`.\n\n---\n\nFollow the following format.\n\nTask Candidate: ${task_candidate}\nGuidance: ${guidance}\nTask Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"description\": \"Objective, programatically verifiable criterion for evaluating a task\\nimplementation. Criteria may call a function and use outputs and observed\\nside effects to determine if the criterion is satisfied, but will\\notherwise not be able to make any assumptions about how the implementation\\nworks - it is a black box.\\nCriteria should never be subjective or require examining the actual code in \\nthe implementation. It must be strictly empirical.\", \"properties\": {\"criterion\": {\"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"\\nList of criteria for evaluating task implementation; importance should sum to 100.\\n\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}\n\n---\n\nTask Candidate: {\"name\":\"Implement multi-crop test-time augmentation\",\"description\":\"Implement the multi-crop test-time augmentation technique described in the paper, which involves evaluating the model on multiple augmented versions of each test image.\",\"baseline\":\"No test-time augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Test-Time Augmentation\"],\"relevant_paper_text\":\"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\",\"scoring_feasibility\":8,\"llm_tractability\":0.8,\"expert_tractability\":0.9,\"layman_tractability\":0.4,\"scoring_llm_tractability\":0.9,\"asset_prerequisites\":[\"CIFAR-10 dataset\",\"Trained model\"]}\nGuidance:\nSelf Critique: The candidate output provides a well-structured TaskRubric with seven TaskCriterion objects, which is a good start. However, there are several areas where the rubric could be improved to better meet the requirements:\n\n1. Objectivity: Some criteria, such as \"reasonable time frame\" and \"handles edge cases correctly,\" are somewhat subjective and may not be easily verifiable programmatically.\n\n2. Verifiability: While many criteria are objectively verifiable, the rubric doesn't specify exact methods or functions to be called for verification. For example, it doesn't detail how to check if the implementation produces 6 sets of predictions or how to verify the weighted average calculation.\n\n3. Black Box Assumption: Some criteria, like checking for 6 sets of predictions, might require examining the implementation's internals, which violates the black box requirement.\n\n4. Completeness: The rubric covers most aspects of the task but doesn't explicitly address the specific augmentations mentioned in the paper (e.g., translations and mirroring).\n\n5. Precision: Some criteria could be more precise. For instance, instead of \"consistent with the expected format,\" it could specify the exact expected output shape.\n\n6. Baseline Comparison: While the rubric includes a criterion for comparing against the baseline, it doesn't specify how this comparison should be conducted or what metrics should be used.\nTask Rubric:"
  },
  {
    "Response": "{\n  \"rubric\": [\n    {\n      \"criterion\": \"The implementation produces output predictions for the CIFAR-10 test set with the same shape as the baseline model (10000, 10).\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"The implementation's predictions are different from the baseline model's predictions for at least 95% of the test samples.\",\n      \"importance\": 10\n    },\n    {\n      \"criterion\": \"The implementation's average prediction time per sample is less than 10 times the baseline model's average prediction time.\",\n      \"importance\": 10\n    },\n    {\n      \"criterion\": \"The implementation's accuracy on the CIFAR-10 test set is higher than the baseline model's accuracy.\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"When provided with a single, unmodified test image, the implementation produces 6 different sets of predictions.\",\n      \"importance\": 15\n    },\n    {\n      \"criterion\": \"The final prediction for each test sample is a weighted average where two predictions have weights of 0.25 each, and four predictions have weights of 0.125 each.\",\n      \"importance\": 20\n    },\n    {\n      \"criterion\": \"When provided with a horizontally flipped test image, the implementation produces predictions that are consistent with the original image's predictions (i.e., the class probabilities are flipped accordingly).\",\n      \"importance\": 10\n    }\n  ]\n}"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.\n\n---\n\nFollow the following format.\n\nTask: ${task}\n\nRubric: ${rubric}\n\nReasoning: Let's think step by step in order to ${produce the justification}. We ...\n\nScorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified? The scoring function will only have access to the function implementing the task, and we cannot make any assumptions about how the task is implemented by the agent. We only want to validate correctness.\n\nJustification: Justification for scorable judgement\n\n---\n\nTask: {\"name\":\"Implement multi-crop test-time augmentation\",\"description\":\"Implement the multi-crop test-time augmentation technique described in the paper, which involves evaluating the model on multiple augmented versions of each test image.\",\"baseline\":\"No test-time augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Test-Time Augmentation\"],\"relevant_paper_text\":\"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\",\"scoring_feasibility\":8,\"llm_tractability\":0.8,\"expert_tractability\":0.9,\"layman_tractability\":0.4,\"scoring_llm_tractability\":0.9,\"asset_prerequisites\":[\"CIFAR-10 dataset\",\"Trained model\"]}\n\nRubric: criterion='The implementation produces output predictions for the CIFAR-10 test set with the same shape as the baseline model (10000, 10).' importance=15.0 criterion=\"The implementation's predictions are different from the baseline model's predictions for at least 95% of the test samples.\" importance=10.0 criterion=\"The implementation's average prediction time per sample is less than 10 times the baseline model's average prediction time.\" importance=10.0 criterion=\"The implementation's accuracy on the CIFAR-10 test set is higher than the baseline model's accuracy.\" importance=20.0 criterion='When provided with a single, unmodified test image, the implementation produces 6 different sets of predictions.' importance=15.0 criterion='The final prediction for each test sample is a weighted average where two predictions have weights of 0.25 each, and four predictions have weights of 0.125 each.' importance=20.0 criterion=\"When provided with a horizontally flipped test image, the implementation produces predictions that are consistent with the original image's predictions (i.e., the class probabilities are flipped accordingly).\" importance=10.0\n\nReasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification:\n\n1. We need to determine if it's possible to write a Python function that can objectively score an implementation of this task.\n\n2. The task involves implementing multi-crop test-time augmentation for image classification.\n\n3. The rubric provides several specific criteria that can be objectively measured:\n   - Output shape (10000, 10) for CIFAR-10 test set\n   - Difference from baseline predictions\n   - Prediction time compared to baseline\n   - Accuracy improvement over baseline\n   - Generation of 6 different sets of predictions for a single image\n   - Correct weighting of predictions (0.25, 0.25, 0.125, 0.125, 0.125, 0.125)\n   - Consistency of predictions for horizontally flipped images\n\n4. All of these criteria can be programmatically verified:\n   - Output shapes can be checked\n   - Predictions can be compared to baseline\n   - Execution time can be measured\n   - Accuracy can be calculated and compared\n   - Number of prediction sets can be counted\n   - Weights can be verified\n   - Flipped image predictions can be tested for consistency\n\n5. The scoring function would have access to the implementation, the baseline model, and the CIFAR-10 test set, which is sufficient to verify all criteria.\n\n6. No assumptions about the internal implementation are required; we only need to validate the correctness of the output and behavior.\n\nGiven these considerations, it appears that an objective scoring function can be written.\n\nScorable[True/False]: True\n\nJustification: It is possible to write a Python function to objectively score an implementation of this task. The rubric provides clear, measurable criteria that can be programmatically verified without making assumptions about the internal implementation. The scoring function can access the implementation, baseline model, and CIFAR-10 test set to validate output shapes, prediction differences, execution time, accuracy improvement, number of prediction sets, weighting of predictions, and consistency with flipped images. All of these checks can be performed objectively, focusing on the correctness of the output and behavior rather than the specific implementation details."
  },
  {
    "Request": "Given the fields `candidate_input`, `candidate_output`, `requirements`, `attempt_num`, `previous_critiques`, produce the fields `self_critique`, `should_retry`.\n\n---\n\nFollow the following format.\n\nCandidate Input: ${candidate_input}\n\nCandidate Output: ${candidate_output}\n\nRequirements: ${requirements}\n\nAttempt Num: ${attempt_num}\n\nPrevious Critiques: ${previous_critiques}\n\nSelf Critique: Self-critique of how well the candidate output satisfies the requirements given the input\n\nShould Retry: Should we attempt to generate the output again taking this critique into account? True/False (Respond with true or false)\n\n---\n\nCandidate Input: {\"name\":\"Implement multi-crop test-time augmentation\",\"description\":\"Implement the multi-crop test-time augmentation technique described in the paper, which involves evaluating the model on multiple augmented versions of each test image.\",\"baseline\":\"No test-time augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Test-Time Augmentation\"],\"relevant_paper_text\":\"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\",\"scoring_feasibility\":8,\"llm_tractability\":0.8,\"expert_tractability\":0.9,\"layman_tractability\":0.4,\"scoring_llm_tractability\":0.9,\"asset_prerequisites\":[\"CIFAR-10 dataset\",\"Trained model\"]}\n\nCandidate Output:\nPrediction(\n    task_rubric=TaskRubric(rubric=[TaskCriterion(criterion='The implementation produces output predictions for the CIFAR-10 test set with the same shape as the baseline model (10000, 10).', importance=15.0), TaskCriterion(criterion=\"The implementation's predictions are different from the baseline model's predictions for at least 95% of the test samples.\", importance=10.0), TaskCriterion(criterion=\"The implementation's average prediction time per sample is less than 10 times the baseline model's average prediction time.\", importance=10.0), TaskCriterion(criterion=\"The implementation's accuracy on the CIFAR-10 test set is higher than the baseline model's accuracy.\", importance=20.0), TaskCriterion(criterion='When provided with a single, unmodified test image, the implementation produces 6 different sets of predictions.', importance=15.0), TaskCriterion(criterion='The final prediction for each test sample is a weighted average where two predictions have weights of 0.25 each, and four predictions have weights of 0.125 each.', importance=20.0), TaskCriterion(criterion=\"When provided with a horizontally flipped test image, the implementation produces predictions that are consistent with the original image's predictions (i.e., the class probabilities are flipped accordingly).\", importance=10.0)])\n)\n\nRequirements:\nDefine a rubric for programming task evaluation, where criteria satisfy \n    Objective, programatically verifiable criterion for evaluating a task\n    implementation. Criteria may call a function and use outputs and observed\n    side effects to determine if the criterion is satisfied, but will\n    otherwise not be able to make any assumptions about how the implementation\n    works - it is a black box.\n    Criteria should never be subjective or require examining the actual code in \n    the implementation. It must be strictly empirical.\n    \n\nAttempt Num: 3\n\nPrevious Critiques:\nSelf Critique:\n\n1. Relevance and Scope: The task candidates extracted are highly relevant to the paper and reflect actual research tasks undertaken. They cover key innovations described in the paper: alternating flip data augmentation, patch-whitening initialization, and multi-crop test-time augmentation.\n\n2. Implementability: All three tasks are implementable programming tasks that modify baseline implementations. They require research and engineering expertise to implement correctly.\n\n3. Scoring Feasibility: The tasks seem feasible to score objectively. For example, the alternating flip augmentation could be scored based on whether the implementation correctly flips images in the described deterministic pattern across epochs. The patch-whitening initialization could be scored based on whether the first layer's weights match the described eigenvector initialization. The multi-crop TTA could be scored based on whether it correctly generates and weights the six augmented views.\n\n4. Difficulty Balance: The tasks appear to strike a good balance between being challenging to implement but relatively straightforward to score. Implementing these techniques correctly requires understanding the concepts and translating them into code, while scoring can be done by checking if the implementation matches the paper's description.\n\n5. Completeness: The tasks cover three major innovations from the paper. However, there may be other potential task candidates that could have been extracted, such as implementing the decoupled learning rate and weight decay optimization or the Lookahead optimizer.\n\n6. Detail Level: The task descriptions provide a good overview, but could potentially benefit from more specific implementation details or evaluation criteria.\n\n7. Baseline Specification: Each task includes a baseline implementation, which is helpful for understanding the starting point and the required modifications.\n\nOverall, the extracted task candidates appear to be well-suited for evaluating research and engineering capabilities related to the paper's innovations. They require expertise to implement correctly while being feasible to score objectively.\n\nSelf Critique:"
  },
  {
    "Response": "Self Critique: The candidate output provides a good starting point for a task rubric, but it has some areas for improvement:\n\n1. Comprehensiveness: The rubric covers key aspects of the multi-crop test-time augmentation technique, including the number of augmented views, the weighting of predictions, and the impact on accuracy. This aligns well with the requirements from the input.\n\n2. Objectivity: Most criteria are objective and programmatically verifiable, which is a strength. For example, checking the output shape, comparing predictions to the baseline, and measuring prediction time are all quantifiable.\n\n3. Specificity: Some criteria could be more specific. For instance, the criterion about producing 6 different sets of predictions could specify the exact augmentations (original, two translations, and their mirrored versions).\n\n4. Alignment with paper: The rubric correctly incorporates the weighting scheme mentioned in the paper (two predictions weighted 0.25, four weighted 0.125). However, it doesn't explicitly mention the specific translations (up-and-to-the-left by one pixel, down-and-to-the-right by one pixel) described in the paper.\n\n5. Performance metrics: The rubric includes a criterion for improved accuracy, which is good. However, it could benefit from a more specific threshold for improvement.\n\n6. Implementation independence: The criteria are generally implementation-agnostic, focusing on inputs, outputs, and performance rather than specific code structures, which aligns with the requirements.\n\n7. Completeness: While the rubric covers many aspects, it could potentially include a criterion to verify that the implementation handles edge cases correctly (e.g., images of different sizes).\n\nShould Retry: true"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.\n\n---\n\nFollow the following format.\n\nTask: ${task}\n\nRubric: ${rubric}\n\nReasoning: Let's think step by step in order to ${produce the justification}. We ...\n\nScorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified? The scoring function will only have access to the function implementing the task, and we cannot make any assumptions about how the task is implemented by the agent. We only want to validate correctness.\n\nJustification: Justification for scorable judgement\n\n---\n\nTask: {\"name\":\"Implement multi-crop test-time augmentation\",\"description\":\"Implement the multi-crop test-time augmentation technique described in the paper, which involves evaluating the model on multiple augmented versions of each test image.\",\"baseline\":\"No test-time augmentation\",\"skills\":[\"PyTorch\",\"Computer Vision\",\"Test-Time Augmentation\"],\"relevant_paper_text\":\"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\",\"scoring_feasibility\":8,\"llm_tractability\":0.8,\"expert_tractability\":0.9,\"layman_tractability\":0.4,\"scoring_llm_tractability\":0.9,\"asset_prerequisites\":[\"CIFAR-10 dataset\",\"Trained model\"]}\n\nRubric: criterion='The implementation produces output predictions for the CIFAR-10 test set with the same shape as the baseline model (10000, 10).' importance=15.0 criterion=\"The implementation's predictions are different from the baseline model's predictions for at least 95% of the test samples.\" importance=10.0 criterion=\"The implementation's average prediction time per sample is less than 10 times the baseline model's average prediction time.\" importance=10.0 criterion=\"The implementation's accuracy on the CIFAR-10 test set is higher than the baseline model's accuracy.\" importance=20.0 criterion='When provided with a single, unmodified test image, the implementation produces 6 different sets of predictions.' importance=15.0 criterion='The final prediction for each test sample is a weighted average where two predictions have weights of 0.25 each, and four predictions have weights of 0.125 each.' importance=20.0 criterion=\"When provided with a horizontally flipped test image, the implementation produces predictions that are consistent with the original image's predictions (i.e., the class probabilities are flipped accordingly).\" importance=10.0\n\nReasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification:\n\n1. We need to determine if it's possible to write a Python function that can objectively score an implementation of this task.\n\n2. The task involves implementing multi-crop test-time augmentation for image classification.\n\n3. The rubric provides several specific criteria that can be objectively measured:\n   - Output shape (10000, 10) for CIFAR-10 test set\n   - Difference from baseline predictions\n   - Prediction time compared to baseline\n   - Accuracy improvement over baseline\n   - Generation of 6 different sets of predictions for a single image\n   - Correct weighting of predictions (0.25, 0.25, 0.125, 0.125, 0.125, 0.125)\n   - Consistency of predictions for horizontally flipped images\n\n4. All of these criteria can be programmatically verified:\n   - Output shapes can be checked\n   - Predictions can be compared to baseline\n   - Execution time can be measured\n   - Accuracy can be calculated and compared\n   - Number of prediction sets can be counted\n   - Weights can be verified\n   - Flipped image predictions can be tested for consistency\n\n5. The scoring function would have access to the implementation, the baseline model, and the CIFAR-10 test set, which is sufficient to verify all criteria.\n\n6. No assumptions about the internal implementation are required; we only need to validate the correctness of the output and behavior.\n\nGiven these considerations, it appears that an objective scoring function can be written.\n\nScorable[True/False]: True\n\nJustification: It is possible to write a Python function to objectively score an implementation of this task. The rubric provides clear, measurable criteria that can be programmatically verified without making assumptions about the internal implementation. The scoring function can access the implementation, baseline model, and CIFAR-10 test set to validate output shapes, prediction differences, execution time, accuracy improvement, number of prediction sets, weighting of predictions, and consistency with flipped images. All of these checks can be performed objectively, focusing on the correctness of the output and behavior rather than the specific implementation details."
  }
]