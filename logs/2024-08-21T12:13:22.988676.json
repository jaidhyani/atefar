[
  {
    "Request": "Given the fields `paper_text`, produce the fields `task_candidates`.

---

Follow the following format.

Paper Text: Full text from research paper PDF
Task Candidates: JSON list of task candidate dicts with keys 'name', 'description', and 'relevant_paper_text'. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCandidate\": {\"description\": \"Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\
\
A good task candidate:\
- describes a task that an agent can implement\
- typically a programming task, e.g. modifying a 'baseline' implementation\
- which reflects or mirrors actual tasks undertaken to produce the research paper\
- and requires research and/or engineering expertise to implement correctly\
- for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\
- ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\
- the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\", \"properties\": {\"name\": {\"description\": \"Name of the task\", \"title\": \"Name\", \"type\": \"string\"}, \"description\": {\"description\": \"Description of the task\", \"title\": \"Description\", \"type\": \"string\"}, \"relevant_paper_text\": {\"description\": \"Text from the paper that is relevant to the task\", \"title\": \"Relevant Paper Text\", \"type\": \"string\"}}, \"required\": [\"name\", \"description\", \"relevant_paper_text\"], \"title\": \"TaskCandidate\", \"type\": \"object\"}}, \"properties\": {\"tasks\": {\"description\": \"Tasks extracted from the paper\", \"items\": {\"$ref\": \"#/$defs/TaskCandidate\"}, \"title\": \"Tasks\", \"type\": \"array\"}}, \"required\": [\"tasks\"], \"title\": \"TaskCandidates\", \"type\": \"object\"}

---

Paper Text:
94% on CIFAR-10 in 3.29 Seconds on a Single GPU
Keller Jordan
kjordan4077@gmail.com
Abstract
CIFAR-10 is among the most widely used datasets in machine learning, facilitating
thousands of research projects per year. To accelerate research and reduce the cost
of experiments, we introduce training methods for CIFAR-10 which reach 94%
accuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when
run on a single NVIDIA A100 GPU. As one factor contributing to these training
speeds, we propose a derandomized variant of horizontal flipping augmentation,
which we show improves over the standard method in every case where flipping is
beneficial over no flipping at all. Our code is released at https://github.com/
KellerJordan/cifar10-airbench .
1 Introduction
CIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-
ing thousands of research projects per year1. Research can be accelerated and the cost of experiments
reduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In
this paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single
NVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,
2023). To support scenarios where higher performance is needed, we additionally develop methods
targeting 95% and 96% accuracy. We release the following methods in total.
1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).
2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).
3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).
4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).
All runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-
matically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the
first uses torch.compile to improve GPU utilization. It is intended for experiments where many
networks are trained at once in order to amortize the one-time compilation cost. The non-compiled
airbench94 variant can be easily installed and run using the following command.
1pip install airbench
2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"
One motivation for the development of these training methods is that they can accelerate the experi-
mental iteration time of researchers working on compatible projects involving CIFAR-10. Another
motivation is that they can decrease the cost of projects involving a massive number of trained
networks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used
3 million trained networks to demonstrate that the outputs of a trained neural network on a given test
input follow an approximately linear function of the vector of binary choices of which examples the
model was trained on. Another example is Jordan (2023), a study on training variance which used
180 thousand trained networks to show that standard trainings have little variance in performance on
1https://paperswithcode.com/datasets
Preprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024
Figure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal
flipping augmentation, which flips each image with 50% probability per epoch. This results in some images
being redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a
deterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.
their test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds
and 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make
it possible to replicate these studies, or conduct similar ones, with fewer computational resources.
Fast training also enables the rapid accumulation of statistical significance for subtle hyperparameter
comparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10
accuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation
between runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the
improvement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training
this will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.
Our work builds on prior training speed projects. We utilize a modified version of the network,
initialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen
patch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over
prior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-
ure 1, Section 3.6, Section 5.2).
2 Background
Our objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set
in the shortest possible amount of time. Timing begins when the method is first given access to
training data, and ends when it produces test-set predictions. The method is considered valid if its
mean accuracy over repeated runs is at least 94%.
We chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the
2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more
recent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10
seconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page
(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is
tysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,
94% is the level of human accuracy reported by Karpathy (2011).
We note the following consequences of how the method is timed. First, it is permitted for the program
to begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins
when the training data is first accessed. This is helpful because otherwise the first run of training
is typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA
improves the performance of a trained network by running it on multiple augmented views of each
test input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping
TTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods
attain 93.2%, 94.4%, and 95.6% mean accuracy respectively.
The CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.
It is divided into a training set of 50,000 images and a validation set of 10,000 images. As a
matter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was
2
80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than
airbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94
can be attributed entirely to algorithmic progress rather than compute scaling.
3 Methods
3.1 Network architecture and baseline training
We train a convolutional network with a total of 1.97 million parameters, following tysam-code
(2023) with a few small changes. It contains seven convolutions with the latter six being divided into
three blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this
section we offer some comments on the main design choices.
The network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed
entirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,
2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with
no padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather
than the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff
between throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.
Following Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,
and disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is
scaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture
differs only in that we decrease the number of output channels in the third block from 512 to 256,
and we add learnable biases to the first convolution.
As our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of
0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the
maximum at 20% of the way through training, and then decreases to zero. For data augmentation we
use random horizontal flipping alongside 2-pixel random translation. For translation we use reflection
padding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note
that what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a
random 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the
network is run on both a given test image and its mirror and inferences are made based on the average
of the two outputs. With optimized choices of learning rate, momentum, and weight decay, this
baseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.
3.2 Frozen patch-whitening initialization
Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-
whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code
(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches
across the training distribution, so that their outputs have identity covariance matrix. The second 12
Figure 2: The first layer\u2019s weights after whitening
initialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,
so that input information is preserved through the
activation which follows. Figure 2 shows the result.
We do not update this layer\u2019s weights during training.
Departing from tysam-code (2023), we add learn-
able biases to this layer, yielding a small performance
boost. The biases are trained for 3 epochs, after which
we disable their gradient to increase backward-pass
throughput, which improves training speed without
reducing accuracy. We also obtain a slight perfor-
mance boost relative to tysam-code (2023) by re-
ducing the constant added to the eigenvalues during
calculation of the patch-whitening initialization for
the purpose of preventing numerical issues in the case
of a singular patch-covariance matrix.
Patch-whitening initialization is the single most impactful feature. Adding it to the baseline more
than doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.
3
Random reshuffling Alternating flip Mean accuracy
No No 93.40%
No Yes 93.48%
Yes No 93.92%
Yes Yes 94.01%
Table 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating
flip (which we propose) reduce training data redundancy and improve performance.
3.3 Identity initialization
dirac : We initialize all convolutions after the first as partial identity transforms. That is, for a
convolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an
identity transform of the input, and leave the remaining N\u2212Mto their default initialization. In
PyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight
wof each convolutional layer. This method partially follows tysam-code (2023), which used a more
complicated scheme where the identity weights are mixed in with the original initialization, which
we did not find to be more performant. With this feature added, training attains 94% accuracy in 18
epochs taking 6.8 A100-seconds.
3.4 Optimization tricks
scalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor
of64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%
in 13.5 epochs taking 5.1 A100-seconds.
lookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.
We note that Lookahead has also been found effective in prior work on training speed for ResNet-
18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6
A100-seconds.
3.5 Multi-crop evaluation
multicrop : To generate predictions, we run the trained network on six augmented views of each
test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a
version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all
three. Predictions are made using a weighted average of all six outputs, where the two views of the
untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125
each. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.
We note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-
monyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of
evaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using
more crops does improve performance, but the increase to inference time outweighs the potential
training speedup.
3.6 Alternating flip
To speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,
which we motivate as follows. When training neural networks, it is standard practice to organize
training into a set of epochs during which every training example is seen exactly once. This
differs from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),
which calls for data to be repeatedly sampled with-replacement from the training set, resulting in
examples being potentially seen multiple redundant times within a short window of training. The
use of randomly ordered epochs of data for training has a different name, being called the random
reshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our
training dataset consists of Nunique examples, then sampling data with replacement causes every
\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N
unique examples on average. On the other hand, random reshuffling leads to all Nunique examples
being seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason
that it is beneficial to maximize the number of unique inputs seen per window of training time.
4
We extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We
first note that standard random horizontal flipping augmentation can be defined as follows.
1import torch
2def random_flip ( inputs ):
3 # Applies random flipping to a batch of images
4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)
5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]
Listing 1: Random flip
If horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2
which may be seen during training. Potentially, every pair of consecutive epochs could contain every
unique input. But our main observation is that with standard random horizontal flipping, half of the
images will be redundantly flipped the same way during both epochs, so that on average only 1.5N
unique inputs will be seen.
altflip : To address this, we propose to modify standard random horizontal flipping augmentation as
follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},
we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we
flip only those inputs which were flipped in the first epoch. We provide the following implementation
which avoids the need for extra memory by using a pseudorandom function to decide the flips.
1import torch
2import hashlib
3def hash_fn (n, seed =42) :
4 k = n * seed
5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,
16)
6def alternating_flip (inputs , indices , epoch ):
7 # Applies alternating flipping to a batch of images
8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist
() ])
9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)
10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )
Listing 2: Alternating flip
The result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in
Figure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.
Adding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final
training method airbench94.py , the entire contents of which can be found in Section E. It reaches
94% accuracy in 3.83 seconds on an NVIDIA A100.
3.7 Compilation
The final step we take to speed up training is a non-algorithmic one: we compile our training
method using torch.compile in order to more efficiently utilize the GPU. This results in a training
script which is mathematically equivalent (up to small differences in floating point arithmetic) to
the non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29
A100-seconds. The downside is that the one-time compilation process takes up to several minutes to
complete before training runs can begin, so that it is only beneficial when we plan to execute many
runs of training at once. We release this version as airbench94_compiled.py .
4 95% and 96% targets
To address scenarios where somewhat higher performance is desired, we additionally develop methods
targeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .
To attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel
count of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce
the learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%
accuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.
2Assuming none of the training inputs are already mirrors of each other.
5
Figure 4: Training speedups accumulate additively. Removing individual features from airbench94
increases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces
the epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-
94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.
Figure 3: FLOPs vs. error rate tradeoff. Our
three training methods apparently follow a linear
log-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-
Vries & Taylor, 2017) augmentation and raise the
training epochs to 40. We add a third convolution to
each block, and scale the first block to 128 channels
and the second two to 512. We also add a residual
connection across the later two convolutions of each
block, which we find is still beneficial despite the fact
that we are already using identity initialization (Sec-
tion 3.3) to ease gradient flow. Finally, we reduce
the learning rate by a factor of 0.78. These changes
yield airbench96 which attains 96.05% accuracy in
46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.
Figure 3 shows the FLOPs and error rate of each of
our three training methods.
5 Experiments
5.1 Interaction between features
To gain a better sense of the impact of each feature on training speed, we compare two quantities.
First, we measure the number of epochs that can be saved by adding the feature to the whitened
baseline (Section 3.2). Second, we measure the number of epochs that must be added when the
feature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-
tion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it
from the final airbench94 increases epochs-to-94% from 9.9 to 12.8.
Figure 4 shows both quantities for each feature. Surprisingly, we find that for all features except
multi-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even
though the whitened baseline requires more than twice as many epochs as the final configuration.
This indicates that the interaction between most features is additive rather than multiplicative.
5.2 Does alternating flip generalize?
In this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of
training configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all
cases except those where neither alternating nor random flip improve over using no flipping at all.
For CIFAR-10 we consider the performance boost given by alternating flip across the follow-
ing 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and
airbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For
6
Figure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,
the use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%
training speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.
95% confidence intervals are shown around each point.
each configuration we compare the performance of alternating and random flip in terms of their mean
accuracy across n= 400 runs of training.
Figure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip
improves performance in every setting. To get a sense for how big the improvement is, we estimate
the effective speedup for each case, i.e., the fraction of epochs that could be saved by switching
from random to alternating flip while maintaining the level of accuracy of random flip. We begin
by fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of
each random flip-based training configuration. We use these curves to calculate the effective speedup
afforded by switching from random to alternating flip. For example, airbench94 with random flip
and without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The
same configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law
fit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.
Note that using a power-law yields a more conservative estimate relative to using linear interpolation
between the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.
Table 2 shows the result. We observe the following patterns. First, the addition of extra augmentation
(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that
the main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the
same way for many epochs in a row; we speculate that adding extra augmentation reduces the negative
impact of these cases because it increases data diversity. Next, TTA reduces the gap between random
and alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),
indicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training
for longer consistently increases the effective speedup given by alternating flip.
We next study ImageNet trainings with the following experiment. We train a ResNet-18 with a
variety of train and test crops, comparing three flipping options: alternating flip, random flip, and no
flipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192
center crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)
to denote the latter. We also consider two training crops: 192x192 inception-style random resized
crop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area
ranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the
shorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write
7
Baseline Cutout Epochs Speedup Speedup (w/ TTA)
airbench94 No 10 15.0% 5.30%
airbench94 No 20 27.1% 21.3%
airbench94 No 40 38.3% 36.4%
airbench94 No 80 102% 31.8%
airbench94 Yes 10 3.84% 1.13%
airbench94 Yes 20 7.42% 2.00%
airbench94 Yes 40 18.6% 9.28%
airbench94 Yes 80 29.2% 14.25%
airbench96 Yes 10 4.94% 1.11%
airbench96 Yes 20 8.99% 3.58%
airbench96 Yes 40 17.2% 6.48%
airbench96 Yes 80 18.8% Not measured
Table 2: Effective speedups given by switching from random flip to alternating flip. The two configurations
most closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw
accuracy values of the airbench94 experiments.
Flipping augmentation option
Train crop Test crop Epochs TTA None Random Alternating
Heavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28
Heavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28
Light RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26
Light RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26
Heavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28
Heavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28
Light RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26
Light RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26
Light RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16
Heavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4
Table 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for
those trainings where random flip improves significantly over not flipping at all. The single best flipping option
in each row is bolded when the difference is statistically significant.
Heavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided
in Section C.
Table 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC
is better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly
better when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test
resolution discrepancy (Touvron et al., 2019).
For trainings which use Light RRC, we find that switching from random flip to alternating flip
provides a substantial boost to performance, amounting to a training speedup of more than 25%.
In Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to
alternating flip improves performance by more than increasing the training duration from 16 to 20
epochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our
results on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit
from alternating flip. Indeed, even turning flipping off completely does not significantly reduce the
performance of these trainings. We conclude that alternating flip improves over random flip for every
training scenario where the latter improves over no flipping at all.
8
Epochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE
1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312
2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233
1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269
1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533
2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433
1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444
Table 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).
5.3 Variance and class-wise calibration
Previous sections have focused on understanding what factors affect the first moment of accuracy
(the mean). In this section we investigate the second moment, finding that TTA reduces variance at
the cost of calibration.
Our experiment is to execute 10,000 runs of airbench94 training with several hyperparameter
settings. For each setting we report both the variance in test-set accuracy as well as an estimate of the
distribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.
Table 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set
variance, replicating the main finding of Jordan (2023). This is a surprising result because these
trainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7
as much distribution-wise variance when run for a similar duration, and reached a low variance only
when run for 64 epochs. We conclude from this comparison that distribution-wise variance is more
strongly connected to the rate of convergence of a training rather than its duration as such. We also
note that the low distribution-wise variance of airbench94 indicates it has high training stability.
Using TTA significantly reduces the test-set variance, such that all three settings with TTA have lower
test-set variance than any setting without TTA. However, test-set variance is implied by the class-wise
calibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this
reduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,
we compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which
measures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA
has a higher CACE than every setting without TTA, confirming the hypothesis.
6 Discussion
In this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7
faster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the
airbench Python package.
We developed airbench solely with the goal of maximizing training speed on CIFAR-10. In
Section B we find that it also generalizes well to other tasks. For example, without any extra tuning,
airbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.
One factor contributing to the training speed of airbench was our finding that training can be accel-
erated by partially derandomizing the standard random horizontal flipping augmentation, resulting in
the variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating
flip improves the performance of every training we considered (Section 5.2), with the exception of
those trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to
us, the standard ImageNet trainings that we considered do not significantly benefit from horizontal
flipping. Future work might investigate whether it is possible to obtain derandomized improvements
to other augmentations besides horizontal flip.
The methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,
with fixed performance and hardware constraints. These constraints mean that we cannot improve
performance by simply scaling up the amount of computational resources used; instead we are forced
to develop new methods like the alternating flip. We look forward to seeing what other new methods
future work discovers to push training speed further.
9
References
Dimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.
Dan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-
performance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,
2011.
Cody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,
Kunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-
mark and competition. 2017.
Luke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet
or cifar-10. arXiv preprint arXiv:1810.03505 , 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,
pp. 248\u2013255. IEEE, 2009.
Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks
with cutout. arXiv preprint arXiv:1708.04552 , 2017.
Mert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic
gradient descent. Mathematical Programming , 186:49\u201384, 2021.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415 , 2016.
Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-
models: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.
pmlr, 2015.
Yiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of
sgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.
Keller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and
inevitable. arXiv preprint arXiv:2304.01910 , 2023.
Andrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.
io/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for
advanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT
License.
Guillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander
M \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.
Thomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,
Benjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.
Benchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural
Information Processing Systems , 35:25404\u201325421, 2022.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.
Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep
learning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.
David Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/
how-to-train-your-resnet-8-bag-of-tricks/ .
10
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical
Statistics , 22(3):400\u2013407, 1951.
Santiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:
//github.com/apple/ml-cifar-10-faster , 2019.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556 , 2014.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.
tysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/
hlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.
Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution
discrepancy. Advances in neural information processing systems , 32, 2019.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,
2016.
Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps
forward, 1 step back. Advances in neural information processing systems , 32, 2019.
11
A Network architecture
1from torch import nn
2
3class Flatten (nn. Module ):
4 def forward (self , x):
5 return x. view (x. size (0) , -1)
6
7class Mul (nn. Module ):
8 def __init__ (self , scale ):
9 super (). __init__ ()
10 self . scale = scale
11 def forward (self , x):
12 return x * self . scale
13
14def conv (ch_in , ch_out ):
15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,
16 padding =\u2019same \u2019, bias = False )
17
18def make_net ():
19 act = lambda : nn. GELU ()
20 bn = lambda ch: nn. BatchNorm2d (ch)
21 return nn. Sequential (
22 nn. Sequential (
23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),
24 act () ,
25 ),
26 nn. Sequential (
27 conv (24 , 64) ,
28 nn. MaxPool2d (2) ,
29 bn (64) , act () ,
30 conv (64 , 64) ,
31 bn (64) , act () ,
32 ),
33 nn. Sequential (
34 conv (64 , 256) ,
35 nn. MaxPool2d (2) ,
36 bn (256) , act () ,
37 conv (256 , 256) ,
38 bn (256) , act () ,
39 ),
40 nn. Sequential (
41 conv (256 , 256) ,
42 nn. MaxPool2d (2) ,
43 bn (256) , act () ,
44 conv (256 , 256) ,
45 bn (256) , act () ,
46 ),
47 nn. MaxPool2d (3) ,
48 Flatten () ,
49 nn. Linear (256 , 10, bias = False ),
50 Mul (1/9) ,
51 )
Listing 3: Minimal PyTorch code for the network architecture used by airbench94 .
We note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock
time. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel
size of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this
does not improve the wallclock training time on an A100. Reducing the batch size is another easy
way to save FLOPs but not wallclock time.
12
Dataset Flipping? Cutout? ResNet-18 airbench96
CIFAR-10 Yes No 95.55% 95.61%
CIFAR-10 Yes Yes 96.01% 96.05%
CIFAR-100 Yes No 77.54% 79.27%
CIFAR-100 Yes Yes 78.04% 79.76%
CINIC-10 Yes No 87.58% 87.78%
CINIC-10 Yes Yes not measured 88.22%
SVHN No No 97.35% 97.38%
SVHN No Yes not measured 97.64%
Table 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We
directly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off
flipping for SVHN).
B Extra dataset experiments
We developed airbench with the singular goal of maximizing training speed on CIFAR-10. To
find out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its
performance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-
10 (Darlow et al., 2018).
On CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both
the case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not
(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than
ResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say
that it generalizes.
We compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.
We do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal
on CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better
performance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where
airbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is
used and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows
strong generalization to other tasks.
We note that this comparison between airbench96 and ResNet-18 training is fair in the sense that
it does demonstrate that the former has good generalization, but unfair in the sense that it does not
indicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time
augmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training
would outperform airbench96 if it were run using test-time augmentation. However, it also takes
5-10 times longer to complete. The decision of which to use may be situational.
The accuracy values we report for ResNet-18 training are from the following sources. We tried
to select the highest values we could find for each setting. Moreau et al. (2022) reports attaining
95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports
attaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on
CIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.
C ImageNet training details
Our ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/
libffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and
momentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution
160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of
training. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.
13
Hyperparameters Flipping augmentation option
Epochs Cutout TTA None Random Alternating
10 No No 92.3053 93.0988 93.2798
20 No No 92.8166 93.7446 93.8652
40 No No 93.0143 94.0133 94.0729
80 No No 93.0612 94.1169 94.1628
10 No Yes 93.4071 93.9488 94.0186
20 No Yes 93.8528 94.5565 94.6530
40 No Yes 94.0381 94.7803 94.8203
80 No Yes 94.0638 94.8506 94.8676
10 Yes No 91.8487 92.0402 92.1374
20 Yes No 92.8474 93.3825 93.4876
40 Yes No 93.2675 94.1014 94.1952
80 Yes No 93.4193 94.4311 94.5204
10 Yes Yes 92.6455 92.7780 92.8103
20 Yes Yes 93.7862 94.1306 94.1670
40 Yes Yes 94.3090 94.8511 94.8960
80 Yes Yes 94.5253 95.1839 95.2362
Table 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a
mean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise
difference in means is statistically significant.
D Extra tables & figures
Figure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in
Section 5.3.
E Complete training code
1\"\"\"
2airbench94 .py
33.83 s runtime on an A100 ; 0.36 PFLOPs .
4Evidence for validity : 94.01 average accuracy in n =1000 runs .
5
6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB
with the following nvidia -smi:
7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :
11.7
14
8torch . __version__ == \u20192.1.2+ cu118 \u2019
9\"\"\"
10
11# ############################################
12# Setup / Hyperparameters #
13# ############################################
14
15import os
16import sys
17import uuid
18from math import ceil
19
20import torch
21from torch import nn
22import torch .nn. functional as F
23import torchvision
24import torchvision . transforms as T
25
26torch . backends . cudnn . benchmark = True
27
28\"\"\"
29We express the main training hyperparameters ( batch size , learning
rate , momentum , and weight decay ) in decoupled form , so that each
one can be tuned independently . This accomplishes the following :
30* Assuming time - constant gradients , the average step size is decoupled
from everything but the lr.
31* The size of the weight decay update is decoupled from everything but
the wd.
32In constrast , normally when we increase the ( Nesterov ) momentum , this
also scales up the step size proportionally to 1 + 1 / (1 -
momentum ), meaning we cannot change momentum without having to re -
tune the learning rate . Similarly , normally when we increase the
learning rate this also increases the size of the weight decay ,
requiring a proportional decrease in the wd to maintain the same
decay strength .
33
34The practical impact is that hyperparameter tuning is faster , since
this parametrization allows each one to be tuned independently .
See https :// myrtle .ai/ learn /how -to -train -your - resnet -5-
hyperparameters /.
35\"\"\"
36
37hyp = {
38 \u2019opt \u2019: {
39 \u2019 train_epochs \u2019: 9.9 ,
40 \u2019batch_size \u2019: 1024 ,
41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples
42 \u2019momentum \u2019: 0.85 ,
43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (
decoupled from learning rate )
44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not
weight decay ) for BatchNorm biases
45 \u2019 label_smoothing \u2019: 0.2 ,
46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the
whitening layer bias before freezing
47 },
48 \u2019aug \u2019: {
49 \u2019flip \u2019: True ,
50 \u2019translate \u2019: 2,
51 },
52 \u2019net \u2019: {
53 \u2019widths \u2019: {
54 \u2019block1 \u2019: 64,
55 \u2019block2 \u2019: 256 ,
56 \u2019block3 \u2019: 256 ,
15
57 },
58 \u2019 batchnorm_momentum \u2019: 0.6 ,
59 \u2019 scaling_factor \u2019: 1/9 ,
60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :
0= none , 1= mirror , 2= mirror + translate
61 },
62}
63
64# ############################################
65# DataLoader #
66# ############################################
67
68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )
69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )
70
71def batch_flip_lr ( inputs ):
72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .
view (-1, 1, 1, 1)
73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )
74
75def batch_crop (images , crop_size ):
76 r = ( images . size ( -1) - crop_size )//2
77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =
images . device )
78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),
device = images .device , dtype = images . dtype )
79 # The two cropping methods in this if - else produce equivalent
results , but the second is faster for r > 2.
80 if r <= 2:
81 for sy in range (-r, r +1):
82 for sx in range (-r, r +1):
83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)
84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size
, r+sx:r+sx+ crop_size ]
85 else :
86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size
+2* r), device = images .device , dtype = images . dtype )
87 for s in range (-r, r +1):
88 mask = ( shifts [:, 0] == s)
89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]
90 for s in range (-r, r +1):
91 mask = ( shifts [:, 1] == s)
92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+
crop_size ]
93 return images_out
94
95class CifarLoader :
96 \"\"\"
97 GPU - accelerated dataloader for CIFAR -10 which implements
alternating flip augmentation .
98 \"\"\"
99
100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,
drop_last =None , shuffle =None , gpu =0):
101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .
pt \u2019)
102 if not os. path . exists ( data_path ):
103 dset = torchvision . datasets . CIFAR10 (path , download =True ,
train = train )
104 images = torch . tensor ( dset . data )
105 labels = torch . tensor ( dset . targets )
106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:
dset . classes }, data_path )
107
108 data = torch . load ( data_path , map_location = torch . device (gpu))
16
109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [
\u2019labels \u2019], data [\u2019classes \u2019]
110 # It \u2019s faster to load + process uint8 data than to load
preprocessed fp16 data
111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).
to( memory_format = torch . channels_last )
112
113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )
114 self . proc_images = {} # Saved results of image processing to
be done on the first epoch
115 self . epoch = 0
116
117 self . aug = aug or {}
118 for k in self .aug . keys ():
119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019
% k
120
121 self . batch_size = batch_size
122 self . drop_last = train if drop_last is None else drop_last
123 self . shuffle = train if shuffle is None else shuffle
124
125 def __len__ ( self ):
126 return len( self . images )// self . batch_size if self . drop_last
else ceil (len( self . images )/ self . batch_size )
127
128 def __iter__ ( self ):
129
130 if self . epoch == 0:
131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .
images )
132 # Randomly flip all images on the first epoch as according
to definition of alternating flip
133 if self .aug.get(\u2019flip \u2019, False ):
134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (
images )
135 # Pre -pad images to save time when doing random
translation
136 pad = self .aug.get (\u2019translate \u2019, 0)
137 if pad > 0:
138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019
reflect \u2019)
139
140 if self .aug.get(\u2019translate \u2019, 0) > 0:
141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .
shape [ -2])
142 elif self .aug.get (\u2019flip \u2019, False ):
143 images = self . proc_images [\u2019flip \u2019]
144 else :
145 images = self . proc_images [\u2019norm \u2019]
146 if self .aug.get(\u2019flip \u2019, False ):
147 if self . epoch % 2 == 1:
148 images = images . flip ( -1)
149
150 self . epoch += 1
151
152 indices = ( torch . randperm if self . shuffle else torch . arange )(
len ( images ), device = images . device )
153 for i in range (len ( self )):
154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]
155 yield ( images [ idxs ], self . labels [ idxs ])
156
157# ############################################
158# Network Components #
159# ############################################
160
17
161class Flatten (nn. Module ):
162 def forward (self , x):
163 return x. view (x. size (0) , -1)
164
165class Mul (nn. Module ):
166 def __init__ (self , scale ):
167 super (). __init__ ()
168 self . scale = scale
169 def forward (self , x):
170 return x * self . scale
171
172class BatchNorm (nn. BatchNorm2d ):
173 def __init__ (self , num_features , momentum , eps =1e -12 ,
174 weight =False , bias = True ):
175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )
176 self . weight . requires_grad = weight
177 self . bias . requires_grad = bias
178 # Note that PyTorch already initializes the weights to one and
biases to zero
179
180class Conv (nn. Conv2d ):
181 def __init__ (self , in_channels , out_channels , kernel_size =3,
padding =\u2019same \u2019, bias = False ):
182 super (). __init__ ( in_channels , out_channels , kernel_size =
kernel_size , padding = padding , bias = bias )
183
184 def reset_parameters ( self ):
185 super (). reset_parameters ()
186 if self . bias is not None :
187 self . bias . data . zero_ ()
188 w = self . weight . data
189 torch .nn. init . dirac_ (w[:w. size (1) ])
190
191class ConvGroup (nn. Module ):
192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):
193 super (). __init__ ()
194 self . conv1 = Conv ( channels_in , channels_out )
195 self . pool = nn. MaxPool2d (2)
196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )
197 self . conv2 = Conv ( channels_out , channels_out )
198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )
199 self . activ = nn. GELU ()
200
201 def forward (self , x):
202 x = self . conv1 (x)
203 x = self . pool (x)
204 x = self . norm1 (x)
205 x = self . activ (x)
206 x = self . conv2 (x)
207 x = self . norm2 (x)
208 x = self . activ (x)
209 return x
210
211# ############################################
212# Network Definition #
213# ############################################
214
215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019
][ \u2019 batchnorm_momentum \u2019]):
216 whiten_kernel_size = 2
217 whiten_width = 2 * 3 * whiten_kernel_size **2
218 net = nn. Sequential (
219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True
),
220 nn. GELU () ,
18
221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],
batchnorm_momentum ),
222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],
batchnorm_momentum ),
223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],
batchnorm_momentum ),
224 nn. MaxPool2d (3) ,
225 Flatten () ,
226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),
227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,
228 )
229 net [0]. weight . requires_grad = False
230 net = net. half (). cuda ()
231 net = net.to( memory_format = torch . channels_last )
232 for mod in net. modules ():
233 if isinstance (mod , BatchNorm ):
234 mod . float ()
235 return net
236
237# ############################################
238# Whitening Conv Initialization #
239# ############################################
240
241def get_patches (x, patch_shape ):
242 c, (h, w) = x. shape [1] , patch_shape
243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h
,w). float ()
244
245def get_whitening_parameters ( patches ):
246 n,c,h,w = patches . shape
247 patches_flat = patches . view (n, -1)
248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n
249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance
, UPLO =\u2019U\u2019)
250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.
reshape (c*h*w,c,h,w). flip (0)
251
252def init_whitening_conv (layer , train_set , eps =5e -4):
253 patches = get_patches ( train_set , patch_shape = layer . weight . data .
shape [2:])
254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )
255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)
256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -
eigenvectors_scaled ))
257
258# ###########################################
259# Lookahead #
260# ###########################################
261
262class LookaheadState :
263 def __init__ (self , net):
264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().
items ()}
265
266 def update (self , net , decay ):
267 for ema_param , net_param in zip( self . net_ema . values () , net.
state_dict (). values ()):
268 if net_param . dtype in ( torch .half , torch . float ):
269 ema_param . lerp_ ( net_param , 1- decay )
270 net_param . copy_ ( ema_param )
271
272# ###########################################
273# Logging #
274# ###########################################
275
19
276def print_columns ( columns_list , is_head =False , is_final_entry = False ):
277 print_string = \u2019\u2019
278 for col in columns_list :
279 print_string += \u2019| %s \u2019 % col
280 print_string += \u2019|\u2019
281 if is_head :
282 print (\u2019-\u2019*len( print_string ))
283 print ( print_string )
284 if is_head or is_final_entry :
285 print (\u2019-\u2019*len( print_string ))
286
287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,
\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]
288def print_training_details ( variables , is_final_entry ):
289 formatted = []
290 for col in logging_columns_list :
291 var = variables .get (col. strip () , None )
292 if type (var) in (int , str):
293 res = str(var)
294 elif type (var) is float :
295 res = \u2019{:0.4 f}\u2019. format ( var)
296 else :
297 assert var is None
298 res = \u2019\u2019
299 formatted . append (res. rjust (len (col)))
300 print_columns ( formatted , is_final_entry = is_final_entry )
301
302# ###########################################
303# Evaluation #
304# ###########################################
305
306def infer (model , loader , tta_level =0) :
307 \"\"\"
308 Test - time augmentation strategy (for tta_level =2):
309 1. Flip / mirror the image left -to - right (50% of the time ).
310 2. Translate the image by one pixel either up -and - left or down -and
- right (50% of the time , i.e. both happen 25% of the time ).
311
312 This creates 6 views per image ( left / right times the two
translations and no - translation ), which we evaluate and then
weight according to the given probabilities .
313 \"\"\"
314
315 def infer_basic (inputs , net):
316 return net( inputs ). clone ()
317
318 def infer_mirror (inputs , net):
319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))
320
321 def infer_mirror_translate ( inputs , net ):
322 logits = infer_mirror (inputs , net )
323 pad = 1
324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)
325 inputs_translate_list = [
326 padded_inputs [:, :, 0:32 , 0:32] ,
327 padded_inputs [:, :, 2:34 , 2:34] ,
328 ]
329 logits_translate_list = [ infer_mirror ( inputs_translate , net)
330 for inputs_translate in
inputs_translate_list ]
331 logits_translate = torch . stack ( logits_translate_list ). mean (0)
332 return 0.5 * logits + 0.5 * logits_translate
333
334 model . eval ()
335 test_images = loader . normalize ( loader . images )
20
336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][
tta_level ]
337 with torch . no_grad ():
338 return torch .cat ([ infer_fn (inputs , model ) for inputs in
test_images . split (2000) ])
339
340def evaluate (model , loader , tta_level =0) :
341 logits = infer (model , loader , tta_level )
342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()
343
344# ###########################################
345# Training #
346# ###########################################
347
348def main (run):
349
350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]
351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]
352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]
353 # Assuming gradients are constant in time , for Nesterov momentum ,
the below ratio is how much larger the default steps will be than
the underlying per - example gradients . We divide the learning rate
by this ratio in order to ensure steps are the same scale as
gradients , regardless of the choice of momentum .
354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))
355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning
rate for PyTorch SGD
356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale
357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]
358
359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019
label_smoothing \u2019], reduction =\u2019none \u2019)
360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)
361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =
batch_size , aug=hyp[\u2019aug \u2019])
362 if run == \u2019warmup \u2019:
363 # The only purpose of the first run is to warmup , so we can
use dummy data
364 train_loader . labels = torch . randint (0, 10, size =( len (
train_loader . labels ) ,), device = train_loader . labels . device )
365 total_train_steps = ceil (len( train_loader ) * epochs )
366
367 model = make_net ()
368 current_steps = 0
369
370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in
k and p. requires_grad ]
371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019
not in k and p. requires_grad ]
372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,
weight_decay =wd/ lr_biases ),
373 dict ( params = other_params , lr=lr , weight_decay =wd/
lr)]
374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,
nesterov = True )
375
376 def triangle (steps , start =0, end =0, peak =0.5) :
377 xp = torch . tensor ([0 , int( peak * steps ), steps ])
378 fp = torch . tensor ([ start , 1, end ])
379 x = torch . arange (1+ steps )
380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])
381 b = fp [: -1] - (m * xp [: -1])
382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1
383 indices = torch . clamp ( indices , 0, len(m) - 1)
384 return m[ indices ] * x + b[ indices ]
21
385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,
peak =0.23)
386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:
lr_schedule [i])
387
388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /
total_train_steps )**3
389 lookahead_state = LookaheadState ( model )
390
391 # For accurately timing GPU code
392 starter = torch . cuda . Event ( enable_timing = True )
393 ender = torch . cuda . Event ( enable_timing = True )
394 total_time_seconds = 0.0
395
396 # Initialize the first layer using statistics of training images
397 starter . record ()
398 train_images = train_loader . normalize ( train_loader . images [:5000])
399 init_whitening_conv ( model [0] , train_images )
400 ender . record ()
401 torch . cuda . synchronize ()
402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )
403
404 for epoch in range ( ceil ( epochs )):
405
406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019
whiten_bias_epochs \u2019])
407
408 # ###################
409 # Training #
410 # ###################
411
412 starter . record ()
413
414 model . train ()
415 for inputs , labels in train_loader :
416
417 outputs = model ( inputs )
418 loss = loss_fn ( outputs , labels ).sum ()
419 optimizer . zero_grad ( set_to_none = True )
420 loss . backward ()
421 optimizer . step ()
422 scheduler . step ()
423
424 current_steps += 1
425
426 if current_steps % 5 == 0:
427 lookahead_state . update (model , decay = alpha_schedule [
current_steps ]. item ())
428
429 if current_steps >= total_train_steps :
430 if lookahead_state is not None :
431 lookahead_state . update (model , decay =1.0)
432 break
433
434 ender . record ()
435 torch . cuda . synchronize ()
436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )
437
438 # ###################
439 # Evaluation #
440 # ###################
441
442 # Print the accuracy and loss from the last training batch of
the epoch
22
443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().
mean (). item ()
444 train_loss = loss . item () / batch_size
445 val_acc = evaluate (model , test_loader , tta_level =0)
446 print_training_details ( locals () , is_final_entry = False )
447 run = None # Only print the run number once
448
449 # ###################
450 # TTA Evaluation #
451 # ###################
452
453 starter . record ()
454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019
tta_level \u2019])
455 ender . record ()
456 torch . cuda . synchronize ()
457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )
458
459 epoch = \u2019eval \u2019
460 print_training_details ( locals () , is_final_entry = True )
461
462 return tta_val_acc
463
464if __name__ == \" __main__ \":
465 with open (sys. argv [0]) as f:
466 code = f. read ()
467
468 print_columns ( logging_columns_list , is_head = True )
469 main (\u2019warmup \u2019)
470 accs = torch . tensor ([ main (run) for run in range (25) ])
471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))
472
473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }
474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))
475 os. makedirs ( log_dir , exist_ok = True )
476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)
477 print (os. path . abspath ( log_path ))
478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))
Listing 4: airbench94.py
23
Task Candidates:"
  },
  {
    "Response": "Here are some potential task candidates extracted from the paper:

{
  \"tasks\": [
    {
      \"name\": \"Implement alternating flip data augmentation\",
      \"description\": \"Implement the proposed 'alternating flip' data augmentation method, which deterministically alternates which images are flipped horizontally on each epoch after the first epoch. Compare its performance to standard random flipping augmentation.\",
      \"relevant_paper_text\": \"To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6,...}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7,...}, we flip only those inputs which were flipped in the first epoch.\"
    },
    {
      \"name\": \"Implement patch-whitening initialization\",
      \"description\": \"Implement the patch-whitening initialization method for the first convolutional layer, which initializes the filters based on the covariance matrix of patches from the training data. Compare its impact on training speed to standard initialization.\",
      \"relevant_paper_text\": \"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\"
    },
    {
      \"name\": \"Implement multi-crop test-time augmentation\",
      \"description\": \"Implement the proposed multi-crop test-time augmentation method, which evaluates 6 augmented views of each test image. Compare its impact on accuracy to standard single-crop evaluation.\",
      \"relevant_paper_text\": \"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three. Predictions are made using a weighted average of all six outputs, where the two views of the untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125 each.\"
    }
  ]
}"
  },
  {
    "Request": "Make a very succinct json object that validates with the following schema

---

Follow the following format.

Json Schema: ${json_schema}
Json Object: ${json_object}

---

Json Schema: {\"$defs\": {\"TaskCandidate\": {\"description\": \"Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\
\
A good task candidate:\
- describes a task that an agent can implement\
- typically a programming task, e.g. modifying a 'baseline' implementation\
- which reflects or mirrors actual tasks undertaken to produce the research paper\
- and requires research and/or engineering expertise to implement correctly\
- for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\
- ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\
- the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\", \"properties\": {\"name\": {\"description\": \"Name of the task\", \"title\": \"Name\", \"type\": \"string\"}, \"description\": {\"description\": \"Description of the task\", \"title\": \"Description\", \"type\": \"string\"}, \"relevant_paper_text\": {\"description\": \"Text from the paper that is relevant to the task\", \"title\": \"Relevant Paper Text\", \"type\": \"string\"}}, \"required\": [\"name\", \"description\", \"relevant_paper_text\"], \"title\": \"TaskCandidate\", \"type\": \"object\"}}, \"properties\": {\"tasks\": {\"description\": \"Tasks extracted from the paper\", \"items\": {\"$ref\": \"#/$defs/TaskCandidate\"}, \"title\": \"Tasks\", \"type\": \"array\"}}, \"required\": [\"tasks\"], \"title\": \"TaskCandidates\", \"type\": \"object\"}
Json Object:"
  },
  {
    "Response": "Json Object: {
  \"tasks\": [
    {
      \"name\": \"Implement Attention Mechanism\",
      \"description\": \"Develop an attention mechanism for a neural network model\",
      \"relevant_paper_text\": \"We propose a novel attention mechanism that enhances the model's ability to focus on important features.\"
    }
  ]
}"
  },
  {
    "Request": "Given the fields `paper_text`, produce the fields `task_candidates`.

---

Follow the following format.

Paper Text: Full text from research paper PDF
Past Error in Task Candidates: An error to avoid in the future
Task Candidates: JSON list of task candidate dicts with keys 'name', 'description', and 'relevant_paper_text'. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCandidate\": {\"description\": \"Eval task candidate extracted from a research paper to be used as the basis for evaluating research/engineering capabilities\
\
A good task candidate:\
- describes a task that an agent can implement\
- typically a programming task, e.g. modifying a 'baseline' implementation\
- which reflects or mirrors actual tasks undertaken to produce the research paper\
- and requires research and/or engineering expertise to implement correctly\
- for which it is relatively straightforward to write an objective scoring function which aassigns scores to task implementations\
- ideally, it should be much easier to produce an implementation scoring function (given access to the paper) than to implement the task itself (without access to the paper)\
- the ultimate goal is to evaluate the research/engineering capabilities of an agent stronger than the agent writing the task specification\", \"properties\": {\"name\": {\"description\": \"Name of the task\", \"title\": \"Name\", \"type\": \"string\"}, \"description\": {\"description\": \"Description of the task\", \"title\": \"Description\", \"type\": \"string\"}, \"relevant_paper_text\": {\"description\": \"Text from the paper that is relevant to the task\", \"title\": \"Relevant Paper Text\", \"type\": \"string\"}}, \"required\": [\"name\", \"description\", \"relevant_paper_text\"], \"title\": \"TaskCandidate\", \"type\": \"object\"}}, \"properties\": {\"tasks\": {\"description\": \"Tasks extracted from the paper\", \"items\": {\"$ref\": \"#/$defs/TaskCandidate\"}, \"title\": \"Tasks\", \"type\": \"array\"}}, \"required\": [\"tasks\"], \"title\": \"TaskCandidates\", \"type\": \"object\"}

---

Paper Text:
94% on CIFAR-10 in 3.29 Seconds on a Single GPU
Keller Jordan
kjordan4077@gmail.com
Abstract
CIFAR-10 is among the most widely used datasets in machine learning, facilitating
thousands of research projects per year. To accelerate research and reduce the cost
of experiments, we introduce training methods for CIFAR-10 which reach 94%
accuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when
run on a single NVIDIA A100 GPU. As one factor contributing to these training
speeds, we propose a derandomized variant of horizontal flipping augmentation,
which we show improves over the standard method in every case where flipping is
beneficial over no flipping at all. Our code is released at https://github.com/
KellerJordan/cifar10-airbench .
1 Introduction
CIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-
ing thousands of research projects per year1. Research can be accelerated and the cost of experiments
reduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In
this paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single
NVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,
2023). To support scenarios where higher performance is needed, we additionally develop methods
targeting 95% and 96% accuracy. We release the following methods in total.
1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).
2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).
3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).
4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).
All runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-
matically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the
first uses torch.compile to improve GPU utilization. It is intended for experiments where many
networks are trained at once in order to amortize the one-time compilation cost. The non-compiled
airbench94 variant can be easily installed and run using the following command.
1pip install airbench
2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"
One motivation for the development of these training methods is that they can accelerate the experi-
mental iteration time of researchers working on compatible projects involving CIFAR-10. Another
motivation is that they can decrease the cost of projects involving a massive number of trained
networks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used
3 million trained networks to demonstrate that the outputs of a trained neural network on a given test
input follow an approximately linear function of the vector of binary choices of which examples the
model was trained on. Another example is Jordan (2023), a study on training variance which used
180 thousand trained networks to show that standard trainings have little variance in performance on
1https://paperswithcode.com/datasets
Preprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024
Figure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal
flipping augmentation, which flips each image with 50% probability per epoch. This results in some images
being redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a
deterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.
their test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds
and 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make
it possible to replicate these studies, or conduct similar ones, with fewer computational resources.
Fast training also enables the rapid accumulation of statistical significance for subtle hyperparameter
comparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10
accuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation
between runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the
improvement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training
this will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.
Our work builds on prior training speed projects. We utilize a modified version of the network,
initialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen
patch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over
prior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-
ure 1, Section 3.6, Section 5.2).
2 Background
Our objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set
in the shortest possible amount of time. Timing begins when the method is first given access to
training data, and ends when it produces test-set predictions. The method is considered valid if its
mean accuracy over repeated runs is at least 94%.
We chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the
2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more
recent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10
seconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page
(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is
tysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,
94% is the level of human accuracy reported by Karpathy (2011).
We note the following consequences of how the method is timed. First, it is permitted for the program
to begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins
when the training data is first accessed. This is helpful because otherwise the first run of training
is typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA
improves the performance of a trained network by running it on multiple augmented views of each
test input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping
TTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods
attain 93.2%, 94.4%, and 95.6% mean accuracy respectively.
The CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.
It is divided into a training set of 50,000 images and a validation set of 10,000 images. As a
matter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was
2
80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than
airbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94
can be attributed entirely to algorithmic progress rather than compute scaling.
3 Methods
3.1 Network architecture and baseline training
We train a convolutional network with a total of 1.97 million parameters, following tysam-code
(2023) with a few small changes. It contains seven convolutions with the latter six being divided into
three blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this
section we offer some comments on the main design choices.
The network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed
entirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,
2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with
no padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather
than the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff
between throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.
Following Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,
and disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is
scaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture
differs only in that we decrease the number of output channels in the third block from 512 to 256,
and we add learnable biases to the first convolution.
As our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of
0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the
maximum at 20% of the way through training, and then decreases to zero. For data augmentation we
use random horizontal flipping alongside 2-pixel random translation. For translation we use reflection
padding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note
that what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a
random 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the
network is run on both a given test image and its mirror and inferences are made based on the average
of the two outputs. With optimized choices of learning rate, momentum, and weight decay, this
baseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.
3.2 Frozen patch-whitening initialization
Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-
whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code
(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches
across the training distribution, so that their outputs have identity covariance matrix. The second 12
Figure 2: The first layer\u2019s weights after whitening
initialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,
so that input information is preserved through the
activation which follows. Figure 2 shows the result.
We do not update this layer\u2019s weights during training.
Departing from tysam-code (2023), we add learn-
able biases to this layer, yielding a small performance
boost. The biases are trained for 3 epochs, after which
we disable their gradient to increase backward-pass
throughput, which improves training speed without
reducing accuracy. We also obtain a slight perfor-
mance boost relative to tysam-code (2023) by re-
ducing the constant added to the eigenvalues during
calculation of the patch-whitening initialization for
the purpose of preventing numerical issues in the case
of a singular patch-covariance matrix.
Patch-whitening initialization is the single most impactful feature. Adding it to the baseline more
than doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.
3
Random reshuffling Alternating flip Mean accuracy
No No 93.40%
No Yes 93.48%
Yes No 93.92%
Yes Yes 94.01%
Table 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating
flip (which we propose) reduce training data redundancy and improve performance.
3.3 Identity initialization
dirac : We initialize all convolutions after the first as partial identity transforms. That is, for a
convolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an
identity transform of the input, and leave the remaining N\u2212Mto their default initialization. In
PyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight
wof each convolutional layer. This method partially follows tysam-code (2023), which used a more
complicated scheme where the identity weights are mixed in with the original initialization, which
we did not find to be more performant. With this feature added, training attains 94% accuracy in 18
epochs taking 6.8 A100-seconds.
3.4 Optimization tricks
scalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor
of64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%
in 13.5 epochs taking 5.1 A100-seconds.
lookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.
We note that Lookahead has also been found effective in prior work on training speed for ResNet-
18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6
A100-seconds.
3.5 Multi-crop evaluation
multicrop : To generate predictions, we run the trained network on six augmented views of each
test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a
version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all
three. Predictions are made using a weighted average of all six outputs, where the two views of the
untranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125
each. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.
We note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-
monyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of
evaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using
more crops does improve performance, but the increase to inference time outweighs the potential
training speedup.
3.6 Alternating flip
To speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,
which we motivate as follows. When training neural networks, it is standard practice to organize
training into a set of epochs during which every training example is seen exactly once. This
differs from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),
which calls for data to be repeatedly sampled with-replacement from the training set, resulting in
examples being potentially seen multiple redundant times within a short window of training. The
use of randomly ordered epochs of data for training has a different name, being called the random
reshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our
training dataset consists of Nunique examples, then sampling data with replacement causes every
\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N
unique examples on average. On the other hand, random reshuffling leads to all Nunique examples
being seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason
that it is beneficial to maximize the number of unique inputs seen per window of training time.
4
We extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We
first note that standard random horizontal flipping augmentation can be defined as follows.
1import torch
2def random_flip ( inputs ):
3 # Applies random flipping to a batch of images
4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)
5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]
Listing 1: Random flip
If horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2
which may be seen during training. Potentially, every pair of consecutive epochs could contain every
unique input. But our main observation is that with standard random horizontal flipping, half of the
images will be redundantly flipped the same way during both epochs, so that on average only 1.5N
unique inputs will be seen.
altflip : To address this, we propose to modify standard random horizontal flipping augmentation as
follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},
we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we
flip only those inputs which were flipped in the first epoch. We provide the following implementation
which avoids the need for extra memory by using a pseudorandom function to decide the flips.
1import torch
2import hashlib
3def hash_fn (n, seed =42) :
4 k = n * seed
5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,
16)
6def alternating_flip (inputs , indices , epoch ):
7 # Applies alternating flipping to a batch of images
8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist
() ])
9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)
10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )
Listing 2: Alternating flip
The result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in
Figure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.
Adding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final
training method airbench94.py , the entire contents of which can be found in Section E. It reaches
94% accuracy in 3.83 seconds on an NVIDIA A100.
3.7 Compilation
The final step we take to speed up training is a non-algorithmic one: we compile our training
method using torch.compile in order to more efficiently utilize the GPU. This results in a training
script which is mathematically equivalent (up to small differences in floating point arithmetic) to
the non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29
A100-seconds. The downside is that the one-time compilation process takes up to several minutes to
complete before training runs can begin, so that it is only beneficial when we plan to execute many
runs of training at once. We release this version as airbench94_compiled.py .
4 95% and 96% targets
To address scenarios where somewhat higher performance is desired, we additionally develop methods
targeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .
To attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel
count of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce
the learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%
accuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.
2Assuming none of the training inputs are already mirrors of each other.
5
Figure 4: Training speedups accumulate additively. Removing individual features from airbench94
increases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces
the epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-
94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.
Figure 3: FLOPs vs. error rate tradeoff. Our
three training methods apparently follow a linear
log-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-
Vries & Taylor, 2017) augmentation and raise the
training epochs to 40. We add a third convolution to
each block, and scale the first block to 128 channels
and the second two to 512. We also add a residual
connection across the later two convolutions of each
block, which we find is still beneficial despite the fact
that we are already using identity initialization (Sec-
tion 3.3) to ease gradient flow. Finally, we reduce
the learning rate by a factor of 0.78. These changes
yield airbench96 which attains 96.05% accuracy in
46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.
Figure 3 shows the FLOPs and error rate of each of
our three training methods.
5 Experiments
5.1 Interaction between features
To gain a better sense of the impact of each feature on training speed, we compare two quantities.
First, we measure the number of epochs that can be saved by adding the feature to the whitened
baseline (Section 3.2). Second, we measure the number of epochs that must be added when the
feature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-
tion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it
from the final airbench94 increases epochs-to-94% from 9.9 to 12.8.
Figure 4 shows both quantities for each feature. Surprisingly, we find that for all features except
multi-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even
though the whitened baseline requires more than twice as many epochs as the final configuration.
This indicates that the interaction between most features is additive rather than multiplicative.
5.2 Does alternating flip generalize?
In this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of
training configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all
cases except those where neither alternating nor random flip improve over using no flipping at all.
For CIFAR-10 we consider the performance boost given by alternating flip across the follow-
ing 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and
airbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For
6
Figure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,
the use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%
training speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.
95% confidence intervals are shown around each point.
each configuration we compare the performance of alternating and random flip in terms of their mean
accuracy across n= 400 runs of training.
Figure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip
improves performance in every setting. To get a sense for how big the improvement is, we estimate
the effective speedup for each case, i.e., the fraction of epochs that could be saved by switching
from random to alternating flip while maintaining the level of accuracy of random flip. We begin
by fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of
each random flip-based training configuration. We use these curves to calculate the effective speedup
afforded by switching from random to alternating flip. For example, airbench94 with random flip
and without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The
same configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law
fit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.
Note that using a power-law yields a more conservative estimate relative to using linear interpolation
between the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.
Table 2 shows the result. We observe the following patterns. First, the addition of extra augmentation
(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that
the main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the
same way for many epochs in a row; we speculate that adding extra augmentation reduces the negative
impact of these cases because it increases data diversity. Next, TTA reduces the gap between random
and alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),
indicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training
for longer consistently increases the effective speedup given by alternating flip.
We next study ImageNet trainings with the following experiment. We train a ResNet-18 with a
variety of train and test crops, comparing three flipping options: alternating flip, random flip, and no
flipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192
center crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)
to denote the latter. We also consider two training crops: 192x192 inception-style random resized
crop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area
ranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the
shorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write
7
Baseline Cutout Epochs Speedup Speedup (w/ TTA)
airbench94 No 10 15.0% 5.30%
airbench94 No 20 27.1% 21.3%
airbench94 No 40 38.3% 36.4%
airbench94 No 80 102% 31.8%
airbench94 Yes 10 3.84% 1.13%
airbench94 Yes 20 7.42% 2.00%
airbench94 Yes 40 18.6% 9.28%
airbench94 Yes 80 29.2% 14.25%
airbench96 Yes 10 4.94% 1.11%
airbench96 Yes 20 8.99% 3.58%
airbench96 Yes 40 17.2% 6.48%
airbench96 Yes 80 18.8% Not measured
Table 2: Effective speedups given by switching from random flip to alternating flip. The two configurations
most closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw
accuracy values of the airbench94 experiments.
Flipping augmentation option
Train crop Test crop Epochs TTA None Random Alternating
Heavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28
Heavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28
Light RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26
Light RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26
Heavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28
Heavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28
Light RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26
Light RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26
Light RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16
Heavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4
Table 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for
those trainings where random flip improves significantly over not flipping at all. The single best flipping option
in each row is bolded when the difference is statistically significant.
Heavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided
in Section C.
Table 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC
is better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly
better when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test
resolution discrepancy (Touvron et al., 2019).
For trainings which use Light RRC, we find that switching from random flip to alternating flip
provides a substantial boost to performance, amounting to a training speedup of more than 25%.
In Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to
alternating flip improves performance by more than increasing the training duration from 16 to 20
epochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our
results on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit
from alternating flip. Indeed, even turning flipping off completely does not significantly reduce the
performance of these trainings. We conclude that alternating flip improves over random flip for every
training scenario where the latter improves over no flipping at all.
8
Epochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE
1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312
2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233
1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269
1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533
2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433
1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444
Table 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).
5.3 Variance and class-wise calibration
Previous sections have focused on understanding what factors affect the first moment of accuracy
(the mean). In this section we investigate the second moment, finding that TTA reduces variance at
the cost of calibration.
Our experiment is to execute 10,000 runs of airbench94 training with several hyperparameter
settings. For each setting we report both the variance in test-set accuracy as well as an estimate of the
distribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.
Table 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set
variance, replicating the main finding of Jordan (2023). This is a surprising result because these
trainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7
as much distribution-wise variance when run for a similar duration, and reached a low variance only
when run for 64 epochs. We conclude from this comparison that distribution-wise variance is more
strongly connected to the rate of convergence of a training rather than its duration as such. We also
note that the low distribution-wise variance of airbench94 indicates it has high training stability.
Using TTA significantly reduces the test-set variance, such that all three settings with TTA have lower
test-set variance than any setting without TTA. However, test-set variance is implied by the class-wise
calibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this
reduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,
we compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which
measures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA
has a higher CACE than every setting without TTA, confirming the hypothesis.
6 Discussion
In this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7
faster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the
airbench Python package.
We developed airbench solely with the goal of maximizing training speed on CIFAR-10. In
Section B we find that it also generalizes well to other tasks. For example, without any extra tuning,
airbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.
One factor contributing to the training speed of airbench was our finding that training can be accel-
erated by partially derandomizing the standard random horizontal flipping augmentation, resulting in
the variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating
flip improves the performance of every training we considered (Section 5.2), with the exception of
those trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to
us, the standard ImageNet trainings that we considered do not significantly benefit from horizontal
flipping. Future work might investigate whether it is possible to obtain derandomized improvements
to other augmentations besides horizontal flip.
The methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,
with fixed performance and hardware constraints. These constraints mean that we cannot improve
performance by simply scaling up the amount of computational resources used; instead we are forced
to develop new methods like the alternating flip. We look forward to seeing what other new methods
future work discovers to push training speed further.
9
References
Dimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.
Dan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-
performance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,
2011.
Cody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,
Kunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-
mark and competition. 2017.
Luke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet
or cifar-10. arXiv preprint arXiv:1810.03505 , 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,
pp. 248\u2013255. IEEE, 2009.
Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks
with cutout. arXiv preprint arXiv:1708.04552 , 2017.
Mert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic
gradient descent. Mathematical Programming , 186:49\u201384, 2021.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415 , 2016.
Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-
models: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.
pmlr, 2015.
Yiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of
sgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.
Keller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and
inevitable. arXiv preprint arXiv:2304.01910 , 2023.
Andrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.
io/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for
advanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT
License.
Guillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander
M \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.
Thomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,
Benjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.
Benchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural
Information Processing Systems , 35:25404\u201325421, 2022.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.
Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep
learning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.
David Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/
how-to-train-your-resnet-8-bag-of-tricks/ .
10
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical
Statistics , 22(3):400\u2013407, 1951.
Santiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:
//github.com/apple/ml-cifar-10-faster , 2019.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556 , 2014.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.
tysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/
hlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.
Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution
discrepancy. Advances in neural information processing systems , 32, 2019.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,
2016.
Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps
forward, 1 step back. Advances in neural information processing systems , 32, 2019.
11
A Network architecture
1from torch import nn
2
3class Flatten (nn. Module ):
4 def forward (self , x):
5 return x. view (x. size (0) , -1)
6
7class Mul (nn. Module ):
8 def __init__ (self , scale ):
9 super (). __init__ ()
10 self . scale = scale
11 def forward (self , x):
12 return x * self . scale
13
14def conv (ch_in , ch_out ):
15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,
16 padding =\u2019same \u2019, bias = False )
17
18def make_net ():
19 act = lambda : nn. GELU ()
20 bn = lambda ch: nn. BatchNorm2d (ch)
21 return nn. Sequential (
22 nn. Sequential (
23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),
24 act () ,
25 ),
26 nn. Sequential (
27 conv (24 , 64) ,
28 nn. MaxPool2d (2) ,
29 bn (64) , act () ,
30 conv (64 , 64) ,
31 bn (64) , act () ,
32 ),
33 nn. Sequential (
34 conv (64 , 256) ,
35 nn. MaxPool2d (2) ,
36 bn (256) , act () ,
37 conv (256 , 256) ,
38 bn (256) , act () ,
39 ),
40 nn. Sequential (
41 conv (256 , 256) ,
42 nn. MaxPool2d (2) ,
43 bn (256) , act () ,
44 conv (256 , 256) ,
45 bn (256) , act () ,
46 ),
47 nn. MaxPool2d (3) ,
48 Flatten () ,
49 nn. Linear (256 , 10, bias = False ),
50 Mul (1/9) ,
51 )
Listing 3: Minimal PyTorch code for the network architecture used by airbench94 .
We note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock
time. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel
size of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this
does not improve the wallclock training time on an A100. Reducing the batch size is another easy
way to save FLOPs but not wallclock time.
12
Dataset Flipping? Cutout? ResNet-18 airbench96
CIFAR-10 Yes No 95.55% 95.61%
CIFAR-10 Yes Yes 96.01% 96.05%
CIFAR-100 Yes No 77.54% 79.27%
CIFAR-100 Yes Yes 78.04% 79.76%
CINIC-10 Yes No 87.58% 87.78%
CINIC-10 Yes Yes not measured 88.22%
SVHN No No 97.35% 97.38%
SVHN No Yes not measured 97.64%
Table 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We
directly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off
flipping for SVHN).
B Extra dataset experiments
We developed airbench with the singular goal of maximizing training speed on CIFAR-10. To
find out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its
performance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-
10 (Darlow et al., 2018).
On CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both
the case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not
(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than
ResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say
that it generalizes.
We compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.
We do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal
on CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better
performance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where
airbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is
used and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows
strong generalization to other tasks.
We note that this comparison between airbench96 and ResNet-18 training is fair in the sense that
it does demonstrate that the former has good generalization, but unfair in the sense that it does not
indicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time
augmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training
would outperform airbench96 if it were run using test-time augmentation. However, it also takes
5-10 times longer to complete. The decision of which to use may be situational.
The accuracy values we report for ResNet-18 training are from the following sources. We tried
to select the highest values we could find for each setting. Moreau et al. (2022) reports attaining
95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports
attaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on
CIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.
C ImageNet training details
Our ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/
libffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and
momentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution
160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of
training. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.
13
Hyperparameters Flipping augmentation option
Epochs Cutout TTA None Random Alternating
10 No No 92.3053 93.0988 93.2798
20 No No 92.8166 93.7446 93.8652
40 No No 93.0143 94.0133 94.0729
80 No No 93.0612 94.1169 94.1628
10 No Yes 93.4071 93.9488 94.0186
20 No Yes 93.8528 94.5565 94.6530
40 No Yes 94.0381 94.7803 94.8203
80 No Yes 94.0638 94.8506 94.8676
10 Yes No 91.8487 92.0402 92.1374
20 Yes No 92.8474 93.3825 93.4876
40 Yes No 93.2675 94.1014 94.1952
80 Yes No 93.4193 94.4311 94.5204
10 Yes Yes 92.6455 92.7780 92.8103
20 Yes Yes 93.7862 94.1306 94.1670
40 Yes Yes 94.3090 94.8511 94.8960
80 Yes Yes 94.5253 95.1839 95.2362
Table 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a
mean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise
difference in means is statistically significant.
D Extra tables & figures
Figure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in
Section 5.3.
E Complete training code
1\"\"\"
2airbench94 .py
33.83 s runtime on an A100 ; 0.36 PFLOPs .
4Evidence for validity : 94.01 average accuracy in n =1000 runs .
5
6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB
with the following nvidia -smi:
7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :
11.7
14
8torch . __version__ == \u20192.1.2+ cu118 \u2019
9\"\"\"
10
11# ############################################
12# Setup / Hyperparameters #
13# ############################################
14
15import os
16import sys
17import uuid
18from math import ceil
19
20import torch
21from torch import nn
22import torch .nn. functional as F
23import torchvision
24import torchvision . transforms as T
25
26torch . backends . cudnn . benchmark = True
27
28\"\"\"
29We express the main training hyperparameters ( batch size , learning
rate , momentum , and weight decay ) in decoupled form , so that each
one can be tuned independently . This accomplishes the following :
30* Assuming time - constant gradients , the average step size is decoupled
from everything but the lr.
31* The size of the weight decay update is decoupled from everything but
the wd.
32In constrast , normally when we increase the ( Nesterov ) momentum , this
also scales up the step size proportionally to 1 + 1 / (1 -
momentum ), meaning we cannot change momentum without having to re -
tune the learning rate . Similarly , normally when we increase the
learning rate this also increases the size of the weight decay ,
requiring a proportional decrease in the wd to maintain the same
decay strength .
33
34The practical impact is that hyperparameter tuning is faster , since
this parametrization allows each one to be tuned independently .
See https :// myrtle .ai/ learn /how -to -train -your - resnet -5-
hyperparameters /.
35\"\"\"
36
37hyp = {
38 \u2019opt \u2019: {
39 \u2019 train_epochs \u2019: 9.9 ,
40 \u2019batch_size \u2019: 1024 ,
41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples
42 \u2019momentum \u2019: 0.85 ,
43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (
decoupled from learning rate )
44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not
weight decay ) for BatchNorm biases
45 \u2019 label_smoothing \u2019: 0.2 ,
46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the
whitening layer bias before freezing
47 },
48 \u2019aug \u2019: {
49 \u2019flip \u2019: True ,
50 \u2019translate \u2019: 2,
51 },
52 \u2019net \u2019: {
53 \u2019widths \u2019: {
54 \u2019block1 \u2019: 64,
55 \u2019block2 \u2019: 256 ,
56 \u2019block3 \u2019: 256 ,
15
57 },
58 \u2019 batchnorm_momentum \u2019: 0.6 ,
59 \u2019 scaling_factor \u2019: 1/9 ,
60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :
0= none , 1= mirror , 2= mirror + translate
61 },
62}
63
64# ############################################
65# DataLoader #
66# ############################################
67
68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )
69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )
70
71def batch_flip_lr ( inputs ):
72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .
view (-1, 1, 1, 1)
73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )
74
75def batch_crop (images , crop_size ):
76 r = ( images . size ( -1) - crop_size )//2
77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =
images . device )
78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),
device = images .device , dtype = images . dtype )
79 # The two cropping methods in this if - else produce equivalent
results , but the second is faster for r > 2.
80 if r <= 2:
81 for sy in range (-r, r +1):
82 for sx in range (-r, r +1):
83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)
84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size
, r+sx:r+sx+ crop_size ]
85 else :
86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size
+2* r), device = images .device , dtype = images . dtype )
87 for s in range (-r, r +1):
88 mask = ( shifts [:, 0] == s)
89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]
90 for s in range (-r, r +1):
91 mask = ( shifts [:, 1] == s)
92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+
crop_size ]
93 return images_out
94
95class CifarLoader :
96 \"\"\"
97 GPU - accelerated dataloader for CIFAR -10 which implements
alternating flip augmentation .
98 \"\"\"
99
100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,
drop_last =None , shuffle =None , gpu =0):
101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .
pt \u2019)
102 if not os. path . exists ( data_path ):
103 dset = torchvision . datasets . CIFAR10 (path , download =True ,
train = train )
104 images = torch . tensor ( dset . data )
105 labels = torch . tensor ( dset . targets )
106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:
dset . classes }, data_path )
107
108 data = torch . load ( data_path , map_location = torch . device (gpu))
16
109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [
\u2019labels \u2019], data [\u2019classes \u2019]
110 # It \u2019s faster to load + process uint8 data than to load
preprocessed fp16 data
111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).
to( memory_format = torch . channels_last )
112
113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )
114 self . proc_images = {} # Saved results of image processing to
be done on the first epoch
115 self . epoch = 0
116
117 self . aug = aug or {}
118 for k in self .aug . keys ():
119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019
% k
120
121 self . batch_size = batch_size
122 self . drop_last = train if drop_last is None else drop_last
123 self . shuffle = train if shuffle is None else shuffle
124
125 def __len__ ( self ):
126 return len( self . images )// self . batch_size if self . drop_last
else ceil (len( self . images )/ self . batch_size )
127
128 def __iter__ ( self ):
129
130 if self . epoch == 0:
131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .
images )
132 # Randomly flip all images on the first epoch as according
to definition of alternating flip
133 if self .aug.get(\u2019flip \u2019, False ):
134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (
images )
135 # Pre -pad images to save time when doing random
translation
136 pad = self .aug.get (\u2019translate \u2019, 0)
137 if pad > 0:
138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019
reflect \u2019)
139
140 if self .aug.get(\u2019translate \u2019, 0) > 0:
141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .
shape [ -2])
142 elif self .aug.get (\u2019flip \u2019, False ):
143 images = self . proc_images [\u2019flip \u2019]
144 else :
145 images = self . proc_images [\u2019norm \u2019]
146 if self .aug.get(\u2019flip \u2019, False ):
147 if self . epoch % 2 == 1:
148 images = images . flip ( -1)
149
150 self . epoch += 1
151
152 indices = ( torch . randperm if self . shuffle else torch . arange )(
len ( images ), device = images . device )
153 for i in range (len ( self )):
154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]
155 yield ( images [ idxs ], self . labels [ idxs ])
156
157# ############################################
158# Network Components #
159# ############################################
160
17
161class Flatten (nn. Module ):
162 def forward (self , x):
163 return x. view (x. size (0) , -1)
164
165class Mul (nn. Module ):
166 def __init__ (self , scale ):
167 super (). __init__ ()
168 self . scale = scale
169 def forward (self , x):
170 return x * self . scale
171
172class BatchNorm (nn. BatchNorm2d ):
173 def __init__ (self , num_features , momentum , eps =1e -12 ,
174 weight =False , bias = True ):
175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )
176 self . weight . requires_grad = weight
177 self . bias . requires_grad = bias
178 # Note that PyTorch already initializes the weights to one and
biases to zero
179
180class Conv (nn. Conv2d ):
181 def __init__ (self , in_channels , out_channels , kernel_size =3,
padding =\u2019same \u2019, bias = False ):
182 super (). __init__ ( in_channels , out_channels , kernel_size =
kernel_size , padding = padding , bias = bias )
183
184 def reset_parameters ( self ):
185 super (). reset_parameters ()
186 if self . bias is not None :
187 self . bias . data . zero_ ()
188 w = self . weight . data
189 torch .nn. init . dirac_ (w[:w. size (1) ])
190
191class ConvGroup (nn. Module ):
192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):
193 super (). __init__ ()
194 self . conv1 = Conv ( channels_in , channels_out )
195 self . pool = nn. MaxPool2d (2)
196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )
197 self . conv2 = Conv ( channels_out , channels_out )
198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )
199 self . activ = nn. GELU ()
200
201 def forward (self , x):
202 x = self . conv1 (x)
203 x = self . pool (x)
204 x = self . norm1 (x)
205 x = self . activ (x)
206 x = self . conv2 (x)
207 x = self . norm2 (x)
208 x = self . activ (x)
209 return x
210
211# ############################################
212# Network Definition #
213# ############################################
214
215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019
][ \u2019 batchnorm_momentum \u2019]):
216 whiten_kernel_size = 2
217 whiten_width = 2 * 3 * whiten_kernel_size **2
218 net = nn. Sequential (
219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True
),
220 nn. GELU () ,
18
221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],
batchnorm_momentum ),
222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],
batchnorm_momentum ),
223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],
batchnorm_momentum ),
224 nn. MaxPool2d (3) ,
225 Flatten () ,
226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),
227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,
228 )
229 net [0]. weight . requires_grad = False
230 net = net. half (). cuda ()
231 net = net.to( memory_format = torch . channels_last )
232 for mod in net. modules ():
233 if isinstance (mod , BatchNorm ):
234 mod . float ()
235 return net
236
237# ############################################
238# Whitening Conv Initialization #
239# ############################################
240
241def get_patches (x, patch_shape ):
242 c, (h, w) = x. shape [1] , patch_shape
243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h
,w). float ()
244
245def get_whitening_parameters ( patches ):
246 n,c,h,w = patches . shape
247 patches_flat = patches . view (n, -1)
248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n
249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance
, UPLO =\u2019U\u2019)
250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.
reshape (c*h*w,c,h,w). flip (0)
251
252def init_whitening_conv (layer , train_set , eps =5e -4):
253 patches = get_patches ( train_set , patch_shape = layer . weight . data .
shape [2:])
254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )
255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)
256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -
eigenvectors_scaled ))
257
258# ###########################################
259# Lookahead #
260# ###########################################
261
262class LookaheadState :
263 def __init__ (self , net):
264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().
items ()}
265
266 def update (self , net , decay ):
267 for ema_param , net_param in zip( self . net_ema . values () , net.
state_dict (). values ()):
268 if net_param . dtype in ( torch .half , torch . float ):
269 ema_param . lerp_ ( net_param , 1- decay )
270 net_param . copy_ ( ema_param )
271
272# ###########################################
273# Logging #
274# ###########################################
275
19
276def print_columns ( columns_list , is_head =False , is_final_entry = False ):
277 print_string = \u2019\u2019
278 for col in columns_list :
279 print_string += \u2019| %s \u2019 % col
280 print_string += \u2019|\u2019
281 if is_head :
282 print (\u2019-\u2019*len( print_string ))
283 print ( print_string )
284 if is_head or is_final_entry :
285 print (\u2019-\u2019*len( print_string ))
286
287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,
\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]
288def print_training_details ( variables , is_final_entry ):
289 formatted = []
290 for col in logging_columns_list :
291 var = variables .get (col. strip () , None )
292 if type (var) in (int , str):
293 res = str(var)
294 elif type (var) is float :
295 res = \u2019{:0.4 f}\u2019. format ( var)
296 else :
297 assert var is None
298 res = \u2019\u2019
299 formatted . append (res. rjust (len (col)))
300 print_columns ( formatted , is_final_entry = is_final_entry )
301
302# ###########################################
303# Evaluation #
304# ###########################################
305
306def infer (model , loader , tta_level =0) :
307 \"\"\"
308 Test - time augmentation strategy (for tta_level =2):
309 1. Flip / mirror the image left -to - right (50% of the time ).
310 2. Translate the image by one pixel either up -and - left or down -and
- right (50% of the time , i.e. both happen 25% of the time ).
311
312 This creates 6 views per image ( left / right times the two
translations and no - translation ), which we evaluate and then
weight according to the given probabilities .
313 \"\"\"
314
315 def infer_basic (inputs , net):
316 return net( inputs ). clone ()
317
318 def infer_mirror (inputs , net):
319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))
320
321 def infer_mirror_translate ( inputs , net ):
322 logits = infer_mirror (inputs , net )
323 pad = 1
324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)
325 inputs_translate_list = [
326 padded_inputs [:, :, 0:32 , 0:32] ,
327 padded_inputs [:, :, 2:34 , 2:34] ,
328 ]
329 logits_translate_list = [ infer_mirror ( inputs_translate , net)
330 for inputs_translate in
inputs_translate_list ]
331 logits_translate = torch . stack ( logits_translate_list ). mean (0)
332 return 0.5 * logits + 0.5 * logits_translate
333
334 model . eval ()
335 test_images = loader . normalize ( loader . images )
20
336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][
tta_level ]
337 with torch . no_grad ():
338 return torch .cat ([ infer_fn (inputs , model ) for inputs in
test_images . split (2000) ])
339
340def evaluate (model , loader , tta_level =0) :
341 logits = infer (model , loader , tta_level )
342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()
343
344# ###########################################
345# Training #
346# ###########################################
347
348def main (run):
349
350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]
351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]
352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]
353 # Assuming gradients are constant in time , for Nesterov momentum ,
the below ratio is how much larger the default steps will be than
the underlying per - example gradients . We divide the learning rate
by this ratio in order to ensure steps are the same scale as
gradients , regardless of the choice of momentum .
354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))
355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning
rate for PyTorch SGD
356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale
357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]
358
359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019
label_smoothing \u2019], reduction =\u2019none \u2019)
360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)
361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =
batch_size , aug=hyp[\u2019aug \u2019])
362 if run == \u2019warmup \u2019:
363 # The only purpose of the first run is to warmup , so we can
use dummy data
364 train_loader . labels = torch . randint (0, 10, size =( len (
train_loader . labels ) ,), device = train_loader . labels . device )
365 total_train_steps = ceil (len( train_loader ) * epochs )
366
367 model = make_net ()
368 current_steps = 0
369
370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in
k and p. requires_grad ]
371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019
not in k and p. requires_grad ]
372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,
weight_decay =wd/ lr_biases ),
373 dict ( params = other_params , lr=lr , weight_decay =wd/
lr)]
374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,
nesterov = True )
375
376 def triangle (steps , start =0, end =0, peak =0.5) :
377 xp = torch . tensor ([0 , int( peak * steps ), steps ])
378 fp = torch . tensor ([ start , 1, end ])
379 x = torch . arange (1+ steps )
380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])
381 b = fp [: -1] - (m * xp [: -1])
382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1
383 indices = torch . clamp ( indices , 0, len(m) - 1)
384 return m[ indices ] * x + b[ indices ]
21
385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,
peak =0.23)
386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:
lr_schedule [i])
387
388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /
total_train_steps )**3
389 lookahead_state = LookaheadState ( model )
390
391 # For accurately timing GPU code
392 starter = torch . cuda . Event ( enable_timing = True )
393 ender = torch . cuda . Event ( enable_timing = True )
394 total_time_seconds = 0.0
395
396 # Initialize the first layer using statistics of training images
397 starter . record ()
398 train_images = train_loader . normalize ( train_loader . images [:5000])
399 init_whitening_conv ( model [0] , train_images )
400 ender . record ()
401 torch . cuda . synchronize ()
402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )
403
404 for epoch in range ( ceil ( epochs )):
405
406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019
whiten_bias_epochs \u2019])
407
408 # ###################
409 # Training #
410 # ###################
411
412 starter . record ()
413
414 model . train ()
415 for inputs , labels in train_loader :
416
417 outputs = model ( inputs )
418 loss = loss_fn ( outputs , labels ).sum ()
419 optimizer . zero_grad ( set_to_none = True )
420 loss . backward ()
421 optimizer . step ()
422 scheduler . step ()
423
424 current_steps += 1
425
426 if current_steps % 5 == 0:
427 lookahead_state . update (model , decay = alpha_schedule [
current_steps ]. item ())
428
429 if current_steps >= total_train_steps :
430 if lookahead_state is not None :
431 lookahead_state . update (model , decay =1.0)
432 break
433
434 ender . record ()
435 torch . cuda . synchronize ()
436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )
437
438 # ###################
439 # Evaluation #
440 # ###################
441
442 # Print the accuracy and loss from the last training batch of
the epoch
22
443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().
mean (). item ()
444 train_loss = loss . item () / batch_size
445 val_acc = evaluate (model , test_loader , tta_level =0)
446 print_training_details ( locals () , is_final_entry = False )
447 run = None # Only print the run number once
448
449 # ###################
450 # TTA Evaluation #
451 # ###################
452
453 starter . record ()
454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019
tta_level \u2019])
455 ender . record ()
456 torch . cuda . synchronize ()
457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )
458
459 epoch = \u2019eval \u2019
460 print_training_details ( locals () , is_final_entry = True )
461
462 return tta_val_acc
463
464if __name__ == \" __main__ \":
465 with open (sys. argv [0]) as f:
466 code = f. read ()
467
468 print_columns ( logging_columns_list , is_head = True )
469 main (\u2019warmup \u2019)
470 accs = torch . tensor ([ main (run) for run in range (25) ])
471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))
472
473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }
474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))
475 os. makedirs ( log_dir , exist_ok = True )
476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)
477 print (os. path . abspath ( log_path ))
478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))
Listing 4: airbench94.py
23
Past Error in Task Candidates: ValueError('json output should start and end with { and }')
Task Candidates:"
  },
  {
    "Response": "{
  \"tasks\": [
    {
      \"name\": \"Implement alternating flip data augmentation\",
      \"description\": \"Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in a deterministic alternating pattern across epochs, rather than randomly, to reduce redundancy.\",
      \"relevant_paper_text\": \"To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we flip only those inputs which were flipped in the first epoch.\"
    },
    {
      \"name\": \"Implement patch-whitening initialization\",
      \"description\": \"Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper. This involves initializing the filters based on the eigenvectors of the covariance matrix of image patches.\",
      \"relevant_paper_text\": \"Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.\"
    },
    {
      \"name\": \"Implement identity initialization for convolutional layers\",
      \"description\": \"Implement the identity initialization technique for convolutional layers after the first layer, as described in the paper. This involves initializing a portion of each layer's filters as identity transforms.\",
      \"relevant_paper_text\": \"We initialize all convolutions after the first as partial identity transforms. That is, for a convolution with M input channels and N\u2265M outputs, we initialize its first M filters to an identity transform of the input, and leave the remaining N\u2212M to their default initialization.\"
    }
  ]
}"
  },
  {
    "Request": "Given the fields `task_candidate`, produce the fields `task_rubric`.

---

Follow the following format.

Task Candidate: ${task_candidate}
Task Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"properties\": {\"task\": {\"description\": \"Name of the task\", \"title\": \"Task\", \"type\": \"string\"}, \"criterion\": {\"description\": \"Criterion for evaluating the task\", \"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"task\", \"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"List of criteria for evaluating task implementation; importance should sum to 100\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}

---

Task Candidate:
Task: Implement alternating flip data augmentation
Description: Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in a deterministic alternating pattern across epochs, rather than randomly, to reduce redundancy.
Relevant Paper Text: To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we flip only those inputs which were flipped in the first epoch.
Task Rubric:"
  },
  {
    "Response": "Task Rubric: {
  \"rubric\": [
    {
      \"task\": \"Implement alternating flip data augmentation\",
      \"criterion\": \"Correctly implement the deterministic alternating pattern of horizontal flipping across epochs\",
      \"importance\": 30
    },
    {
      \"task\": \"Implement alternating flip data augmentation\",
      \"criterion\": \"Ensure 50% of inputs are randomly flipped in the first epoch\",
      \"importance\": 20
    },
    {
      \"task\": \"Implement alternating flip data augmentation\",
      \"criterion\": \"Correctly flip inputs on even epochs (2,4,6,...) that were not flipped in the first epoch\",
      \"importance\": 20
    },
    {
      \"task\": \"Implement alternating flip data augmentation\",
      \"criterion\": \"Correctly flip inputs on odd epochs (3,5,7,...) that were flipped in the first epoch\",
      \"importance\": 20
    },
    {
      \"task\": \"Implement alternating flip data augmentation\",
      \"criterion\": \"Efficiently track and maintain the flipping state of inputs across epochs\",
      \"importance\": 10
    }
  ]
}"
  },
  {
    "Request": "Make a very succinct json object that validates with the following schema

---

Follow the following format.

Json Schema: ${json_schema}
Json Object: ${json_object}

---

Json Schema: {\"$defs\": {\"TaskCriterion\": {\"properties\": {\"task\": {\"description\": \"Name of the task\", \"title\": \"Task\", \"type\": \"string\"}, \"criterion\": {\"description\": \"Criterion for evaluating the task\", \"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"task\", \"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"List of criteria for evaluating task implementation; importance should sum to 100\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}
Json Object:"
  },
  {
    "Response": "Json Object: {
  \"rubric\": [
    {
      \"task\": \"Write code\",
      \"criterion\": \"Functionality\",
      \"importance\": 50
    },
    {
      \"task\": \"Write code\",
      \"criterion\": \"Readability\",
      \"importance\": 30
    },
    {
      \"task\": \"Write code\",
      \"criterion\": \"Efficiency\",
      \"importance\": 20
    }
  ]
}"
  },
  {
    "Request": "Given the fields `task_candidate`, produce the fields `task_rubric`.

---

Follow the following format.

Task Candidate: ${task_candidate}
Past Error in Task Rubric: An error to avoid in the future
Task Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"properties\": {\"task\": {\"description\": \"Name of the task\", \"title\": \"Task\", \"type\": \"string\"}, \"criterion\": {\"description\": \"Criterion for evaluating the task\", \"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"task\", \"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"List of criteria for evaluating task implementation; importance should sum to 100\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}

---

Task Candidate:
Task: Implement alternating flip data augmentation
Description: Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in a deterministic alternating pattern across epochs, rather than randomly, to reduce redundancy.
Relevant Paper Text: To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we flip only those inputs which were flipped in the first epoch.
Past Error in Task Rubric: ValueError('json output should start and end with { and }')
Task Rubric:"
  },
  {
    "Response": "{
  \"rubric\": [
    {
      \"task\": \"Implement alternating flip data augmentation\",
      \"criterion\": \"Correctly implement the deterministic alternating pattern for horizontal flipping across epochs\",
      \"importance\": 30
    },
    {
      \"task\": \"Implement alternating flip data augmentation\",
      \"criterion\": \"Ensure 50% of inputs are randomly flipped in the first epoch\",
      \"importance\": 20
    },
    {
      \"task\": \"Implement alternating flip data augmentation\",
      \"criterion\": \"Correctly flip inputs on even epochs (2,4,6,...) that were not flipped in the first epoch\",
      \"importance\": 20
    },
    {
      \"task\": \"Implement alternating flip data augmentation\",
      \"criterion\": \"Correctly flip inputs on odd epochs (3,5,7,...) that were flipped in the first epoch\",
      \"importance\": 20
    },
    {
      \"task\": \"Implement alternating flip data augmentation\",
      \"criterion\": \"Ensure the implementation reduces redundancy compared to random flipping\",
      \"importance\": 10
    }
  ]
}"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.

---

Follow the following format.

Task: ${task}

Rubric: ${rubric}

Reasoning: Let's think step by step in order to ${produce the justification}. We ...

Scorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified?

Justification: Justification for scorable judgement

---

Task: Task: Implement alternating flip data augmentation Description: Implement the alternating flip data augmentation technique described in the paper. This involves flipping images horizontally in a deterministic alternating pattern across epochs, rather than randomly, to reduce redundancy. Relevant Paper Text: To address this, we propose to modify standard random horizontal flipping augmentation as follows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .}, we flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we flip only those inputs which were flipped in the first epoch.

Rubric: Task: Implement alternating flip data augmentation Criterion: Correctly implement the deterministic alternating pattern for horizontal flipping across epochs Importance: 30.0 Task: Implement alternating flip data augmentation Criterion: Ensure 50% of inputs are randomly flipped in the first epoch Importance: 20.0 Task: Implement alternating flip data augmentation Criterion: Correctly flip inputs on even epochs (2,4,6,...) that were not flipped in the first epoch Importance: 20.0 Task: Implement alternating flip data augmentation Criterion: Correctly flip inputs on odd epochs (3,5,7,...) that were flipped in the first epoch Importance: 20.0 Task: Implement alternating flip data augmentation Criterion: Ensure the implementation reduces redundancy compared to random flipping Importance: 10.0

Reasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification:

1. The task involves implementing a specific data augmentation technique described in a paper.
2. The technique has a clear, deterministic pattern for flipping images across epochs.
3. There are specific rules for which images to flip in each epoch.
4. The rubric provides clear criteria for evaluating the implementation.
5. Each criterion in the rubric corresponds to a specific, observable aspect of the implementation.
6. The criteria cover all key aspects of the required functionality.
7. The implementation can be tested by running it over multiple epochs and checking the flipping pattern.
8. The results of the implementation can be compared against the expected behavior described in the paper.
9. A Python function could be written to verify each criterion in the rubric objectively.
10. The function could track which images are flipped in each epoch and compare this to the expected pattern.

Scorable[True/False]: True

Justification: This task is objectively scorable because:

1. The alternating flip data augmentation technique has a clear, deterministic pattern that can be programmatically verified.
2. The rubric provides specific, measurable criteria that directly correspond to the required functionality.
3. Each aspect of the implementation (initial random flipping, alternating pattern across epochs, correct flipping based on even/odd epochs) can be objectively tested and verified.
4. A Python function could be written to:
   a. Check if 50% of inputs are randomly flipped in the first epoch
   b. Verify the correct alternating pattern of flipping across subsequent epochs
   c. Ensure that the correct images are flipped on even and odd epochs based on their first-epoch status
   d. Compare the redundancy of this method to random flipping
5. The implementation's output can be directly compared to the expected behavior described in the paper.
6. There are no subjective elements in the task or rubric that would require human judgment to evaluate.

Given these factors, it would be possible to create a Python function that objectively scores an implementation of this task, verifying that all key functionality is implemented as specified in both the task description and the rubric."
  },
  {
    "Request": "Given the fields `task_candidate`, produce the fields `task_rubric`.

---

Follow the following format.

Task Candidate: ${task_candidate}
Task Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"properties\": {\"task\": {\"description\": \"Name of the task\", \"title\": \"Task\", \"type\": \"string\"}, \"criterion\": {\"description\": \"Criterion for evaluating the task\", \"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"task\", \"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"List of criteria for evaluating task implementation; importance should sum to 100\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}

---

Task Candidate:
Task: Implement patch-whitening initialization
Description: Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper. This involves initializing the filters based on the eigenvectors of the covariance matrix of image patches.
Relevant Paper Text: Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.
Task Rubric:"
  },
  {
    "Response": "Task Rubric: {
  \"rubric\": [
    {
      \"task\": \"Implement patch-whitening initialization\",
      \"criterion\": \"Correctly implement 2x2 convolution with 24 channels for the first layer\",
      \"importance\": 20
    },
    {
      \"task\": \"Implement patch-whitening initialization\",
      \"criterion\": \"Accurately compute the covariance matrix of 2x2 patches across the training distribution\",
      \"importance\": 25
    },
    {
      \"task\": \"Implement patch-whitening initialization\",
      \"criterion\": \"Correctly calculate eigenvectors of the covariance matrix\",
      \"importance\": 20
    },
    {
      \"task\": \"Implement patch-whitening initialization\",
      \"criterion\": \"Initialize first 12 filters with eigenvectors to achieve identity covariance matrix output\",
      \"importance\": 15
    },
    {
      \"task\": \"Implement patch-whitening initialization\",
      \"criterion\": \"Initialize second 12 filters as negations of the first 12\",
      \"importance\": 10
    },
    {
      \"task\": \"Implement patch-whitening initialization\",
      \"criterion\": \"Ensure input information preservation through the following activation\",
      \"importance\": 10
    }
  ]
}"
  },
  {
    "Request": "Make a very succinct json object that validates with the following schema

---

Follow the following format.

Json Schema: ${json_schema}
Json Object: ${json_object}

---

Json Schema: {\"$defs\": {\"TaskCriterion\": {\"properties\": {\"task\": {\"description\": \"Name of the task\", \"title\": \"Task\", \"type\": \"string\"}, \"criterion\": {\"description\": \"Criterion for evaluating the task\", \"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"task\", \"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"List of criteria for evaluating task implementation; importance should sum to 100\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}
Json Object:"
  },
  {
    "Response": "Json Object: {
  \"rubric\": [
    {
      \"task\": \"Write code\",
      \"criterion\": \"Functionality\",
      \"importance\": 50
    },
    {
      \"task\": \"Write code\",
      \"criterion\": \"Readability\",
      \"importance\": 30
    },
    {
      \"task\": \"Write code\",
      \"criterion\": \"Efficiency\",
      \"importance\": 20
    }
  ]
}"
  },
  {
    "Request": "Given the fields `task_candidate`, produce the fields `task_rubric`.

---

Follow the following format.

Task Candidate: ${task_candidate}
Past Error in Task Rubric: An error to avoid in the future
Task Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"properties\": {\"task\": {\"description\": \"Name of the task\", \"title\": \"Task\", \"type\": \"string\"}, \"criterion\": {\"description\": \"Criterion for evaluating the task\", \"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"task\", \"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"List of criteria for evaluating task implementation; importance should sum to 100\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}

---

Task Candidate:
Task: Implement patch-whitening initialization
Description: Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper. This involves initializing the filters based on the eigenvectors of the covariance matrix of image patches.
Relevant Paper Text: Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.
Past Error in Task Rubric: ValueError('json output should start and end with { and }')
Task Rubric:"
  },
  {
    "Response": "{
  \"rubric\": [
    {
      \"task\": \"Implement patch-whitening initialization\",
      \"criterion\": \"Correctly compute the covariance matrix of 2x2 patches from the training distribution\",
      \"importance\": 25
    },
    {
      \"task\": \"Implement patch-whitening initialization\",
      \"criterion\": \"Accurately calculate the eigenvectors of the covariance matrix\",
      \"importance\": 25
    },
    {
      \"task\": \"Implement patch-whitening initialization\",
      \"criterion\": \"Initialize the first 12 filters with the computed eigenvectors\",
      \"importance\": 20
    },
    {
      \"task\": \"Implement patch-whitening initialization\",
      \"criterion\": \"Initialize the second 12 filters as the negation of the first 12\",
      \"importance\": 20
    },
    {
      \"task\": \"Implement patch-whitening initialization\",
      \"criterion\": \"Ensure the output of the initialized filters has an identity covariance matrix\",
      \"importance\": 10
    }
  ]
}"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.

---

Follow the following format.

Task: ${task}

Rubric: ${rubric}

Reasoning: Let's think step by step in order to ${produce the justification}. We ...

Scorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified?

Justification: Justification for scorable judgement

---

Task: Task: Implement patch-whitening initialization Description: Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper. This involves initializing the filters based on the eigenvectors of the covariance matrix of image patches. Relevant Paper Text: Following Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-whitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code (2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches across the training distribution, so that their outputs have identity covariance matrix. The second 12 filters are initialized as the negation of the first 12, so that input information is preserved through the activation which follows.

Rubric: Task: Implement patch-whitening initialization Criterion: Correctly compute the covariance matrix of 2x2 patches from the training distribution Importance: 25.0 Task: Implement patch-whitening initialization Criterion: Accurately calculate the eigenvectors of the covariance matrix Importance: 25.0 Task: Implement patch-whitening initialization Criterion: Initialize the first 12 filters with the computed eigenvectors Importance: 20.0 Task: Implement patch-whitening initialization Criterion: Initialize the second 12 filters as the negation of the first 12 Importance: 20.0 Task: Implement patch-whitening initialization Criterion: Ensure the output of the initialized filters has an identity covariance matrix Importance: 10.0

Reasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification:

1. The task involves implementing a specific initialization technique for convolutional layers.
2. The rubric provides clear, objective criteria for evaluating the implementation.
3. Each criterion in the rubric corresponds to a specific, measurable aspect of the implementation:
   a. Computing the covariance matrix
   b. Calculating eigenvectors
   c. Initializing the first 12 filters
   d. Initializing the second 12 filters
   e. Verifying the output covariance matrix
4. These criteria can be objectively verified through code inspection and numerical tests.
5. The implementation can be tested with sample input data to ensure it produces the expected output.
6. The rubric assigns specific importance values to each criterion, allowing for a quantitative scoring system.
7. A Python function could be written to:
   - Check the presence and correctness of each step in the implementation
   - Verify the mathematical properties of the initialized filters
   - Test the output of the initialization against expected results
8. The specificity of the task and rubric makes it possible to create automated tests that can objectively evaluate the implementation.

Scorable[True/False]: True

Justification: This task is scorable because it involves implementing a well-defined mathematical procedure with clear, objective criteria for evaluation. The rubric provides specific, measurable aspects of the implementation that can be verified through code inspection and numerical tests. A Python function can be written to automatically check the presence and correctness of each step in the implementation, verify the mathematical properties of the initialized filters, and test the output against expected results. The quantitative nature of the task (involving matrix computations and specific initialization procedures) lends itself well to automated testing and objective scoring. Therefore, it is possible to write a Python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified."
  },
  {
    "Request": "Given the fields `task_candidate`, produce the fields `task_rubric`.

---

Follow the following format.

Task Candidate: ${task_candidate}
Task Rubric: ${task_rubric}. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCriterion\": {\"properties\": {\"task\": {\"description\": \"Name of the task\", \"title\": \"Task\", \"type\": \"string\"}, \"criterion\": {\"description\": \"Criterion for evaluating the task\", \"title\": \"Criterion\", \"type\": \"string\"}, \"importance\": {\"description\": \"Float in [0, 100] indicating how important satisfying this criterion is to a successful implementation?\", \"title\": \"Importance\", \"type\": \"number\"}}, \"required\": [\"task\", \"criterion\", \"importance\"], \"title\": \"TaskCriterion\", \"type\": \"object\"}}, \"properties\": {\"rubric\": {\"description\": \"List of criteria for evaluating task implementation; importance should sum to 100\", \"items\": {\"$ref\": \"#/$defs/TaskCriterion\"}, \"title\": \"Rubric\", \"type\": \"array\"}}, \"required\": [\"rubric\"], \"title\": \"TaskRubric\", \"type\": \"object\"}

---

Task Candidate:
Task: Implement identity initialization for convolutional layers
Description: Implement the identity initialization technique for convolutional layers after the first layer, as described in the paper. This involves initializing a portion of each layer's filters as identity transforms.
Relevant Paper Text: We initialize all convolutions after the first as partial identity transforms. That is, for a convolution with M input channels and N\u2265M outputs, we initialize its first M filters to an identity transform of the input, and leave the remaining N\u2212M to their default initialization.
Task Rubric:"
  },
  {
    "Response": "{
  \"rubric\": [
    {
      \"task\": \"Implement identity initialization for convolutional layers\",
      \"criterion\": \"Correctly initialize the first M filters of each convolutional layer (after the first) as identity transforms\",
      \"importance\": 40
    },
    {
      \"task\": \"Implement identity initialization for convolutional layers\",
      \"criterion\": \"Properly handle cases where the number of output channels (N) is greater than or equal to the number of input channels (M)\",
      \"importance\": 25
    },
    {
      \"task\": \"Implement identity initialization for convolutional layers\",
      \"criterion\": \"Leave the remaining N-M filters with their default initialization\",
      \"importance\": 20
    },
    {
      \"task\": \"Implement identity initialization for convolutional layers\",
      \"criterion\": \"Apply the initialization only to convolutional layers after the first layer\",
      \"importance\": 10
    },
    {
      \"task\": \"Implement identity initialization for convolutional layers\",
      \"criterion\": \"Ensure the implementation is compatible with the existing neural network architecture\",
      \"importance\": 5
    }
  ]
}"
  },
  {
    "Request": "Given the fields `task`, `rubric`, produce the fields `scorable`, `justification`.

---

Follow the following format.

Task: ${task}

Rubric: ${rubric}

Reasoning: Let's think step by step in order to ${produce the justification}. We ...

Scorable[True/False]: Will it be possible to write a python function to objectively score an implementation of this task, verifying that key functionality is implemented as specified?

Justification: Justification for scorable judgement

---

Task: Task: Implement identity initialization for convolutional layers Description: Implement the identity initialization technique for convolutional layers after the first layer, as described in the paper. This involves initializing a portion of each layer's filters as identity transforms. Relevant Paper Text: We initialize all convolutions after the first as partial identity transforms. That is, for a convolution with M input channels and N\u2265M outputs, we initialize its first M filters to an identity transform of the input, and leave the remaining N\u2212M to their default initialization.

Rubric: Task: Implement identity initialization for convolutional layers Criterion: Correctly initialize the first M filters of each convolutional layer (after the first) as identity transforms Importance: 40.0 Task: Implement identity initialization for convolutional layers Criterion: Properly handle cases where the number of output channels (N) is greater than or equal to the number of input channels (M) Importance: 25.0 Task: Implement identity initialization for convolutional layers Criterion: Leave the remaining N-M filters with their default initialization Importance: 20.0 Task: Implement identity initialization for convolutional layers Criterion: Apply the initialization only to convolutional layers after the first layer Importance: 10.0 Task: Implement identity initialization for convolutional layers Criterion: Ensure the implementation is compatible with the existing neural network architecture Importance: 5.0

Reasoning: Let's think step by step in order to"
  },
  {
    "Response": "Reasoning: Let's think step by step in order to produce the justification:

1. The task is clearly defined and involves implementing a specific initialization technique for convolutional layers.

2. The rubric provides clear, objective criteria for evaluating the implementation:
   - Correctly initializing the first M filters as identity transforms
   - Handling cases where N \u2265 M
   - Leaving N-M filters with default initialization
   - Applying only to layers after the first
   - Compatibility with existing architecture

3. These criteria can be objectively verified through code inspection and testing:
   - We can check if the correct number of filters are initialized as identity transforms
   - We can verify the handling of different N and M values
   - We can confirm that remaining filters retain default initialization
   - We can ensure the first layer is not affected
   - We can test compatibility with the existing network structure

4. The implementation details are specific enough to write test cases that verify each criterion.

5. The task involves concrete, measurable outcomes rather than subjective assessments.

6. A Python function could be written to automate these checks, comparing the implemented initialization against the expected behavior.

Scorable[True/False]: True

Justification: This task is scorable because it involves implementing a specific technical feature with clear, objective criteria as outlined in the rubric. Each aspect of the implementation can be verified programmatically:

1. We can check if the correct number of filters (M) are initialized as identity transforms for each layer.
2. We can test with various combinations of input (M) and output (N) channels to ensure correct handling.
3. We can verify that N-M filters retain their default initialization.
4. We can confirm that the initialization is only applied to layers after the first one.
5. We can test the implementation within the existing neural network architecture to ensure compatibility.

A Python function can be written to perform these checks automatically, comparing the implemented initialization against the expected behavior described in the task and rubric. This allows for objective scoring of the implementation, verifying that all key functionality is correctly implemented as specified."
  }
]