[{"Request": "Given the fields `paper_text`, produce the fields `task_candidates`.\n\n---\n\nFollow the following format.\n\nPaper Text: Full text from research paper PDF\nTask Candidates: JSON list of task candidate dicts with keys 'name', 'description', and 'relevant_paper_text'. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCandidate\": {\"properties\": {\"name\": {\"description\": \"Name of the task\", \"title\": \"Name\", \"type\": \"string\"}, \"description\": {\"description\": \"Description of the task\", \"title\": \"Description\", \"type\": \"string\"}, \"relevant_paper_text\": {\"description\": \"Text from the paper that is relevant to the task\", \"title\": \"Relevant Paper Text\", \"type\": \"string\"}}, \"required\": [\"name\", \"description\", \"relevant_paper_text\"], \"title\": \"TaskCandidate\", \"type\": \"object\"}}, \"properties\": {\"tasks\": {\"description\": \"Tasks extracted from the paper\", \"items\": {\"$ref\": \"#/$defs/TaskCandidate\"}, \"title\": \"Tasks\", \"type\": \"array\"}}, \"required\": [\"tasks\"], \"title\": \"TaskCandidates\", \"type\": \"object\"}\n\n---\n\nPaper Text:\n94% on CIFAR-10 in 3.29 Seconds on a Single GPU\nKeller Jordan\nkjordan4077@gmail.com\nAbstract\nCIFAR-10 is among the most widely used datasets in machine learning, facilitating\nthousands of research projects per year. To accelerate research and reduce the cost\nof experiments, we introduce training methods for CIFAR-10 which reach 94%\naccuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when\nrun on a single NVIDIA A100 GPU. As one factor contributing to these training\nspeeds, we propose a derandomized variant of horizontal flipping augmentation,\nwhich we show improves over the standard method in every case where flipping is\nbeneficial over no flipping at all. Our code is released at https://github.com/\nKellerJordan/cifar10-airbench .\n1 Introduction\nCIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-\ning thousands of research projects per year1. Research can be accelerated and the cost of experiments\nreduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In\nthis paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single\nNVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,\n2023). To support scenarios where higher performance is needed, we additionally develop methods\ntargeting 95% and 96% accuracy. We release the following methods in total.\n1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).\n2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).\n3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).\n4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).\nAll runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-\nmatically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the\nfirst uses torch.compile to improve GPU utilization. It is intended for experiments where many\nnetworks are trained at once in order to amortize the one-time compilation cost. The non-compiled\nairbench94 variant can be easily installed and run using the following command.\n1pip install airbench\n2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"\nOne motivation for the development of these training methods is that they can accelerate the experi-\nmental iteration time of researchers working on compatible projects involving CIFAR-10. Another\nmotivation is that they can decrease the cost of projects involving a massive number of trained\nnetworks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used\n3 million trained networks to demonstrate that the outputs of a trained neural network on a given test\ninput follow an approximately linear function of the vector of binary choices of which examples the\nmodel was trained on. Another example is Jordan (2023), a study on training variance which used\n180 thousand trained networks to show that standard trainings have little variance in performance on\n1https://paperswithcode.com/datasets\nPreprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024\nFigure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal\nflipping augmentation, which flips each image with 50% probability per epoch. This results in some images\nbeing redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a\ndeterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\ntheir test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds\nand 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make\nit possible to replicate these studies, or conduct similar ones, with fewer computational resources.\nFast training also enables the rapid accumulation of statistical significance for subtle hyperparameter\ncomparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10\naccuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation\nbetween runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the\nimprovement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training\nthis will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.\nOur work builds on prior training speed projects. We utilize a modified version of the network,\ninitialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen\npatch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over\nprior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-\nure 1, Section 3.6, Section 5.2).\n2 Background\nOur objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set\nin the shortest possible amount of time. Timing begins when the method is first given access to\ntraining data, and ends when it produces test-set predictions. The method is considered valid if its\nmean accuracy over repeated runs is at least 94%.\nWe chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the\n2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more\nrecent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10\nseconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page\n(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is\ntysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,\n94% is the level of human accuracy reported by Karpathy (2011).\nWe note the following consequences of how the method is timed. First, it is permitted for the program\nto begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins\nwhen the training data is first accessed. This is helpful because otherwise the first run of training\nis typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA\nimproves the performance of a trained network by running it on multiple augmented views of each\ntest input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping\nTTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods\nattain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\nThe CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.\nIt is divided into a training set of 50,000 images and a validation set of 10,000 images. As a\nmatter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was\n2\n80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than\nairbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94\ncan be attributed entirely to algorithmic progress rather than compute scaling.\n3 Methods\n3.1 Network architecture and baseline training\nWe train a convolutional network with a total of 1.97 million parameters, following tysam-code\n(2023) with a few small changes. It contains seven convolutions with the latter six being divided into\nthree blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this\nsection we offer some comments on the main design choices.\nThe network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed\nentirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,\n2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with\nno padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather\nthan the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff\nbetween throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\nFollowing Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,\nand disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is\nscaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture\ndiffers only in that we decrease the number of output channels in the third block from 512 to 256,\nand we add learnable biases to the first convolution.\nAs our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of\n0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the\nmaximum at 20% of the way through training, and then decreases to zero. For data augmentation we\nuse random horizontal flipping alongside 2-pixel random translation. For translation we use reflection\npadding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note\nthat what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a\nrandom 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the\nnetwork is run on both a given test image and its mirror and inferences are made based on the average\nof the two outputs. With optimized choices of learning rate, momentum, and weight decay, this\nbaseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.\n3.2 Frozen patch-whitening initialization\nFollowing Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-\nwhitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code\n(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches\nacross the training distribution, so that their outputs have identity covariance matrix. The second 12\nFigure 2: The first layer\u2019s weights after whitening\ninitialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,\nso that input information is preserved through the\nactivation which follows. Figure 2 shows the result.\nWe do not update this layer\u2019s weights during training.\nDeparting from tysam-code (2023), we add learn-\nable biases to this layer, yielding a small performance\nboost. The biases are trained for 3 epochs, after which\nwe disable their gradient to increase backward-pass\nthroughput, which improves training speed without\nreducing accuracy. We also obtain a slight perfor-\nmance boost relative to tysam-code (2023) by re-\nducing the constant added to the eigenvalues during\ncalculation of the patch-whitening initialization for\nthe purpose of preventing numerical issues in the case\nof a singular patch-covariance matrix.\nPatch-whitening initialization is the single most impactful feature. Adding it to the baseline more\nthan doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.\n3\nRandom reshuffling Alternating flip Mean accuracy\nNo No 93.40%\nNo Yes 93.48%\nYes No 93.92%\nYes Yes 94.01%\nTable 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating\nflip (which we propose) reduce training data redundancy and improve performance.\n3.3 Identity initialization\ndirac : We initialize all convolutions after the first as partial identity transforms. That is, for a\nconvolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an\nidentity transform of the input, and leave the remaining N\u2212Mto their default initialization. In\nPyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight\nwof each convolutional layer. This method partially follows tysam-code (2023), which used a more\ncomplicated scheme where the identity weights are mixed in with the original initialization, which\nwe did not find to be more performant. With this feature added, training attains 94% accuracy in 18\nepochs taking 6.8 A100-seconds.\n3.4 Optimization tricks\nscalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor\nof64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%\nin 13.5 epochs taking 5.1 A100-seconds.\nlookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.\nWe note that Lookahead has also been found effective in prior work on training speed for ResNet-\n18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6\nA100-seconds.\n3.5 Multi-crop evaluation\nmulticrop : To generate predictions, we run the trained network on six augmented views of each\ntest image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a\nversion which is translated down-and-to-the-right by one pixel, and the mirrored versions of all\nthree. Predictions are made using a weighted average of all six outputs, where the two views of the\nuntranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125\neach. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.\nWe note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-\nmonyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of\nevaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using\nmore crops does improve performance, but the increase to inference time outweighs the potential\ntraining speedup.\n3.6 Alternating flip\nTo speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,\nwhich we motivate as follows. When training neural networks, it is standard practice to organize\ntraining into a set of epochs during which every training example is seen exactly once. This\ndiffers from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),\nwhich calls for data to be repeatedly sampled with-replacement from the training set, resulting in\nexamples being potentially seen multiple redundant times within a short window of training. The\nuse of randomly ordered epochs of data for training has a different name, being called the random\nreshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our\ntraining dataset consists of Nunique examples, then sampling data with replacement causes every\n\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N\nunique examples on average. On the other hand, random reshuffling leads to all Nunique examples\nbeing seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason\nthat it is beneficial to maximize the number of unique inputs seen per window of training time.\n4\nWe extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We\nfirst note that standard random horizontal flipping augmentation can be defined as follows.\n1import torch\n2def random_flip ( inputs ):\n3 # Applies random flipping to a batch of images\n4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)\n5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]\nListing 1: Random flip\nIf horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2\nwhich may be seen during training. Potentially, every pair of consecutive epochs could contain every\nunique input. But our main observation is that with standard random horizontal flipping, half of the\nimages will be redundantly flipped the same way during both epochs, so that on average only 1.5N\nunique inputs will be seen.\naltflip : To address this, we propose to modify standard random horizontal flipping augmentation as\nfollows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},\nwe flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we\nflip only those inputs which were flipped in the first epoch. We provide the following implementation\nwhich avoids the need for extra memory by using a pseudorandom function to decide the flips.\n1import torch\n2import hashlib\n3def hash_fn (n, seed =42) :\n4 k = n * seed\n5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,\n16)\n6def alternating_flip (inputs , indices , epoch ):\n7 # Applies alternating flipping to a batch of images\n8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist\n() ])\n9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)\n10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\nListing 2: Alternating flip\nThe result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in\nFigure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.\nAdding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final\ntraining method airbench94.py , the entire contents of which can be found in Section E. It reaches\n94% accuracy in 3.83 seconds on an NVIDIA A100.\n3.7 Compilation\nThe final step we take to speed up training is a non-algorithmic one: we compile our training\nmethod using torch.compile in order to more efficiently utilize the GPU. This results in a training\nscript which is mathematically equivalent (up to small differences in floating point arithmetic) to\nthe non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29\nA100-seconds. The downside is that the one-time compilation process takes up to several minutes to\ncomplete before training runs can begin, so that it is only beneficial when we plan to execute many\nruns of training at once. We release this version as airbench94_compiled.py .\n4 95% and 96% targets\nTo address scenarios where somewhat higher performance is desired, we additionally develop methods\ntargeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .\nTo attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel\ncount of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce\nthe learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%\naccuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.\n2Assuming none of the training inputs are already mirrors of each other.\n5\nFigure 4: Training speedups accumulate additively. Removing individual features from airbench94\nincreases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces\nthe epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-\n94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.\nFigure 3: FLOPs vs. error rate tradeoff. Our\nthree training methods apparently follow a linear\nlog-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-\nVries & Taylor, 2017) augmentation and raise the\ntraining epochs to 40. We add a third convolution to\neach block, and scale the first block to 128 channels\nand the second two to 512. We also add a residual\nconnection across the later two convolutions of each\nblock, which we find is still beneficial despite the fact\nthat we are already using identity initialization (Sec-\ntion 3.3) to ease gradient flow. Finally, we reduce\nthe learning rate by a factor of 0.78. These changes\nyield airbench96 which attains 96.05% accuracy in\n46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.\nFigure 3 shows the FLOPs and error rate of each of\nour three training methods.\n5 Experiments\n5.1 Interaction between features\nTo gain a better sense of the impact of each feature on training speed, we compare two quantities.\nFirst, we measure the number of epochs that can be saved by adding the feature to the whitened\nbaseline (Section 3.2). Second, we measure the number of epochs that must be added when the\nfeature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-\ntion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it\nfrom the final airbench94 increases epochs-to-94% from 9.9 to 12.8.\nFigure 4 shows both quantities for each feature. Surprisingly, we find that for all features except\nmulti-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even\nthough the whitened baseline requires more than twice as many epochs as the final configuration.\nThis indicates that the interaction between most features is additive rather than multiplicative.\n5.2 Does alternating flip generalize?\nIn this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of\ntraining configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all\ncases except those where neither alternating nor random flip improve over using no flipping at all.\nFor CIFAR-10 we consider the performance boost given by alternating flip across the follow-\ning 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and\nairbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For\n6\nFigure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,\nthe use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%\ntraining speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.\n95% confidence intervals are shown around each point.\neach configuration we compare the performance of alternating and random flip in terms of their mean\naccuracy across n= 400 runs of training.\nFigure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip\nimproves performance in every setting. To get a sense for how big the improvement is, we estimate\nthe effective speedup for each case, i.e., the fraction of epochs that could be saved by switching\nfrom random to alternating flip while maintaining the level of accuracy of random flip. We begin\nby fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of\neach random flip-based training configuration. We use these curves to calculate the effective speedup\nafforded by switching from random to alternating flip. For example, airbench94 with random flip\nand without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The\nsame configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law\nfit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.\nNote that using a power-law yields a more conservative estimate relative to using linear interpolation\nbetween the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.\nTable 2 shows the result. We observe the following patterns. First, the addition of extra augmentation\n(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that\nthe main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the\nsame way for many epochs in a row; we speculate that adding extra augmentation reduces the negative\nimpact of these cases because it increases data diversity. Next, TTA reduces the gap between random\nand alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),\nindicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training\nfor longer consistently increases the effective speedup given by alternating flip.\nWe next study ImageNet trainings with the following experiment. We train a ResNet-18 with a\nvariety of train and test crops, comparing three flipping options: alternating flip, random flip, and no\nflipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192\ncenter crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)\nto denote the latter. We also consider two training crops: 192x192 inception-style random resized\ncrop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area\nranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the\nshorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write\n7\nBaseline Cutout Epochs Speedup Speedup (w/ TTA)\nairbench94 No 10 15.0% 5.30%\nairbench94 No 20 27.1% 21.3%\nairbench94 No 40 38.3% 36.4%\nairbench94 No 80 102% 31.8%\nairbench94 Yes 10 3.84% 1.13%\nairbench94 Yes 20 7.42% 2.00%\nairbench94 Yes 40 18.6% 9.28%\nairbench94 Yes 80 29.2% 14.25%\nairbench96 Yes 10 4.94% 1.11%\nairbench96 Yes 20 8.99% 3.58%\nairbench96 Yes 40 17.2% 6.48%\nairbench96 Yes 80 18.8% Not measured\nTable 2: Effective speedups given by switching from random flip to alternating flip. The two configurations\nmost closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw\naccuracy values of the airbench94 experiments.\nFlipping augmentation option\nTrain crop Test crop Epochs TTA None Random Alternating\nHeavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28\nHeavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28\nLight RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26\nLight RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26\nHeavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28\nHeavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28\nLight RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26\nLight RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26\nLight RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16\nHeavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4\nTable 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for\nthose trainings where random flip improves significantly over not flipping at all. The single best flipping option\nin each row is bolded when the difference is statistically significant.\nHeavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided\nin Section C.\nTable 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC\nis better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly\nbetter when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test\nresolution discrepancy (Touvron et al., 2019).\nFor trainings which use Light RRC, we find that switching from random flip to alternating flip\nprovides a substantial boost to performance, amounting to a training speedup of more than 25%.\nIn Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to\nalternating flip improves performance by more than increasing the training duration from 16 to 20\nepochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our\nresults on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit\nfrom alternating flip. Indeed, even turning flipping off completely does not significantly reduce the\nperformance of these trainings. We conclude that alternating flip improves over random flip for every\ntraining scenario where the latter improves over no flipping at all.\n8\nEpochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE\n1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312\n2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233\n1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269\n1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533\n2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433\n1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444\nTable 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).\n5.3 Variance and class-wise calibration\nPrevious sections have focused on understanding what factors affect the first moment of accuracy\n(the mean). In this section we investigate the second moment, finding that TTA reduces variance at\nthe cost of calibration.\nOur experiment is to execute 10,000 runs of airbench94 training with several hyperparameter\nsettings. For each setting we report both the variance in test-set accuracy as well as an estimate of the\ndistribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.\nTable 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set\nvariance, replicating the main finding of Jordan (2023). This is a surprising result because these\ntrainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7\nas much distribution-wise variance when run for a similar duration, and reached a low variance only\nwhen run for 64 epochs. We conclude from this comparison that distribution-wise variance is more\nstrongly connected to the rate of convergence of a training rather than its duration as such. We also\nnote that the low distribution-wise variance of airbench94 indicates it has high training stability.\nUsing TTA significantly reduces the test-set variance, such that all three settings with TTA have lower\ntest-set variance than any setting without TTA. However, test-set variance is implied by the class-wise\ncalibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this\nreduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,\nwe compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which\nmeasures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA\nhas a higher CACE than every setting without TTA, confirming the hypothesis.\n6 Discussion\nIn this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7\nfaster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the\nairbench Python package.\nWe developed airbench solely with the goal of maximizing training speed on CIFAR-10. In\nSection B we find that it also generalizes well to other tasks. For example, without any extra tuning,\nairbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.\nOne factor contributing to the training speed of airbench was our finding that training can be accel-\nerated by partially derandomizing the standard random horizontal flipping augmentation, resulting in\nthe variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating\nflip improves the performance of every training we considered (Section 5.2), with the exception of\nthose trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to\nus, the standard ImageNet trainings that we considered do not significantly benefit from horizontal\nflipping. Future work might investigate whether it is possible to obtain derandomized improvements\nto other augmentations besides horizontal flip.\nThe methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,\nwith fixed performance and hardware constraints. These constraints mean that we cannot improve\nperformance by simply scaling up the amount of computational resources used; instead we are forced\nto develop new methods like the alternating flip. We look forward to seeing what other new methods\nfuture work discovers to push training speed further.\n9\nReferences\nDimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.\nDan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-\nperformance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,\n2011.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\nmark and competition. 2017.\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet\nor cifar-10. arXiv preprint arXiv:1810.03505 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\npp. 248\u2013255. IEEE, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552 , 2017.\nMert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic\ngradient descent. Mathematical Programming , 186:49\u201384, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 , 2016.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.\npmlr, 2015.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\nsgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.\nKeller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and\ninevitable. arXiv preprint arXiv:2304.01910 , 2023.\nAndrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.\nio/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for\nadvanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT\nLicense.\nGuillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander\nM \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,\nBenjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.\nBenchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural\nInformation Processing Systems , 35:25404\u201325421, 2022.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.\nDavid Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/\nhow-to-train-your-resnet-8-bag-of-tricks/ .\n10\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics , 22(3):400\u2013407, 1951.\nSantiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:\n//github.com/apple/ml-cifar-10-faster , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.\ntysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/\nhlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. Advances in neural information processing systems , 32, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,\n2016.\nMichael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps\nforward, 1 step back. Advances in neural information processing systems , 32, 2019.\n11\nA Network architecture\n1from torch import nn\n2\n3class Flatten (nn. Module ):\n4 def forward (self , x):\n5 return x. view (x. size (0) , -1)\n6\n7class Mul (nn. Module ):\n8 def __init__ (self , scale ):\n9 super (). __init__ ()\n10 self . scale = scale\n11 def forward (self , x):\n12 return x * self . scale\n13\n14def conv (ch_in , ch_out ):\n15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,\n16 padding =\u2019same \u2019, bias = False )\n17\n18def make_net ():\n19 act = lambda : nn. GELU ()\n20 bn = lambda ch: nn. BatchNorm2d (ch)\n21 return nn. Sequential (\n22 nn. Sequential (\n23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),\n24 act () ,\n25 ),\n26 nn. Sequential (\n27 conv (24 , 64) ,\n28 nn. MaxPool2d (2) ,\n29 bn (64) , act () ,\n30 conv (64 , 64) ,\n31 bn (64) , act () ,\n32 ),\n33 nn. Sequential (\n34 conv (64 , 256) ,\n35 nn. MaxPool2d (2) ,\n36 bn (256) , act () ,\n37 conv (256 , 256) ,\n38 bn (256) , act () ,\n39 ),\n40 nn. Sequential (\n41 conv (256 , 256) ,\n42 nn. MaxPool2d (2) ,\n43 bn (256) , act () ,\n44 conv (256 , 256) ,\n45 bn (256) , act () ,\n46 ),\n47 nn. MaxPool2d (3) ,\n48 Flatten () ,\n49 nn. Linear (256 , 10, bias = False ),\n50 Mul (1/9) ,\n51 )\nListing 3: Minimal PyTorch code for the network architecture used by airbench94 .\nWe note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock\ntime. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel\nsize of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this\ndoes not improve the wallclock training time on an A100. Reducing the batch size is another easy\nway to save FLOPs but not wallclock time.\n12\nDataset Flipping? Cutout? ResNet-18 airbench96\nCIFAR-10 Yes No 95.55% 95.61%\nCIFAR-10 Yes Yes 96.01% 96.05%\nCIFAR-100 Yes No 77.54% 79.27%\nCIFAR-100 Yes Yes 78.04% 79.76%\nCINIC-10 Yes No 87.58% 87.78%\nCINIC-10 Yes Yes not measured 88.22%\nSVHN No No 97.35% 97.38%\nSVHN No Yes not measured 97.64%\nTable 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We\ndirectly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off\nflipping for SVHN).\nB Extra dataset experiments\nWe developed airbench with the singular goal of maximizing training speed on CIFAR-10. To\nfind out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its\nperformance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-\n10 (Darlow et al., 2018).\nOn CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both\nthe case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not\n(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than\nResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say\nthat it generalizes.\nWe compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.\nWe do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal\non CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better\nperformance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where\nairbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is\nused and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows\nstrong generalization to other tasks.\nWe note that this comparison between airbench96 and ResNet-18 training is fair in the sense that\nit does demonstrate that the former has good generalization, but unfair in the sense that it does not\nindicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time\naugmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training\nwould outperform airbench96 if it were run using test-time augmentation. However, it also takes\n5-10 times longer to complete. The decision of which to use may be situational.\nThe accuracy values we report for ResNet-18 training are from the following sources. We tried\nto select the highest values we could find for each setting. Moreau et al. (2022) reports attaining\n95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports\nattaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on\nCIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.\nC ImageNet training details\nOur ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/\nlibffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and\nmomentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution\n160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of\ntraining. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.\n13\nHyperparameters Flipping augmentation option\nEpochs Cutout TTA None Random Alternating\n10 No No 92.3053 93.0988 93.2798\n20 No No 92.8166 93.7446 93.8652\n40 No No 93.0143 94.0133 94.0729\n80 No No 93.0612 94.1169 94.1628\n10 No Yes 93.4071 93.9488 94.0186\n20 No Yes 93.8528 94.5565 94.6530\n40 No Yes 94.0381 94.7803 94.8203\n80 No Yes 94.0638 94.8506 94.8676\n10 Yes No 91.8487 92.0402 92.1374\n20 Yes No 92.8474 93.3825 93.4876\n40 Yes No 93.2675 94.1014 94.1952\n80 Yes No 93.4193 94.4311 94.5204\n10 Yes Yes 92.6455 92.7780 92.8103\n20 Yes Yes 93.7862 94.1306 94.1670\n40 Yes Yes 94.3090 94.8511 94.8960\n80 Yes Yes 94.5253 95.1839 95.2362\nTable 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a\nmean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise\ndifference in means is statistically significant.\nD Extra tables & figures\nFigure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in\nSection 5.3.\nE Complete training code\n1\"\"\"\n2airbench94 .py\n33.83 s runtime on an A100 ; 0.36 PFLOPs .\n4Evidence for validity : 94.01 average accuracy in n =1000 runs .\n5\n6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB\nwith the following nvidia -smi:\n7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :\n11.7\n14\n8torch . __version__ == \u20192.1.2+ cu118 \u2019\n9\"\"\"\n10\n11# ############################################\n12# Setup / Hyperparameters #\n13# ############################################\n14\n15import os\n16import sys\n17import uuid\n18from math import ceil\n19\n20import torch\n21from torch import nn\n22import torch .nn. functional as F\n23import torchvision\n24import torchvision . transforms as T\n25\n26torch . backends . cudnn . benchmark = True\n27\n28\"\"\"\n29We express the main training hyperparameters ( batch size , learning\nrate , momentum , and weight decay ) in decoupled form , so that each\none can be tuned independently . This accomplishes the following :\n30* Assuming time - constant gradients , the average step size is decoupled\nfrom everything but the lr.\n31* The size of the weight decay update is decoupled from everything but\nthe wd.\n32In constrast , normally when we increase the ( Nesterov ) momentum , this\nalso scales up the step size proportionally to 1 + 1 / (1 -\nmomentum ), meaning we cannot change momentum without having to re -\ntune the learning rate . Similarly , normally when we increase the\nlearning rate this also increases the size of the weight decay ,\nrequiring a proportional decrease in the wd to maintain the same\ndecay strength .\n33\n34The practical impact is that hyperparameter tuning is faster , since\nthis parametrization allows each one to be tuned independently .\nSee https :// myrtle .ai/ learn /how -to -train -your - resnet -5-\nhyperparameters /.\n35\"\"\"\n36\n37hyp = {\n38 \u2019opt \u2019: {\n39 \u2019 train_epochs \u2019: 9.9 ,\n40 \u2019batch_size \u2019: 1024 ,\n41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples\n42 \u2019momentum \u2019: 0.85 ,\n43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (\ndecoupled from learning rate )\n44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not\nweight decay ) for BatchNorm biases\n45 \u2019 label_smoothing \u2019: 0.2 ,\n46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the\nwhitening layer bias before freezing\n47 },\n48 \u2019aug \u2019: {\n49 \u2019flip \u2019: True ,\n50 \u2019translate \u2019: 2,\n51 },\n52 \u2019net \u2019: {\n53 \u2019widths \u2019: {\n54 \u2019block1 \u2019: 64,\n55 \u2019block2 \u2019: 256 ,\n56 \u2019block3 \u2019: 256 ,\n15\n57 },\n58 \u2019 batchnorm_momentum \u2019: 0.6 ,\n59 \u2019 scaling_factor \u2019: 1/9 ,\n60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :\n0= none , 1= mirror , 2= mirror + translate\n61 },\n62}\n63\n64# ############################################\n65# DataLoader #\n66# ############################################\n67\n68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )\n69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )\n70\n71def batch_flip_lr ( inputs ):\n72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .\nview (-1, 1, 1, 1)\n73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\n74\n75def batch_crop (images , crop_size ):\n76 r = ( images . size ( -1) - crop_size )//2\n77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =\nimages . device )\n78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),\ndevice = images .device , dtype = images . dtype )\n79 # The two cropping methods in this if - else produce equivalent\nresults , but the second is faster for r > 2.\n80 if r <= 2:\n81 for sy in range (-r, r +1):\n82 for sx in range (-r, r +1):\n83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)\n84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size\n, r+sx:r+sx+ crop_size ]\n85 else :\n86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size\n+2* r), device = images .device , dtype = images . dtype )\n87 for s in range (-r, r +1):\n88 mask = ( shifts [:, 0] == s)\n89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]\n90 for s in range (-r, r +1):\n91 mask = ( shifts [:, 1] == s)\n92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+\ncrop_size ]\n93 return images_out\n94\n95class CifarLoader :\n96 \"\"\"\n97 GPU - accelerated dataloader for CIFAR -10 which implements\nalternating flip augmentation .\n98 \"\"\"\n99\n100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,\ndrop_last =None , shuffle =None , gpu =0):\n101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .\npt \u2019)\n102 if not os. path . exists ( data_path ):\n103 dset = torchvision . datasets . CIFAR10 (path , download =True ,\ntrain = train )\n104 images = torch . tensor ( dset . data )\n105 labels = torch . tensor ( dset . targets )\n106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:\ndset . classes }, data_path )\n107\n108 data = torch . load ( data_path , map_location = torch . device (gpu))\n16\n109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [\n\u2019labels \u2019], data [\u2019classes \u2019]\n110 # It \u2019s faster to load + process uint8 data than to load\npreprocessed fp16 data\n111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).\nto( memory_format = torch . channels_last )\n112\n113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )\n114 self . proc_images = {} # Saved results of image processing to\nbe done on the first epoch\n115 self . epoch = 0\n116\n117 self . aug = aug or {}\n118 for k in self .aug . keys ():\n119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019\n% k\n120\n121 self . batch_size = batch_size\n122 self . drop_last = train if drop_last is None else drop_last\n123 self . shuffle = train if shuffle is None else shuffle\n124\n125 def __len__ ( self ):\n126 return len( self . images )// self . batch_size if self . drop_last\nelse ceil (len( self . images )/ self . batch_size )\n127\n128 def __iter__ ( self ):\n129\n130 if self . epoch == 0:\n131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .\nimages )\n132 # Randomly flip all images on the first epoch as according\nto definition of alternating flip\n133 if self .aug.get(\u2019flip \u2019, False ):\n134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (\nimages )\n135 # Pre -pad images to save time when doing random\ntranslation\n136 pad = self .aug.get (\u2019translate \u2019, 0)\n137 if pad > 0:\n138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019\nreflect \u2019)\n139\n140 if self .aug.get(\u2019translate \u2019, 0) > 0:\n141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .\nshape [ -2])\n142 elif self .aug.get (\u2019flip \u2019, False ):\n143 images = self . proc_images [\u2019flip \u2019]\n144 else :\n145 images = self . proc_images [\u2019norm \u2019]\n146 if self .aug.get(\u2019flip \u2019, False ):\n147 if self . epoch % 2 == 1:\n148 images = images . flip ( -1)\n149\n150 self . epoch += 1\n151\n152 indices = ( torch . randperm if self . shuffle else torch . arange )(\nlen ( images ), device = images . device )\n153 for i in range (len ( self )):\n154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]\n155 yield ( images [ idxs ], self . labels [ idxs ])\n156\n157# ############################################\n158# Network Components #\n159# ############################################\n160\n17\n161class Flatten (nn. Module ):\n162 def forward (self , x):\n163 return x. view (x. size (0) , -1)\n164\n165class Mul (nn. Module ):\n166 def __init__ (self , scale ):\n167 super (). __init__ ()\n168 self . scale = scale\n169 def forward (self , x):\n170 return x * self . scale\n171\n172class BatchNorm (nn. BatchNorm2d ):\n173 def __init__ (self , num_features , momentum , eps =1e -12 ,\n174 weight =False , bias = True ):\n175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )\n176 self . weight . requires_grad = weight\n177 self . bias . requires_grad = bias\n178 # Note that PyTorch already initializes the weights to one and\nbiases to zero\n179\n180class Conv (nn. Conv2d ):\n181 def __init__ (self , in_channels , out_channels , kernel_size =3,\npadding =\u2019same \u2019, bias = False ):\n182 super (). __init__ ( in_channels , out_channels , kernel_size =\nkernel_size , padding = padding , bias = bias )\n183\n184 def reset_parameters ( self ):\n185 super (). reset_parameters ()\n186 if self . bias is not None :\n187 self . bias . data . zero_ ()\n188 w = self . weight . data\n189 torch .nn. init . dirac_ (w[:w. size (1) ])\n190\n191class ConvGroup (nn. Module ):\n192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):\n193 super (). __init__ ()\n194 self . conv1 = Conv ( channels_in , channels_out )\n195 self . pool = nn. MaxPool2d (2)\n196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )\n197 self . conv2 = Conv ( channels_out , channels_out )\n198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )\n199 self . activ = nn. GELU ()\n200\n201 def forward (self , x):\n202 x = self . conv1 (x)\n203 x = self . pool (x)\n204 x = self . norm1 (x)\n205 x = self . activ (x)\n206 x = self . conv2 (x)\n207 x = self . norm2 (x)\n208 x = self . activ (x)\n209 return x\n210\n211# ############################################\n212# Network Definition #\n213# ############################################\n214\n215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019\n][ \u2019 batchnorm_momentum \u2019]):\n216 whiten_kernel_size = 2\n217 whiten_width = 2 * 3 * whiten_kernel_size **2\n218 net = nn. Sequential (\n219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True\n),\n220 nn. GELU () ,\n18\n221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],\nbatchnorm_momentum ),\n222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],\nbatchnorm_momentum ),\n223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],\nbatchnorm_momentum ),\n224 nn. MaxPool2d (3) ,\n225 Flatten () ,\n226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),\n227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,\n228 )\n229 net [0]. weight . requires_grad = False\n230 net = net. half (). cuda ()\n231 net = net.to( memory_format = torch . channels_last )\n232 for mod in net. modules ():\n233 if isinstance (mod , BatchNorm ):\n234 mod . float ()\n235 return net\n236\n237# ############################################\n238# Whitening Conv Initialization #\n239# ############################################\n240\n241def get_patches (x, patch_shape ):\n242 c, (h, w) = x. shape [1] , patch_shape\n243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h\n,w). float ()\n244\n245def get_whitening_parameters ( patches ):\n246 n,c,h,w = patches . shape\n247 patches_flat = patches . view (n, -1)\n248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n\n249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance\n, UPLO =\u2019U\u2019)\n250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.\nreshape (c*h*w,c,h,w). flip (0)\n251\n252def init_whitening_conv (layer , train_set , eps =5e -4):\n253 patches = get_patches ( train_set , patch_shape = layer . weight . data .\nshape [2:])\n254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )\n255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)\n256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -\neigenvectors_scaled ))\n257\n258# ###########################################\n259# Lookahead #\n260# ###########################################\n261\n262class LookaheadState :\n263 def __init__ (self , net):\n264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().\nitems ()}\n265\n266 def update (self , net , decay ):\n267 for ema_param , net_param in zip( self . net_ema . values () , net.\nstate_dict (). values ()):\n268 if net_param . dtype in ( torch .half , torch . float ):\n269 ema_param . lerp_ ( net_param , 1- decay )\n270 net_param . copy_ ( ema_param )\n271\n272# ###########################################\n273# Logging #\n274# ###########################################\n275\n19\n276def print_columns ( columns_list , is_head =False , is_final_entry = False ):\n277 print_string = \u2019\u2019\n278 for col in columns_list :\n279 print_string += \u2019| %s \u2019 % col\n280 print_string += \u2019|\u2019\n281 if is_head :\n282 print (\u2019-\u2019*len( print_string ))\n283 print ( print_string )\n284 if is_head or is_final_entry :\n285 print (\u2019-\u2019*len( print_string ))\n286\n287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,\n\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]\n288def print_training_details ( variables , is_final_entry ):\n289 formatted = []\n290 for col in logging_columns_list :\n291 var = variables .get (col. strip () , None )\n292 if type (var) in (int , str):\n293 res = str(var)\n294 elif type (var) is float :\n295 res = \u2019{:0.4 f}\u2019. format ( var)\n296 else :\n297 assert var is None\n298 res = \u2019\u2019\n299 formatted . append (res. rjust (len (col)))\n300 print_columns ( formatted , is_final_entry = is_final_entry )\n301\n302# ###########################################\n303# Evaluation #\n304# ###########################################\n305\n306def infer (model , loader , tta_level =0) :\n307 \"\"\"\n308 Test - time augmentation strategy (for tta_level =2):\n309 1. Flip / mirror the image left -to - right (50% of the time ).\n310 2. Translate the image by one pixel either up -and - left or down -and\n- right (50% of the time , i.e. both happen 25% of the time ).\n311\n312 This creates 6 views per image ( left / right times the two\ntranslations and no - translation ), which we evaluate and then\nweight according to the given probabilities .\n313 \"\"\"\n314\n315 def infer_basic (inputs , net):\n316 return net( inputs ). clone ()\n317\n318 def infer_mirror (inputs , net):\n319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))\n320\n321 def infer_mirror_translate ( inputs , net ):\n322 logits = infer_mirror (inputs , net )\n323 pad = 1\n324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)\n325 inputs_translate_list = [\n326 padded_inputs [:, :, 0:32 , 0:32] ,\n327 padded_inputs [:, :, 2:34 , 2:34] ,\n328 ]\n329 logits_translate_list = [ infer_mirror ( inputs_translate , net)\n330 for inputs_translate in\ninputs_translate_list ]\n331 logits_translate = torch . stack ( logits_translate_list ). mean (0)\n332 return 0.5 * logits + 0.5 * logits_translate\n333\n334 model . eval ()\n335 test_images = loader . normalize ( loader . images )\n20\n336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][\ntta_level ]\n337 with torch . no_grad ():\n338 return torch .cat ([ infer_fn (inputs , model ) for inputs in\ntest_images . split (2000) ])\n339\n340def evaluate (model , loader , tta_level =0) :\n341 logits = infer (model , loader , tta_level )\n342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()\n343\n344# ###########################################\n345# Training #\n346# ###########################################\n347\n348def main (run):\n349\n350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]\n351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]\n352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]\n353 # Assuming gradients are constant in time , for Nesterov momentum ,\nthe below ratio is how much larger the default steps will be than\nthe underlying per - example gradients . We divide the learning rate\nby this ratio in order to ensure steps are the same scale as\ngradients , regardless of the choice of momentum .\n354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))\n355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning\nrate for PyTorch SGD\n356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale\n357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]\n358\n359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019\nlabel_smoothing \u2019], reduction =\u2019none \u2019)\n360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)\n361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =\nbatch_size , aug=hyp[\u2019aug \u2019])\n362 if run == \u2019warmup \u2019:\n363 # The only purpose of the first run is to warmup , so we can\nuse dummy data\n364 train_loader . labels = torch . randint (0, 10, size =( len (\ntrain_loader . labels ) ,), device = train_loader . labels . device )\n365 total_train_steps = ceil (len( train_loader ) * epochs )\n366\n367 model = make_net ()\n368 current_steps = 0\n369\n370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in\nk and p. requires_grad ]\n371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019\nnot in k and p. requires_grad ]\n372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,\nweight_decay =wd/ lr_biases ),\n373 dict ( params = other_params , lr=lr , weight_decay =wd/\nlr)]\n374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,\nnesterov = True )\n375\n376 def triangle (steps , start =0, end =0, peak =0.5) :\n377 xp = torch . tensor ([0 , int( peak * steps ), steps ])\n378 fp = torch . tensor ([ start , 1, end ])\n379 x = torch . arange (1+ steps )\n380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])\n381 b = fp [: -1] - (m * xp [: -1])\n382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1\n383 indices = torch . clamp ( indices , 0, len(m) - 1)\n384 return m[ indices ] * x + b[ indices ]\n21\n385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,\npeak =0.23)\n386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:\nlr_schedule [i])\n387\n388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /\ntotal_train_steps )**3\n389 lookahead_state = LookaheadState ( model )\n390\n391 # For accurately timing GPU code\n392 starter = torch . cuda . Event ( enable_timing = True )\n393 ender = torch . cuda . Event ( enable_timing = True )\n394 total_time_seconds = 0.0\n395\n396 # Initialize the first layer using statistics of training images\n397 starter . record ()\n398 train_images = train_loader . normalize ( train_loader . images [:5000])\n399 init_whitening_conv ( model [0] , train_images )\n400 ender . record ()\n401 torch . cuda . synchronize ()\n402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n403\n404 for epoch in range ( ceil ( epochs )):\n405\n406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019\nwhiten_bias_epochs \u2019])\n407\n408 # ###################\n409 # Training #\n410 # ###################\n411\n412 starter . record ()\n413\n414 model . train ()\n415 for inputs , labels in train_loader :\n416\n417 outputs = model ( inputs )\n418 loss = loss_fn ( outputs , labels ).sum ()\n419 optimizer . zero_grad ( set_to_none = True )\n420 loss . backward ()\n421 optimizer . step ()\n422 scheduler . step ()\n423\n424 current_steps += 1\n425\n426 if current_steps % 5 == 0:\n427 lookahead_state . update (model , decay = alpha_schedule [\ncurrent_steps ]. item ())\n428\n429 if current_steps >= total_train_steps :\n430 if lookahead_state is not None :\n431 lookahead_state . update (model , decay =1.0)\n432 break\n433\n434 ender . record ()\n435 torch . cuda . synchronize ()\n436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n437\n438 # ###################\n439 # Evaluation #\n440 # ###################\n441\n442 # Print the accuracy and loss from the last training batch of\nthe epoch\n22\n443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().\nmean (). item ()\n444 train_loss = loss . item () / batch_size\n445 val_acc = evaluate (model , test_loader , tta_level =0)\n446 print_training_details ( locals () , is_final_entry = False )\n447 run = None # Only print the run number once\n448\n449 # ###################\n450 # TTA Evaluation #\n451 # ###################\n452\n453 starter . record ()\n454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019\ntta_level \u2019])\n455 ender . record ()\n456 torch . cuda . synchronize ()\n457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n458\n459 epoch = \u2019eval \u2019\n460 print_training_details ( locals () , is_final_entry = True )\n461\n462 return tta_val_acc\n463\n464if __name__ == \" __main__ \":\n465 with open (sys. argv [0]) as f:\n466 code = f. read ()\n467\n468 print_columns ( logging_columns_list , is_head = True )\n469 main (\u2019warmup \u2019)\n470 accs = torch . tensor ([ main (run) for run in range (25) ])\n471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))\n472\n473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }\n474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))\n475 os. makedirs ( log_dir , exist_ok = True )\n476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)\n477 print (os. path . abspath ( log_path ))\n478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))\nListing 4: airbench94.py\n23\nTask Candidates:"}, {"Response": "Here is a JSON object containing task candidates extracted from the paper:\n\n{\n  \"tasks\": [\n    {\n      \"name\": \"Fast CIFAR-10 Training\",\n      \"description\": \"Develop methods to train neural networks to 94% accuracy on CIFAR-10 in the shortest possible time on a single GPU.\",\n      \"relevant_paper_text\": \"To accelerate research and reduce the cost of experiments, we introduce training methods for CIFAR-10 which reach 94% accuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when run on a single NVIDIA A100 GPU.\"\n    },\n    {\n      \"name\": \"Derandomized Data Augmentation\",\n      \"description\": \"Develop a derandomized variant of horizontal flipping augmentation to improve training speed and performance.\",\n      \"relevant_paper_text\": \"As one factor contributing to these training speeds, we propose a derandomized variant of horizontal flipping augmentation, which we show improves over the standard method in every case where flipping is beneficial over no flipping at all.\"\n    },\n    {\n      \"name\": \"Efficient Network Architecture Design\",\n      \"description\": \"Design an efficient convolutional neural network architecture optimized for fast training on CIFAR-10.\",\n      \"relevant_paper_text\": \"We train a convolutional network with a total of 1.97 million parameters, following tysam-code (2023) with a few small changes. It contains seven convolutions with the latter six being divided into three blocks of two.\"\n    },\n    {\n      \"name\": \"Hyperparameter Optimization\",\n      \"description\": \"Optimize hyperparameters like learning rate, momentum, and weight decay for fast and stable training.\",\n      \"relevant_paper_text\": \"We express the main training hyperparameters (batch size, learning rate, momentum, and weight decay) in decoupled form, so that each one can be tuned independently.\"\n    },\n    {\n      \"name\": \"Test-Time Augmentation\",\n      \"description\": \"Develop efficient test-time augmentation methods to improve model performance without significantly increasing inference time.\",\n      \"relevant_paper_text\": \"To generate predictions, we run the trained network on six augmented views of each test image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a version which is translated down-and-to-the-right by one pixel, and the mirrored versions of all three.\"\n    }\n  ]\n}"}, {"Request": "Make a very succinct json object that validates with the following schema\n\n---\n\nFollow the following format.\n\nJson Schema: ${json_schema}\nJson Object: ${json_object}\n\n---\n\nJson Schema: {\"$defs\": {\"TaskCandidate\": {\"properties\": {\"name\": {\"description\": \"Name of the task\", \"title\": \"Name\", \"type\": \"string\"}, \"description\": {\"description\": \"Description of the task\", \"title\": \"Description\", \"type\": \"string\"}, \"relevant_paper_text\": {\"description\": \"Text from the paper that is relevant to the task\", \"title\": \"Relevant Paper Text\", \"type\": \"string\"}}, \"required\": [\"name\", \"description\", \"relevant_paper_text\"], \"title\": \"TaskCandidate\", \"type\": \"object\"}}, \"properties\": {\"tasks\": {\"description\": \"Tasks extracted from the paper\", \"items\": {\"$ref\": \"#/$defs/TaskCandidate\"}, \"title\": \"Tasks\", \"type\": \"array\"}}, \"required\": [\"tasks\"], \"title\": \"TaskCandidates\", \"type\": \"object\"}\nJson Object:"}, {"Response": "Json Object: {\n  \"tasks\": [\n    {\n      \"name\": \"Image Classification\",\n      \"description\": \"Categorize images into predefined classes\",\n      \"relevant_paper_text\": \"We applied our model to image classification tasks.\"\n    }\n  ]\n}"}, {"Request": "Given the fields `paper_text`, produce the fields `task_candidates`.\n\n---\n\nFollow the following format.\n\nPaper Text: Full text from research paper PDF\nPast Error in Task Candidates: An error to avoid in the future\nTask Candidates: JSON list of task candidate dicts with keys 'name', 'description', and 'relevant_paper_text'. Respond with a single JSON object. JSON Schema: {\"$defs\": {\"TaskCandidate\": {\"properties\": {\"name\": {\"description\": \"Name of the task\", \"title\": \"Name\", \"type\": \"string\"}, \"description\": {\"description\": \"Description of the task\", \"title\": \"Description\", \"type\": \"string\"}, \"relevant_paper_text\": {\"description\": \"Text from the paper that is relevant to the task\", \"title\": \"Relevant Paper Text\", \"type\": \"string\"}}, \"required\": [\"name\", \"description\", \"relevant_paper_text\"], \"title\": \"TaskCandidate\", \"type\": \"object\"}}, \"properties\": {\"tasks\": {\"description\": \"Tasks extracted from the paper\", \"items\": {\"$ref\": \"#/$defs/TaskCandidate\"}, \"title\": \"Tasks\", \"type\": \"array\"}}, \"required\": [\"tasks\"], \"title\": \"TaskCandidates\", \"type\": \"object\"}\n\n---\n\nPaper Text:\n94% on CIFAR-10 in 3.29 Seconds on a Single GPU\nKeller Jordan\nkjordan4077@gmail.com\nAbstract\nCIFAR-10 is among the most widely used datasets in machine learning, facilitating\nthousands of research projects per year. To accelerate research and reduce the cost\nof experiments, we introduce training methods for CIFAR-10 which reach 94%\naccuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when\nrun on a single NVIDIA A100 GPU. As one factor contributing to these training\nspeeds, we propose a derandomized variant of horizontal flipping augmentation,\nwhich we show improves over the standard method in every case where flipping is\nbeneficial over no flipping at all. Our code is released at https://github.com/\nKellerJordan/cifar10-airbench .\n1 Introduction\nCIFAR-10 (Krizhevsky et al., 2009) is one of the most popular datasets in machine learning, facilitat-\ning thousands of research projects per year1. Research can be accelerated and the cost of experiments\nreduced if the speed at which it is possible to train neural networks on CIFAR-10 is improved. In\nthis paper we introduce a training method which reaches 94% accuracy in 3.29 seconds on a single\nNVIDIA A100 GPU, which is a 1.9\u00d7improvement over the prior state-of-the-art ( tysam-code ,\n2023). To support scenarios where higher performance is needed, we additionally develop methods\ntargeting 95% and 96% accuracy. We release the following methods in total.\n1.airbench94_compiled.py :94.01% accuracy in 3.29 seconds (3.6\u00d71014FLOPs).\n2.airbench94.py : 94.01% accuracy in 3.83 seconds ( 3.6\u00d71014FLOPs).\n3.airbench95.py : 95.01% accuracy in 10.4 seconds ( 1.4\u00d71015FLOPs).\n4.airbench96.py : 96.05% accuracy in 46.3 seconds ( 7.2\u00d71015FLOPs).\nAll runtimes are measured on a single NVIDIA A100. We note that the first two scripts are mathe-\nmatically equivalent ( i.e., yield the same distribution of trained networks), and differ only in that the\nfirst uses torch.compile to improve GPU utilization. It is intended for experiments where many\nnetworks are trained at once in order to amortize the one-time compilation cost. The non-compiled\nairbench94 variant can be easily installed and run using the following command.\n1pip install airbench\n2python -c \" import airbench as ab; ab. warmup94 (); ab. train94 ()\"\nOne motivation for the development of these training methods is that they can accelerate the experi-\nmental iteration time of researchers working on compatible projects involving CIFAR-10. Another\nmotivation is that they can decrease the cost of projects involving a massive number of trained\nnetworks. One example of such a project is Ilyas et al. (2022), a study on data attribution which used\n3 million trained networks to demonstrate that the outputs of a trained neural network on a given test\ninput follow an approximately linear function of the vector of binary choices of which examples the\nmodel was trained on. Another example is Jordan (2023), a study on training variance which used\n180 thousand trained networks to show that standard trainings have little variance in performance on\n1https://paperswithcode.com/datasets\nPreprint. Under review.arXiv:2404.00498v2  [cs.LG]  5 Apr 2024\nFigure 1: Alternating flip. In computer vision we typically train neural networks using random horizontal\nflipping augmentation, which flips each image with 50% probability per epoch. This results in some images\nbeing redundantly flipped the same way for many epochs in a row. We propose (Section 3.6) to flip images in a\ndeterministically alternating manner after the first epoch, avoiding this redundancy and speeding up training.\ntheir test-distributions. These studies were based on trainings which reach 93% in 34 A100-seconds\nand 94.4% in 72 A100-seconds, respectively. The training methods we introduce in this paper make\nit possible to replicate these studies, or conduct similar ones, with fewer computational resources.\nFast training also enables the rapid accumulation of statistical significance for subtle hyperparameter\ncomparisons. For example, if changing a given hyperparameter subtly improves mean CIFAR-10\naccuracy by 0.02% compared to a baseline, then (assuming a typical 0.14% standard deviation\nbetween runs (Jordan, 2023)) we will need on average N= 133 runs of training to confirm the\nimprovement at a statistical significance of p= 0.05. For a standard 5-minute ResNet-18 training\nthis will take 11.1 GPU-hours; airbench94 shrinks this to a more convenient time of 7.3 minutes.\nOur work builds on prior training speed projects. We utilize a modified version of the network,\ninitialization, and optimizer from tysam-code (2023), as well as the optimization tricks and frozen\npatch-whitening layer from Page (2019); tysam-code (2023). The final \u223c10% of our speedup over\nprior work is obtained from a novel improvement to standard horizontal flipping augmentation (Fig-\nure 1, Section 3.6, Section 5.2).\n2 Background\nOur objective is to develop a training method which reaches 94% accuracy on the CIFAR-10 test-set\nin the shortest possible amount of time. Timing begins when the method is first given access to\ntraining data, and ends when it produces test-set predictions. The method is considered valid if its\nmean accuracy over repeated runs is at least 94%.\nWe chose the goal of 94% accuracy because this was the target used by the CIFAR-10 track of the\n2017-2020 Stanford DAWNBench training speed competition (Coleman et al., 2017), as well as more\nrecent work ( tysam-code , 2023). The final winning DAWNBench submission reached 94% in 10\nseconds on 8 V100s (Serrano et al., 2019) ( \u224832A100-seconds), using a modified version of Page\n(2019), which itself runs in 26 V100-seconds ( \u224810.4A100-seconds). The prior state-of-the-art is\ntysam-code (2023) which attains 94% in 6.3 A100-seconds. As another motivation for the goal,\n94% is the level of human accuracy reported by Karpathy (2011).\nWe note the following consequences of how the method is timed. First, it is permitted for the program\nto begin by executing a run using dummy data in order to \u201cwarm up\u201d the GPU, since timing begins\nwhen the training data is first accessed. This is helpful because otherwise the first run of training\nis typically a bit slower. Additionally, arbitrary test-time augmentation (TTA) is permitted. TTA\nimproves the performance of a trained network by running it on multiple augmented views of each\ntest input. Prior works (Page, 2019; Serrano et al., 2019; tysam-code , 2023) use horizontal flipping\nTTA; we use horizontal flipping and two extra crops. Without any TTA our three training methods\nattain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\nThe CIFAR-10 dataset contains 60,000 32x32 color images, each labeled as one of ten classes.\nIt is divided into a training set of 50,000 images and a validation set of 10,000 images. As a\nmatter of historical interest, we note that in 2011 the state-of-the-art accuracy on CIFAR-10 was\n2\n80.5% (Cire\u00b8 san et al., 2011), using a training method which consumes 26\u00d7more FLOPs than\nairbench94 . Therefore, the progression from 80.5% in 2011 to the 94% accuracy of airbench94\ncan be attributed entirely to algorithmic progress rather than compute scaling.\n3 Methods\n3.1 Network architecture and baseline training\nWe train a convolutional network with a total of 1.97 million parameters, following tysam-code\n(2023) with a few small changes. It contains seven convolutions with the latter six being divided into\nthree blocks of two. The precise architecture is given as simple PyTorch code in Section A; in this\nsection we offer some comments on the main design choices.\nThe network is VGG (Simonyan & Zisserman, 2014)-like in the sense that its main body is composed\nentirely of 3x3 convolutions and 2x2 max-pooling layers, alongside BatchNorm (Ioffe & Szegedy,\n2015) layers and activations. Following tysam-code (2023) the first layer is a 2x2 convolution with\nno padding, causing the shape of the internal feature maps to be 31x31 \u219215x15\u21927x7\u21923x3 rather\nthan the more typical 32x32 \u219216x16\u21928x8\u21924x4, resulting in a slightly more favorable tradeoff\nbetween throughput and performance. We use GELU (Hendrycks & Gimpel, 2016) activations.\nFollowing Page (2019); tysam-code (2023), we disable the biases of convolutional and linear layers,\nand disable the affine scale parameters of BatchNorm layers. The output of the final linear layer is\nscaled down by a constant factor of 1/9. Relative to tysam-code (2023), our network architecture\ndiffers only in that we decrease the number of output channels in the third block from 512 to 256,\nand we add learnable biases to the first convolution.\nAs our baseline, we train using Nesterov SGD at batch size 1024, with a label smoothing rate of\n0.2. We use a triangular learning rate schedule which starts at 0.2\u00d7the maximum rate, reaches the\nmaximum at 20% of the way through training, and then decreases to zero. For data augmentation we\nuse random horizontal flipping alongside 2-pixel random translation. For translation we use reflection\npadding (Zagoruyko & Komodakis, 2016) which we found to be better than zero-padding. Note\nthat what we call 2-pixel random translation is equivalent to padding with 2 pixels and then taking a\nrandom 32x32 crop. During evaluation we use horizontal flipping test-time augmentation, where the\nnetwork is run on both a given test image and its mirror and inferences are made based on the average\nof the two outputs. With optimized choices of learning rate, momentum, and weight decay, this\nbaseline training configuration yields 94% mean accuracy in 45 epochs taking 18.3 A100-seconds.\n3.2 Frozen patch-whitening initialization\nFollowing Page (2019); tysam-code (2023) we initialize the first convolutional layer as a patch-\nwhitening transformation. The layer is a 2x2 convolution with 24 channels. Following tysam-code\n(2023) the first 12 filters are initialized as the eigenvectors of the covariance matrix of 2x2 patches\nacross the training distribution, so that their outputs have identity covariance matrix. The second 12\nFigure 2: The first layer\u2019s weights after whitening\ninitialization ( tysam-code , 2023; Page, 2019)filters are initialized as the negation of the first 12,\nso that input information is preserved through the\nactivation which follows. Figure 2 shows the result.\nWe do not update this layer\u2019s weights during training.\nDeparting from tysam-code (2023), we add learn-\nable biases to this layer, yielding a small performance\nboost. The biases are trained for 3 epochs, after which\nwe disable their gradient to increase backward-pass\nthroughput, which improves training speed without\nreducing accuracy. We also obtain a slight perfor-\nmance boost relative to tysam-code (2023) by re-\nducing the constant added to the eigenvalues during\ncalculation of the patch-whitening initialization for\nthe purpose of preventing numerical issues in the case\nof a singular patch-covariance matrix.\nPatch-whitening initialization is the single most impactful feature. Adding it to the baseline more\nthan doubles training speed so that we reach 94% accuracy in 21 epochs taking 8.0 A100-seconds.\n3\nRandom reshuffling Alternating flip Mean accuracy\nNo No 93.40%\nNo Yes 93.48%\nYes No 93.92%\nYes Yes 94.01%\nTable 1: Training distribution options (Section 3.6). Both random reshuffling (which is standard) and alternating\nflip (which we propose) reduce training data redundancy and improve performance.\n3.3 Identity initialization\ndirac : We initialize all convolutions after the first as partial identity transforms. That is, for a\nconvolution with Minput channels and N\u2265Moutputs, we initialize its first Mfilters to an\nidentity transform of the input, and leave the remaining N\u2212Mto their default initialization. In\nPyTorch code, this amounts to running torch.nn.init.dirac_(w[:w.size(1)]) on the weight\nwof each convolutional layer. This method partially follows tysam-code (2023), which used a more\ncomplicated scheme where the identity weights are mixed in with the original initialization, which\nwe did not find to be more performant. With this feature added, training attains 94% accuracy in 18\nepochs taking 6.8 A100-seconds.\n3.4 Optimization tricks\nscalebias : We increase the learning rate for the learnable biases of all BatchNorm layers by a factor\nof64\u00d7, following Page (2019); tysam-code (2023). With this feature added, training reaches 94%\nin 13.5 epochs taking 5.1 A100-seconds.\nlookahead : Following tysam-code (2023), we use Lookahead (Zhang et al., 2019) optimization.\nWe note that Lookahead has also been found effective in prior work on training speed for ResNet-\n18 (Moreau et al., 2022). With this feature added, training reaches 94% in 12.0 epochs taking 4.6\nA100-seconds.\n3.5 Multi-crop evaluation\nmulticrop : To generate predictions, we run the trained network on six augmented views of each\ntest image: the unmodified input, a version which is translated up-and-to-the-left by one pixel, a\nversion which is translated down-and-to-the-right by one pixel, and the mirrored versions of all\nthree. Predictions are made using a weighted average of all six outputs, where the two views of the\nuntranslated image are weighted by 0.25 each, and the remaining four views are weighted by 0.125\neach. With this feature added, training reaches 94% in 10.8 epochs taking 4.2 A100-seconds.\nWe note that multi-crop inference is a classic method for ImageNet (Deng et al., 2009) trainings (Si-\nmonyan & Zisserman, 2014; Szegedy et al., 2014), where performance improves as the number of\nevaluated crops is increased, even up to 144 crops (Szegedy et al., 2014). In our experiments, using\nmore crops does improve performance, but the increase to inference time outweighs the potential\ntraining speedup.\n3.6 Alternating flip\nTo speed up training, we propose a derandomized variant of standard horizontal flipping augmentation,\nwhich we motivate as follows. When training neural networks, it is standard practice to organize\ntraining into a set of epochs during which every training example is seen exactly once. This\ndiffers from the textbook definition of stochastic gradient descent (SGD) (Robbins & Monro, 1951),\nwhich calls for data to be repeatedly sampled with-replacement from the training set, resulting in\nexamples being potentially seen multiple redundant times within a short window of training. The\nuse of randomly ordered epochs of data for training has a different name, being called the random\nreshuffling method in the optimization literature (G\u00fcrb\u00fczbalaban et al., 2021; Bertsekas, 2015). If our\ntraining dataset consists of Nunique examples, then sampling data with replacement causes every\n\u201cepoch\u201d of Nsampled examples to contain only (1\u2212(1\u22121/N)N)N\u2248(1\u22121/e)N\u22480.632N\nunique examples on average. On the other hand, random reshuffling leads to all Nunique examples\nbeing seen every epoch. Given that random reshuffling is empirically successful (Table 1), we reason\nthat it is beneficial to maximize the number of unique inputs seen per window of training time.\n4\nWe extend this reasoning to design a new variant of horizontal flipping augmentation, as follows. We\nfirst note that standard random horizontal flipping augmentation can be defined as follows.\n1import torch\n2def random_flip ( inputs ):\n3 # Applies random flipping to a batch of images\n4 flip_mask = ( torch . rand (len ( inputs )) < 0.5) . view (-1, 1, 1, 1)\n5 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )]\nListing 1: Random flip\nIf horizontal flipping is the only augmentation used, then there are exactly 2Npossible unique inputs2\nwhich may be seen during training. Potentially, every pair of consecutive epochs could contain every\nunique input. But our main observation is that with standard random horizontal flipping, half of the\nimages will be redundantly flipped the same way during both epochs, so that on average only 1.5N\nunique inputs will be seen.\naltflip : To address this, we propose to modify standard random horizontal flipping augmentation as\nfollows. For the first epoch, we randomly flip 50% of inputs as usual. Then on epochs {2,4,6, . . .},\nwe flip only those inputs which were not flipped in the first epoch, and on epochs {3,5,7, . . .}, we\nflip only those inputs which were flipped in the first epoch. We provide the following implementation\nwhich avoids the need for extra memory by using a pseudorandom function to decide the flips.\n1import torch\n2import hashlib\n3def hash_fn (n, seed =42) :\n4 k = n * seed\n5 return int( hashlib .md5( bytes (str(k), \u2019utf -8 \u2019)). hexdigest () [ -8:] ,\n16)\n6def alternating_flip (inputs , indices , epoch ):\n7 # Applies alternating flipping to a batch of images\n8 hashed_indices = torch . tensor ([ hash_fn (i) for i in indices . tolist\n() ])\n9 flip_mask = (( hashed_indices + epoch ) % 2 == 0). view (-1, 1, 1, 1)\n10 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\nListing 2: Alternating flip\nThe result is that every pair of consecutive epochs contains all 2Nunique inputs, as we can see in\nFigure 1. We demonstrate the effectiveness of this method across a variety of scenarios in Section 5.2.\nAdding this feature allows us to shorten training to its final duration of 9.9 epochs, yielding our final\ntraining method airbench94.py , the entire contents of which can be found in Section E. It reaches\n94% accuracy in 3.83 seconds on an NVIDIA A100.\n3.7 Compilation\nThe final step we take to speed up training is a non-algorithmic one: we compile our training\nmethod using torch.compile in order to more efficiently utilize the GPU. This results in a training\nscript which is mathematically equivalent (up to small differences in floating point arithmetic) to\nthe non-compiled variant while being significantly faster: training time is reduced by 14% to 3.29\nA100-seconds. The downside is that the one-time compilation process takes up to several minutes to\ncomplete before training runs can begin, so that it is only beneficial when we plan to execute many\nruns of training at once. We release this version as airbench94_compiled.py .\n4 95% and 96% targets\nTo address scenarios where somewhat higher performance is desired, we additionally develop methods\ntargeting 95% and 96% accuracy. Both are straightforward modifications airbench94 .\nTo attain 95% accuracy, we increase training epochs from 9.9 to 15, and we scale the output channel\ncount of the first block from 64 to 128 and of the second two blocks from 256 to 384. We reduce\nthe learning rate by a factor of 0.87. These modifications yield airbench95 which attains 95.01%\naccuracy in 10.4 A100-seconds, consuming 1.4\u00d71015FLOPs.\n2Assuming none of the training inputs are already mirrors of each other.\n5\nFigure 4: Training speedups accumulate additively. Removing individual features from airbench94\nincreases the epochs-to-94%. Adding the same features to the whitened baseline training (Section 3.2) reduces\nthe epochs-to-94%. For every feature except multi-crop TTA (Section 3.5), these two changes in in epochs-to-\n94% are roughly the same, suggesting that training speedups accumulate additively rather than multiplicatively.\nFigure 3: FLOPs vs. error rate tradeoff. Our\nthree training methods apparently follow a linear\nlog-log relationship between FLOPs and error rate.To attain 96% accuracy, we add 12-pixel Cutout (De-\nVries & Taylor, 2017) augmentation and raise the\ntraining epochs to 40. We add a third convolution to\neach block, and scale the first block to 128 channels\nand the second two to 512. We also add a residual\nconnection across the later two convolutions of each\nblock, which we find is still beneficial despite the fact\nthat we are already using identity initialization (Sec-\ntion 3.3) to ease gradient flow. Finally, we reduce\nthe learning rate by a factor of 0.78. These changes\nyield airbench96 which attains 96.05% accuracy in\n46.3 A100-seconds, consuming 7.2\u00d71015FLOPs.\nFigure 3 shows the FLOPs and error rate of each of\nour three training methods.\n5 Experiments\n5.1 Interaction between features\nTo gain a better sense of the impact of each feature on training speed, we compare two quantities.\nFirst, we measure the number of epochs that can be saved by adding the feature to the whitened\nbaseline (Section 3.2). Second, we measure the number of epochs that must be added when the\nfeature is removed from the final airbench94 (Section 3.6). For example, adding identity initializa-\ntion (Section 3.3) to the whitened baseline reduces the epochs-to-94% from 21 to 18, and removing it\nfrom the final airbench94 increases epochs-to-94% from 9.9 to 12.8.\nFigure 4 shows both quantities for each feature. Surprisingly, we find that for all features except\nmulti-crop TTA, the change in epochs attributable to a given feature is similar in both cases, even\nthough the whitened baseline requires more than twice as many epochs as the final configuration.\nThis indicates that the interaction between most features is additive rather than multiplicative.\n5.2 Does alternating flip generalize?\nIn this section we investigate the effectiveness of alternating flip (Section 3.6) across a variety of\ntraining configurations on CIFAR-10 and ImageNet. We find that it improves training speed in all\ncases except those where neither alternating nor random flip improve over using no flipping at all.\nFor CIFAR-10 we consider the performance boost given by alternating flip across the follow-\ning 24 training configurations: airbench94 ,airbench94 with extra Cutout augmentation, and\nairbench96 , each with epochs in the range {10,20,40,80}and TTA (Section 3.5) in {yes,no}. For\n6\nFigure 5: Alternating flip boosts performance. Across a variety of settings for airbench94 andairbench96 ,\nthe use of alternating flip rather than random flip consistently boosts performance by the equivalent of a 0-25%\ntraining speedup. The benefit generalizes to ImageNet trainings which use light augmentation other than flipping.\n95% confidence intervals are shown around each point.\neach configuration we compare the performance of alternating and random flip in terms of their mean\naccuracy across n= 400 runs of training.\nFigure 5 shows the result (see Table 6 for raw numbers). Switching from random flip to alternating flip\nimproves performance in every setting. To get a sense for how big the improvement is, we estimate\nthe effective speedup for each case, i.e., the fraction of epochs that could be saved by switching\nfrom random to alternating flip while maintaining the level of accuracy of random flip. We begin\nby fitting power law curves of the form error = c+b\u00b7epochsato the epochs-to-error curves of\neach random flip-based training configuration. We use these curves to calculate the effective speedup\nafforded by switching from random to alternating flip. For example, airbench94 with random flip\nand without TTA attains 6.26% error when run for 20 epochs and 5.99% when run for 40 epochs. The\nsame configuration with alternating flip attains 6.13% when run for 20 epochs, which a power-law\nfit predicts would take 25.3 epochs to attain using random flip. So we report a speedup of 27%.\nNote that using a power-law yields a more conservative estimate relative to using linear interpolation\nbetween the observed epochs vs. error datapoints, which would yield a predicted speedup of 52%.\nTable 2 shows the result. We observe the following patterns. First, the addition of extra augmentation\n(Cutout) somewhat closes the gap between random and alternating flip. To explain this, we note that\nthe main effect of alternating flip is that it eliminates cases where an image is redundantly flipped the\nsame way for many epochs in a row; we speculate that adding extra augmentation reduces the negative\nimpact of these cases because it increases data diversity. Next, TTA reduces the gap between random\nand alternating flip. It also reduces the gap between random flip and no flipping at all (Table 6),\nindicating that TTA simply reduces the importance of flipping augmentation as such. Finally, training\nfor longer consistently increases the effective speedup given by alternating flip.\nWe next study ImageNet trainings with the following experiment. We train a ResNet-18 with a\nvariety of train and test crops, comparing three flipping options: alternating flip, random flip, and no\nflipping at all. We consider two test crops: 256x256 center crop with crop ratio 0.875, and 192x192\ncenter crop with crop ratio 1.0. We write CC(256, 0.875) to denote the former and CC(192, 1.0)\nto denote the latter. We also consider two training crops: 192x192 inception-style random resized\ncrop (Szegedy et al., 2014), which has aspect ratio ranging from 0.75 to 1.33 and covers an area\nranging from 8% to 100% of the image, and a less aggressive random crop, which first resizes the\nshorter side of the image to 192 pixels, and then selects a random 192x192 square crop. We write\n7\nBaseline Cutout Epochs Speedup Speedup (w/ TTA)\nairbench94 No 10 15.0% 5.30%\nairbench94 No 20 27.1% 21.3%\nairbench94 No 40 38.3% 36.4%\nairbench94 No 80 102% 31.8%\nairbench94 Yes 10 3.84% 1.13%\nairbench94 Yes 20 7.42% 2.00%\nairbench94 Yes 40 18.6% 9.28%\nairbench94 Yes 80 29.2% 14.25%\nairbench96 Yes 10 4.94% 1.11%\nairbench96 Yes 20 8.99% 3.58%\nairbench96 Yes 40 17.2% 6.48%\nairbench96 Yes 80 18.8% Not measured\nTable 2: Effective speedups given by switching from random flip to alternating flip. The two configurations\nmost closely corresponding to airbench94.py andairbench96.py are italicized. See Table 6 for the raw\naccuracy values of the airbench94 experiments.\nFlipping augmentation option\nTrain crop Test crop Epochs TTA None Random Alternating\nHeavy RRC CC(256, 0.875) 16 No 66.78% n=8 66.54% n=28 66.58% n=28\nHeavy RRC CC(192, 1.0) 16 No 64.43% n=8 64.62% n=28 64.63% n=28\nLight RRC CC(256, 0.875) 16 No 59.02% n=4 61.84% n=26 62.19% n=26\nLight RRC CC(192, 1.0) 16 No 61.79% n=4 64.50% n=26 64.93% n=26\nHeavy RRC CC(256, 0.875) 16 Yes 67.52% n=8 67.65% n=28 67.60% n=28\nHeavy RRC CC(192, 1.0) 16 Yes 65.36% n=8 65.48% n=28 65.51% n=28\nLight RRC CC(256, 0.875) 16 Yes 61.08% n=4 62.89% n=26 63.08% n=26\nLight RRC CC(192, 1.0) 16 Yes 63.91% n=4 65.63% n=26 65.87% n=26\nLight RRC CC(192, 1.0) 20 Yes not measured 65.80% n=16 66.02% n=16\nHeavy RRC CC(256, 0.875) 88 Yes 72.34% n=2 72.45% n=4 72.46% n=4\nTable 3: ImageNet validation accuracy for ResNet-18 trainings. Alternating flip improves over random flip for\nthose trainings where random flip improves significantly over not flipping at all. The single best flipping option\nin each row is bolded when the difference is statistically significant.\nHeavy RRC to denote the former and Light RRC to denote the latter. Full training details are provided\nin Section C.\nTable 3 reports the mean top-1 validation accuracy of each case. We first note that Heavy RRC\nis better when networks are evaluated with the CC(256, 0.875) crop, and Light RRC is slightly\nbetter when CC(192, 1.0) is used. This is fairly unsurprising given the standard theory of train-test\nresolution discrepancy (Touvron et al., 2019).\nFor trainings which use Light RRC, we find that switching from random flip to alternating flip\nprovides a substantial boost to performance, amounting to a training speedup of more than 25%.\nIn Figure 5 we visualize the improvement for short trainings with Light RRC, where switching to\nalternating flip improves performance by more than increasing the training duration from 16 to 20\nepochs. The boost is higher when horizontal flipping TTA is turned off, which is consistent with our\nresults on CIFAR-10. On the other hand, trainings which use Heavy RRC see no significant benefit\nfrom alternating flip. Indeed, even turning flipping off completely does not significantly reduce the\nperformance of these trainings. We conclude that alternating flip improves over random flip for every\ntraining scenario where the latter improves over no flipping at all.\n8\nEpochs Width TTA Mean accuracy Test-set stddev Dist-wise stddev CACE\n1\u00d7 1\u00d7 No 93.25% 0.157% 0.037% 0.0312\n2\u00d7 1\u00d7 No 93.86% 0.152% 0.025% 0.0233\n1.5\u00d7 1.5\u00d7 No 94.32% 0.142% 0.020% 0.0269\n1\u00d7 1\u00d7 Yes 94.01% 0.128% 0.029% 0.0533\n2\u00d7 1\u00d7 Yes 94.65% 0.124% 0.022% 0.0433\n1.5\u00d7 1.5\u00d7 Yes 94.97% 0.116% 0.018% 0.0444\nTable 4: Statistical metrics for airbench94 trainings (n=10,000 runs each).\n5.3 Variance and class-wise calibration\nPrevious sections have focused on understanding what factors affect the first moment of accuracy\n(the mean). In this section we investigate the second moment, finding that TTA reduces variance at\nthe cost of calibration.\nOur experiment is to execute 10,000 runs of airbench94 training with several hyperparameter\nsettings. For each setting we report both the variance in test-set accuracy as well as an estimate of the\ndistribution-wise variance (Jordan, 2023). Figure 6 shows the raw accuracy distributions.\nTable 4 shows the results. Every case has at least 5\u00d7less distribution-wise variance than test-set\nvariance, replicating the main finding of Jordan (2023). This is a surprising result because these\ntrainings are at most 20 epochs, whereas the more standard training studied by Jordan (2023) had 5\u00d7\nas much distribution-wise variance when run for a similar duration, and reached a low variance only\nwhen run for 64 epochs. We conclude from this comparison that distribution-wise variance is more\nstrongly connected to the rate of convergence of a training rather than its duration as such. We also\nnote that the low distribution-wise variance of airbench94 indicates it has high training stability.\nUsing TTA significantly reduces the test-set variance, such that all three settings with TTA have lower\ntest-set variance than any setting without TTA. However, test-set variance is implied by the class-wise\ncalibration property (Jordan, 2023; Jiang et al., 2021), so contrapositively, we hypothesize that this\nreduction in test-set variance must come at the cost of class-wise calibration. To test this hypothesis,\nwe compute the class-aggregated calibration error (CACE) (Jiang et al., 2021) of each setting, which\nmeasures deviation from class-wise calibration. Table 4 shows the results. Every setting with TTA\nhas a higher CACE than every setting without TTA, confirming the hypothesis.\n6 Discussion\nIn this paper we introduced a new training method for CIFAR-10. It reaches 94% accuracy 1.9\u00d7\nfaster than the prior state-of-the-art, while being calibrated and highly stable. It is released as the\nairbench Python package.\nWe developed airbench solely with the goal of maximizing training speed on CIFAR-10. In\nSection B we find that it also generalizes well to other tasks. For example, without any extra tuning,\nairbench96 attains 1.7% better performance than standard ResNet-18 when training on CIFAR-100.\nOne factor contributing to the training speed of airbench was our finding that training can be accel-\nerated by partially derandomizing the standard random horizontal flipping augmentation, resulting in\nthe variant that we call alternating flip (Figure 1, Section 3.6). Replacing random flip with alternating\nflip improves the performance of every training we considered (Section 5.2), with the exception of\nthose trainings which do not benefit from horizontal flipping at all. We note that, surprisingly to\nus, the standard ImageNet trainings that we considered do not significantly benefit from horizontal\nflipping. Future work might investigate whether it is possible to obtain derandomized improvements\nto other augmentations besides horizontal flip.\nThe methods we introduced in this work improve the state-of-the-art for training speed on CIFAR-10,\nwith fixed performance and hardware constraints. These constraints mean that we cannot improve\nperformance by simply scaling up the amount of computational resources used; instead we are forced\nto develop new methods like the alternating flip. We look forward to seeing what other new methods\nfuture work discovers to push training speed further.\n9\nReferences\nDimitri Bertsekas. Convex optimization algorithms . Athena Scientific, 2015.\nDan C Cire\u00b8 san, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J\u00fcrgen Schmidhuber. High-\nperformance neural networks for visual object classification. arXiv preprint arXiv:1102.0183 ,\n2011.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\u00e9, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\nmark and competition. 2017.\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet\nor cifar-10. arXiv preprint arXiv:1810.03505 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\npp. 248\u2013255. IEEE, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552 , 2017.\nMert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic\ngradient descent. Mathematical Programming , 186:49\u201384, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 , 2016.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning , pp. 448\u2013456.\npmlr, 2015.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\nsgd via disagreement. arXiv preprint arXiv:2106.13799 , 2021.\nKeller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and\ninevitable. arXiv preprint arXiv:2304.01910 , 2023.\nAndrej Karpathy. Lessons learned from manually classifying cifar-10. https://karpathy.github.\nio/2011/04/27/manually-classifying-cifar10/ , April 2011. Accessed: 2024-03-15.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 and cifar-10 (canadian institute for\nadvanced research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html . MIT\nLicense.\nGuillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander\nM \u02db adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 12011\u201312020, 2023.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,\nBenjamin Charlier, Mathieu Dagr\u00e9ou, Tom Dupre la Tour, Ghislain Durif, Cassio F Dantas, et al.\nBenchopt: Reproducible, efficient and collaborative optimization benchmarks. Advances in Neural\nInformation Processing Systems , 35:25404\u201325421, 2022.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS workshop on deep\nlearning and unsupervised feature learning , volume 2011, pp. 7. Granada, Spain, 2011.\nDavid Page. How to train your resnet 8: Back of tricks, 2019. URL https://myrtle.ai/\nhow-to-train-your-resnet-8-bag-of-tricks/ .\n10\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics , 22(3):400\u2013407, 1951.\nSantiaga Serrano, Hadi Ansari, Vipul Gupta, and Dennis DeCoste. ml-cifar-10-faster. https:\n//github.com/apple/ml-cifar-10-faster , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1\u20139, 2014.\ntysam-code . Cifar10 hyperlightspeedbench. https://github.com/tysam-code/\nhlb-CIFAR10/commit/ad103b43d29f08b348b522ad89d38beba8955f7c , 2023.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. Advances in neural information processing systems , 32, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,\n2016.\nMichael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps\nforward, 1 step back. Advances in neural information processing systems , 32, 2019.\n11\nA Network architecture\n1from torch import nn\n2\n3class Flatten (nn. Module ):\n4 def forward (self , x):\n5 return x. view (x. size (0) , -1)\n6\n7class Mul (nn. Module ):\n8 def __init__ (self , scale ):\n9 super (). __init__ ()\n10 self . scale = scale\n11 def forward (self , x):\n12 return x * self . scale\n13\n14def conv (ch_in , ch_out ):\n15 return nn. Conv2d (ch_in , ch_out , kernel_size =3,\n16 padding =\u2019same \u2019, bias = False )\n17\n18def make_net ():\n19 act = lambda : nn. GELU ()\n20 bn = lambda ch: nn. BatchNorm2d (ch)\n21 return nn. Sequential (\n22 nn. Sequential (\n23 nn. Conv2d (3, 24, kernel_size =2, padding =0, bias = True ),\n24 act () ,\n25 ),\n26 nn. Sequential (\n27 conv (24 , 64) ,\n28 nn. MaxPool2d (2) ,\n29 bn (64) , act () ,\n30 conv (64 , 64) ,\n31 bn (64) , act () ,\n32 ),\n33 nn. Sequential (\n34 conv (64 , 256) ,\n35 nn. MaxPool2d (2) ,\n36 bn (256) , act () ,\n37 conv (256 , 256) ,\n38 bn (256) , act () ,\n39 ),\n40 nn. Sequential (\n41 conv (256 , 256) ,\n42 nn. MaxPool2d (2) ,\n43 bn (256) , act () ,\n44 conv (256 , 256) ,\n45 bn (256) , act () ,\n46 ),\n47 nn. MaxPool2d (3) ,\n48 Flatten () ,\n49 nn. Linear (256 , 10, bias = False ),\n50 Mul (1/9) ,\n51 )\nListing 3: Minimal PyTorch code for the network architecture used by airbench94 .\nWe note that there exist various tweaks to the architecture which reduce FLOP usage but not wallclock\ntime. For example, we can lower the FLOPs of airbench96 by almost 20% by reducing the kernel\nsize of the first convolution in each block from 3 to 2 and increasing epochs from 40 to 45. But this\ndoes not improve the wallclock training time on an A100. Reducing the batch size is another easy\nway to save FLOPs but not wallclock time.\n12\nDataset Flipping? Cutout? ResNet-18 airbench96\nCIFAR-10 Yes No 95.55% 95.61%\nCIFAR-10 Yes Yes 96.01% 96.05%\nCIFAR-100 Yes No 77.54% 79.27%\nCIFAR-100 Yes Yes 78.04% 79.76%\nCINIC-10 Yes No 87.58% 87.78%\nCINIC-10 Yes Yes not measured 88.22%\nSVHN No No 97.35% 97.38%\nSVHN No Yes not measured 97.64%\nTable 5: Comparison of airbench96 to standard ResNet-18 training across a variety of tasks. We\ndirectly apply airbench96 to each task without re-tuning any hyperparameters (besides turning off\nflipping for SVHN).\nB Extra dataset experiments\nWe developed airbench with the singular goal of maximizing training speed on CIFAR-10. To\nfind out whether this has resulted in it being \u201coverfit\u201d to CIFAR-10, in this section we evaluate its\nperformance on CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and CINIC-\n10 (Darlow et al., 2018).\nOn CIFAR-10, airbench96 attains comparable accuracy to a standard ResNet-18 training, in both\nthe case where both trainings use Cutout (DeVries & Taylor, 2017) and the case where both do not\n(Table 5). So, if we evaluate airbench96 on other tasks and find that it attains worse accuracy than\nResNet-18, then we can say that airbench96 must be overfit to CIFAR-10, otherwise we can say\nthat it generalizes.\nWe compare to the best accuracy numbers we can find in the literature for ResNet-18 on each task.\nWe do not tune the hyperparameters of airbench96 at all: we use the same values that were optimal\non CIFAR-10. Table 5 shows the result. It turns out that in every case, airbench96 attains better\nperformance than ResNet-18 training. Particularly impressive are results on CIFAR-100 where\nairbench96 attains 1.7% higher accuracy than ResNet-18 training, both in the case that Cutout is\nused and the case that it is not. We conclude that airbench is not overfit to CIFAR-10, since it shows\nstrong generalization to other tasks.\nWe note that this comparison between airbench96 and ResNet-18 training is fair in the sense that\nit does demonstrate that the former has good generalization, but unfair in the sense that it does not\nindicate that airbench96 is the superior training as such. In particular, airbench96 uses test-time\naugmentation whereas standard ResNet-18 training does not. It is likely that ResNet-18 training\nwould outperform airbench96 if it were run using test-time augmentation. However, it also takes\n5-10 times longer to complete. The decision of which to use may be situational.\nThe accuracy values we report for ResNet-18 training are from the following sources. We tried\nto select the highest values we could find for each setting. Moreau et al. (2022) reports attaining\n95.55% on CIFAR-10 without Cutout, and 97.35% on SVHN. DeVries & Taylor (2017) reports\nattaining 96.01% on CIFAR-10 with Cutout, 77.54% on CIFAR-100 without Cutout, and 78.04% on\nCIFAR-100 with Cutout. Darlow et al. (2018) report attaining 87.58% on CINIC-10 without Cutout.\nC ImageNet training details\nOur ImageNet trainings follow the 16 and 88-epoch configurations from https://github.com/\nlibffcv/ffcv-imagenet . In particular, we use a batch size of 1024 and learning rate 0.5 and\nmomentum 0.9, with a linear warmup and decay schedule for the learning rate. We train at resolution\n160 for the majority of training and then ramp up to resolution 192 for roughly the last 30% of\ntraining. We use label smoothing of 0.1. We use the FFCV (Leclerc et al., 2023) data loader.\n13\nHyperparameters Flipping augmentation option\nEpochs Cutout TTA None Random Alternating\n10 No No 92.3053 93.0988 93.2798\n20 No No 92.8166 93.7446 93.8652\n40 No No 93.0143 94.0133 94.0729\n80 No No 93.0612 94.1169 94.1628\n10 No Yes 93.4071 93.9488 94.0186\n20 No Yes 93.8528 94.5565 94.6530\n40 No Yes 94.0381 94.7803 94.8203\n80 No Yes 94.0638 94.8506 94.8676\n10 Yes No 91.8487 92.0402 92.1374\n20 Yes No 92.8474 93.3825 93.4876\n40 Yes No 93.2675 94.1014 94.1952\n80 Yes No 93.4193 94.4311 94.5204\n10 Yes Yes 92.6455 92.7780 92.8103\n20 Yes Yes 93.7862 94.1306 94.1670\n40 Yes Yes 94.3090 94.8511 94.8960\n80 Yes Yes 94.5253 95.1839 95.2362\nTable 6: Raw accuracy values for airbench94 flipping augmentation experiments. Each value is a\nmean over n= 400 runs. The 95% confidence intervals are roughly \u00b10.014, so that every row-wise\ndifference in means is statistically significant.\nD Extra tables & figures\nFigure 6: Accuracy distributions for the three airbench94 variations (with TTA) described in\nSection 5.3.\nE Complete training code\n1\"\"\"\n2airbench94 .py\n33.83 s runtime on an A100 ; 0.36 PFLOPs .\n4Evidence for validity : 94.01 average accuracy in n =1000 runs .\n5\n6We recorded the runtime of 3.83 seconds on an NVIDIA A100 -SXM4 -80 GB\nwith the following nvidia -smi:\n7NVIDIA - SMI 515.105.01 Driver Version : 515.105.01 CUDA Version :\n11.7\n14\n8torch . __version__ == \u20192.1.2+ cu118 \u2019\n9\"\"\"\n10\n11# ############################################\n12# Setup / Hyperparameters #\n13# ############################################\n14\n15import os\n16import sys\n17import uuid\n18from math import ceil\n19\n20import torch\n21from torch import nn\n22import torch .nn. functional as F\n23import torchvision\n24import torchvision . transforms as T\n25\n26torch . backends . cudnn . benchmark = True\n27\n28\"\"\"\n29We express the main training hyperparameters ( batch size , learning\nrate , momentum , and weight decay ) in decoupled form , so that each\none can be tuned independently . This accomplishes the following :\n30* Assuming time - constant gradients , the average step size is decoupled\nfrom everything but the lr.\n31* The size of the weight decay update is decoupled from everything but\nthe wd.\n32In constrast , normally when we increase the ( Nesterov ) momentum , this\nalso scales up the step size proportionally to 1 + 1 / (1 -\nmomentum ), meaning we cannot change momentum without having to re -\ntune the learning rate . Similarly , normally when we increase the\nlearning rate this also increases the size of the weight decay ,\nrequiring a proportional decrease in the wd to maintain the same\ndecay strength .\n33\n34The practical impact is that hyperparameter tuning is faster , since\nthis parametrization allows each one to be tuned independently .\nSee https :// myrtle .ai/ learn /how -to -train -your - resnet -5-\nhyperparameters /.\n35\"\"\"\n36\n37hyp = {\n38 \u2019opt \u2019: {\n39 \u2019 train_epochs \u2019: 9.9 ,\n40 \u2019batch_size \u2019: 1024 ,\n41 \u2019lr \u2019: 11.5 , # learning rate per 1024 examples\n42 \u2019momentum \u2019: 0.85 ,\n43 \u2019 weight_decay \u2019: 0.0153 , # weight decay per 1024 examples (\ndecoupled from learning rate )\n44 \u2019bias_scaler \u2019: 64.0 , # scales up learning rate (but not\nweight decay ) for BatchNorm biases\n45 \u2019 label_smoothing \u2019: 0.2 ,\n46 \u2019 whiten_bias_epochs \u2019: 3, # how many epochs to train the\nwhitening layer bias before freezing\n47 },\n48 \u2019aug \u2019: {\n49 \u2019flip \u2019: True ,\n50 \u2019translate \u2019: 2,\n51 },\n52 \u2019net \u2019: {\n53 \u2019widths \u2019: {\n54 \u2019block1 \u2019: 64,\n55 \u2019block2 \u2019: 256 ,\n56 \u2019block3 \u2019: 256 ,\n15\n57 },\n58 \u2019 batchnorm_momentum \u2019: 0.6 ,\n59 \u2019 scaling_factor \u2019: 1/9 ,\n60 \u2019tta_level \u2019: 2, # the level of test - time augmentation :\n0= none , 1= mirror , 2= mirror + translate\n61 },\n62}\n63\n64# ############################################\n65# DataLoader #\n66# ############################################\n67\n68CIFAR_MEAN = torch . tensor ((0.4914 , 0.4822 , 0.4465) )\n69CIFAR_STD = torch . tensor ((0.2470 , 0.2435 , 0.2616) )\n70\n71def batch_flip_lr ( inputs ):\n72 flip_mask = ( torch . rand (len ( inputs ), device = inputs . device ) < 0.5) .\nview (-1, 1, 1, 1)\n73 return torch . where ( flip_mask , inputs . flip ( -1) , inputs )\n74\n75def batch_crop (images , crop_size ):\n76 r = ( images . size ( -1) - crop_size )//2\n77 shifts = torch . randint (-r, r+1, size =( len( images ), 2) , device =\nimages . device )\n78 images_out = torch . empty (( len( images ), 3, crop_size , crop_size ),\ndevice = images .device , dtype = images . dtype )\n79 # The two cropping methods in this if - else produce equivalent\nresults , but the second is faster for r > 2.\n80 if r <= 2:\n81 for sy in range (-r, r +1):\n82 for sx in range (-r, r +1):\n83 mask = ( shifts [:, 0] == sy) & ( shifts [:, 1] == sx)\n84 images_out [ mask ] = images [mask , :, r+sy:r+sy+ crop_size\n, r+sx:r+sx+ crop_size ]\n85 else :\n86 images_tmp = torch . empty (( len( images ), 3, crop_size , crop_size\n+2* r), device = images .device , dtype = images . dtype )\n87 for s in range (-r, r +1):\n88 mask = ( shifts [:, 0] == s)\n89 images_tmp [ mask ] = images [mask , :, r+s:r+s+ crop_size , :]\n90 for s in range (-r, r +1):\n91 mask = ( shifts [:, 1] == s)\n92 images_out [ mask ] = images_tmp [mask , :, :, r+s:r+s+\ncrop_size ]\n93 return images_out\n94\n95class CifarLoader :\n96 \"\"\"\n97 GPU - accelerated dataloader for CIFAR -10 which implements\nalternating flip augmentation .\n98 \"\"\"\n99\n100 def __init__ (self , path , train =True , batch_size =500 , aug =None ,\ndrop_last =None , shuffle =None , gpu =0):\n101 data_path = os. path . join (path , \u2019train .pt \u2019 if train else \u2019test .\npt \u2019)\n102 if not os. path . exists ( data_path ):\n103 dset = torchvision . datasets . CIFAR10 (path , download =True ,\ntrain = train )\n104 images = torch . tensor ( dset . data )\n105 labels = torch . tensor ( dset . targets )\n106 torch . save ({ \u2019images \u2019: images , \u2019labels \u2019: labels , \u2019classes \u2019:\ndset . classes }, data_path )\n107\n108 data = torch . load ( data_path , map_location = torch . device (gpu))\n16\n109 self . images , self .labels , self . classes = data [\u2019images \u2019], data [\n\u2019labels \u2019], data [\u2019classes \u2019]\n110 # It \u2019s faster to load + process uint8 data than to load\npreprocessed fp16 data\n111 self . images = ( self . images . half () / 255) . permute (0, 3, 1, 2).\nto( memory_format = torch . channels_last )\n112\n113 self . normalize = T. Normalize ( CIFAR_MEAN , CIFAR_STD )\n114 self . proc_images = {} # Saved results of image processing to\nbe done on the first epoch\n115 self . epoch = 0\n116\n117 self . aug = aug or {}\n118 for k in self .aug . keys ():\n119 assert k in [\u2019flip \u2019, \u2019translate \u2019], \u2019 Unrecognized key: %s\u2019\n% k\n120\n121 self . batch_size = batch_size\n122 self . drop_last = train if drop_last is None else drop_last\n123 self . shuffle = train if shuffle is None else shuffle\n124\n125 def __len__ ( self ):\n126 return len( self . images )// self . batch_size if self . drop_last\nelse ceil (len( self . images )/ self . batch_size )\n127\n128 def __iter__ ( self ):\n129\n130 if self . epoch == 0:\n131 images = self . proc_images [\u2019norm \u2019] = self . normalize ( self .\nimages )\n132 # Randomly flip all images on the first epoch as according\nto definition of alternating flip\n133 if self .aug.get(\u2019flip \u2019, False ):\n134 images = self . proc_images [\u2019flip \u2019] = batch_flip_lr (\nimages )\n135 # Pre -pad images to save time when doing random\ntranslation\n136 pad = self .aug.get (\u2019translate \u2019, 0)\n137 if pad > 0:\n138 self . proc_images [\u2019pad \u2019] = F.pad(images , (pad ,) *4, \u2019\nreflect \u2019)\n139\n140 if self .aug.get(\u2019translate \u2019, 0) > 0:\n141 images = batch_crop ( self . proc_images [\u2019pad \u2019], self . images .\nshape [ -2])\n142 elif self .aug.get (\u2019flip \u2019, False ):\n143 images = self . proc_images [\u2019flip \u2019]\n144 else :\n145 images = self . proc_images [\u2019norm \u2019]\n146 if self .aug.get(\u2019flip \u2019, False ):\n147 if self . epoch % 2 == 1:\n148 images = images . flip ( -1)\n149\n150 self . epoch += 1\n151\n152 indices = ( torch . randperm if self . shuffle else torch . arange )(\nlen ( images ), device = images . device )\n153 for i in range (len ( self )):\n154 idxs = indices [i* self . batch_size :(i+1)* self . batch_size ]\n155 yield ( images [ idxs ], self . labels [ idxs ])\n156\n157# ############################################\n158# Network Components #\n159# ############################################\n160\n17\n161class Flatten (nn. Module ):\n162 def forward (self , x):\n163 return x. view (x. size (0) , -1)\n164\n165class Mul (nn. Module ):\n166 def __init__ (self , scale ):\n167 super (). __init__ ()\n168 self . scale = scale\n169 def forward (self , x):\n170 return x * self . scale\n171\n172class BatchNorm (nn. BatchNorm2d ):\n173 def __init__ (self , num_features , momentum , eps =1e -12 ,\n174 weight =False , bias = True ):\n175 super (). __init__ ( num_features , eps=eps , momentum =1- momentum )\n176 self . weight . requires_grad = weight\n177 self . bias . requires_grad = bias\n178 # Note that PyTorch already initializes the weights to one and\nbiases to zero\n179\n180class Conv (nn. Conv2d ):\n181 def __init__ (self , in_channels , out_channels , kernel_size =3,\npadding =\u2019same \u2019, bias = False ):\n182 super (). __init__ ( in_channels , out_channels , kernel_size =\nkernel_size , padding = padding , bias = bias )\n183\n184 def reset_parameters ( self ):\n185 super (). reset_parameters ()\n186 if self . bias is not None :\n187 self . bias . data . zero_ ()\n188 w = self . weight . data\n189 torch .nn. init . dirac_ (w[:w. size (1) ])\n190\n191class ConvGroup (nn. Module ):\n192 def __init__ (self , channels_in , channels_out , batchnorm_momentum ):\n193 super (). __init__ ()\n194 self . conv1 = Conv ( channels_in , channels_out )\n195 self . pool = nn. MaxPool2d (2)\n196 self . norm1 = BatchNorm ( channels_out , batchnorm_momentum )\n197 self . conv2 = Conv ( channels_out , channels_out )\n198 self . norm2 = BatchNorm ( channels_out , batchnorm_momentum )\n199 self . activ = nn. GELU ()\n200\n201 def forward (self , x):\n202 x = self . conv1 (x)\n203 x = self . pool (x)\n204 x = self . norm1 (x)\n205 x = self . activ (x)\n206 x = self . conv2 (x)\n207 x = self . norm2 (x)\n208 x = self . activ (x)\n209 return x\n210\n211# ############################################\n212# Network Definition #\n213# ############################################\n214\n215def make_net ( widths =hyp [\u2019net \u2019][ \u2019widths \u2019], batchnorm_momentum =hyp[\u2019net \u2019\n][ \u2019 batchnorm_momentum \u2019]):\n216 whiten_kernel_size = 2\n217 whiten_width = 2 * 3 * whiten_kernel_size **2\n218 net = nn. Sequential (\n219 Conv (3, whiten_width , whiten_kernel_size , padding =0, bias = True\n),\n220 nn. GELU () ,\n18\n221 ConvGroup ( whiten_width , widths [\u2019block1 \u2019],\nbatchnorm_momentum ),\n222 ConvGroup ( widths [\u2019block1 \u2019], widths [\u2019block2 \u2019],\nbatchnorm_momentum ),\n223 ConvGroup ( widths [\u2019block2 \u2019], widths [\u2019block3 \u2019],\nbatchnorm_momentum ),\n224 nn. MaxPool2d (3) ,\n225 Flatten () ,\n226 nn. Linear ( widths [\u2019block3 \u2019], 10, bias = False ),\n227 Mul (hyp[\u2019net \u2019][ \u2019 scaling_factor \u2019]) ,\n228 )\n229 net [0]. weight . requires_grad = False\n230 net = net. half (). cuda ()\n231 net = net.to( memory_format = torch . channels_last )\n232 for mod in net. modules ():\n233 if isinstance (mod , BatchNorm ):\n234 mod . float ()\n235 return net\n236\n237# ############################################\n238# Whitening Conv Initialization #\n239# ############################################\n240\n241def get_patches (x, patch_shape ):\n242 c, (h, w) = x. shape [1] , patch_shape\n243 return x. unfold (2,h ,1). unfold (3,w ,1) . transpose (1 ,3). reshape (-1,c,h\n,w). float ()\n244\n245def get_whitening_parameters ( patches ):\n246 n,c,h,w = patches . shape\n247 patches_flat = patches . view (n, -1)\n248 est_patch_covariance = ( patches_flat .T @ patches_flat ) / n\n249 eigenvalues , eigenvectors = torch . linalg . eigh ( est_patch_covariance\n, UPLO =\u2019U\u2019)\n250 return eigenvalues . flip (0). view (-1, 1, 1, 1) , eigenvectors .T.\nreshape (c*h*w,c,h,w). flip (0)\n251\n252def init_whitening_conv (layer , train_set , eps =5e -4):\n253 patches = get_patches ( train_set , patch_shape = layer . weight . data .\nshape [2:])\n254 eigenvalues , eigenvectors = get_whitening_parameters ( patches )\n255 eigenvectors_scaled = eigenvectors / torch . sqrt ( eigenvalues + eps)\n256 layer . weight . data [:] = torch . cat (( eigenvectors_scaled , -\neigenvectors_scaled ))\n257\n258# ###########################################\n259# Lookahead #\n260# ###########################################\n261\n262class LookaheadState :\n263 def __init__ (self , net):\n264 self . net_ema = {k: v. clone () for k, v in net . state_dict ().\nitems ()}\n265\n266 def update (self , net , decay ):\n267 for ema_param , net_param in zip( self . net_ema . values () , net.\nstate_dict (). values ()):\n268 if net_param . dtype in ( torch .half , torch . float ):\n269 ema_param . lerp_ ( net_param , 1- decay )\n270 net_param . copy_ ( ema_param )\n271\n272# ###########################################\n273# Logging #\n274# ###########################################\n275\n19\n276def print_columns ( columns_list , is_head =False , is_final_entry = False ):\n277 print_string = \u2019\u2019\n278 for col in columns_list :\n279 print_string += \u2019| %s \u2019 % col\n280 print_string += \u2019|\u2019\n281 if is_head :\n282 print (\u2019-\u2019*len( print_string ))\n283 print ( print_string )\n284 if is_head or is_final_entry :\n285 print (\u2019-\u2019*len( print_string ))\n286\n287logging_columns_list = [\u2019run \u2019, \u2019epoch \u2019, \u2019train_loss \u2019, \u2019train_acc \u2019,\n\u2019val_acc \u2019, \u2019 tta_val_acc \u2019, \u2019 total_time_seconds \u2019]\n288def print_training_details ( variables , is_final_entry ):\n289 formatted = []\n290 for col in logging_columns_list :\n291 var = variables .get (col. strip () , None )\n292 if type (var) in (int , str):\n293 res = str(var)\n294 elif type (var) is float :\n295 res = \u2019{:0.4 f}\u2019. format ( var)\n296 else :\n297 assert var is None\n298 res = \u2019\u2019\n299 formatted . append (res. rjust (len (col)))\n300 print_columns ( formatted , is_final_entry = is_final_entry )\n301\n302# ###########################################\n303# Evaluation #\n304# ###########################################\n305\n306def infer (model , loader , tta_level =0) :\n307 \"\"\"\n308 Test - time augmentation strategy (for tta_level =2):\n309 1. Flip / mirror the image left -to - right (50% of the time ).\n310 2. Translate the image by one pixel either up -and - left or down -and\n- right (50% of the time , i.e. both happen 25% of the time ).\n311\n312 This creates 6 views per image ( left / right times the two\ntranslations and no - translation ), which we evaluate and then\nweight according to the given probabilities .\n313 \"\"\"\n314\n315 def infer_basic (inputs , net):\n316 return net( inputs ). clone ()\n317\n318 def infer_mirror (inputs , net):\n319 return 0.5 * net( inputs ) + 0.5 * net( inputs . flip ( -1))\n320\n321 def infer_mirror_translate ( inputs , net ):\n322 logits = infer_mirror (inputs , net )\n323 pad = 1\n324 padded_inputs = F.pad(inputs , (pad ,) *4, \u2019reflect \u2019)\n325 inputs_translate_list = [\n326 padded_inputs [:, :, 0:32 , 0:32] ,\n327 padded_inputs [:, :, 2:34 , 2:34] ,\n328 ]\n329 logits_translate_list = [ infer_mirror ( inputs_translate , net)\n330 for inputs_translate in\ninputs_translate_list ]\n331 logits_translate = torch . stack ( logits_translate_list ). mean (0)\n332 return 0.5 * logits + 0.5 * logits_translate\n333\n334 model . eval ()\n335 test_images = loader . normalize ( loader . images )\n20\n336 infer_fn = [ infer_basic , infer_mirror , infer_mirror_translate ][\ntta_level ]\n337 with torch . no_grad ():\n338 return torch .cat ([ infer_fn (inputs , model ) for inputs in\ntest_images . split (2000) ])\n339\n340def evaluate (model , loader , tta_level =0) :\n341 logits = infer (model , loader , tta_level )\n342 return ( logits . argmax (1) == loader . labels ). float (). mean (). item ()\n343\n344# ###########################################\n345# Training #\n346# ###########################################\n347\n348def main (run):\n349\n350 batch_size = hyp[\u2019opt \u2019][ \u2019batch_size \u2019]\n351 epochs = hyp[\u2019opt \u2019][ \u2019 train_epochs \u2019]\n352 momentum = hyp[\u2019opt \u2019][ \u2019momentum \u2019]\n353 # Assuming gradients are constant in time , for Nesterov momentum ,\nthe below ratio is how much larger the default steps will be than\nthe underlying per - example gradients . We divide the learning rate\nby this ratio in order to ensure steps are the same scale as\ngradients , regardless of the choice of momentum .\n354 kilostep_scale = 1024 * (1 + 1 / (1 - momentum ))\n355 lr = hyp[\u2019opt \u2019][ \u2019lr \u2019] / kilostep_scale # un - decoupled learning\nrate for PyTorch SGD\n356 wd = hyp[\u2019opt \u2019][ \u2019 weight_decay \u2019] * batch_size / kilostep_scale\n357 lr_biases = lr * hyp [\u2019opt \u2019][ \u2019bias_scaler \u2019]\n358\n359 loss_fn = nn. CrossEntropyLoss ( label_smoothing =hyp[\u2019opt \u2019][ \u2019\nlabel_smoothing \u2019], reduction =\u2019none \u2019)\n360 test_loader = CifarLoader (\u2019cifar10 \u2019, train =False , batch_size =2000)\n361 train_loader = CifarLoader (\u2019cifar10 \u2019, train =True , batch_size =\nbatch_size , aug=hyp[\u2019aug \u2019])\n362 if run == \u2019warmup \u2019:\n363 # The only purpose of the first run is to warmup , so we can\nuse dummy data\n364 train_loader . labels = torch . randint (0, 10, size =( len (\ntrain_loader . labels ) ,), device = train_loader . labels . device )\n365 total_train_steps = ceil (len( train_loader ) * epochs )\n366\n367 model = make_net ()\n368 current_steps = 0\n369\n370 norm_biases = [p for k, p in model . named_parameters () if \u2019norm \u2019 in\nk and p. requires_grad ]\n371 other_params = [p for k, p in model . named_parameters () if \u2019norm \u2019\nnot in k and p. requires_grad ]\n372 param_configs = [ dict ( params = norm_biases , lr= lr_biases ,\nweight_decay =wd/ lr_biases ),\n373 dict ( params = other_params , lr=lr , weight_decay =wd/\nlr)]\n374 optimizer = torch . optim .SGD ( param_configs , momentum = momentum ,\nnesterov = True )\n375\n376 def triangle (steps , start =0, end =0, peak =0.5) :\n377 xp = torch . tensor ([0 , int( peak * steps ), steps ])\n378 fp = torch . tensor ([ start , 1, end ])\n379 x = torch . arange (1+ steps )\n380 m = (fp [1:] - fp [: -1]) / (xp [1:] - xp [: -1])\n381 b = fp [: -1] - (m * xp [: -1])\n382 indices = torch .sum( torch .ge(x[:, None ], xp[None , :]) , 1) - 1\n383 indices = torch . clamp ( indices , 0, len(m) - 1)\n384 return m[ indices ] * x + b[ indices ]\n21\n385 lr_schedule = triangle ( total_train_steps , start =0.2 , end =0.07 ,\npeak =0.23)\n386 scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lambda i:\nlr_schedule [i])\n387\n388 alpha_schedule = 0.95**5 * ( torch . arange ( total_train_steps +1) /\ntotal_train_steps )**3\n389 lookahead_state = LookaheadState ( model )\n390\n391 # For accurately timing GPU code\n392 starter = torch . cuda . Event ( enable_timing = True )\n393 ender = torch . cuda . Event ( enable_timing = True )\n394 total_time_seconds = 0.0\n395\n396 # Initialize the first layer using statistics of training images\n397 starter . record ()\n398 train_images = train_loader . normalize ( train_loader . images [:5000])\n399 init_whitening_conv ( model [0] , train_images )\n400 ender . record ()\n401 torch . cuda . synchronize ()\n402 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n403\n404 for epoch in range ( ceil ( epochs )):\n405\n406 model [0]. bias . requires_grad = ( epoch < hyp [\u2019opt \u2019][ \u2019\nwhiten_bias_epochs \u2019])\n407\n408 # ###################\n409 # Training #\n410 # ###################\n411\n412 starter . record ()\n413\n414 model . train ()\n415 for inputs , labels in train_loader :\n416\n417 outputs = model ( inputs )\n418 loss = loss_fn ( outputs , labels ).sum ()\n419 optimizer . zero_grad ( set_to_none = True )\n420 loss . backward ()\n421 optimizer . step ()\n422 scheduler . step ()\n423\n424 current_steps += 1\n425\n426 if current_steps % 5 == 0:\n427 lookahead_state . update (model , decay = alpha_schedule [\ncurrent_steps ]. item ())\n428\n429 if current_steps >= total_train_steps :\n430 if lookahead_state is not None :\n431 lookahead_state . update (model , decay =1.0)\n432 break\n433\n434 ender . record ()\n435 torch . cuda . synchronize ()\n436 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n437\n438 # ###################\n439 # Evaluation #\n440 # ###################\n441\n442 # Print the accuracy and loss from the last training batch of\nthe epoch\n22\n443 train_acc = ( outputs . detach (). argmax (1) == labels ). float ().\nmean (). item ()\n444 train_loss = loss . item () / batch_size\n445 val_acc = evaluate (model , test_loader , tta_level =0)\n446 print_training_details ( locals () , is_final_entry = False )\n447 run = None # Only print the run number once\n448\n449 # ###################\n450 # TTA Evaluation #\n451 # ###################\n452\n453 starter . record ()\n454 tta_val_acc = evaluate (model , test_loader , tta_level = hyp[\u2019net \u2019][ \u2019\ntta_level \u2019])\n455 ender . record ()\n456 torch . cuda . synchronize ()\n457 total_time_seconds += 1e -3 * starter . elapsed_time ( ender )\n458\n459 epoch = \u2019eval \u2019\n460 print_training_details ( locals () , is_final_entry = True )\n461\n462 return tta_val_acc\n463\n464if __name__ == \" __main__ \":\n465 with open (sys. argv [0]) as f:\n466 code = f. read ()\n467\n468 print_columns ( logging_columns_list , is_head = True )\n469 main (\u2019warmup \u2019)\n470 accs = torch . tensor ([ main (run) for run in range (25) ])\n471 print (\u2019Mean : %.4f Std: %.4f\u2019 % ( accs . mean () , accs .std ()))\n472\n473 log = {\u2019code \u2019: code , \u2019accs \u2019: accs }\n474 log_dir = os. path . join (\u2019logs \u2019, str ( uuid . uuid4 ()))\n475 os. makedirs ( log_dir , exist_ok = True )\n476 log_path = os. path . join ( log_dir , \u2019log.pt \u2019)\n477 print (os. path . abspath ( log_path ))\n478 torch . save (log , os. path . join ( log_dir , \u2019log.pt \u2019))\nListing 4: airbench94.py\n23\nPast Error in Task Candidates: ValueError('json output should start and end with { and }')\nTask Candidates:"}, {"Response": "{\n  \"tasks\": [\n    {\n      \"name\": \"Fast CIFAR-10 Training\",\n      \"description\": \"Develop a method to train a neural network to 94% accuracy on CIFAR-10 in under 4 seconds on a single GPU.\",\n      \"relevant_paper_text\": \"We introduce training methods for CIFAR-10 which reach 94% accuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when run on a single NVIDIA A100 GPU.\"\n    },\n    {\n      \"name\": \"Derandomized Data Augmentation\",\n      \"description\": \"Develop a derandomized variant of horizontal flipping augmentation that improves training speed and performance.\",\n      \"relevant_paper_text\": \"As one factor contributing to these training speeds, we propose a derandomized variant of horizontal flipping augmentation, which we show improves over the standard method in every case where flipping is beneficial over no flipping at all.\"\n    },\n    {\n      \"name\": \"Efficient Network Architecture Design\",\n      \"description\": \"Design an efficient convolutional neural network architecture optimized for fast training on CIFAR-10.\",\n      \"relevant_paper_text\": \"We train a convolutional network with a total of 1.97 million parameters, following tysam-code (2023) with a few small changes. It contains seven convolutions with the latter six being divided into three blocks of two.\"\n    },\n    {\n      \"name\": \"Hyperparameter Optimization\",\n      \"description\": \"Optimize hyperparameters like learning rate, momentum, and weight decay for fast and stable training.\",\n      \"relevant_paper_text\": \"We express the main training hyperparameters (batch size, learning rate, momentum, and weight decay) in decoupled form, so that each one can be tuned independently.\"\n    },\n    {\n      \"name\": \"Test-Time Augmentation\",\n      \"description\": \"Implement and evaluate test-time augmentation techniques to improve model performance.\",\n      \"relevant_paper_text\": \"Without any TTA our three training methods attain 93.2%, 94.4%, and 95.6% mean accuracy respectively.\"\n    }\n  ]\n}"}, {"Request": "Given the fields `task`, produce the fields `scorable`.\n\n---\n\nFollow the following format.\n\nTask: ${task}\nReasoning: Let's think step by step in order to ${produce the scorable}. We ...\nScorable[True/False]: Will it be possible to write a function to objectively score an implementation of this task, verifying that key functionality is implemented?\n\n---\n\nTask: Task: Fast CIFAR-10 Training Description: Develop a method to train a neural network to 94% accuracy on CIFAR-10 in under 4 seconds on a single GPU. Relevant Paper Text: We introduce training methods for CIFAR-10 which reach 94% accuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when run on a single NVIDIA A100 GPU.\nReasoning: Let's think step by step in order to"}, {"Response": "Reasoning: Let's think step by step in order to determine if this task is scorable:\n\n1. The task has a clear, measurable goal: 94% accuracy on CIFAR-10.\n2. There's a specific time constraint: under 4 seconds.\n3. The hardware is specified: a single GPU.\n4. CIFAR-10 is a standardized dataset, ensuring consistency in evaluation.\n5. Accuracy is an objective metric that can be calculated programmatically.\n6. The time taken for training can be measured precisely using system clocks.\n7. The process can be repeated and verified independently.\n8. There are no subjective elements to the task or its evaluation.\n9. The relevant paper provides benchmark times, which can be used for comparison.\n\nGiven these factors, it would be possible to create an objective scoring function that:\na) Measures the training time\nb) Calculates the accuracy on the CIFAR-10 test set\nc) Verifies if both the time and accuracy meet the specified criteria\n\nScorable[True/False]: True"}]