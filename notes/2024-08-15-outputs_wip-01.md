title:
94% on CIFAR-10 in 3.29 Seconds on a Single GPU


abstract_plus:
Title: 94% on CIFAR-10 in 3.29 Seconds on a Single GPU

Abstract Plus: This paper introduces fast training methods for the CIFAR-10 dataset, achieving 94% accuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds on a single NVIDIA A100 GPU. The authors propose a derandomized variant of horizontal flipping augmentation called "alternating flip," which improves performance over standard random flipping in all cases where flipping is beneficial. The methods build upon prior work, incorporating optimizations such as a modified network architecture, initialization techniques, and frozen patch-whitening layers. The paper presents a comprehensive analysis of the training process, including ablation studies and experiments on other datasets to demonstrate generalization.

Additional high-level summaries:
1. The paper provides a detailed breakdown of the training method, including hyperparameter choices, network architecture, and optimization techniques.
2. The authors introduce a decoupled form of expressing hyperparameters, allowing for more efficient tuning.
3. The research demonstrates that the proposed methods not only achieve fast training times on CIFAR-10 but also generalize well to other datasets like CIFAR-100, SVHN, and CINIC-10.
4. The paper includes an analysis of the statistical properties of the trained models, including variance and class-wise calibration.
5. The authors provide open-source implementations of their methods, making them easily accessible for researchers and practitioners.




====================


quantitative_results_json:
[
  {
    "units": "seconds",
    "value": 3.29,
    "description": "Time to reach 94% accuracy on CIFAR-10",
    "method": "airbench94_compiled",
    "notes": "Run on a single NVIDIA A100 GPU",
    "comparison": {"baseline": 6.3, "improvement": "1.9x faster"}
  },
  {
    "units": "seconds",
    "value": 10.4,
    "description": "Time to reach 95% accuracy on CIFAR-10",
    "method": "airbench95",
    "notes": "Run on a single NVIDIA A100 GPU"
  },
  {
    "units": "seconds",
    "value": 46.3,
    "description": "Time to reach 96% accuracy on CIFAR-10",
    "method": "airbench96",
    "notes": "Run on a single NVIDIA A100 GPU"
  },
  {
    "units": "FLOPs",
    "value": 3.6e14,
    "description": "Computational cost for 94% accuracy",
    "method": "airbench94_compiled"
  },
  {
    "units": "FLOPs",
    "value": 1.4e15,
    "description": "Computational cost for 95% accuracy",
    "method": "airbench95"
  },
  {
    "units": "FLOPs",
    "value": 7.2e15,
    "description": "Computational cost for 96% accuracy",
    "method": "airbench96"
  },
  {
    "units": "accuracy",
    "value": 0.9401,
    "description": "Mean accuracy on CIFAR-10",
    "method": "airbench94 with alternating flip",
    "comparison": {"baseline": 0.9392, "improvement": "+0.09%"}
  },
  {
    "units": "accuracy",
    "value": 0.7976,
    "description": "Accuracy on CIFAR-100",
    "method": "airbench96",
    "comparison": {"baseline": 0.7804, "improvement": "+1.72%"}
  },
  {
    "units": "accuracy",
    "value": 0.8822,
    "description": "Accuracy on CINIC-10",
    "method": "airbench96",
    "comparison": {"baseline": 0.8758, "improvement": "+0.64%"}
  },
  {
    "units": "accuracy",
    "value": 0.9764,
    "description": "Accuracy on SVHN",
    "method": "airbench96",
    "comparison": {"baseline": 0.9735, "improvement": "+0.29%"}
  },
  {
    "units": "test_set_stddev",
    "value": 0.00128,
    "description": "Standard deviation of test set accuracy",
    "method": "airbench94 with TTA",
    "notes": "Based on 10,000 runs"
  },
  {
    "units": "CACE",
    "value": 0.0533,
    "description": "Class-aggregated calibration error",
    "method": "airbench94 with TTA",
    "notes": "Higher CACE indicates lower class-wise calibration"
  }
]




====================


core_ideas_json:
{
  "fast_cifar10_training": "Develop methods for extremely fast training on CIFAR-10, reaching high accuracy in seconds on a single GPU",
  "alternating_flip": "A novel derandomized variant of horizontal flipping augmentation that improves over standard random flipping",
  "optimized_training_pipeline": "Combine and optimize various techniques including network architecture, initialization, and optimization tricks to achieve fast training",
  "generalization_to_other_datasets": "Demonstrate that the fast training methods generalize well to other datasets like CIFAR-100, SVHN, and CINIC-10",
  "decoupled_hyperparameter_tuning": "Introduce a decoupled form of expressing hyperparameters for more efficient tuning",
  "comprehensive_analysis": "Provide detailed ablation studies and analysis of statistical properties of the trained models, including variance and calibration"
}




====================


metrics_json:
[
  {
    "name": "Training Time to 94% Accuracy",
    "description": "Time required to train a model to reach 94% accuracy on CIFAR-10",
    "unit": "seconds",
    "measurement_details": "Measured on a single NVIDIA A100 GPU using airbench94_compiled method"
  },
  {
    "name": "Training Time to 95% Accuracy",
    "description": "Time required to train a model to reach 95% accuracy on CIFAR-10",
    "unit": "seconds",
    "measurement_details": "Measured on a single NVIDIA A100 GPU using airbench95 method"
  },
  {
    "name": "Training Time to 96% Accuracy",
    "description": "Time required to train a model to reach 96% accuracy on CIFAR-10",
    "unit": "seconds",
    "measurement_details": "Measured on a single NVIDIA A100 GPU using airbench96 method"
  },
  {
    "name": "Computational Cost for 94% Accuracy",
    "description": "Number of floating point operations required to train a model to 94% accuracy",
    "unit": "FLOPs",
    "measurement_details": "Calculated for airbench94_compiled method"
  },
  {
    "name": "Computational Cost for 95% Accuracy",
    "description": "Number of floating point operations required to train a model to 95% accuracy",
    "unit": "FLOPs",
    "measurement_details": "Calculated for airbench95 method"
  },
  {
    "name": "Computational Cost for 96% Accuracy",
    "description": "Number of floating point operations required to train a model to 96% accuracy",
    "unit": "FLOPs",
    "measurement_details": "Calculated for airbench96 method"
  },
  {
    "name": "CIFAR-10 Accuracy",
    "description": "Classification accuracy on the CIFAR-10 dataset",
    "unit": "percentage",
    "measurement_details": "Measured using airbench94 with alternating flip"
  },
  {
    "name": "CIFAR-100 Accuracy",
    "description": "Classification accuracy on the CIFAR-100 dataset",
    "unit": "percentage",
    "measurement_details": "Measured using airbench96"
  },
  {
    "name": "CINIC-10 Accuracy",
    "description": "Classification accuracy on the CINIC-10 dataset",
    "unit": "percentage",
    "measurement_details": "Measured using airbench96"
  },
  {
    "name": "SVHN Accuracy",
    "description": "Classification accuracy on the SVHN dataset",
    "unit": "percentage",
    "measurement_details": "Measured using airbench96"
  },
  {
    "name": "Test Set Standard Deviation",
    "description": "Standard deviation of test set accuracy across multiple runs",
    "unit": "percentage",
    "measurement_details": "Calculated for airbench94 with TTA over 10,000 runs"
  },
  {
    "name": "Class-Aggregated Calibration Error",
    "description": "Measure of deviation from class-wise calibration",
    "unit": "CACE",
    "measurement_details": "Calculated for airbench94 with TTA"
  }
]




====================


baseline_methods_json:
{
  "tysam_code_2023": {
    "description": "The prior state-of-the-art method for fast CIFAR-10 training",
    "key_components": [
      "Modified network architecture",
      "Specialized initialization technique",
      "Optimized SGD optimizer",
      "Frozen patch-whitening layer",
      "Random horizontal flipping augmentation"
    ],
    "architecture_details": {
      "first_layer": "2x2 convolution with no padding",
      "main_body": "VGG-like structure with 3x3 convolutions and 2x2 max-pooling",
      "activation": "GELU",
      "normalization": "BatchNorm with disabled affine scale parameters",
      "final_layer_scaling": "1/9"
    },
    "training_details": {
      "optimizer": "Nesterov SGD",
      "batch_size": 1024,
      "label_smoothing": 0.2,
      "learning_rate": "Triangular schedule",
      "data_augmentation": "Random horizontal flipping and 2-pixel random translation"
    },
    "performance": {
      "accuracy": "94%",
      "training_time": "6.3 A100-seconds"
    }
  },
  "page_2019": {
    "description": "An earlier fast CIFAR-10 training method that introduced several optimization techniques",
    "key_components": [
      "Frozen patch-whitening layer",
      "Optimization tricks for BatchNorm layers"
    ],
    "training_details": {
      "batch_norm_bias_learning_rate": "64x higher than standard learning rate"
    },
    "performance": {
      "training_time": "26 V100-seconds (â‰ˆ10.4 A100-seconds)"
    }
  },
  "standard_random_flipping": {
    "description": "The conventional data augmentation technique for image classification tasks",
    "key_components": [
      "Random horizontal flipping of input images"
    ],
    "implementation_details": {
      "flip_probability": 0.5,
      "application": "Applied independently to each image in each epoch"
    }
  }
}

This JSON object captures the key baseline methods mentioned in the paper, including their main components, architectural details, and training specifics where available. It provides a comprehensive overview of the approaches that the authors' work builds upon and improves.




====================


experimental_methods_json:
[
  {
    "name": "airbench94",
    "baseline": "tysam_code_2023",
    "modifications": [
      "Derandomized horizontal flipping (alternating flip)",
      "Optimized network architecture",
      "Decoupled hyperparameter tuning",
      "Multi-crop evaluation"
    ],
    "expected_improvements": [
      "1.9x faster training time to 94% accuracy",
      "Reduced computational cost"
    ],
    "target_metrics": ["Training time to 94% accuracy", "FLOPs"]
  },
  {
    "name": "airbench95",
    "baseline": "airbench94",
    "modifications": [
      "Increased training epochs from 9.9 to 15",
      "Scaled output channel count in network blocks",
      "Reduced learning rate"
    ],
    "expected_improvements": [
      "Achieve 95% accuracy with minimal increase in training time"
    ],
    "target_metrics": ["Training time to 95% accuracy", "FLOPs"]
  },
  {
    "name": "airbench96",
    "baseline": "airbench95",
    "modifications": [
      "Added 12-pixel Cutout augmentation",
      "Increased training epochs to 40",
      "Added third convolution to each block",
      "Scaled channel counts in network blocks",
      "Added residual connections"
    ],
    "expected_improvements": [
      "Achieve 96% accuracy while maintaining relatively fast training time"
    ],
    "target_metrics": ["Training time to 96% accuracy", "FLOPs"]
  },
  {
    "name": "Alternating Flip",
    "baseline": "standard_random_flipping",
    "modifications": [
      "Deterministic flipping pattern after first epoch",
      "Ensures all unique flipped versions are seen in consecutive epochs"
    ],
    "expected_improvements": [
      "Reduced redundancy in augmented data",
      "Improved training speed and accuracy"
    ],
    "target_metrics": ["Training time", "Accuracy"]
  },
  {
    "name": "Decoupled Hyperparameter Tuning",
    "baseline": "Standard hyperparameter tuning",
    "modifications": [
      "Express main training hyperparameters in decoupled form",
      "Allow independent tuning of batch size, learning rate, momentum, and weight decay"
    ],
    "expected_improvements": [
      "More efficient hyperparameter optimization",
      "Faster convergence to optimal training configuration"
    ],
    "target_metrics": ["Hyperparameter tuning efficiency", "Training time"]
  }
]

This JSON structure captures the main experimental methods and techniques introduced in the paper, along with their key components, expected improvements, and target metrics.




====================


method_metric_results:
{
  "airbench94_compiled": {
    "time_to_94_percent": 3.29,
    "FLOPs": 3.6e14
  },
  "airbench94": {
    "time_to_94_percent": 3.83,
    "FLOPs": 3.6e14,
    "mean_accuracy": 94.01
  },
  "airbench95": {
    "time_to_95_percent": 10.4,
    "FLOPs": 1.4e15
  },
  "airbench96": {
    "time_to_96_percent": 46.3,
    "FLOPs": 7.2e15,
    "CIFAR100_accuracy": 79.76,
    "CINIC10_accuracy": 88.22,
    "SVHN_accuracy": 97.64
  },
  "alternating_flip": {
    "mean_accuracy": 94.01
  },
  "standard_random_flip": {
    "mean_accuracy": 93.92
  }
}

This JSON structure captures the key metrics reported for each method in the paper, allowing for easy comparison and analysis.




====================


hw_agnostic_metrics_json:
[
  {
    "name": "Epochs to 94% accuracy",
    "description": "Number of training epochs required to reach 94% accuracy on CIFAR-10",
    "corresponding_hw_metric": "Training time to 94% accuracy",
    "unit": "epochs"
  },
  {
    "name": "FLOPs for 94% accuracy",
    "description": "Number of floating point operations required to train a model to 94% accuracy",
    "corresponding_hw_metric": "Computational cost for 94% accuracy",
    "unit": "FLOPs"
  },
  {
    "name": "Epochs to 95% accuracy",
    "description": "Number of training epochs required to reach 95% accuracy on CIFAR-10",
    "corresponding_hw_metric": "Training time to 95% accuracy",
    "unit": "epochs"
  },
  {
    "name": "FLOPs for 95% accuracy",
    "description": "Number of floating point operations required to train a model to 95% accuracy",
    "corresponding_hw_metric": "Computational cost for 95% accuracy",
    "unit": "FLOPs"
  },
  {
    "name": "Epochs to 96% accuracy",
    "description": "Number of training epochs required to reach 96% accuracy on CIFAR-10",
    "corresponding_hw_metric": "Training time to 96% accuracy",
    "unit": "epochs"
  },
  {
    "name": "FLOPs for 96% accuracy",
    "description": "Number of floating point operations required to train a model to 96% accuracy",
    "corresponding_hw_metric": "Computational cost for 96% accuracy",
    "unit": "FLOPs"
  },
  {
    "name": "Model size",
    "description": "Total number of trainable parameters in the model",
    "corresponding_hw_metric": "None",
    "unit": "parameters"
  },
  {
    "name": "CIFAR-10 Accuracy",
    "description": "Classification accuracy on the CIFAR-10 dataset",
    "corresponding_hw_metric": "None",
    "unit": "percentage"
  },
  {
    "name": "Test Set Standard Deviation",
    "description": "Standard deviation of test set accuracy across multiple runs",
    "corresponding_hw_metric": "None",
    "unit": "percentage"
  },
  {
    "name": "Class-Aggregated Calibration Error",
    "description": "Measure of deviation from class-wise calibration",
    "corresponding_hw_metric": "None",
    "unit": "CACE"
  }
]

This list of hardware-agnostic metrics captures the essential performance measures from the paper without relying on specific hardware characteristics. It includes both the computational aspects (epochs, FLOPs) and the model performance aspects (accuracy, calibration) that are key to evaluating the methods presented in the paper.




====================


tasks_json:
[
  {
    "name": "implement_alternating_flip",
    "inputs": [
      {
        "name": "inputs",
        "type": "torch.Tensor",
        "shape": "(batch_size, channels, height, width)"
      },
      {
        "name": "indices",
        "type": "torch.Tensor",
        "shape": "(batch_size,)"
      },
      {
        "name": "epoch",
        "type": "int"
      }
    ],
    "outputs": [
      {
        "name": "flipped_inputs",
        "type": "torch.Tensor",
        "shape": "(batch_size, channels, height, width)"
      }
    ],
    "description": "Implement the alternating flip data augmentation technique as described in the paper",
    "constraints": [
      "Use a deterministic flipping pattern based on the epoch number",
      "Ensure all unique flipped versions are seen in consecutive epochs",
      "Use a hash function to determine which images to flip in each epoch",
      "The function should be compatible with PyTorch's DataLoader"
    ]
  },
  {
    "name": "init_whitening_conv",
    "inputs": [
      {
        "name": "layer",
        "type": "torch.nn.Conv2d"
      },
      {
        "name": "train_set",
        "type": "torch.Tensor",
        "shape": "(num_samples, channels, height, width)"
      },
      {
        "name": "eps",
        "type": "float",
        "default": 5e-4
      }
    ],
    "outputs": [
      {
        "name": "initialized_layer",
        "type": "torch.nn.Conv2d"
      }
    ],
    "description": "Initialize the first convolutional layer as a patch-whitening transformation",
    "constraints": [
      "Compute the covariance matrix of 2x2 patches across the training distribution",
      "Initialize the first half of the filters as eigenvectors of the covariance matrix",
      "Initialize the second half as the negation of the first half",
      "Ensure the output has identity covariance matrix"
    ]
  },
  {
    "name": "decoupled_hyperparameters",
    "inputs": [
      {
        "name": "batch_size",
        "type": "int"
      },
      {
        "name": "momentum",
        "type": "float"
      },
      {
        "name": "learning_rate",
        "type": "float"
      },
      {
        "name": "weight_decay",
        "type": "float"
      }
    ],
    "outputs": [
      {
        "name": "decoupled_params",
        "type": "dict"
      }
    ],
    "description": "Implement the decoupled form of expressing hyperparameters as described in the paper",
    "constraints": [
      "Decouple the learning rate from momentum effects",
      "Decouple the weight decay from the learning rate",
      "Return a dictionary with the decoupled hyperparameters ready for use in PyTorch optimizers"
    ]
  }
]

This tasks_json provides three implementation tasks that capture key aspects of the paper:
1. The alternating flip augmentation technique
2. The whitening initialization for the first convolutional layer
3. The decoupled hyperparameter expression

Each task includes the necessary inputs, outputs, a description, and specific constraints to guide the implementation.




====================


task_eval_planning_json:
[
  {
    "task_name": "implement_alternating_flip",
    "provided_inputs": {
      "sample_data": "A batch of CIFAR-10 images (torch.Tensor of shape (1024, 3, 32, 32))",
      "sample_indices": "Indices for the batch (torch.Tensor of shape (1024,))",
      "epochs": "A list of epoch numbers to test (e.g., [0, 1, 2, 3, 4])"
    },
    "expected_outputs": {
      "flipped_images": "Flipped versions of the input images for each epoch"
    },
    "evaluation_criteria": [
      "Correctness: Verify that the flipping pattern is deterministic and consistent across epochs",
      "Correctness: Check that all unique flipped versions are seen in consecutive epochs",
      "Performance: Measure the execution time compared to standard random flipping",
      "Quality: Compare the impact on model accuracy when used in a full training loop"
    ],
    "additional_tests": [
      "Test with different batch sizes",
      "Verify behavior at epoch boundaries"
    ]
  },
  {
    "task_name": "init_whitening_conv",
    "provided_inputs": {
      "sample_layer": "A torch.nn.Conv2d layer with kernel_size=2 and out_channels=24",
      "sample_train_set": "A subset of the CIFAR-10 training data (e.g., 5000 images)"
    },
    "expected_outputs": {
      "initialized_layer": "The Conv2d layer with weights initialized as a patch-whitening transformation"
    },
    "evaluation_criteria": [
      "Correctness: Verify that the output of the layer has approximately identity covariance matrix",
      "Correctness: Check that the second half of the filters are the negation of the first half",
      "Performance: Measure the initialization time",
      "Quality: Compare the impact on early training loss and accuracy compared to standard initialization"
    ],
    "additional_tests": [
      "Test with different epsilon values",
      "Verify behavior with different input distributions"
    ]
  },
  {
    "task_name": "decoupled_hyperparameters",
    "provided_inputs": {
      "sample_hyperparameters": {
        "batch_size": 1024,
        "momentum": 0.85,
        "learning_rate": 11.5,
        "weight_decay": 0.0153
      }
    },
    "expected_outputs": {
      "decoupled_params": "A dictionary with decoupled hyperparameters ready for use in PyTorch optimizers"
    },
    "evaluation_criteria": [
      "Correctness: Verify that the learning rate is properly decoupled from momentum effects",
      "Correctness: Check that the weight decay is correctly decoupled from the learning rate",
      "Quality: Compare the training dynamics (loss curves, accuracy progression) using decoupled vs. standard hyperparameters"
    ],
    "additional_tests": [
      "Test with a range of hyperparameter values",
      "Verify that changing one hyperparameter doesn't affect others unintentionally"
    ]
  }
]

This task_eval_planning_json provides a comprehensive plan for evaluating each of the three tasks. It includes specific inputs to be provided, expected outputs, detailed evaluation criteria, and additional tests to ensure robustness. These plans will allow for thorough testing of the implementations, covering correctness, performance, and quality aspects as they relate to the paper's contributions.




====================


task_eval_baseline_implementation_json:
{
  "implement_alternating_flip": "```python\nimport torch\nimport hashlib\n\ndef hash_fn(n, seed=42):\n    k = n * seed\n    return int(hashlib.md5(bytes(str(k), 'utf-8')).hexdigest()[-8:], 16)\n\ndef alternating_flip(inputs, indices, epoch):\n    # Applies alternating flipping to a batch of images\n    hashed_indices = torch.tensor([hash_fn(i) for i in indices.tolist()])\n    flip_mask = ((hashed_indices + epoch) % 2 == 0).view(-1, 1, 1, 1)\n    return torch.where(flip_mask, inputs.flip(-1), inputs)\n```",

  "init_whitening_conv": "```python\nimport torch\nimport torch.nn.functional as F\n\ndef get_patches(x, patch_shape):\n    c, (h, w) = x.shape[1], patch_shape\n    return x.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1,c,h,w).float()\n\ndef get_whitening_parameters(patches):\n    n, c, h, w = patches.shape\n    patches_flat = patches.view(n, -1)\n    est_patch_covariance = (patches_flat.T @ patches_flat) / n\n    eigenvalues, eigenvectors = torch.linalg.eigh(est_patch_covariance, UPLO='U')\n    return eigenvalues.flip(0).view(-1, 1, 1, 1), eigenvectors.T.reshape(c*h*w,c,h,w).flip(0)\n\ndef init_whitening_conv(layer, train_set, eps=5e-4):\n    patches = get_patches(train_set, patch_shape=layer.weight.data.shape[2:])\n    eigenvalues, eigenvectors = get_whitening_parameters(patches)\n    eigenvectors_scaled = eigenvectors / torch.sqrt(eigenvalues + eps)\n    layer.weight.data[:] = torch.cat((eigenvectors_scaled, -eigenvectors_scaled))\n    return layer\n```",

  "decoupled_hyperparameters": "```python\ndef decoupled_hyperparameters(batch_size, momentum, learning_rate, weight_decay):\n    kilostep_scale = 1024 * (1 + 1 / (1 - momentum))\n    lr = learning_rate / kilostep_scale\n    wd = weight_decay * batch_size / kilostep_scale\n    \n    return {\n        'batch_size': batch_size,\n        'momentum': momentum,\n        'learning_rate': lr,\n        'weight_decay': wd\n    }\n```"
}

This JSON object provides baseline implementations for each of the three tasks, which can be used as starting points for further development or as reference implementations for evaluation.




====================


task_eval_instructions_json:
{
  "implement_alternating_flip": "Implement the alternating flip data augmentation technique as described in the paper. Your implementation should:

1. Take as input a batch of images, their indices, and the current epoch number
2. Use a deterministic flipping pattern based on the epoch number
3. Ensure all unique flipped versions are seen in consecutive epochs
4. Use a hash function to determine which images to flip in each epoch
5. Be compatible with PyTorch's DataLoader

Key points to implement:
- For the first epoch, randomly flip 50% of inputs
- On even epochs (2, 4, 6, ...), flip only those inputs not flipped in the first epoch
- On odd epochs (3, 5, 7, ...), flip only those inputs flipped in the first epoch
- Use a hash function to consistently determine which images to flip based on their indices

Your implementation should be efficient and work with batched inputs. Focus on modifying the provided baseline implementation to incorporate the alternating flip logic.",

  "init_whitening_conv": "Implement the initialization of the first convolutional layer as a patch-whitening transformation. Your implementation should:

1. Take as input a convolutional layer and a subset of the training data
2. Compute the covariance matrix of 2x2 patches across the training distribution
3. Initialize the first half of the filters as eigenvectors of the covariance matrix
4. Initialize the second half as the negation of the first half
5. Ensure the output has approximately identity covariance matrix

Key steps to implement:
- Extract 2x2 patches from the input images
- Compute the covariance matrix of these patches
- Perform eigendecomposition on the covariance matrix
- Scale the eigenvectors to whiten the input
- Set the layer weights using the scaled eigenvectors and their negations

Your implementation should be numerically stable and efficient. Use the provided baseline implementation as a starting point and focus on correctly implementing the whitening transformation.",

  "decoupled_hyperparameters": "Implement the decoupled form of expressing hyperparameters as described in the paper. Your implementation should:

1. Take as input the batch size, momentum, learning rate, and weight decay
2. Decouple the learning rate from momentum effects
3. Decouple the weight decay from the learning rate
4. Return a dictionary with the decoupled hyperparameters ready for use in PyTorch optimizers

Key calculations to implement:
- Compute the 'kilostep_scale' factor: 1024 * (1 + 1 / (1 - momentum))
- Adjust the learning rate: lr = learning_rate / kilostep_scale
- Adjust the weight decay: wd = weight_decay * batch_size / kilostep_scale

Ensure your implementation allows each hyperparameter to be tuned independently. Use the provided baseline implementation as a reference and focus on correctly applying the decoupling formulas."
}

These instructions provide clear guidance for each task, highlighting the key aspects to implement and any important details from the paper. They also reference the baseline implementations, allowing engineers to focus on the specific modifications required to implement the paper's novel contributions.




====================


task_eval_correctness_scoring_json:
{
  "implement_alternating_flip": "```python
def score_alternating_flip(implementation):
    score = 0.0
    # Test data
    batch_size = 1024
    inputs = torch.randn(batch_size, 3, 32, 32)
    indices = torch.arange(batch_size)
    
    # Check flipping pattern for 5 epochs
    flipped = [implementation(inputs, indices, epoch) for epoch in range(5)]
    
    # Check if first epoch has ~50% flipped
    first_epoch_flipped = (flipped[0] != inputs).any(dim=(1,2,3)).float().mean()
    if 0.45 < first_epoch_flipped < 0.55:
        score += 0.2
    
    # Check alternating pattern
    for i in range(1, 5):
        if torch.allclose(flipped[i], flipped[i-2]):
            score += 0.1
    
    # Check if all images are flipped in two consecutive epochs
    all_flipped = (flipped[1] != inputs) | (flipped[2] != inputs)
    if all_flipped.all():
        score += 0.2
    
    # Check consistency using hashing
    hashed_indices = torch.tensor([hash_fn(i) for i in indices.tolist()])
    for epoch in range(1, 5):
        expected_flip = ((hashed_indices + epoch) % 2 == 0).view(-1, 1, 1, 1)
        actual_flip = (flipped[epoch] != inputs)
        if torch.allclose(expected_flip.float(), actual_flip.float()):
            score += 0.1
    
    return score
```",

  "init_whitening_conv": "```python
def score_init_whitening_conv(implementation):
    score = 0.0
    
    # Test data
    train_set = torch.randn(5000, 3, 32, 32)
    layer = nn.Conv2d(3, 24, kernel_size=2, padding=0, bias=False)
    
    # Initialize the layer
    implementation(layer, train_set)
    
    # Check if output has approximately identity covariance matrix
    with torch.no_grad():
        output = layer(train_set)
        output_flat = output.view(-1, output.shape[1])
        cov = torch.cov(output_flat.T)
        if torch.allclose(cov, torch.eye(cov.shape[0]), atol=1e-2):
            score += 0.4
    
    # Check if second half of filters are negation of first half
    weights = layer.weight.data
    if torch.allclose(weights[:12], -weights[12:]):
        score += 0.3
    
    # Check if filters are orthogonal
    filters = weights.view(24, -1)
    gram = filters @ filters.T
    if torch.allclose(gram, torch.eye(24), atol=1e-2):
        score += 0.3
    
    return score
```",

  "decoupled_hyperparameters": "```python
def score_decoupled_hyperparameters(implementation):
    score = 0.0
    
    # Test cases
    test_cases = [
        {'batch_size': 1024, 'momentum': 0.85, 'learning_rate': 11.5, 'weight_decay': 0.0153},
        {'batch_size': 512, 'momentum': 0.9, 'learning_rate': 5.0, 'weight_decay': 0.01},
        {'batch_size': 2048, 'momentum': 0.8, 'learning_rate': 20.0, 'weight_decay': 0.02}
    ]
    
    for case in test_cases:
        result = implementation(**case)
        
        # Check if learning rate is decoupled from momentum
        kilostep_scale = 1024 * (1 + 1 / (1 - case['momentum']))
        expected_lr = case['learning_rate'] / kilostep_scale
        if abs(result['learning_rate'] - expected_lr) < 1e-6:
            score += 0.1
        
        # Check if weight decay is decoupled from learning rate
        expected_wd = case['weight_decay'] * case['batch_size'] / kilostep_scale
        if abs(result['weight_decay'] - expected_wd) < 1e-6:
            score += 0.1
        
        # Check if momentum is unchanged
        if result['momentum'] == case['momentum']:
            score += 0.05
        
        # Check if batch size is unchanged
        if result['batch_size'] == case['batch_size']:
            score += 0.05
    
    return score
```"
}

These scoring functions evaluate the correctness of each implementation based on the key aspects described in the paper. They return a score between 0 and 1, with higher scores indicating better adherence to the expected behavior.




====================


task_eval_metric_scoring_json:
{
  "implement_alternating_flip": "```python
def score_alternating_flip_metrics(alternating_flip_fn, random_flip_fn):
    score = 0.0
    
    # Setup a simple CNN for CIFAR-10
    model = SimpleCNN()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
    
    # Load CIFAR-10 data
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)
    
    # Train with alternating flip
    start_time = time.time()
    alt_accuracy = train_model(model, trainloader, criterion, optimizer, alternating_flip_fn, epochs=5)
    alt_time = time.time() - start_time
    
    # Reset model and train with random flip
    model = SimpleCNN()
    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
    start_time = time.time()
    rand_accuracy = train_model(model, trainloader, criterion, optimizer, random_flip_fn, epochs=5)
    rand_time = time.time() - start_time
    
    # Score based on accuracy improvement
    if alt_accuracy > rand_accuracy:
        score += 0.5 * (alt_accuracy - rand_accuracy) / rand_accuracy
    
    # Score based on time improvement
    if alt_time < rand_time:
        score += 0.5 * (rand_time - alt_time) / rand_time
    
    return score

def train_model(model, dataloader, criterion, optimizer, flip_fn, epochs):
    for epoch in range(epochs):
        for inputs, labels in dataloader:
            inputs = flip_fn(inputs, torch.arange(len(inputs)), epoch)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    
    return evaluate_model(model, dataloader)

def evaluate_model(model, dataloader):
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in dataloader:
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    return correct / total
```",

  "init_whitening_conv": "```python
def score_whitening_conv_metrics(whitening_init_fn, standard_init_fn):
    score = 0.0
    
    # Setup a simple CNN for CIFAR-10
    model_whitening = SimpleCNN()
    model_standard = SimpleCNN()
    criterion = nn.CrossEntropyLoss()
    
    # Load CIFAR-10 data
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)
    
    # Apply initializations
    whitening_init_fn(model_whitening.features[0], trainset.data[:5000])
    standard_init_fn(model_standard.features[0])
    
    # Train both models for a few steps
    optimizer_whitening = optim.SGD(model_whitening.parameters(), lr=0.01)
    optimizer_standard = optim.SGD(model_standard.parameters(), lr=0.01)
    
    whitening_losses = []
    standard_losses = []
    
    for inputs, labels in itertools.islice(trainloader, 100):  # Train for 100 batches
        # Train whitening model
        outputs = model_whitening(inputs)
        loss = criterion(outputs, labels)
        optimizer_whitening.zero_grad()
        loss.backward()
        optimizer_whitening.step()
        whitening_losses.append(loss.item())
        
        # Train standard model
        outputs = model_standard(inputs)
        loss = criterion(outputs, labels)
        optimizer_standard.zero_grad()
        loss.backward()
        optimizer_standard.step()
        standard_losses.append(loss.item())
    
    # Compare early training loss
    if np.mean(whitening_losses) < np.mean(standard_losses):
        score += 0.5 * (np.mean(standard_losses) - np.mean(whitening_losses)) / np.mean(standard_losses)
    
    # Compare final accuracy
    whitening_accuracy = evaluate_model(model_whitening, trainloader)
    standard_accuracy = evaluate_model(model_standard, trainloader)
    
    if whitening_accuracy > standard_accuracy:
        score += 0.5 * (whitening_accuracy - standard_accuracy) / standard_accuracy
    
    return score
```",

  "decoupled_hyperparameters": "```python
def score_decoupled_hyperparameters_metrics(decoupled_fn, standard_fn):
    score = 0.0
    
    # Setup a simple CNN for CIFAR-10
    model_decoupled = SimpleCNN()
    model_standard = SimpleCNN()
    criterion = nn.CrossEntropyLoss()
    
    # Load CIFAR-10 data
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)
    
    # Define base hyperparameters
    base_params = {
        'batch_size': 128,
        'momentum': 0.9,
        'learning_rate': 0.1,
        'weight_decay': 0.0005
    }
    
    # Get decoupled and standard hyperparameters
    decoupled_params = decoupled_fn(**base_params)
    standard_params = standard_fn(**base_params)
    
    # Create optimizers
    optimizer_decoupled = optim.SGD(model_decoupled.parameters(), **decoupled_params)
    optimizer_standard = optim.SGD(model_standard.parameters(), **standard_params)
    
    # Train both models
    decoupled_losses = []
    standard_losses = []
    decoupled_accuracies = []
    standard_accuracies = []
    
    for epoch in range(5):  # Train for 5 epochs
        for inputs, labels in trainloader:
            # Train decoupled model
            outputs = model_decoupled(inputs)
            loss = criterion(outputs, labels)
            optimizer_decoupled.zero_grad()
            loss.backward()
            optimizer_decoupled.step()
            decoupled_losses.append(loss.item())
            
            # Train standard model
            outputs = model_standard(inputs)
            loss = criterion(outputs, labels)
            optimizer_standard.zero_grad()
            loss.backward()
            optimizer_standard.step()
            standard_losses.append(loss.item())
        
        # Evaluate after each epoch
        decoupled_accuracies.append(evaluate_model(model_decoupled, trainloader))
        standard_accuracies.append(evaluate_model(model_standard, trainloader))
    
    # Compare final accuracy
    if decoupled_accuracies[-1] > standard_accuracies[-1]:
        score += 0.5 * (decoupled_accuracies[-1] - standard_accuracies[-1]) / standard_accuracies[-1]
    
    # Compare convergence speed (accuracy after first epoch)
    if decoupled_accuracies[0] > standard_accuracies[0]:
        score += 0.3 * (decoupled_accuracies[0] - standard_accuracies[0]) / standard_accuracies[0]
    
    # Compare loss curve smoothness
    decoupled_smoothness = np.mean(np.abs(np.diff(decoupled_losses)))
    standard_smoothness = np.mean(np.abs(np.diff(standard_losses)))
    if decoupled_smoothness < standard_smoothness:
        score += 0.2 * (standard_smoothness - decoupled_smoothness) / standard_smoothness
    
    return score
```"
}

This task_eval_metric_scoring_json provides scoring functions for each task that evaluate the performance impact of the implementations. Each function compares the implementation against a baseline or standard method, focusing on relevant metrics such as training speed, accuracy, and convergence characteristics. The scores are normalized to be between 0 and 1, with higher scores indicating better performance relative to the baseline.




====================


setup_description:
To implement and evaluate the tasks from this paper, the following setup is required:

1. Python environment with PyTorch (version 2.1.2 or later) and torchvision installed
2. NVIDIA GPU with CUDA support (A100 recommended for replicating paper results)
3. CIFAR-10 dataset (will be automatically downloaded by torchvision if not present)
4. A basic CNN architecture for CIFAR-10 (provided below)
5. Standard Python libraries: numpy, time, itertools

No pre-trained models or additional datasets are required. The CIFAR-10 dataset will be automatically downloaded when first accessed through torchvision.

Here's a basic CNN architecture for CIFAR-10 that can be used for evaluations:

```python
import torch
import torch.nn as nn

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.classifier = nn.Sequential(
            nn.Linear(256 * 4 * 4, 1024),
            nn.ReLU(inplace=True),
            nn.Linear(1024, 10)
        )

    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x
```

This setup provides all the necessary components to implement and evaluate the tasks described in the paper, focusing on the CIFAR-10 dataset and fast training methods.




====================

