title:
94% on CIFAR-10 in 3.29 Seconds on a Single GPU


abstract_plus:
Title: 94% on CIFAR-10 in 3.29 Seconds on a Single GPU

Abstract Plus: This paper introduces fast training methods for the CIFAR-10 dataset, achieving 94% accuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds on a single NVIDIA A100 GPU. The authors propose a derandomized variant of horizontal flipping augmentation called "alternating flip," which improves performance over standard random flipping in all cases where flipping is beneficial. The methods build upon prior work, incorporating optimizations such as a modified network architecture, initialization techniques, and frozen patch-whitening layers. The paper presents a comprehensive analysis of the training process, including ablation studies and experiments on other datasets to demonstrate generalization.

Additional high-level summaries:
1. The paper provides a detailed breakdown of the training method, including hyperparameter choices, network architecture, and optimization techniques.
2. The authors introduce a decoupled form of expressing hyperparameters, allowing for more efficient tuning.
3. The research demonstrates that the proposed methods not only achieve fast training times on CIFAR-10 but also generalize well to other datasets like CIFAR-100, SVHN, and CINIC-10.
4. The paper includes an analysis of the statistical properties of the trained models, including variance and class-wise calibration.
5. The authors provide open-source implementations of their methods, making them easily accessible for researchers and practitioners.




====================


quantitative_results_json:
[
  {
    "units": "seconds",
    "value": 3.29,
    "description": "Time to reach 94% accuracy on CIFAR-10",
    "method": "airbench94_compiled",
    "notes": "Run on a single NVIDIA A100 GPU",
    "comparison": {"baseline": 6.3, "improvement": "1.9x faster"}
  },
  {
    "units": "seconds",
    "value": 10.4,
    "description": "Time to reach 95% accuracy on CIFAR-10",
    "method": "airbench95",
    "notes": "Run on a single NVIDIA A100 GPU"
  },
  {
    "units": "seconds",
    "value": 46.3,
    "description": "Time to reach 96% accuracy on CIFAR-10",
    "method": "airbench96",
    "notes": "Run on a single NVIDIA A100 GPU"
  },
  {
    "units": "FLOPs",
    "value": 3.6e14,
    "description": "Computational cost for 94% accuracy method",
    "method": "airbench94_compiled"
  },
  {
    "units": "FLOPs",
    "value": 1.4e15,
    "description": "Computational cost for 95% accuracy method",
    "method": "airbench95"
  },
  {
    "units": "FLOPs",
    "value": 7.2e15,
    "description": "Computational cost for 96% accuracy method",
    "method": "airbench96"
  },
  {
    "units": "percentage_points",
    "value": 0.13,
    "description": "Improvement in accuracy from random flip to alternating flip",
    "method": "Alternating flip",
    "notes": "For airbench94 without TTA, 20 epochs"
  },
  {
    "units": "percentage",
    "value": 27.1,
    "description": "Effective speedup from random flip to alternating flip",
    "method": "Alternating flip",
    "notes": "For airbench94 without TTA, 20 epochs"
  },
  {
    "units": "accuracy",
    "value": 0.7927,
    "description": "Accuracy on CIFAR-100",
    "method": "airbench96",
    "comparison": {"baseline": 0.7754, "improvement": "+1.73%"}
  },
  {
    "units": "test_set_accuracy_stddev",
    "value": 0.00128,
    "description": "Standard deviation of test-set accuracy",
    "method": "airbench94 with TTA",
    "notes": "Based on 10,000 runs"
  },
  {
    "units": "distribution_wise_accuracy_stddev",
    "value": 0.00029,
    "description": "Standard deviation of distribution-wise accuracy",
    "method": "airbench94 with TTA",
    "notes": "Based on 10,000 runs"
  }
]




====================


core_ideas_json:
{
  "rapid_training": "Develop extremely fast training methods for image classification on CIFAR-10, achieving high accuracy in seconds",
  "alternating_flip": "A derandomized variant of horizontal flipping augmentation that improves performance over standard random flipping",
  "optimized_architecture": "Incorporate and refine existing optimizations in network architecture, initialization, and training process",
  "efficiency": "Design methods that are efficient in both time and computational resources (FLOPs)",
  "generalization": "Demonstrate that the rapid training methods generalize well to other datasets beyond CIFAR-10",
  "decoupled_hyperparameters": "Introduce a decoupled form of expressing hyperparameters for more efficient tuning",
  "statistical_analysis": "Analyze statistical properties of trained models, including variance and class-wise calibration"
}




====================


metrics_json:
[
  {
    "name": "Training Time (94% accuracy)",
    "description": "Time required to reach 94% accuracy on CIFAR-10",
    "unit": "seconds",
    "measurement_details": "Measured on a single NVIDIA A100 GPU",
    "justification": "Demonstrates the speed of the proposed method in reaching a high accuracy benchmark"
  },
  {
    "name": "Training Time (95% accuracy)",
    "description": "Time required to reach 95% accuracy on CIFAR-10",
    "unit": "seconds",
    "measurement_details": "Measured on a single NVIDIA A100 GPU",
    "justification": "Shows the method's performance for a higher accuracy target"
  },
  {
    "name": "Training Time (96% accuracy)",
    "description": "Time required to reach 96% accuracy on CIFAR-10",
    "unit": "seconds",
    "measurement_details": "Measured on a single NVIDIA A100 GPU",
    "justification": "Demonstrates the method's capability for very high accuracy scenarios"
  },
  {
    "name": "Computational Cost",
    "description": "Number of floating point operations required for training",
    "unit": "FLOPs",
    "measurement_details": "Calculated for each accuracy level (94%, 95%, 96%)",
    "justification": "Provides a hardware-independent measure of computational efficiency"
  },
  {
    "name": "Alternating Flip Improvement",
    "description": "Accuracy improvement from using alternating flip instead of random flip",
    "unit": "percentage points",
    "measurement_details": "Measured for various configurations and training durations",
    "justification": "Quantifies the benefit of the proposed alternating flip augmentation"
  },
  {
    "name": "Alternating Flip Speedup",
    "description": "Effective training speedup from using alternating flip",
    "unit": "percentage",
    "measurement_details": "Calculated based on equivalent accuracy achieved with fewer epochs",
    "justification": "Demonstrates the efficiency gain from the alternating flip method"
  },
  {
    "name": "CIFAR-100 Accuracy",
    "description": "Classification accuracy on the CIFAR-100 dataset",
    "unit": "percentage",
    "measurement_details": "Measured using the airbench96 method without retuning",
    "justification": "Shows the generalization capability of the method to other datasets"
  },
  {
    "name": "Test-set Accuracy Variance",
    "description": "Standard deviation of test-set accuracy across multiple runs",
    "unit": "percentage",
    "measurement_details": "Calculated from 10,000 runs of airbench94 with TTA",
    "justification": "Indicates the consistency and reliability of the training method"
  },
  {
    "name": "Distribution-wise Accuracy Variance",
    "description": "Standard deviation of distribution-wise accuracy across multiple runs",
    "unit": "percentage",
    "measurement_details": "Calculated from 10,000 runs of airbench94 with TTA",
    "justification": "Provides insight into the inherent variability of the training process"
  }
]




====================


hw_agnostic_metrics_json:
[
  {
    "name": "Epochs to 94% accuracy",
    "description": "Number of training epochs required to reach 94% test set accuracy on CIFAR-10",
    "corresponding_hw_metric": "Training Time (94% accuracy)",
    "unit": "Epochs",
    "equivalence_justification": "Epochs are a hardware-independent measure of training progress, directly related to training time"
  },
  {
    "name": "Epochs to 95% accuracy",
    "description": "Number of training epochs required to reach 95% test set accuracy on CIFAR-10",
    "corresponding_hw_metric": "Training Time (95% accuracy)",
    "unit": "Epochs",
    "equivalence_justification": "Epochs are a hardware-independent measure of training progress, directly related to training time"
  },
  {
    "name": "Epochs to 96% accuracy",
    "description": "Number of training epochs required to reach 96% test set accuracy on CIFAR-10",
    "corresponding_hw_metric": "Training Time (96% accuracy)",
    "unit": "Epochs",
    "equivalence_justification": "Epochs are a hardware-independent measure of training progress, directly related to training time"
  },
  {
    "name": "Computational Cost",
    "description": "Number of floating point operations required for training",
    "corresponding_hw_metric": "Computational Cost",
    "unit": "FLOPs",
    "equivalence_justification": "FLOPs are already a hardware-independent measure of computational complexity"
  },
  {
    "name": "Alternating Flip Improvement",
    "description": "Accuracy improvement from using alternating flip instead of random flip",
    "corresponding_hw_metric": "Alternating Flip Improvement",
    "unit": "percentage points",
    "equivalence_justification": "This metric is already hardware-independent"
  },
  {
    "name": "Alternating Flip Speedup",
    "description": "Effective training speedup from using alternating flip",
    "corresponding_hw_metric": "Alternating Flip Speedup",
    "unit": "percentage",
    "equivalence_justification": "This metric is already expressed as a percentage, which is hardware-independent"
  },
  {
    "name": "CIFAR-100 Accuracy",
    "description": "Classification accuracy on the CIFAR-100 dataset",
    "corresponding_hw_metric": "CIFAR-100 Accuracy",
    "unit": "percentage",
    "equivalence_justification": "Accuracy is a hardware-independent metric"
  },
  {
    "name": "Test-set Accuracy Variance",
    "description": "Standard deviation of test-set accuracy across multiple runs",
    "corresponding_hw_metric": "Test-set Accuracy Variance",
    "unit": "percentage",
    "equivalence_justification": "Variance in accuracy is hardware-independent"
  },
  {
    "name": "Distribution-wise Accuracy Variance",
    "description": "Standard deviation of distribution-wise accuracy across multiple runs",
    "corresponding_hw_metric": "Distribution-wise Accuracy Variance",
    "unit": "percentage",
    "equivalence_justification": "Variance in accuracy is hardware-independent"
  }
]




====================


baseline_methods_json:
{
  "tysam-code_2023": {
    "description": "The prior state-of-the-art method for fast CIFAR-10 training",
    "key_components": [
      "Modified network architecture",
      "Initialization techniques",
      "Optimizer configuration",
      "Frozen patch-whitening layer"
    ],
    "architecture_details": {
      "first_layer": "2x2 convolution with no padding",
      "main_body": "VGG-like structure with 3x3 convolutions and 2x2 max-pooling",
      "final_block": "512 output channels"
    },
    "training_details": {
      "optimizer": "Nesterov SGD",
      "batch_size": 1024,
      "augmentation": "Random horizontal flipping, 2-pixel random translation"
    },
    "target_metrics": ["Accuracy", "Training time"],
    "experimental_methods": [
      "airbench94",
      "airbench95",
      "airbench96"
    ]
  },
  "airbench94": {
    "description": "Fast training method reaching 94% accuracy on CIFAR-10",
    "key_components": [
      "Modified tysam-code_2023 architecture",
      "Alternating flip augmentation",
      "Decoupled hyperparameter expression",
      "Lookahead optimization"
    ],
    "architecture_details": {
      "first_layer": "2x2 convolution with no padding, learnable biases",
      "main_body": "VGG-like structure with 3x3 convolutions and 2x2 max-pooling",
      "final_block": "256 output channels"
    },
    "training_details": {
      "optimizer": "Nesterov SGD with Lookahead",
      "batch_size": 1024,
      "augmentation": "Alternating flip, 2-pixel random translation",
      "epochs": 9.9,
      "learning_rate": "Triangular schedule"
    },
    "target_metrics": ["Accuracy", "Training time", "FLOPs"],
    "experimental_methods": [
      "airbench94_compiled"
    ]
  },
  "airbench95": {
    "description": "Fast training method reaching 95% accuracy on CIFAR-10",
    "key_components": [
      "Modified airbench94 architecture",
      "Increased training duration"
    ],
    "architecture_details": {
      "first_block": "128 output channels",
      "later_blocks": "384 output channels"
    },
    "training_details": {
      "epochs": 15,
      "learning_rate": "0.87 * airbench94 learning rate"
    },
    "target_metrics": ["Accuracy", "Training time", "FLOPs"]
  },
  "airbench96": {
    "description": "Fast training method reaching 96% accuracy on CIFAR-10",
    "key_components": [
      "Further modified airbench94 architecture",
      "Additional augmentation",
      "Residual connections"
    ],
    "architecture_details": {
      "first_block": "128 output channels",
      "later_blocks": "512 output channels",
      "convolutions_per_block": 3,
      "residual_connections": "Across later two convolutions of each block"
    },
    "training_details": {
      "epochs": 40,
      "learning_rate": "0.78 * airbench94 learning rate",
      "augmentation": "Alternating flip, 2-pixel random translation, 12-pixel Cutout"
    },
    "target_metrics": ["Accuracy", "Training time", "FLOPs"]
  }
}




====================


experimental_methods_json:
[
  {
    "name": "airbench94",
    "baseline": "tysam-code_2023",
    "modifications": [
      "Alternating flip augmentation instead of random flip",
      "Reduced output channels in the third block from 512 to 256",
      "Added learnable biases to the first convolution",
      "Decoupled hyperparameter expression",
      "Lookahead optimization"
    ],
    "expected_improvements": [
      "Faster training time to reach 94% accuracy",
      "Reduced computational cost (FLOPs)"
    ],
    "target_metrics": ["Accuracy", "Training time", "FLOPs"]
  },
  {
    "name": "airbench95",
    "baseline": "airbench94",
    "modifications": [
      "Increased training epochs from 9.9 to 15",
      "Scaled output channel count of the first block from 64 to 128",
      "Scaled output channel count of the second two blocks from 256 to 384",
      "Reduced learning rate by a factor of 0.87"
    ],
    "expected_improvements": [
      "Higher accuracy (95%) with a trade-off in training time"
    ],
    "target_metrics": ["Accuracy", "Training time", "FLOPs"]
  },
  {
    "name": "airbench96",
    "baseline": "airbench94",
    "modifications": [
      "Added 12-pixel Cutout augmentation",
      "Increased training epochs to 40",
      "Added a third convolution to each block",
      "Scaled first block to 128 channels and second two to 512",
      "Added residual connections across later two convolutions of each block",
      "Reduced learning rate by a factor of 0.78"
    ],
    "expected_improvements": [
      "Higher accuracy (96%) with a larger trade-off in training time"
    ],
    "target_metrics": ["Accuracy", "Training time", "FLOPs"]
  },
  {
    "name": "airbench94_compiled",
    "baseline": "airbench94",
    "modifications": [
      "Use of torch.compile for more efficient GPU utilization"
    ],
    "expected_improvements": [
      "14% reduction in training time compared to non-compiled version"
    ],
    "target_metrics": ["Training time"]
  }
]




====================


method_metric_results:
{
  "tysam-code_2023": {
    "accuracy": 94.0,
    "training_time": 6.3
  },
  "airbench94": {
    "accuracy": 94.01,
    "training_time": 3.83,
    "computational_cost": 3.6e14
  },
  "airbench95": {
    "accuracy": 95.01,
    "training_time": 10.4,
    "computational_cost": 1.4e15
  },
  "airbench96": {
    "accuracy": 96.05,
    "training_time": 46.3,
    "computational_cost": 7.2e15
  },
  "airbench94_compiled": {
    "accuracy": 94.01,
    "training_time": 3.29,
    "computational_cost": 3.6e14
  }
}




====================


task_candidates_json:
[
  {
    "name": "implement_alternating_flip",
    "description": "Implement the alternating flip augmentation technique described in the paper",
    "corresponding_method": "airbench94",
    "inputs": [
      {
        "name": "images",
        "type": "torch.Tensor",
        "shape": "(batch_size, channels, height, width)"
      },
      {
        "name": "epoch",
        "type": "int"
      }
    ],
    "outputs": [
      {
        "name": "augmented_images",
        "type": "torch.Tensor",
        "shape": "(batch_size, channels, height, width)"
      }
    ],
    "skills_tested": ["PyTorch", "Data augmentation", "Image processing"],
    "assets_provided": ["baseline_random_flip_implementation"],
    "minimum_hardware_requirements": "CPU only; 8GB RAM",
    "evaluation_criteria": [
      "Correctness: Compare output with expected alternating flip pattern",
      "Efficiency: Measure execution time compared to random flip"
    ],
    "provided_baseline": "Standard random flip implementation",
    "instructions_short": "Modify the provided random flip function to implement the alternating flip augmentation as described in the paper.",
    "time_to_complete": 2,
    "difficulty": 2,
    "feasibility": 5,
    "research_ability": 3
  },
  {
    "name": "implement_decoupled_hyperparameters",
    "description": "Implement a system for decoupled hyperparameter expression as described in the paper",
    "corresponding_method": "airbench94",
    "inputs": [
      {
        "name": "hyperparameters",
        "type": "dict",
        "contents": "learning_rate, momentum, weight_decay, batch_size"
      }
    ],
    "outputs": [
      {
        "name": "decoupled_hyperparameters",
        "type": "dict",
        "contents": "Decoupled versions of input hyperparameters"
      }
    ],
    "skills_tested": ["PyTorch", "Optimization theory", "Hyperparameter tuning"],
    "assets_provided": ["hyperparameter_conversion_formulas"],
    "minimum_hardware_requirements": "CPU only; 4GB RAM",
    "evaluation_criteria": [
      "Correctness: Compare output with expected decoupled values",
      "Completeness: Ensure all necessary hyperparameters are decoupled"
    ],
    "provided_baseline": "Standard hyperparameter setup for PyTorch optimizers",
    "instructions_short": "Create a function that converts standard hyperparameters to their decoupled form as described in the paper.",
    "time_to_complete": 3,
    "difficulty": 3,
    "feasibility": 5,
    "research_ability": 4
  },
  {
    "name": "implement_lookahead_optimizer",
    "description": "Implement the Lookahead optimization technique used in the paper",
    "corresponding_method": "airbench94",
    "inputs": [
      {
        "name": "base_optimizer",
        "type": "torch.optim.Optimizer"
      },
      {
        "name": "model",
        "type": "torch.nn.Module"
      },
      {
        "name": "lookahead_steps",
        "type": "int"
      },
      {
        "name": "lookahead_alpha",
        "type": "float"
      }
    ],
    "outputs": [
      {
        "name": "lookahead_optimizer",
        "type": "custom Optimizer class"
      }
    ],
    "skills_tested": ["PyTorch", "Optimization algorithms", "Custom optimizer implementation"],
    "assets_provided": ["base_optimizer_implementation"],
    "minimum_hardware_requirements": "CPU or GPU; 8GB RAM",
    "evaluation_criteria": [
      "Correctness: Compare optimization results with expected values",
      "Efficiency: Measure training time improvement over base optimizer"
    ],
    "provided_baseline": "Standard PyTorch optimizer (e.g., SGD with momentum)",
    "instructions_short": "Implement the Lookahead optimization technique as a wrapper around the provided base optimizer.",
    "time_to_complete": 4,
    "difficulty": 4,
    "feasibility": 4,
    "research_ability": 4
  },
  {
    "name": "implement_custom_cnn_architecture",
    "description": "Implement the custom CNN architecture described in the paper",
    "corresponding_method": "airbench94",
    "inputs": [
      {
        "name": "input_channels",
        "type": "int"
      },
      {
        "name": "num_classes",
        "type": "int"
      }
    ],
    "outputs": [
      {
        "name": "model",
        "type": "torch.nn.Module"
      }
    ],
    "skills_tested": ["PyTorch", "CNN architecture design", "Model implementation"],
    "assets_provided": ["architecture_diagram", "layer_specifications"],
    "minimum_hardware_requirements": "CPU or GPU; 8GB RAM",
    "evaluation_criteria": [
      "Correctness: Compare model structure with paper description",
      "Efficiency: Measure forward pass time and parameter count"
    ],
    "provided_baseline": "Basic CNN architecture for CIFAR-10",
    "instructions_short": "Implement the custom CNN architecture as described in the paper, including the specific modifications mentioned.",
    "time_to_complete": 3,
    "difficulty": 3,
    "feasibility": 5,
    "research_ability": 3
  },
  {
    "name": "implement_test_time_augmentation",
    "description": "Implement the test-time augmentation (TTA) technique used in the paper",
    "corresponding_method": "airbench94",
    "inputs": [
      {
        "name": "model",
        "type": "torch.nn.Module"
      },
      {
        "name": "test_images",
        "type": "torch.Tensor",
        "shape": "(batch_size, channels, height, width)"
      },
      {
        "name": "tta_level",
        "type": "int"
      }
    ],
    "outputs": [
      {
        "name": "tta_predictions",
        "type": "torch.Tensor",
        "shape": "(batch_size, num_classes)"
      }
    ],
    "skills_tested": ["PyTorch", "Data augmentation", "Inference optimization"],
    "assets_provided": ["base_inference_function"],
    "minimum_hardware_requirements": "GPU recommended; 16GB RAM",
    "evaluation_criteria": [
      "Correctness: Compare TTA results with expected improvements",
      "Efficiency: Measure inference time with and without TTA"
    ],
    "provided_baseline": "Standard inference without augmentation",
    "instructions_short": "Implement the test-time augmentation technique as described in the paper, supporting different levels of augmentation.",
    "time_to_complete": 3,
    "difficulty": 3,
    "feasibility": 4,
    "research_ability": 3
  },
  {
    "name": "implement_training_loop_with_lr_schedule",
    "description": "Implement a training loop with the triangular learning rate schedule described in the paper",
    "corresponding_method": "airbench94",
    "inputs": [
      {
        "name": "model",
        "type": "torch.nn.Module"
      },
      {
        "name": "train_loader",
        "type": "torch.utils.data.DataLoader"
      },
      {
        "name": "optimizer",
        "type": "torch.optim.Optimizer"
      },
      {
        "name": "num_epochs",
        "type": "int"
      }
    ],
    "outputs": [
      {
        "name": "trained_model",
        "type": "torch.nn.Module"
      },
      {
        "name": "training_history",
        "type": "dict",
        "contents": "Epoch-wise loss and accuracy"
      }
    ],
    "skills_tested": ["PyTorch", "Training loop implementation", "Learning rate scheduling"],
    "assets_provided": ["basic_training_loop", "lr_schedule_formula"],
    "minimum_hardware_requirements": "GPU recommended; 16GB RAM",
    "evaluation_criteria": [
      "Correctness: Compare learning rate values with expected schedule",
      "Efficiency: Measure training time and final model performance"
    ],
    "provided_baseline": "Basic training loop with constant learning rate",
    "instructions_short": "Modify the provided training loop to incorporate the triangular learning rate schedule as described in the paper.",
    "time_to_complete": 4,
    "difficulty": 3,
    "feasibility": 5,
    "research_ability": 3
  },
  {
    "name": "implement_patch_whitening_initialization",
    "description": "Implement the patch-whitening initialization technique for the first convolutional layer",
    "corresponding_method": "airbench94",
    "inputs": [
      {
        "name": "conv_layer",
        "type": "torch.nn.Conv2d"
      },
      {
        "name": "train_images",
        "type": "torch.Tensor",
        "shape": "(num_images, channels, height, width)"
      }
    ],
    "outputs": [
      {
        "name": "initialized_conv_layer",
        "type": "torch.nn.Conv2d"
      }
    ],
    "skills_tested": ["PyTorch", "Initialization techniques", "Linear algebra"],
    "assets_provided": ["patch_extraction_function"],
    "minimum_hardware_requirements": "GPU recommended; 16GB RAM",
    "evaluation_criteria": [
      "Correctness: Compare initialized weights with expected statistics",
      "Efficiency: Measure initialization time"
    ],
    "provided_baseline": "Standard Kaiming initialization",
    "instructions_short": "Implement the patch-whitening initialization technique for the first convolutional layer as described in the paper.",
    "time_to_complete": 5,
    "difficulty": 4,
    "feasibility": 4,
    "research_ability": 4
  }
]




====================


task_prerequisites_json:
{
  "implement_alternating_flip": ["PyTorch", "CIFAR-10 dataset"],
  "implement_decoupled_hyperparameters": ["PyTorch"],
  "implement_lookahead_optimizer": ["PyTorch"],
  "implement_custom_cnn_architecture": ["PyTorch"],
  "implement_test_time_augmentation": ["PyTorch", "CIFAR-10 dataset"],
  "implement_training_loop_with_lr_schedule": ["PyTorch", "CIFAR-10 dataset"],
  "implement_patch_whitening_initialization": ["PyTorch", "CIFAR-10 dataset", "NumPy"]
}




====================


task_eval_instructions_json:
{
  "implement_alternating_flip": "Implement the alternating flip augmentation technique as follows:
1. Create a function that takes a batch of images and the current epoch number as inputs.
2. For the first epoch (epoch 0), randomly flip 50% of the images horizontally.
3. For subsequent epochs:
   - On even epochs (2, 4, 6, ...), flip only those images that were not flipped in the first epoch.
   - On odd epochs (3, 5, 7, ...), flip only those images that were flipped in the first epoch.
4. Use a deterministic method (e.g., hashing the image index) to decide which images to flip in the first epoch, ensuring consistency across epochs.
5. Implement this without storing additional memory for flip decisions by using a pseudorandom function.
6. Ensure that the function is efficient and can handle large batch sizes.
7. Test the implementation by running it for multiple epochs and verifying the flip pattern.

Your function should have the following signature:
def alternating_flip(images: torch.Tensor, indices: torch.Tensor, epoch: int) -> torch.Tensor:
    # Your implementation here

Ensure that your implementation is compatible with the provided random flip baseline and can be easily integrated into the existing data loading pipeline.",

  "implement_decoupled_hyperparameters": "Implement a function to convert standard hyperparameters to their decoupled form:
1. Create a function that takes a dictionary of standard hyperparameters as input.
2. The input dictionary should contain: 'batch_size', 'learning_rate', 'momentum', and 'weight_decay'.
3. Implement the following conversions:
   a. Decoupled learning rate = learning_rate / (batch_size * (1 + 1 / (1 - momentum)))
   b. Decoupled weight decay = weight_decay * batch_size / (1 + 1 / (1 - momentum))
4. Return a new dictionary with the decoupled values, keeping the original values as well.
5. Add a 'bias_lr_scale' key to the output dictionary, set to 64 as per the paper.
6. Ensure that the function handles edge cases, such as momentum being 1.0.
7. Include docstrings and type hints for clarity.

Your function should have the following signature:
def decouple_hyperparameters(hyperparams: Dict[str, float]) -> Dict[str, float]:
    # Your implementation here

Test your implementation with various input values and verify that the output matches the expected decoupled values.",

  "implement_lookahead_optimizer": "Implement the Lookahead optimization technique as a wrapper around an existing optimizer:
1. Create a new class called 'Lookahead' that inherits from torch.optim.Optimizer.
2. The constructor should take the following arguments:
   - base_optimizer: The optimizer to wrap (e.g., SGD, Adam)
   - model: The neural network model
   - lookahead_steps: Number of steps before updating the slow weights (default: 5)
   - lookahead_alpha: The slow weights step size (default: 0.8)
3. In the constructor, create a copy of the model's parameters for the slow weights.
4. Implement the 'step' method to do the following:
   a. Call the base optimizer's step method.
   b. Increment an internal counter.
   c. If the counter reaches lookahead_steps:
      - Update the slow weights using the formula: slow_weights = slow_weights + lookahead_alpha * (fast_weights - slow_weights)
      - Copy the slow weights back to the model's parameters.
      - Reset the counter.
5. Implement a 'state_dict' method to save the optimizer state, including the slow weights.
6. Implement a 'load_state_dict' method to load the optimizer state.
7. Ensure that the Lookahead wrapper is compatible with various base optimizers.

Your class should be implemented as follows:
class Lookahead(torch.optim.Optimizer):
    def __init__(self, base_optimizer, model, lookahead_steps=5, lookahead_alpha=0.8):
        # Your implementation here

    def step(self):
        # Your implementation here

    def state_dict(self):
        # Your implementation here

    def load_state_dict(self, state_dict):
        # Your implementation here

Test your implementation by training a model with and without Lookahead and compare the convergence speed and final performance.",

  "implement_custom_cnn_architecture": "Implement the custom CNN architecture described in the paper:
1. Create a new class that inherits from torch.nn.Module.
2. The constructor should take input_channels and num_classes as arguments.
3. Implement the following architecture:
   a. First layer: 2x2 convolution with 24 output channels, no padding, and learnable biases.
   b. Activation function: GELU
   c. Three blocks, each consisting of:
      - Conv2d (3x3, same padding, no bias)
      - MaxPool2d (2x2)
      - BatchNorm2d (no affine parameters)
      - GELU activation
      - Conv2d (3x3, same padding, no bias)
      - BatchNorm2d (no affine parameters)
      - GELU activation
   d. Final MaxPool2d (3x3)
   e. Flatten layer
   f. Linear layer (output size: num_classes, no bias)
   g. Scaling layer (multiply output by 1/9)
4. Use the following output channel counts for the blocks:
   - First block: 64 channels
   - Second block: 256 channels
   - Third block: 256 channels
5. Implement the forward method to pass the input through all layers.
6. Initialize the convolution layers using the dirac initialization for the first min(in_channels, out_channels) filters.
7. Freeze the weights of the first convolutional layer (they will be set using patch whitening later).

Your class should be implemented as follows:
class CustomCNN(torch.nn.Module):
    def __init__(self, input_channels: int, num_classes: int):
        # Your implementation here

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Your implementation here

Test your implementation by creating an instance of the model and performing a forward pass with a dummy input to ensure the output shape is correct.",

  "implement_test_time_augmentation": "Implement the test-time augmentation (TTA) technique as described in the paper:
1. Create a function that takes a model, a batch of test images, and a TTA level as inputs.
2. Implement three levels of TTA:
   - Level 0: No augmentation (basic inference)
   - Level 1: Horizontal flip only
   - Level 2: Horizontal flip and translations
3. For Level 1 (Horizontal flip):
   a. Run the model on the original images
   b. Run the model on the horizontally flipped images
   c. Average the two predictions
4. For Level 2 (Horizontal flip and translations):
   a. Create 6 versions of each image:
      - Original
      - Horizontally flipped
      - Translated up-and-left by 1 pixel
      - Translated down-and-right by 1 pixel
      - Horizontally flipped version of the up-and-left translation
      - Horizontally flipped version of the down-and-right translation
   b. Run the model on all 6 versions
   c. Compute a weighted average of the predictions:
      - Weight the original and flipped versions by 0.25 each
      - Weight the four translated versions by 0.125 each
5. Ensure that the function handles batches of images efficiently.
6. Implement the function to work with models in evaluation mode.

Your function should have the following signature:
def test_time_augmentation(model: torch.nn.Module, images: torch.Tensor, tta_level: int) -> torch.Tensor:
    # Your implementation here

Test your implementation by applying it to a trained model and comparing the results with and without TTA.",

  "implement_training_loop_with_lr_schedule": "Implement a training loop with the triangular learning rate schedule as described in the paper:
1. Create a function that takes a model, train_loader, optimizer, and num_epochs as inputs.
2. Implement the triangular learning rate schedule:
   a. Start at 20% of the maximum learning rate
   b. Linearly increase to the maximum rate at 20% of total training steps
   c. Linearly decrease to 7% of the maximum rate by the end of training
3. Use torch.optim.lr_scheduler.LambdaLR to implement the schedule.
4. In the training loop:
   a. Iterate through the train_loader for the specified number of epochs
   b. For each batch:
      - Perform a forward pass
      - Compute the loss (use CrossEntropyLoss with label smoothing of 0.2)
      - Perform backward pass and optimizer step
      - Update the learning rate according to the schedule
   c. After every 5 steps, perform a Lookahead update if using Lookahead optimizer
5. Implement basic logging of training loss and accuracy.
6. Return the trained model and a dictionary containing training history.

Your function should have the following signature:
def train_with_lr_schedule(
    model: torch.nn.Module,
    train_loader: torch.utils.data.DataLoader,
    optimizer: torch.optim.Optimizer,
    num_epochs: int
) -> Tuple[torch.nn.Module, Dict[str, List[float]]]:
    # Your implementation here

Test your implementation by training a model and plotting the learning rate over time to verify the triangular schedule.",

  "implement_patch_whitening_initialization": "Implement the patch-whitening initialization technique for the first convolutional layer:
1. Create a function that takes a Conv2d layer and a batch of training images as inputs.
2. Extract 2x2 patches from the input images:
   a. Use torch.unfold to efficiently extract patches
   b. Reshape the patches to (num_patches, channels, 2, 2)
3. Compute the covariance matrix of the flattened patches:
   a. Flatten each patch to a vector
   b. Compute the mean of the patches and subtract it
   c. Compute the covariance matrix using torch.mm
4. Perform eigendecomposition on the covariance matrix:
   a. Use torch.linalg.eigh to get eigenvalues and eigenvectors
5. Compute the whitening matrix:
   a. Scale the eigenvectors by the inverse square root of the eigenvalues
   b. Add a small epsilon (e.g., 5e-4) to the eigenvalues for numerical stability
6. Initialize the convolutional layer weights:
   a. Set the first half of the filters to the whitening matrix
   b. Set the second half to the negation of the whitening matrix
7. Ensure that the weights of the layer are set to not require gradients.

Your function should have the following signature:
def init_whitening_conv(
    conv_layer: torch.nn.Conv2d,
    train_images: torch.Tensor,
    eps: float = 5e-4
) -> None:
    # Your implementation here

Test your implementation by applying it to the first layer of your custom CNN and visualizing the initialized filters to ensure they look like edge detectors."
}




====================


task_eval_baseline_implementation_json:
{
  "implement_alternating_flip": "```python\ndef random_flip(images: torch.Tensor) -> torch.Tensor:\n    flip_mask = (torch.rand(len(images)) < 0.5).view(-1, 1, 1, 1)\n    return torch.where(flip_mask, images.flip(-1), images)\n```",
  
  "implement_decoupled_hyperparameters": "```python\ndef basic_hyperparameters(hyperparams: Dict[str, float]) -> Dict[str, float]:\n    return hyperparams.copy()\n```",
  
  "implement_lookahead_optimizer": "```python\nclass BasicOptimizerWrapper(torch.optim.Optimizer):\n    def __init__(self, base_optimizer, model):\n        self.base_optimizer = base_optimizer\n        self.model = model\n\n    def step(self):\n        self.base_optimizer.step()\n\n    def state_dict(self):\n        return self.base_optimizer.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self.base_optimizer.load_state_dict(state_dict)\n```",
  
  "implement_custom_cnn_architecture": "```python\nclass BasicCNN(nn.Module):\n    def __init__(self, input_channels: int, num_classes: int):\n        super().__init__()\n        self.conv1 = nn.Conv2d(input_channels, 32, 3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc = nn.Linear(64 * 4 * 4, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = x.view(-1, 64 * 4 * 4)\n        x = self.fc(x)\n        return x\n```",
  
  "implement_test_time_augmentation": "```python\ndef basic_inference(model: nn.Module, images: torch.Tensor) -> torch.Tensor:\n    model.eval()\n    with torch.no_grad():\n        return model(images)\n```",
  
  "implement_training_loop_with_lr_schedule": "```python\ndef basic_training_loop(model: nn.Module, train_loader: DataLoader, optimizer: torch.optim.Optimizer, num_epochs: int):\n    criterion = nn.CrossEntropyLoss()\n    for epoch in range(num_epochs):\n        for inputs, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n    return model\n```",
  
  "implement_patch_whitening_initialization": "```python\ndef kaiming_init_conv(conv_layer: nn.Conv2d):\n    nn.init.kaiming_normal_(conv_layer.weight, mode='fan_out', nonlinearity='relu')\n    if conv_layer.bias is not None:\n        nn.init.constant_(conv_layer.bias, 0)\n```"
}




====================


task_eval_correctness_scoring_json:
{
  "implement_alternating_flip": "```python
def score_alternating_flip(implementation, baseline):
    score = 0.0
    # Test flip pattern
    images = torch.randn(1000, 3, 32, 32)
    indices = torch.arange(1000)
    flipped_images = [implementation(images, indices, epoch) for epoch in range(10)]
    correct_pattern = all(
        torch.allclose(flipped_images[0], flipped_images[2*i]) and
        torch.allclose(flipped_images[1], flipped_images[2*i+1])
        for i in range(1, 5)
    )
    score += 0.5 if correct_pattern else 0.0

    # Test efficiency
    start_time = time.time()
    for _ in range(100):
        implementation(images, indices, 0)
    impl_time = time.time() - start_time

    start_time = time.time()
    for _ in range(100):
        baseline(images)
    base_time = time.time() - start_time

    score += 0.3 if impl_time <= base_time else 0.3 * (base_time / impl_time)

    # Check memory usage
    torch.cuda.empty_cache()
    start_mem = torch.cuda.memory_allocated()
    implementation(images, indices, 0)
    end_mem = torch.cuda.memory_allocated()
    score += 0.2 if (end_mem - start_mem) <= (images.numel() * 4 + 1000 * 4) else 0.0

    return score
```",

  "implement_decoupled_hyperparameters": "```python
def score_decoupled_hyperparameters(implementation):
    score = 0.0
    test_cases = [
        {'batch_size': 128, 'learning_rate': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001},
        {'batch_size': 256, 'learning_rate': 0.01, 'momentum': 0.99, 'weight_decay': 0.0005},
        {'batch_size': 64, 'learning_rate': 1.0, 'momentum': 0.0, 'weight_decay': 0.0},
    ]
    
    for case in test_cases:
        result = implementation(case)
        expected = {
            'batch_size': case['batch_size'],
            'learning_rate': case['learning_rate'],
            'momentum': case['momentum'],
            'weight_decay': case['weight_decay'],
            'decoupled_lr': case['learning_rate'] / (case['batch_size'] * (1 + 1 / (1 - case['momentum']))),
            'decoupled_wd': case['weight_decay'] * case['batch_size'] / (1 + 1 / (1 - case['momentum'])),
            'bias_lr_scale': 64.0
        }
        if all(torch.isclose(torch.tensor(result[k]), torch.tensor(expected[k])) for k in expected):
            score += 0.25
        
    # Check edge case (momentum = 1.0)
    edge_case = {'batch_size': 128, 'learning_rate': 0.1, 'momentum': 1.0, 'weight_decay': 0.0001}
    try:
        result = implementation(edge_case)
        if 'decoupled_lr' in result and 'decoupled_wd' in result:
            score += 0.25
    except:
        pass

    return score
```",

  "implement_lookahead_optimizer": "```python
def score_lookahead_optimizer(implementation):
    score = 0.0
    model = torch.nn.Linear(10, 1)
    base_opt = torch.optim.SGD(model.parameters(), lr=0.1)
    lookahead_opt = implementation(base_opt, model, lookahead_steps=5, lookahead_alpha=0.8)

    # Test slow weights update
    initial_weights = model.weight.clone()
    for i in range(10):
        lookahead_opt.step()
        if i == 4:
            if not torch.allclose(model.weight, initial_weights):
                score += 0.3
            slow_weights = model.weight.clone()
        if i == 5:
            if torch.allclose(model.weight, slow_weights):
                score += 0.3

    # Test state dict
    state = lookahead_opt.state_dict()
    new_opt = implementation(base_opt, model, lookahead_steps=5, lookahead_alpha=0.8)
    new_opt.load_state_dict(state)
    if torch.allclose(new_opt.state[model.weight]['slow_buffer'], lookahead_opt.state[model.weight]['slow_buffer']):
        score += 0.2

    # Test convergence
    def train(optimizer, steps):
        losses = []
        for _ in range(steps):
            def closure():
                optimizer.zero_grad()
                output = model(torch.randn(32, 10))
                loss = output.pow(2).sum()
                loss.backward()
                return loss
            loss = optimizer.step(closure)
            losses.append(loss.item())
        return losses

    base_losses = train(base_opt, 100)
    lookahead_losses = train(lookahead_opt, 100)
    if sum(lookahead_losses) < sum(base_losses):
        score += 0.2

    return score
```",

  "implement_custom_cnn_architecture": "```python
def score_custom_cnn(implementation):
    score = 0.0
    model = implementation(3, 10)

    # Check model structure
    expected_layers = [
        torch.nn.Conv2d, torch.nn.GELU,
        torch.nn.Conv2d, torch.nn.MaxPool2d, torch.nn.BatchNorm2d, torch.nn.GELU,
        torch.nn.Conv2d, torch.nn.BatchNorm2d, torch.nn.GELU,
        torch.nn.Conv2d, torch.nn.MaxPool2d, torch.nn.BatchNorm2d, torch.nn.GELU,
        torch.nn.Conv2d, torch.nn.BatchNorm2d, torch.nn.GELU,
        torch.nn.Conv2d, torch.nn.MaxPool2d, torch.nn.BatchNorm2d, torch.nn.GELU,
        torch.nn.Conv2d, torch.nn.BatchNorm2d, torch.nn.GELU,
        torch.nn.MaxPool2d, torch.nn.Flatten, torch.nn.Linear
    ]
    if all(isinstance(m, expected) for m, expected in zip(model.modules(), expected_layers)):
        score += 0.3

    # Check initialization
    for m in model.modules():
        if isinstance(m, torch.nn.Conv2d):
            if m.weight.requires_grad == (m != model[0]):
                score += 0.1
            if torch.allclose(m.weight[:m.in_channels], torch.eye(m.in_channels).view(m.in_channels, m.in_channels, 1, 1)):
                score += 0.1

    # Check forward pass
    x = torch.randn(1, 3, 32, 32)
    try:
        out = model(x)
        if out.shape == (1, 10):
            score += 0.2
    except:
        pass

    # Check parameter count
    if sum(p.numel() for p in model.parameters()) < 2e6:
        score += 0.2

    return score
```",

  "implement_test_time_augmentation": "```python
def score_test_time_augmentation(implementation):
    score = 0.0
    model = torch.nn.Sequential(
        torch.nn.Conv2d(3, 16, 3, padding=1),
        torch.nn.ReLU(),
        torch.nn.AdaptiveAvgPool2d(1),
        torch.nn.Flatten(),
        torch.nn.Linear(16, 10)
    )
    images = torch.randn(10, 3, 32, 32)

    # Check TTA levels
    for level in [0, 1, 2]:
        out = implementation(model, images, level)
        if out.shape == (10, 10):
            score += 0.1

    # Check augmentations
    model.eval()
    with torch.no_grad():
        base_out = model(images)
        tta_out = implementation(model, images, 2)
        if not torch.allclose(base_out, tta_out) and tta_out.std() < base_out.std():
            score += 0.3

    # Check efficiency
    start_time = time.time()
    for _ in range(10):
        implementation(model, images, 2)
    tta_time = time.time() - start_time

    start_time = time.time()
    for _ in range(10):
        model(images)
    base_time = time.time() - start_time

    if tta_time < 6 * base_time:
        score += 0.3

    return score
```",

  "implement_training_loop_with_lr_schedule": "```python
def score_training_loop_with_lr_schedule(implementation):
    score = 0.0
    model = torch.nn.Linear(10, 2)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
    train_loader = [(torch.randn(32, 10), torch.randint(0, 2, (32,))) for _ in range(100)]

    trained_model, history = implementation(model, train_loader, optimizer, num_epochs=5)

    # Check learning rate schedule
    if 'lr' in history:
        lr_values = history['lr']
        if lr_values[0] < lr_values[len(lr_values)//5] > lr_values[-1]:
            score += 0.3

    # Check loss decrease
    if 'loss' in history and history['loss'][0] > history['loss'][-1]:
        score += 0.2

    # Check accuracy increase
    if 'accuracy' in history and history['accuracy'][0] < history['accuracy'][-1]:
        score += 0.2

    # Check if Lookahead is used (assuming it's wrapped around the optimizer)
    if hasattr(optimizer, 'base_optimizer'):
        score += 0.1

    # Check model improvement
    x = torch.randn(100, 10)
    y = (x.sum(dim=1) > 0).long()
    initial_loss = torch.nn.functional.cross_entropy(model(x), y)
    final_loss = torch.nn.functional.cross_entropy(trained_model(x), y)
    if final_loss < initial_loss:
        score += 0.2

    return score
```",

  "implement_patch_whitening_initialization": "```python
def score_patch_whitening_initialization(implementation):
    score = 0.0
    conv_layer = torch.nn.Conv2d(3, 24, kernel_size=2, bias=False)
    train_images = torch.randn(1000, 3, 32, 32)

    implementation(conv_layer, train_images)

    # Check if weights are set to not require gradients
    if not conv_layer.weight.requires_grad:
        score += 0.2

    # Check if the weights form a whitening transformation
    patches = train_images.unfold(2, 2, 1).unfold(3, 2, 1).reshape(-1, 3*2*2)
    whitened = torch.mm(patches, conv_layer.weight.reshape(24, -1).t())
    cov = torch.mm(whitened.t(), whitened) / whitened.size(0)
    if torch.allclose(cov, torch.eye(24), atol=1e-2):
        score += 0.4

    # Check if half of the filters are negations of the other half
    if torch.allclose(conv_layer.weight[:12], -conv_layer.weight[12:], atol=1e-6):
        score += 0.2

    # Compare initial training steps against standard initialization
    def train_steps(model, steps):
        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
        losses = []
        for _ in range(steps):
            optimizer.zero_grad()
            loss = model(torch.randn(32, 3, 32, 32)).pow(2).sum()
            loss.backward()
            optimizer.step()
            losses.append(loss.item())
        return sum(losses)

    whitened_model = torch.nn.Sequential(conv_layer, torch.nn.Flatten(), torch.nn.Linear(24*31*31, 10))
    standard_model = torch.nn.Sequential(torch.nn.Conv2d(3, 24, kernel_size=2, bias=False), torch.nn.Flatten(), torch.nn.Linear(24*31*31, 10))

    if train_steps(whitened_model, 100) < train_steps(standard_model, 100):
        score += 0.2

    return score
```"
}




====================


task_eval_metric_scoring_json:
{
  "implement_alternating_flip": "```python
def score_alternating_flip_metrics(implementation, baseline, num_images=10000, num_epochs=10):
    images = torch.randn(num_images, 3, 32, 32)
    indices = torch.arange(num_images)
    
    # Measure efficiency improvement
    start_time = time.time()
    for epoch in range(num_epochs):
        implementation(images, indices, epoch)
    impl_time = time.time() - start_time
    
    start_time = time.time()
    for _ in range(num_epochs):
        baseline(images)
    base_time = time.time() - start_time
    
    speedup = base_time / impl_time
    
    # Measure memory efficiency
    torch.cuda.empty_cache()
    start_mem = torch.cuda.memory_allocated()
    implementation(images, indices, 0)
    impl_mem = torch.cuda.memory_allocated() - start_mem
    
    torch.cuda.empty_cache()
    start_mem = torch.cuda.memory_allocated()
    baseline(images)
    base_mem = torch.cuda.memory_allocated() - start_mem
    
    mem_efficiency = base_mem / impl_mem
    
    # Calculate score (higher is better)
    score = min(1.0, speedup / 1.5) * 0.7 + min(1.0, mem_efficiency / 1.2) * 0.3
    return score
```",

  "implement_decoupled_hyperparameters": "```python
def score_decoupled_hyperparameters_metrics(implementation):
    test_cases = [
        {'batch_size': 128, 'learning_rate': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001},
        {'batch_size': 256, 'learning_rate': 0.01, 'momentum': 0.99, 'weight_decay': 0.0005},
        {'batch_size': 64, 'learning_rate': 1.0, 'momentum': 0.0, 'weight_decay': 0.0},
    ]
    
    score = 0.0
    for case in test_cases:
        result = implementation(case)
        expected = {
            'decoupled_lr': case['learning_rate'] / (case['batch_size'] * (1 + 1 / (1 - case['momentum']))),
            'decoupled_wd': case['weight_decay'] * case['batch_size'] / (1 + 1 / (1 - case['momentum'])),
        }
        score += sum(torch.isclose(torch.tensor(result[k]), torch.tensor(expected[k])) for k in expected) / (2 * len(test_cases))
    
    # Test training improvement
    model = torch.nn.Linear(10, 1)
    x = torch.randn(1000, 10)
    y = torch.randn(1000, 1)
    
    def train(hyperparams, epochs=100):
        model.reset_parameters()
        optimizer = torch.optim.SGD(model.parameters(), **hyperparams)
        losses = []
        for _ in range(epochs):
            optimizer.zero_grad()
            loss = torch.nn.functional.mse_loss(model(x), y)
            loss.backward()
            optimizer.step()
            losses.append(loss.item())
        return losses[-1]
    
    standard_loss = train(test_cases[0])
    decoupled_loss = train(implementation(test_cases[0]))
    
    loss_improvement = (standard_loss - decoupled_loss) / standard_loss
    score += min(1.0, loss_improvement / 0.1)  # Expect up to 10% improvement
    
    return score / 2  # Normalize to [0, 1]
```",

  "implement_lookahead_optimizer": "```python
def score_lookahead_optimizer_metrics(implementation):
    model = torch.nn.Linear(10, 1)
    x = torch.randn(1000, 10)
    y = torch.randn(1000, 1)
    base_optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
    lookahead_optimizer = implementation(base_optimizer, model, lookahead_steps=5, lookahead_alpha=0.8)
    
    def train(optimizer, epochs=100):
        model.reset_parameters()
        losses = []
        for _ in range(epochs):
            optimizer.zero_grad()
            loss = torch.nn.functional.mse_loss(model(x), y)
            loss.backward()
            optimizer.step()
            losses.append(loss.item())
        return losses
    
    base_losses = train(base_optimizer)
    lookahead_losses = train(lookahead_optimizer)
    
    # Measure convergence speed improvement
    base_min_loss = min(base_losses)
    lookahead_min_loss = min(lookahead_losses)
    convergence_improvement = (base_min_loss - lookahead_min_loss) / base_min_loss
    
    # Measure final loss improvement
    final_loss_improvement = (base_losses[-1] - lookahead_losses[-1]) / base_losses[-1]
    
    # Calculate score (higher is better)
    score = min(1.0, convergence_improvement / 0.2) * 0.5 + min(1.0, final_loss_improvement / 0.1) * 0.5
    return score
```",

  "implement_custom_cnn_architecture": "```python
def score_custom_cnn_metrics(implementation):
    model = implementation(3, 10)
    
    # Measure parameter efficiency
    param_count = sum(p.numel() for p in model.parameters())
    param_efficiency = min(1.0, 2e6 / param_count)  # 2M parameters as reference
    
    # Measure forward pass speed
    x = torch.randn(128, 3, 32, 32)
    start_time = time.time()
    for _ in range(100):
        model(x)
    forward_time = (time.time() - start_time) / 100
    speed_score = min(1.0, 0.01 / forward_time)  # 10ms as reference
    
    # Measure accuracy on CIFAR-10
    transform = torchvision.transforms.Compose([
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
    ])
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)
    
    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    accuracy = correct / total
    accuracy_score = min(1.0, accuracy / 0.94)  # 94% as reference
    
    # Calculate overall score
    score = param_efficiency * 0.3 + speed_score * 0.3 + accuracy_score * 0.4
    return score
```",

  "implement_test_time_augmentation": "```python
def score_test_time_augmentation_metrics(implementation):
    model = torchvision.models.resnet18(pretrained=True)
    model.eval()
    
    transform = torchvision.transforms.Compose([
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
    ])
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)
    
    def evaluate(use_tta):
        correct = 0
        total = 0
        with torch.no_grad():
            for data in testloader:
                images, labels = data
                if use_tta:
                    outputs = implementation(model, images, tta_level=2)
                else:
                    outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        return correct / total
    
    base_accuracy = evaluate(False)
    tta_accuracy = evaluate(True)
    
    accuracy_improvement = (tta_accuracy - base_accuracy) / base_accuracy
    
    # Measure inference time
    images = torch.randn(100, 3, 32, 32)
    start_time = time.time()
    for _ in range(10):
        implementation(model, images, tta_level=2)
    tta_time = (time.time() - start_time) / 10
    
    start_time = time.time()
    for _ in range(10):
        model(images)
    base_time = (time.time() - start_time) / 10
    
    time_overhead = tta_time / base_time
    
    # Calculate score (higher is better)
    score = min(1.0, accuracy_improvement / 0.02) * 0.7 + min(1.0, 6 / time_overhead) * 0.3
    return score
```",

  "implement_training_loop_with_lr_schedule": "```python
def score_training_loop_with_lr_schedule_metrics(implementation):
    model = torchvision.models.resnet18(num_classes=10)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
    
    transform_train = torchvision.transforms.Compose([
        torchvision.transforms.RandomCrop(32, padding=4),
        torchvision.transforms.RandomHorizontalFlip(),
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
    ])
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)
    
    trained_model, history = implementation(model, trainloader, optimizer, num_epochs=200)
    
    # Check learning rate schedule
    lr_score = 0
    if 'lr' in history:
        lr_values = history['lr']
        if lr_values[0] < lr_values[len(lr_values)//5] > lr_values[-1]:
            lr_score = 1.0
    
    # Check final accuracy
    transform_test = torchvision.transforms.Compose([
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
    ])
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)
    
    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            outputs = trained_model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    accuracy = correct / total
    accuracy_score = min(1.0, accuracy / 0.94)  # 94% as reference
    
    # Calculate overall score
    score = lr_score * 0.3 + accuracy_score * 0.7
    return score
```",

  "implement_patch_whitening_initialization": "```python
def score_patch_whitening_initialization_metrics(implementation):
    conv_layer = torch.nn.Conv2d(3, 24, kernel_size=2, bias=False)
    train_images = torch.randn(1000, 3, 32, 32)
    implementation(conv_layer, train_images)
    
    # Check if the weights form a whitening transformation
    patches = train_images.unfold(2, 2, 1).unfold(3, 2, 1).reshape(-1, 3*2*2)
    whitened = torch.mm(patches, conv_layer.weight.reshape(24, -1).t())
    cov = torch.mm(whitened.t(), whitened) / whitened.size(0)
    whitening_score = 1.0 - torch.norm(cov - torch.eye(24)).item() / 5  # Normalize by expected max difference
    
    # Compare training performance
    def create_model(init_func):
        model = torch.nn.Sequential(
            torch.nn.Conv2d(3, 24, kernel_size=2, bias=False),
            torch.nn.ReLU(),
            torch.nn.Flatten(),
            torch.nn.Linear(24*31*31, 10)
        )
        init_func(model[0], train_images)
        return model
    
    def train_model(model, steps=1000):
        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
        criterion = torch.nn.CrossEntropyLoss()
        x = torch.randn(128, 3, 32, 32)
        y = torch.randint(0, 10, (128,))
        losses = []
        for _ in range(steps):
            optimizer.zero_grad()
            loss = criterion(model(x), y)
            loss.backward()
            optimizer.step()
            losses.append(loss.item())
        return losses
    
    whitened_model = create_model(implementation)
    standard_model = create_model(lambda c, _: torch.nn.init.kaiming_normal_(c.weight))
    
    whitened_losses = train_model(whitened_model)
    standard_losses = train_model(standard_model)
    
    convergence_improvement = (sum(standard_losses) - sum(whitened_losses)) / sum(standard_losses)
    
    # Calculate overall score
    score = whitening_score * 0.5 + min(1.0, convergence_improvement / 0.2) * 0.5
    return score
```"
}

This JSON provides metric scoring functions for each task, focusing on the key performance aspects highlighted in the paper. The functions compare the implementation's performance to baselines or expected results, providing continuous scores where possible.




====================


task_eval_combined_scoring_json:
{
  "implement_alternating_flip": "```python
def score_alternating_flip_combined(implementation, baseline):
    try:
        correctness_score = score_alternating_flip(implementation, baseline)
        metric_score = score_alternating_flip_metrics(implementation, baseline)
        return 0.6 * correctness_score + 0.4 * metric_score
    except Exception as e:
        print(f'Error in scoring alternating flip: {e}')
        return 0.0
```",

  "implement_decoupled_hyperparameters": "```python
def score_decoupled_hyperparameters_combined(implementation):
    try:
        correctness_score = score_decoupled_hyperparameters(implementation)
        metric_score = score_decoupled_hyperparameters_metrics(implementation)
        return 0.6 * correctness_score + 0.4 * metric_score
    except Exception as e:
        print(f'Error in scoring decoupled hyperparameters: {e}')
        return 0.0
```",

  "implement_lookahead_optimizer": "```python
def score_lookahead_optimizer_combined(implementation):
    try:
        correctness_score = score_lookahead_optimizer(implementation)
        metric_score = score_lookahead_optimizer_metrics(implementation)
        return 0.6 * correctness_score + 0.4 * metric_score
    except Exception as e:
        print(f'Error in scoring lookahead optimizer: {e}')
        return 0.0
```",

  "implement_custom_cnn_architecture": "```python
def score_custom_cnn_combined(implementation):
    try:
        correctness_score = score_custom_cnn(implementation)
        metric_score = score_custom_cnn_metrics(implementation)
        return 0.6 * correctness_score + 0.4 * metric_score
    except Exception as e:
        print(f'Error in scoring custom CNN architecture: {e}')
        return 0.0
```",

  "implement_test_time_augmentation": "```python
def score_test_time_augmentation_combined(implementation):
    try:
        correctness_score = score_test_time_augmentation(implementation)
        metric_score = score_test_time_augmentation_metrics(implementation)
        return 0.6 * correctness_score + 0.4 * metric_score
    except Exception as e:
        print(f'Error in scoring test-time augmentation: {e}')
        return 0.0
```",

  "implement_training_loop_with_lr_schedule": "```python
def score_training_loop_with_lr_schedule_combined(implementation):
    try:
        correctness_score = score_training_loop_with_lr_schedule(implementation)
        metric_score = score_training_loop_with_lr_schedule_metrics(implementation)
        return 0.6 * correctness_score + 0.4 * metric_score
    except Exception as e:
        print(f'Error in scoring training loop with LR schedule: {e}')
        return 0.0
```",

  "implement_patch_whitening_initialization": "```python
def score_patch_whitening_initialization_combined(implementation):
    try:
        correctness_score = score_patch_whitening_initialization(implementation)
        metric_score = score_patch_whitening_initialization_metrics(implementation)
        return 0.6 * correctness_score + 0.4 * metric_score
    except Exception as e:
        print(f'Error in scoring patch whitening initialization: {e}')
        return 0.0
```"
}




====================


task_setup_script:
```bash
#!/bin/bash

# Help message
if [ "$#" -ne 1 ]; then
    echo "Usage: $0 <output_directory>"
    exit 1
fi

# Set variables
OUTPUT_DIR="$1"
VENV_DIR="$OUTPUT_DIR/venv"

# Create output directory if it doesn't exist
mkdir -p "$OUTPUT_DIR"

# Function to create files
create_files() {
    # Create instructions.txt
    cat > "$OUTPUT_DIR/instructions.txt" << EOL
Task Instructions:

1. Implement Alternating Flip:
${Task Eval Instructions Json["implement_alternating_flip"]}

2. Implement Decoupled Hyperparameters:
${Task Eval Instructions Json["implement_decoupled_hyperparameters"]}

3. Implement Lookahead Optimizer:
${Task Eval Instructions Json["implement_lookahead_optimizer"]}

4. Implement Custom CNN Architecture:
${Task Eval Instructions Json["implement_custom_cnn_architecture"]}

5. Implement Test-Time Augmentation:
${Task Eval Instructions Json["implement_test_time_augmentation"]}

6. Implement Training Loop with LR Schedule:
${Task Eval Instructions Json["implement_training_loop_with_lr_schedule"]}

7. Implement Patch Whitening Initialization:
${Task Eval Instructions Json["implement_patch_whitening_initialization"]}
EOL

    # Create solution.py
    cat > "$OUTPUT_DIR/solution.py" << EOL
import torch
import torch.nn as nn
import torch.nn.functional as F

# Baseline implementations

${Task Eval Baseline Implementation Json["implement_alternating_flip"]}

${Task Eval Baseline Implementation Json["implement_decoupled_hyperparameters"]}

${Task Eval Baseline Implementation Json["implement_lookahead_optimizer"]}

${Task Eval Baseline Implementation Json["implement_custom_cnn_architecture"]}

${Task Eval Baseline Implementation Json["implement_test_time_augmentation"]}

${Task Eval Baseline Implementation Json["implement_training_loop_with_lr_schedule"]}

${Task Eval Baseline Implementation Json["implement_patch_whitening_initialization"]}

# TODO: Implement your solutions here
EOL

    # Create scoring.py
    cat > "$OUTPUT_DIR/scoring.py" << EOL
import torch
import torch.nn as nn
import torch.nn.functional as F
import time
import torchvision

# Correctness scoring functions

${Task Eval Correctness Scoring Json["implement_alternating_flip"]}

${Task Eval Correctness Scoring Json["implement_decoupled_hyperparameters"]}

${Task Eval Correctness Scoring Json["implement_lookahead_optimizer"]}

${Task Eval Correctness Scoring Json["implement_custom_cnn_architecture"]}

${Task Eval Correctness Scoring Json["implement_test_time_augmentation"]}

${Task Eval Correctness Scoring Json["implement_training_loop_with_lr_schedule"]}

${Task Eval Correctness Scoring Json["implement_patch_whitening_initialization"]}

# Metric scoring functions

${Task Eval Metric Scoring Json["implement_alternating_flip"]}

${Task Eval Metric Scoring Json["implement_decoupled_hyperparameters"]}

${Task Eval Metric Scoring Json["implement_lookahead_optimizer"]}

${Task Eval Metric Scoring Json["implement_custom_cnn_architecture"]}

${Task Eval Metric Scoring Json["implement_test_time_augmentation"]}

${Task Eval Metric Scoring Json["implement_training_loop_with_lr_schedule"]}

${Task Eval Metric Scoring Json["implement_patch_whitening_initialization"]}

# Combined scoring functions

${Task Eval Combined Scoring Json["implement_alternating_flip"]}

${Task Eval Combined Scoring Json["implement_decoupled_hyperparameters"]}

${Task Eval Combined Scoring Json["implement_lookahead_optimizer"]}

${Task Eval Combined Scoring Json["implement_custom_cnn_architecture"]}

${Task Eval Combined Scoring Json["implement_test_time_augmentation"]}

${Task Eval Combined Scoring Json["implement_training_loop_with_lr_schedule"]}

${Task Eval Combined Scoring Json["implement_patch_whitening_initialization"]}
EOL

    # Create requirements.txt
    cat > "$OUTPUT_DIR/requirements.txt" << EOL
torch
torchvision
numpy
EOL
}

# Function to set up Python environment
setup_python_env() {
    python3 -m venv "$VENV_DIR"
    source "$VENV_DIR/bin/activate"
    pip install -r "$OUTPUT_DIR/requirements.txt"
}

# Main execution
main() {
    echo "Setting up evaluation environment in $OUTPUT_DIR"
    
    create_files
    if [ $? -ne 0 ]; then
        echo "Error: Failed to create necessary files"
        exit 1
    fi
    
    setup_python_env
    if [ $? -ne 0 ]; then
        echo "Error: Failed to set up Python environment"
        exit 1
    fi
    
    echo "Setup complete. Evaluation environment is ready in $OUTPUT_DIR"
}

# Run the script
main
```

This script creates the necessary files, sets up a Python virtual environment, and installs the required packages. It uses the task information provided in the context to populate the files with instructions, baseline implementations, and scoring functions. The script also includes error handling and provides a help message when run without arguments.




====================

