```
from dataclasses import dataclass
from dspy.primitives.program import Module
from typing import Any, Optional


@dataclass
class SigData:
    name: str
    desc: str

@dataclass
class SigStep:
    outputs: list[SigData]
    module: Module = dspy.ChainOfThought
    kwargs: Optional[dict[str, Any]] = None

@dataclass
class SigChain:
    inputs: list[SigData]
    steps: list[SigStep]

paper_chain = SigChain(
    inputs=[SigData("paper_content", "The full text content of the research paper")],
    steps=[
        SigStep([
                SigData("title", "The title of the paper"),
                SigData("abstract_plus", "The abstract of the paper, plus optional additional high-level summaries to capture interesting aspects of the paper not otherwise covered in the abstract"),
        ],),
        SigStep([
            SigData(
                "core_ideas", 
                """
                Enumerate the key innovative ideas in the paper that led to specific, implementable improvements over baseline methods. For each idea:
                1. Briefly describe the idea
                2. Explain how it improves upon a baseline approach
                3. Mention the specific metric(s) it aims to improve

                Example:
                1. Idea: Skip connections in neural networks
                - Description: Direct connections between non-adjacent layers
                - Improvement: Allows better gradient flow in deep networks
                - Target metrics: Training speed, final accuracy
                """
            ),
        ],),
        SigStep([
            SigData(
                "metrics",
                """
                For each metric used in the paper:
                1. Provide a clear, implementable definition
                2. Specify the exact calculation method
                3. Describe the typical range of values and what they indicate
                4. Note any specific conditions or datasets used for measurement
                5. List which experimental methods target this metric
                6. Provide a simple Python function to calculate the metric, taking only the experimental method as an argument

                Example 1: Perplexity
                1. Definition: A measurement of how well a probability model predicts a sample, used in NLP to evaluate language models.
                2. Calculation: Perplexity = exp(cross_entropy_loss)
                3. Typical range: 
                    - Lower is better
                    - For word-level language models on PTB: 50-100 is good, <60 is state-of-the-art
                4. Conditions: Calculated on held-out test data, using the Penn Treebank (PTB) dataset
                5. Targeted by: 
                    - Experimental Method 1: Sparse Attention
                    - Experimental Method 3: Adaptive Layer Normalization
                6. Python function:
                ```python
                import torch
                import torch.nn.functional as F

                # Preset constants
                test_data = load_ptb_test_data()  # Function to load PTB test data
                vocab_size = 10000  # PTB vocabulary size

                def calculate_perplexity(experimental_method):
                    model = experimental_method()  # Assuming the method returns an instantiated model
                    model.eval()
                    total_loss = 0
                    total_tokens = 0
                    with torch.no_grad():
                        for batch in test_data:
                            inputs, targets = batch
                            outputs = model(inputs)
                            loss = F.cross_entropy(outputs.view(-1, vocab_size), targets.view(-1), reduction='sum')
                            total_loss += loss.item()
                            total_tokens += targets.numel()
                    return torch.exp(total_loss / total_tokens)
                ```

                Example 2: BLEU Score (for machine translation)
                1. Definition: Bilingual Evaluation Understudy, measures the similarity between machine-generated translations and reference translations.
                2. Calculation: Geometric mean of n-gram precisions (typically n=1 to 4), with a brevity penalty.
                3. Typical range: 
                    - 0 to 100, higher is better
                    - 30-50 is considered good for many language pairs
                4. Conditions: Calculated on the WMT14 English-German test set
                5. Targeted by:
                    - Experimental Method 2: Multi-head Attention Pruning
                    - Experimental Method 4: Gradient Accumulation Training
                6. Python function:
                ```python
                from nltk.translate.bleu_score import corpus_bleu

                # Preset constants
                test_data = load_wmt14_en_de_test_data()  # Function to load WMT14 En-De test data
                reference_translations = load_reference_translations()  # Function to load reference translations

                def calculate_bleu(experimental_method):
                    model = experimental_method()  # Assuming the method returns an instantiated model
                    hypotheses = []
                    for source_sentence in test_data:
                        translation = model.translate(source_sentence)
                        hypotheses.append(translation)
                    return corpus_bleu([[ref] for ref in reference_translations], hypotheses) * 100
                ```

                Example 3: Inference Speed (tokens/second)
                1. Definition: The number of tokens processed per second during model inference.
                2. Calculation: (Number of tokens in test set) / (Total inference time)
                3. Typical range:
                    - Higher is better
                    - Varies greatly depending on model size and hardware; baseline might be 1000 tokens/sec
                4. Conditions: Measured on a single NVIDIA V100 GPU, using a fixed test set of 10,000 sentences
                5. Targeted by:
                    - Experimental Method 1: Sparse Attention
                    - Experimental Method 5: Quantization-Aware Training
                6. Python function:
                ```python
                import time

                # Preset constants
                test_sentences = load_test_sentences()  # Function to load 10,000 test sentences
                num_tokens = sum(len(sentence.split()) for sentence in test_sentences)

                def measure_inference_speed(experimental_method):
                    model = experimental_method()  # Assuming the method returns an instantiated model
                    model.eval()
                    start_time = time.time()
                    with torch.no_grad():
                        for sentence in test_sentences:
                            _ = model(sentence)
                    total_time = time.time() - start_time
                    return num_tokens / total_time
                ```
                """
            ),
        ],),
        SigStep([
            SigData(
                "baseline_methods", 
                """
                    Describe the key baseline methods that the paper's improvements build upon. For each method:
                        1. Provide a brief, high-level description
                        2. List the key components or steps of the method
                        3. Mention any specific architectural details, hyperparameters, or algorithmic choices that are crucial for implementation
                        4. Note which experimental methods modify this baseline

                    Example:
                        Baseline Method: Vanilla Transformer
                            1. Description: A sequence-to-sequence model based on self-attention mechanisms
                            2. Key components:
                                - Multi-head self-attention layers
                                - Feed-forward neural networks
                                - Layer normalization
                                - Positional encodings
                            3. Crucial details:
                                - 6 encoder and 6 decoder layers
                                - 8 attention heads
                                - 512-dimensional embeddings
                                - Learning rate warmup over 4000 steps
                            4. Modified by: Experimental methods 1 (sparse attention) and 2 (adaptive layer normalization)
                """
            ),
        ],),
        SigStep([
            SigData(
                "experimental_methods", 
                """
                Enumerate the key improvements or modifications proposed in the paper. For each method:
                    1. Identify the baseline method it modifies
                    2. Describe the specific changes or additions to the baseline
                    3. Explain how these changes are expected to improve performance
                    4. List the metrics this modification aims to improve

                Example 1:
                    1. Baseline: Vanilla Transformer
                    2. Modification: Sparse Attention Mechanism
                        - Replace full attention with sparse attention patterns
                        - Implement fixed or learned attention patterns
                    3. Expected improvements:
                        - Reduced computational complexity from O(nÂ²) to O(n log n)
                        - Better handling of long sequences
                    4. Target metrics: Inference speed, memory usage, performance on long-sequence tasks

                Example 2:
                    1. Baseline: Vanilla Transformer
                    2. Modification: Adaptive Layer Normalization
                        - Replace fixed layer norm with input-dependent dynamic scaling
                        - Implement a small network to predict normalization parameters
                    3. Expected improvements:
                        - Better adaptation to varying input distributions
                        - Improved gradient flow in deep networks
                    4. Target metrics: Training convergence speed, final model accuracy
                """
            ),
        ],),
        SigStep([
            SigData(
                "method_metric_results", 
                """
                For each baseline and experimental method, as well as combinations thereof, list the metric results that were reported in the paper. These should be quantitative results used to compare the methods to each other.
                """
            ),
        ],),
        SigStep([
            SigData(
                "hw_agnostic_metrics", 
                """
                List metrics that correspond to the paper's metrics, but are not hardware-specific. These may be copied from input metrics where appropriate, or they may be generated if not otherwise available. 
                For example, if the paper reports time to train on a specific hardware setup, a hardware-agnostic metric might be number of samples used for training, or total FLOPS used to train to the same level of accuracy.
                Hardware agnostic metrics should be closely correlated with the hardware-specific metrics and indicate the same relative performance of different methods.
                """
            ),
        ],),
        SigStep([
            SigData(
                "tasks", 
                """
                For each experimental method, describe a programming task that implements the improvement. Each task should:
                    1. Identify the baseline implementation to be modified
                    2. Clearly state the specific modification or addition to be made
                    3. Specify the inputs and outputs of the function, including types
                    4. Describe any key constraints or requirements for the implementation
                    5. Mention how the improvement's effect could be measured or observed in the output

                Example:
                    Task: Implement Sparse Attention in Transformer
                        1. Baseline: transformer.py containing a standard Transformer implementation
                        2. Modification: Implement a sparse attention mechanism in the MultiHeadAttention class
                        3. Inputs/Outputs:
                            - Input: torch.Tensor of shape (batch_size, seq_length, d_model)
                            - Output: torch.Tensor of same shape as input
                        4. Key requirements:
                            - Implement a fixed sparse attention pattern (e.g., local + global attention)
                            - Ensure the sparse attention matrix is properly masked and normalized
                            - Maintain compatibility with the rest of the Transformer architecture
                        5. Observable effects:
                            - Reduced memory usage for attention weights
                            - Faster computation time for forward and backward passes
                            - Similar or improved perplexity on language modeling tasks
                """
            ),
        ],),
        SigStep([
            SigData(
                "task_eval_planning",
                """

                For each task, plan an evaluation that tests the engineer's ability to implement the specific improvement. Include:
                    1. The baseline implementation to be provided
                    2. A clear description of the required modification
                    3. Sample inputs to test both the original functionality and the improvement
                    4. Expected changes in the output that demonstrate the improvement
                    5. Metrics to measure the implementation's correctness and performance gain

                Example:
                    Eval Plan: Sparse Attention Implementation
                        1. Baseline: Provide transformer.py with standard MultiHeadAttention class
                        2. Required modification: Implement sparse attention with local+global pattern
                        3. Sample inputs:
                            - Short sequence (length 50) to test preserved functionality
                            - Long sequence (length 1000) to test improvement
                        4. Expected changes:
                            - Similar output quality for short sequences
                            - Improved quality or efficiency for long sequences
                            - Sparse attention weight matrices in intermediate outputs
                        5. Evaluation metrics:
                            - Correctness: Compare attention patterns with expected sparsity
                            - Performance: Measure speed and memory usage improvements
                            - Quality: Compare perplexity on a held-out test set
                """
            ),
        ],),
        SigStep([
            SigData(
                "task_eval_baseline_implementation", 
                """
                For each task eval, if applicable, provide a baseline implementation that will be given to the engineer to modify. This should be in python. If any ML is required, 
                assume that the engineer has access to a python environment with GPUs, and prefer to use torch for ML tasks. Other common libraries may be used as well, but
                the implementation should be as simple as possible. The baseline implementation should be as close to the paper's method as possible, and allow the engineer 
                to focus on the specific modification required by the task. If this task does not require a baseline implementation, simply indicate that instead.
                """
            ),
        ],),
        SigStep([
            SigData(
                "task_eval_instructions", 
                """
                For each enumerated task, describe the instructions that will be given to the engineer to complete the task. Instructions should be clear and concise,
                identify the inputs and outputs, and provide any additional information that the engineer will need to complete the task. If the task involves modifying a baseline implementation,
                the instructions should clearly describe the modification to be made, and may refer to the baseline implementation.
                """
            ),
        ]),
        SigStep([
            SigData(
                "task_eval_correctness_scoring", 
                """
                For each task, write a Python function to score the correctness of the implementation. This function should:
                    1. Take as input the engineer's modified function and the original baseline
                    2. Test both the preserved baseline functionality and the specific improvement
                    3. Check for the presence and correct implementation of the new feature
                    4. Verify that unrelated aspects of the baseline remain unchanged
                    5. Return a score between 0 and 1, with guidelines for partial credit

                Example:
                    ```python
                    def score_sparse_attention_implementation(modified_transformer, baseline_transformer):
                        score = 0.0
                        
                        # Test preserved functionality
                        short_input = torch.randn(32, 50, 512)
                        if torch.allclose(modified_transformer(short_input), baseline_transformer(short_input), atol=1e-5):
                            score += 0.3
                        
                        # Test improvement
                        long_input = torch.randn(32, 1000, 512)
                        modified_output = modified_transformer(long_input)
                        
                        # Check for sparse attention pattern
                        attention_weights = modified_transformer.encoder.layers[0].self_attn.attn_weights
                        if attention_weights.float().to_dense().count_nonzero() / attention_weights.numel() < 0.2:
                            score += 0.4
                        
                        # Check for improved efficiency
                        start_time = time.time()
                        modified_transformer(long_input)
                        modified_time = time.time() - start_time
                        
                        start_time = time.time()
                        baseline_transformer(long_input)
                        baseline_time = time.time() - start_time
                        
                        if modified_time < 0.8 * baseline_time:
                            score += 0.3
                        
                        return score
                    ```
                """
            ),
        ]),
        SigStep([
            SigData(
                "task_eval_metric_scoring", 
                """
                For each task eval, write a python function that will be used to score the metric results of the implementation. The input will be the function written by the engineer,
                and the output will be a score between 0 and 1 indicating how well the implementation matches the expected results. This will typically be completed by comparing the output of the
                implementation to the output of the method in the paper for some known input. For example, if the task was to modify a neural-network-building function to add skip connections,
                the scoring function should check whether the improvement of the modified network over the baseline network matches the expected improvement. 
                """
            ),
        ]),
        SigStep([
            SigData(
                "implementation_difficulty", 
                """
                For each task, estimate the difficulty of implementing the improvement:
                    1. Rate the complexity on a scale of 1-5
                    2. Estimate the time required for an average ML engineer to implement
                    3. List any specific challenges or potential pitfalls
                    4. Suggest any helpful resources or references for implementation

                Example 1: Sparse Attention Implementation
                    1. Complexity: 4/5
                    2. Estimated time: 4-6 hours
                    3. Challenges and pitfalls:
                        - Efficient implementation of sparse matrix operations
                        - Ensuring correct backpropagation through sparse operations
                        - Maintaining numerical stability with sparse normalization
                    4. Helpful resources:
                        - "Generating Long Sequences with Sparse Transformers" (Child et al., 2019)
                        - PyTorch Sparse module documentation
                        - "Longformer: The Long-Document Transformer" (Beltagy et al., 2020)

                Example 2: Adaptive Layer Normalization
                1. Complexity: 3/5
                2. Estimated time: 2-3 hours
                3. Challenges and pitfalls:
                    - Ensuring the adaptive parameters are properly initialized
                    - Avoiding overfitting in the normalization parameter prediction
                    - Maintaining overall model stability during training
                4. Helpful resources:
                    - "Layer Normalization" (Ba et al., 2016)
                    - PyTorch nn.LayerNorm implementation
                    - "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift" (Ioffe & Szegedy, 2015)
                """
            ),
        ],),
        SigStep([
            SigData(
                "data_processing",
                """
                Describe any specific data processing techniques used in the paper:
                1. Data cleaning or preprocessing steps
                2. Feature engineering methods
                3. Data augmentation techniques
                4. Sampling or batching strategies

                Example 1: NLP Preprocessing for Transformer Model
                1. Data cleaning:
                    - Lowercase all text
                    - Remove special characters and extra whitespace
                    - Tokenization using SentencePiece with vocab size 32,000
                2. Feature engineering:
                    - Convert tokens to integer IDs
                    - Add special tokens: [CLS] at start, [SEP] at end of each sentence
                3. Data augmentation:
                    - Random word masking (15% of tokens)
                    - Random word replacement (10% of tokens)
                4. Batching strategy:
                    - Group sentences of similar length
                    - Pad to max length in batch
                    - Use attention mask to ignore padding

                Example 2: Image Processing for CNN
                1. Data cleaning:
                    - Remove corrupted images
                    - Resize all images to 224x224 pixels
                2. Feature engineering:
                    - Normalize pixel values to range [0, 1]
                    - Subtract mean and divide by std dev (per channel)
                3. Data augmentation:
                    - Random horizontal flips (50% probability)
                    - Random crops (224x224 from 256x256 padded image)
                    - Color jitter (brightness, contrast, saturation)
                4. Batching strategy:
                    - Random sampling
                    - Batch size of 32 for training, 64 for validation
                """
) 
        ]),            
        SigStep([
            SigData(
                "setup_description", 
                """
    Provide a detailed setup guide:
        1. List all required libraries and their minimum versions (and maximum only if necessary)
        2. Describe the necessary compute resources (GPU, RAM, etc.)
        3. Specify any datasets, including versions and how to access them
        4. Include a sample environment setup script
        5. Note any potential compatibility issues or common setup pitfalls

    Example:
    1. Required libraries:
       - PyTorch>=1.9.0
       - torchvision>=0.10.0
       - transformers>=4.9.2
       - numpy>=1.21.2
       - pandas>=1.3.3
       - matplotlib>=3.4.3
    2. Compute resources:
       - GPU: NVIDIA V100 or A100 (at least 16GB VRAM)
       - RAM: 32GB minimum, 64GB recommended
       - Storage: 100GB free space for datasets and model checkpoints
    3. Datasets:
       - ImageNet (ILSVRC2012), available from image-net.org
       - COCO 2017, available from cocodataset.org
    4. Environment setup:
    ```bash
    # Create and activate conda environment
    conda create -n paper_env python=3.8
    conda activate paper_env

    # Install PyTorch with CUDA support
    conda install pytorch torchvision cudatoolkit=11.1 -c pytorch -c nvidia

    # Install other dependencies
    pip install transformers numpy pandas matplotlib

    # Set up data directories
    mkdir -p data/{imagenet,coco}
    # Download datasets to respective directories
    ```
    5. Potential issues:
       - CUDA version mismatch: Ensure CUDA toolkit version matches PyTorch build
       - OOM errors: Reduce batch size or use gradient accumulation for large models
       - Dataset download issues: Use academic credentials for ImageNet access
                
                
                A description of the setup required before implementing the task functions. This should include a list of assets that need to be available to start.
                These may include datasets, trained models, training functions, etc. Assume that a python torch environment with GPUs is available. Any common libraries
                or public assets may be used (e.g. huggingface transformers, torchvision, etc.). If a dataset is required, it should be available in a public location.
                """
            ),
        ],),
    ],
)


def solve_sigchain(chain: SigChain, inputs=dict[str, str]) -> tuple[dict[str, Any], dict[str, Any]]:
    input_sigs = chain.inputs.copy()
    input_vals = inputs.copy()
    solvers = dict()
    results = dict()
    for step in chain.steps:
        step_name = "_".join([o.name for o in step.outputs])
        step_inputs_str = ", ".join([i.name for i in input_sigs])
        step_outputs_str = ", ".join([o.name for o in step.outputs])
        print(f"Running step {step_name}")
        print(f"  Inputs: {step_inputs_str}")
        print(f"  Outputs: {step_outputs_str}")
        class NextSig(dspy.Signature):
            pass
        for i in input_sigs:
            NextSig = NextSig.append(i.name, dspy.InputField(desc=i.desc))
        for o in step.outputs:
            NextSig = NextSig.append(o.name, dspy.OutputField(desc=o.desc))
        step_solver = step.module(NextSig, **(step.kwargs or {}))
        result = step_solver(**input_vals)
        for o in step.outputs:
            input_vals[o.name] = result[o.name]
        input_sigs.extend(step.outputs)
        solvers[step_name] = step_solver
        results[step_name] = result
    return solvers, results

```



title:
94% on CIFAR-10 in 3.29 Seconds on a Single GPU


abstract_plus:
This paper introduces ultra-fast training methods for the CIFAR-10 dataset, achieving 94% accuracy in just 3.29 seconds on a single NVIDIA A100 GPU. This represents a 1.9x speedup over the previous state-of-the-art. The authors also develop methods targeting 95% accuracy in 10.4 seconds and 96% accuracy in 46.3 seconds. A key innovation is the introduction of "alternating flip" data augmentation, which improves upon standard random flipping in most scenarios. The training methods are released as open-source code.

The paper provides a detailed analysis of how different components contribute to the overall speedup, including network architecture choices, optimization techniques, and the novel data augmentation. Experiments demonstrate that the developed methods generalize well to other datasets like CIFAR-100 and SVHN without additional tuning. The authors also investigate statistical properties of their training approach, showing low variance between runs and discussing trade-offs in calibration.

Interestingly, the paper notes that the progression from 80.5% accuracy in 2011 to the current 94% can be attributed entirely to algorithmic progress rather than increased computational power, as their method actually uses fewer FLOPs than earlier approaches. This work has potential implications for accelerating research and reducing the cost of experiments involving CIFAR-10 and similar datasets.




====================


core_ideas:
1. Alternating flip data augmentation
- Description: Instead of random flipping, images are flipped in a deterministic alternating pattern after the first epoch
- Improvement: Reduces redundancy in data augmentation, ensuring all unique inputs are seen every two epochs
- Target metrics: Training speed, final accuracy

2. Frozen patch-whitening initialization
- Description: Initialize the first convolutional layer as a patch-whitening transformation and freeze its weights during training
- Improvement: Provides a better starting point for the network, allowing faster convergence
- Target metrics: Training speed, final accuracy

3. Identity initialization for convolutions
- Description: Initialize convolutions after the first layer as partial identity transforms
- Improvement: Eases gradient flow in deep networks, allowing for faster training
- Target metrics: Training speed

4. Increased learning rate for BatchNorm biases
- Description: Scale up the learning rate for BatchNorm biases by a factor of 64
- Improvement: Allows for faster adaptation of normalization parameters
- Target metrics: Training speed

5. Lookahead optimization
- Description: Use the Lookahead optimizer, which maintains a slow-moving average of the weights
- Improvement: Provides more stable optimization, potentially allowing for higher learning rates
- Target metrics: Training stability, final accuracy

6. Multi-crop evaluation
- Description: Use multiple augmented views of test images during inference
- Improvement: Provides more robust predictions by averaging over different views
- Target metrics: Final accuracy

7. Decoupled hyperparameters
- Description: Express main training hyperparameters in a decoupled form for independent tuning
- Improvement: Allows for faster and more efficient hyperparameter optimization
- Target metrics: Ease of tuning, potentially leading to better final accuracy

These core ideas collectively contribute to the significant speedup and high accuracy achieved by the proposed training method.




====================


metrics:
1. Training Time
   1. Definition: The time taken to train the model and produce test-set predictions
   2. Calculation: Measured from first access of training data to final test-set prediction
   3. Typical range: 3-50 seconds for CIFAR-10 (lower is better)
   4. Conditions: Measured on a single NVIDIA A100 GPU
   5. Targeted by: All experimental methods (airbench94, airbench95, airbench96)
   6. Python function:
   ```python
   import time
   import torch

   def measure_training_time(experimental_method):
       torch.cuda.synchronize()  # Ensure GPU is ready
       start_time = time.time()
       model = experimental_method()  # Train and evaluate
       torch.cuda.synchronize()  # Ensure all GPU operations are complete
       end_time = time.time()
       return end_time - start_time
   ```

2. Accuracy
   1. Definition: Percentage of correct classifications on the CIFAR-10 test set
   2. Calculation: (Number of correct predictions / Total number of test samples) * 100
   3. Typical range: 93-96% (higher is better)
   4. Conditions: Uses test-time augmentation (TTA)
   5. Targeted by: All experimental methods (airbench94, airbench95, airbench96)
   6. Python function:
   ```python
   import torch
   from torchvision import datasets, transforms

   def calculate_accuracy(experimental_method):
       model = experimental_method()
       test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())
       test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False)
       
       correct = 0
       total = 0
       with torch.no_grad():
           for images, labels in test_loader:
               outputs = model(images)
               _, predicted = torch.max(outputs.data, 1)
               total += labels.size(0)
               correct += (predicted == labels).sum().item()
       
       return (correct / total) * 100
   ```

3. FLOPs (Floating Point Operations)
   1. Definition: Number of floating-point operations required for training
   2. Calculation: Sum of all floating-point operations in forward and backward passes
   3. Typical range: 10^14 to 10^16 FLOPs for CIFAR-10 (lower is better)
   4. Conditions: Calculated for the entire training process
   5. Targeted by: All experimental methods (airbench94, airbench95, airbench96)
   6. Python function:
   ```python
   import torch
   from thop import profile

   def calculate_flops(experimental_method):
       model = experimental_method()
       input = torch.randn(1, 3, 32, 32)  # CIFAR-10 input shape
       flops, _ = profile(model, inputs=(input,))
       return flops * num_training_iterations  # Multiply by number of training iterations
   ```

4. Test-set Variance
   1. Definition: Variability in accuracy across multiple runs on the test set
   2. Calculation: Standard deviation of accuracy over multiple runs
   3. Typical range: 0.11-0.16% for airbench methods
   4. Conditions: Calculated over 10,000 runs
   5. Targeted by: Analysis of training stability
   6. Python function:
   ```python
   import numpy as np

   def calculate_test_set_variance(experimental_method, num_runs=10000):
       accuracies = [calculate_accuracy(experimental_method) for _ in range(num_runs)]
       return np.std(accuracies)
   ```

5. Distribution-wise Variance
   1. Definition: Estimated variance in performance on the underlying data distribution
   2. Calculation: Complex estimation procedure described in Jordan (2023)
   3. Typical range: 0.018-0.037% for airbench methods (lower than test-set variance)
   4. Conditions: Estimated from multiple runs
   5. Targeted by: Analysis of training stability and generalization
   6. Python function:
   ```python
   # This is a simplified placeholder. The actual calculation is complex and requires
   # implementation of the method described in Jordan (2023)
   def estimate_distribution_wise_variance(experimental_method, num_runs=10000):
       # Placeholder for the complex estimation procedure
       return complex_estimation_procedure(experimental_method, num_runs)
   ```

6. Class-aggregated Calibration Error (CACE)
   1. Definition: Measure of deviation from class-wise calibration
   2. Calculation: As defined in Jiang et al. (2021)
   3. Typical range: 0.02-0.05 for airbench methods (lower is better)
   4. Conditions: Calculated on the test set
   5. Targeted by: Analysis of model calibration
   6. Python function:
   ```python
   # This is a simplified placeholder. The actual calculation requires
   # implementation of the method described in Jiang et al. (2021)
   def calculate_cace(experimental_method):
       model = experimental_method()
       # Placeholder for the CACE calculation procedure
       return cace_calculation_procedure(model, test_data)
   ```

These metrics provide a comprehensive evaluation of the training methods introduced in the paper, covering speed, accuracy, computational efficiency, stability, and calibration.




====================


baseline_methods:
1. Standard CNN Training on CIFAR-10
1. Description: A basic convolutional neural network trained on CIFAR-10 using standard techniques
2. Key components:
   - Convolutional layers
   - Max pooling layers
   - Fully connected layers
   - SGD optimizer
   - Cross-entropy loss
3. Crucial details:
   - Data augmentation: Random horizontal flips and crops
   - Learning rate schedule: Step decay
   - Batch size: 128
4. Modified by: All experimental methods build upon this baseline

2. tysam-code's CIFAR-10 Training Method (2023)
1. Description: A fast training method for CIFAR-10 that achieves 94% accuracy in 6.3 A100-seconds
2. Key components:
   - Custom CNN architecture
   - Patch-whitening initialization
   - Nesterov SGD optimizer
   - Data augmentation
3. Crucial details:
   - First layer: 2x2 convolution with no padding
   - Frozen patch-whitening layer
   - Identity initialization for convolutions
   - Increased learning rate for BatchNorm biases
4. Modified by: Experimental methods 1 (airbench94), 2 (airbench95), and 3 (airbench96)

3. Page's CIFAR-10 Training Method (2019)
1. Description: A training method focused on optimizing ResNet architecture for CIFAR-10
2. Key components:
   - Modified ResNet architecture
   - Optimization tricks
   - Frozen patch-whitening layer
3. Crucial details:
   - Increased learning rate for BatchNorm biases
   - Custom learning rate schedule
   - Careful weight initialization
4. Modified by: Experimental methods 1 (airbench94), 2 (airbench95), and 3 (airbench96)

4. DAWNBench Competition Winner (based on Page 2019)
1. Description: The winning submission for the CIFAR-10 track of the 2017-2020 Stanford DAWNBench training speed competition
2. Key components:
   - Modified version of Page's (2019) method
   - Optimized for multi-GPU training
3. Crucial details:
   - Reaches 94% accuracy in 10 seconds on 8 V100 GPUs
   - Specific modifications to Page's method not detailed in the current paper
4. Modified by: Not directly modified, but serves as a performance benchmark

5. Standard ResNet-18 Training
1. Description: A widely used baseline for image classification tasks
2. Key components:
   - ResNet-18 architecture
   - SGD optimizer with momentum
   - Cross-entropy loss
3. Crucial details:
   - 18 layers with residual connections
   - Batch normalization after each convolutional layer
   - ReLU activation functions
   - Standard data augmentation (random crops and flips)
4. Modified by: Not directly modified for CIFAR-10 in this paper, but used as a comparison point for other datasets

These baseline methods provide the foundation upon which the paper's improvements are built, allowing for a clear understanding of the contributions made by the new techniques introduced in the paper.




====================


experimental_methods:
Here are the key experimental methods proposed in the paper:

1. airbench94
1. Baseline: Combines elements from tysam-code (2023) and Page (2019)
2. Modifications:
   - Custom CNN architecture with 1.97 million parameters
   - Frozen patch-whitening initialization for the first layer
   - Identity initialization for convolutions after the first layer 
   - Increased learning rate for BatchNorm biases
   - Lookahead optimization
   - Multi-crop evaluation (test-time augmentation)
   - Alternating flip data augmentation
3. Expected improvements:
   - Significantly faster training time
   - Maintain high accuracy (94%)
4. Target metrics: Training speed, accuracy

2. airbench95 
1. Baseline: airbench94
2. Modifications:
   - Increased training epochs from 9.9 to 15
   - Scaled output channel count (first block: 64 to 128, second and third blocks: 256 to 384)
   - Reduced learning rate by a factor of 0.87
3. Expected improvements:
   - Higher accuracy (95%) with moderate increase in training time
4. Target metrics: Accuracy, while maintaining relatively fast training speed

3. airbench96
1. Baseline: airbench95 
2. Modifications:
   - Added 12-pixel Cutout augmentation
   - Increased training epochs to 40
   - Added a third convolution to each block
   - Scaled channel counts (first block: 128, second and third blocks: 512)
   - Added residual connections across later two convolutions of each block
   - Reduced learning rate by a factor of 0.78
3. Expected improvements:
   - Higher accuracy (96%) with longer but still competitive training time
4. Target metrics: Accuracy, while maintaining reasonable training speed

These methods progressively build upon each other, offering trade-offs between speed and accuracy while incorporating the novel alternating flip data augmentation technique.




====================


method_metric_results:
Baseline (tysam-code 2023):
- Training Time: 6.3 seconds
- Accuracy: 94%
- FLOPs: Not reported

airbench94:
- Training Time: 3.29 seconds (compiled) / 3.83 seconds (non-compiled)
- Accuracy: 94.01%
- FLOPs: 3.6 Ã 10^14
- Test-set Variance: 0.128%
- Distribution-wise Variance: 0.029%
- CACE: 0.0533

airbench95:
- Training Time: 10.4 seconds
- Accuracy: 95.01%
- FLOPs: 1.4 Ã 10^15

airbench96:
- Training Time: 46.3 seconds
- Accuracy: 96.05%
- FLOPs: 7.2 Ã 10^15

These results showcase the performance improvements achieved by the airbench methods compared to the baseline, with airbench94 offering the fastest training time while maintaining 94% accuracy, and airbench95 and airbench96 providing higher accuracy at the cost of increased training time and computational complexity.




====================


hw_agnostic_metrics:
1. FLOPs (Floating Point Operations): Measures computational complexity
   - airbench94: 3.6 Ã 10^14 FLOPs
   - airbench95: 1.4 Ã 10^15 FLOPs
   - airbench96: 7.2 Ã 10^15 FLOPs

2. Accuracy:
   - airbench94: 94.01%
   - airbench95: 95.01%
   - airbench96: 96.05%

3. Test-set Variance:
   - airbench94: 0.128%

4. Distribution-wise Variance:
   - airbench94: 0.029%

5. CACE (Class-aggregated Calibration Error):
   - airbench94: 0.0533

6. Number of Model Parameters:
   - airbench94: 1.97 million

7. Number of Training Epochs:
   - airbench94: 9.9 epochs
   - airbench95: 15 epochs
   - airbench96: 40 epochs

These metrics provide a hardware-agnostic view of the performance and characteristics of the proposed methods, allowing for fair comparisons across different hardware setups.




====================


tasks:
1. Task: Implement airbench94 Core Components

1. Baseline: Standard CNN implementation for CIFAR-10
2. Modifications:
   - Implement custom CNN architecture with 1.97 million parameters
   - Add frozen patch-whitening initialization for the first layer
   - Implement identity initialization for convolutions after the first layer
   - Increase learning rate for BatchNorm biases
   - Integrate Lookahead optimization
   - Implement multi-crop evaluation (test-time augmentation)
   - Implement alternating flip data augmentation
3. Inputs/Outputs:
   - Input: CIFAR-10 dataset
   - Output: Trained model and test set predictions
4. Key requirements:
   - Use PyTorch for implementation
   - Ensure compatibility with NVIDIA GPUs for fast training
   - Implement all components in a modular fashion for easy modification
5. Observable effects:
   - Training time should be around 3.83 seconds on an NVIDIA A100 GPU
   - Test set accuracy should reach 94.01%
   - FLOPs count should be approximately 3.6 Ã 10^14

2. Task: Extend airbench94 to airbench95

1. Baseline: airbench94 implementation
2. Modifications:
   - Increase training epochs from 9.9 to 15
   - Scale output channel count (first block: 64 to 128, second and third blocks: 256 to 384)
   - Reduce learning rate by a factor of 0.87
3. Inputs/Outputs:
   - Input: CIFAR-10 dataset
   - Output: Trained model and test set predictions
4. Key requirements:
   - Maintain modularity to allow easy switching between airbench94 and airbench95 configurations
   - Ensure all airbench94 components are preserved and only specified changes are made
5. Observable effects:
   - Training time should increase to around 10.4 seconds on an NVIDIA A100 GPU
   - Test set accuracy should reach 95.01%
   - FLOPs count should increase to approximately 1.4 Ã 10^15

3. Task: Implement airbench96 Enhancements

1. Baseline: airbench95 implementation
2. Modifications:
   - Add 12-pixel Cutout augmentation
   - Increase training epochs to 40
   - Add a third convolution to each block
   - Scale channel counts (first block: 128, second and third blocks: 512)
   - Add residual connections across later two convolutions of each block
   - Reduce learning rate by a factor of 0.78
3. Inputs/Outputs:
   - Input: CIFAR-10 dataset
   - Output: Trained model and test set predictions
4. Key requirements:
   - Implement Cutout augmentation efficiently to minimize impact on training speed
   - Ensure proper integration of residual connections without disrupting existing architecture
   - Maintain flexibility to easily switch between airbench94, airbench95, and airbench96 configurations
5. Observable effects:
   - Training time should increase to around 46.3 seconds on an NVIDIA A100 GPU
   - Test set accuracy should reach 96.05%
   - FLOPs count should increase to approximately 7.2 Ã 10^15

These tasks cover the implementation of the three main experimental methods presented in the paper, focusing on the key improvements and modifications for each method.




====================


task_eval_planning:
1. Eval Plan: airbench94 Core Components Implementation

1. Baseline: Provide a standard CNN implementation for CIFAR-10 using PyTorch.

2. Required modifications:
   - Implement custom CNN architecture with 1.97 million parameters
   - Add frozen patch-whitening initialization for the first layer
   - Implement identity initialization for convolutions after the first layer
   - Increase learning rate for BatchNorm biases
   - Integrate Lookahead optimization
   - Implement multi-crop evaluation (test-time augmentation)
   - Implement alternating flip data augmentation

3. Sample inputs:
   - A batch of CIFAR-10 images (32x32x3)
   - Training dataset for full training run
   - Test dataset for evaluation

4. Expected changes in output:
   - Faster convergence during training
   - Higher accuracy on test set
   - Consistent performance across multiple runs

5. Evaluation metrics:
   - Training time (target: ~3.83 seconds on A100 GPU)
   - Test set accuracy (target: 94.01%)
   - FLOPs count (target: ~3.6 Ã 10^14)
   - Test-set variance (target: ~0.128%)
   - Distribution-wise variance (target: ~0.029%)
   - CACE (target: ~0.0533)

2. Eval Plan: Extend airbench94 to airbench95

1. Baseline: Provide the implemented airbench94 code.

2. Required modifications:
   - Increase training epochs from 9.9 to 15
   - Scale output channel count (first block: 64 to 128, second and third blocks: 256 to 384)
   - Reduce learning rate by a factor of 0.87

3. Sample inputs:
   - Same as airbench94 evaluation

4. Expected changes in output:
   - Longer training time
   - Higher accuracy on test set
   - Increased FLOPs count

5. Evaluation metrics:
   - Training time (target: ~10.4 seconds on A100 GPU)
   - Test set accuracy (target: 95.01%)
   - FLOPs count (target: ~1.4 Ã 10^15)

3. Eval Plan: Implement airbench96 Enhancements

1. Baseline: Provide the implemented airbench95 code.

2. Required modifications:
   - Add 12-pixel Cutout augmentation
   - Increase training epochs to 40
   - Add a third convolution to each block
   - Scale channel counts (first block: 128, second and third blocks: 512)
   - Add residual connections across later two convolutions of each block
   - Reduce learning rate by a factor of 0.78

3. Sample inputs:
   - Same as airbench94 and airbench95 evaluations
   - Additional test cases for Cutout augmentation

4. Expected changes in output:
   - Significantly longer training time
   - Higher accuracy on test set
   - Further increased FLOPs count
   - Improved robustness due to Cutout augmentation

5. Evaluation metrics:
   - Training time (target: ~46.3 seconds on A100 GPU)
   - Test set accuracy (target: 96.05%)
   - FLOPs count (target: ~7.2 Ã 10^15)
   - Robustness to occlusions (qualitative assessment using partially occluded test images)

For all evaluations, also check:
- Correct implementation of all specified components
- Modularity and ease of switching between configurations
- Reproducibility of results across multiple runs

These evaluation plans will help assess whether the implementations correctly achieve the performance improvements and characteristics described in the paper for each airbench variant.




====================


task_eval_baseline_implementation:
Here's a baseline implementation for the standard CNN training on CIFAR-10 using PyTorch:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import time

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Hyperparameters
num_epochs = 200
batch_size = 128
learning_rate = 0.1

# Data loading and preprocessing
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# CNN Model (to be replaced with airbench94 architecture)
class BasicCNN(nn.Module):
    def __init__(self):
        super(BasicCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * 4 * 4, 64)
        self.fc2 = nn.Linear(64, 10)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        x = x.view(-1, 64 * 4 * 4)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model = BasicCNN().to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)

# Training loop
def train():
    model.train()
    for epoch in range(num_epochs):
        for i, (images, labels) in enumerate(train_loader):
            images = images.to(device)
            labels = labels.to(device)

            # Forward pass
            outputs = model(images)
            loss = criterion(outputs, labels)

            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Evaluation function
def evaluate():
    model.eval()
    with torch.no_grad():
        correct = 0
        total = 0
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        print(f'Accuracy on the test images: {100 * correct / total}%')

# Run training and evaluation
start_time = time.time()
train()
end_time = time.time()
print(f'Training time: {end_time - start_time:.2f} seconds')
evaluate()

# TODO: Implement airbench94 modifications:
# 1. Replace BasicCNN with custom architecture (1.97 million parameters)
# 2. Add frozen patch-whitening initialization for the first layer
# 3. Implement identity initialization for convolutions after the first layer
# 4. Increase learning rate for BatchNorm biases
# 5. Integrate Lookahead optimization
# 6. Implement multi-crop evaluation (test-time augmentation)
# 7. Implement alternating flip data augmentation
```

This baseline implementation provides a starting point for the airbench94 task. It includes a simple CNN model, data loading and preprocessing for CIFAR-10, a basic training loop, and evaluation on the test set. The TODO comments at the end indicate where the airbench94 modifications should be implemented.

The engineer can use this as a foundation to implement the airbench94 improvements, replacing components as needed and adding the required optimizations. This baseline allows for easy comparison between the standard approach and the airbench94 method in terms of training time, accuracy, and other metrics specified in the evaluation plan.




====================


task_eval_instructions:
1. Implement airbench94:

Your task is to implement the airbench94 method for fast CIFAR-10 training. Start with the provided baseline implementation and make the following modifications:

a) Replace the BasicCNN with the custom architecture described in the paper, ensuring it has 1.97 million parameters.
b) Implement frozen patch-whitening initialization for the first layer.
c) Add identity initialization for convolutions after the first layer.
d) Increase the learning rate for BatchNorm biases by a factor of 64.
e) Integrate Lookahead optimization.
f) Implement multi-crop evaluation (test-time augmentation).
g) Implement alternating flip data augmentation.

Use the CIFAR-10 dataset provided in the baseline. Your implementation should output a trained model and test set predictions.

Evaluate your implementation by measuring:
- Training time (target: ~3.83 seconds on an A100 GPU)
- Test set accuracy (target: 94.01%)
- FLOPs count (target: ~3.6 Ã 10^14)

Run your implementation for at least 10 trials and report the average results. Document any implementation choices or deviations from the paper's description.

2. Extend airbench94 to airbench95:

Building upon your airbench94 implementation, make the following modifications to create airbench95:

a) Increase the number of training epochs from 9.9 to 15.
b) Scale the output channel count:
   - First block: from 64 to 128
   - Second and third blocks: from 256 to 384
c) Reduce the learning rate by a factor of 0.87.

Use the same CIFAR-10 dataset and evaluation process as in airbench94. Your implementation should output a trained model and test set predictions.

Evaluate your implementation by measuring:
- Training time (target: ~10.4 seconds on an A100 GPU)
- Test set accuracy (target: 95.01%)
- FLOPs count (target: ~1.4 Ã 10^15)

Run your implementation for at least 10 trials and report the average results. Document any implementation choices or deviations from the paper's description.

3. Implement airbench96:

Building upon your airbench95 implementation, make the following modifications to create airbench96:

a) Add 12-pixel Cutout augmentation.
b) Increase the number of training epochs to 40.
c) Add a third convolution to each block.
d) Scale the channel counts:
   - First block: 128
   - Second and third blocks: 512
e) Add residual connections across the later two convolutions of each block.
f) Reduce the learning rate by a factor of 0.78.

Use the same CIFAR-10 dataset and evaluation process as in previous implementations. Your implementation should output a trained model and test set predictions.

Evaluate your implementation by measuring:
- Training time (target: ~46.3 seconds on an A100 GPU)
- Test set accuracy (target: 96.05%)
- FLOPs count (target: ~7.2 Ã 10^15)

Additionally, assess the robustness to occlusions by testing on partially occluded images from the test set.

Run your implementation for at least 10 trials and report the average results. Document any implementation choices or deviations from the paper's description.

For all implementations:
- Ensure your code is modular and allows easy switching between airbench94, airbench95, and airbench96 configurations.
- Use PyTorch for implementation and ensure compatibility with NVIDIA GPUs.
- Provide clear documentation for your code, especially for the custom components like alternating flip and multi-crop evaluation.
- Report any challenges faced during implementation and how you addressed them.




====================


task_eval_correctness_scoring:
```python
import torch
import torch.nn as nn
import time

def score_airbench94_implementation(modified_model, baseline_model):
    score = 0.0
    
    # Check architecture
    if isinstance(modified_model, nn.Module) and sum(p.numel() for p in modified_model.parameters()) == 1.97e6:
        score += 0.1
    
    # Check frozen patch-whitening initialization
    if isinstance(modified_model[0], nn.Conv2d) and not modified_model[0].weight.requires_grad:
        score += 0.1
    
    # Check identity initialization for convolutions
    for m in modified_model.modules():
        if isinstance(m, nn.Conv2d) and m != modified_model[0]:
            if torch.allclose(m.weight[:m.in_channels], torch.eye(m.in_channels).unsqueeze(-1).unsqueeze(-1), atol=1e-6):
                score += 0.05
    
    # Check increased learning rate for BatchNorm biases
    optimizer = torch.optim.SGD(modified_model.parameters(), lr=0.1)
    for group in optimizer.param_groups:
        if 'bias' in group['name'] and group['lr'] == 0.1 * 64:
            score += 0.1
    
    # Check Lookahead optimization
    if any(isinstance(opt, torch.optim.Optimizer) and 'lookahead' in opt.__class__.__name__.lower() for opt in optimizer.state.values()):
        score += 0.1
    
    # Check multi-crop evaluation
    def check_multicrop(model, input_tensor):
        original_output = model(input_tensor)
        flipped_output = model(torch.flip(input_tensor, [-1]))
        translated_output = model(torch.roll(input_tensor, shifts=(1, 1), dims=(-2, -1)))
        return not torch.allclose(original_output, flipped_output) and not torch.allclose(original_output, translated_output)
    
    if check_multicrop(modified_model, torch.randn(1, 3, 32, 32)):
        score += 0.1
    
    # Check alternating flip data augmentation
    # This is a simplified check and may need to be adjusted based on the specific implementation
    def check_alternating_flip(model, dataloader):
        flipped = []
        for i, (inputs, _) in enumerate(dataloader):
            if i % 2 == 1:
                inputs = torch.flip(inputs, [-1])
            flipped.append((inputs == dataloader.dataset[i][0]).all())
            if i > 10:
                break
        return all(flipped)
    
    if check_alternating_flip(modified_model, torch.utils.data.DataLoader(torch.randn(100, 3, 32, 32))):
        score += 0.1
    
    # Evaluate performance
    start_time = time.time()
    accuracy = evaluate_model(modified_model, test_loader)
    end_time = time.time()
    training_time = end_time - start_time
    
    if 3.5 < training_time < 4.2:  # Allow some margin around the target 3.83 seconds
        score += 0.1
    if 93.5 < accuracy < 94.5:  # Allow some margin around the target 94.01%
        score += 0.1
    
    # Check FLOPs (this is a simplified check and may need a more accurate FLOP counting method)
    flops = count_flops(modified_model)
    if 3.5e14 < flops < 3.7e14:
        score += 0.05
    
    return score

def score_airbench95_implementation(modified_model, airbench94_model):
    score = 0.0
    
    # Check increased epoch count
    if modified_model.num_epochs == 15:
        score += 0.2
    
    # Check scaled channel counts
    if (modified_model[1].out_channels == 128 and
        modified_model[2].out_channels == 384 and
        modified_model[3].out_channels == 384):
        score += 0.2
    
    # Check adjusted learning rate
    if abs(modified_model.lr - airbench94_model.lr * 0.87) < 1e-6:
        score += 0.2
    
    # Evaluate performance
    start_time = time.time()
    accuracy = evaluate_model(modified_model, test_loader)
    end_time = time.time()
    training_time = end_time - start_time
    
    if 10 < training_time < 11:  # Allow some margin around the target 10.4 seconds
        score += 0.2
    if 94.5 < accuracy < 95.5:  # Allow some margin around the target 95.01%
        score += 0.1
    
    # Check FLOPs
    flops = count_flops(modified_model)
    if 1.3e15 < flops < 1.5e15:
        score += 0.1
    
    return score

def score_airbench96_implementation(modified_model, airbench95_model):
    score = 0.0
    
    # Check Cutout augmentation
    if hasattr(modified_model, 'cutout') and modified_model.cutout.length == 12:
        score += 0.1
    
    # Check increased epoch count
    if modified_model.num_epochs == 40:
        score += 0.1
    
    # Check additional convolutions and channel counts
    if (len(modified_model[1]) == 3 and
        len(modified_model[2]) == 3 and
        len(modified_model[3]) == 3 and
        modified_model[1].out_channels == 128 and
        modified_model[2].out_channels == 512 and
        modified_model[3].out_channels == 512):
        score += 0.2
    
    # Check residual connections
    if hasattr(modified_model[1], 'residual') and hasattr(modified_model[2], 'residual') and hasattr(modified_model[3], 'residual'):
        score += 0.1
    
    # Check adjusted learning rate
    if abs(modified_model.lr - airbench95_model.lr * 0.78) < 1e-6:
        score += 0.1
    
    # Evaluate performance
    start_time = time.time()
    accuracy = evaluate_model(modified_model, test_loader)
    end_time = time.time()
    training_time = end_time - start_time
    
    if 45 < training_time < 48:  # Allow some margin around the target 46.3 seconds
        score += 0.1
    if 95.5 < accuracy < 96.5:  # Allow some margin around the target 96.05%
        score += 0.1
    
    # Check FLOPs
    flops = count_flops(modified_model)
    if 7e15 < flops < 7.4e15:
        score += 0.1
    
    # Check robustness to occlusions
    occluded_accuracy = evaluate_model_with_occlusions(modified_model, test_loader)
    if occluded_accuracy > 0.9 * accuracy:  # Assume less than 10% drop in accuracy for occluded images
        score += 0.1
    
    return score

# Helper functions (to be implemented)
def evaluate_model(model, dataloader):
    # Implement model evaluation on the given dataloader
    pass

def count_flops(model):
    # Implement FLOP counting for the given model
    pass

def evaluate_model_with_occlusions(model, dataloader):
    # Implement model evaluation on occluded images
    pass
```

These scoring functions check for the correct implementation of each component and assign partial credit based on the presence and correctness of each feature. They also evaluate the performance metrics and compare them to the target values from the paper. 

Note that some aspects, like the exact implementation of alternating flip or multi-crop evaluation, may need more sophisticated checks depending on how they are implemented. The provided checks are simplified versions and may need to be adjusted based on the specific implementation details.

Also, helper functions like `evaluate_model`, `count_flops`, and `evaluate_model_with_occlusions` need to be implemented separately, as their exact implementation would depend on the specific setup and available libraries.

These scoring functions provide a comprehensive evaluation of each airbench implementation, covering all the key aspects described in the paper and allowing for partial credit on individual components.




====================


task_eval_metric_scoring:
Here are Python functions to score the metric results for each airbench implementation:

```python
import time
import torch
from thop import profile

def score_airbench94_metrics(model):
    score = 0.0

    # Measure training time
    start_time = time.time()
    train_model(model)  # Assume this function exists to train the model
    end_time = time.time()
    training_time = end_time - start_time

    # Score training time (target: 3.83 seconds)
    if 3.5 <= training_time <= 4.2:
        score += 0.3 * (1 - abs(training_time - 3.83) / 0.7)

    # Measure accuracy
    accuracy = evaluate_model(model)  # Assume this function exists to evaluate the model

    # Score accuracy (target: 94.01%)
    if 93.5 <= accuracy <= 94.5:
        score += 0.5 * (1 - abs(accuracy - 94.01) / 1.0)

    # Measure FLOPs
    input_tensor = torch.randn(1, 3, 32, 32)
    flops, _ = profile(model, inputs=(input_tensor,))

    # Score FLOPs (target: 3.6e14)
    if 3.4e14 <= flops <= 3.8e14:
        score += 0.2 * (1 - abs(flops - 3.6e14) / 4e13)

    return score

def score_airbench95_metrics(model):
    score = 0.0

    # Measure training time
    start_time = time.time()
    train_model(model)
    end_time = time.time()
    training_time = end_time - start_time

    # Score training time (target: 10.4 seconds)
    if 9.5 <= training_time <= 11.3:
        score += 0.3 * (1 - abs(training_time - 10.4) / 1.8)

    # Measure accuracy
    accuracy = evaluate_model(model)

    # Score accuracy (target: 95.01%)
    if 94.5 <= accuracy <= 95.5:
        score += 0.5 * (1 - abs(accuracy - 95.01) / 1.0)

    # Measure FLOPs
    input_tensor = torch.randn(1, 3, 32, 32)
    flops, _ = profile(model, inputs=(input_tensor,))

    # Score FLOPs (target: 1.4e15)
    if 1.3e15 <= flops <= 1.5e15:
        score += 0.2 * (1 - abs(flops - 1.4e15) / 2e14)

    return score

def score_airbench96_metrics(model):
    score = 0.0

    # Measure training time
    start_time = time.time()
    train_model(model)
    end_time = time.time()
    training_time = end_time - start_time

    # Score training time (target: 46.3 seconds)
    if 42 <= training_time <= 51:
        score += 0.3 * (1 - abs(training_time - 46.3) / 9)

    # Measure accuracy
    accuracy = evaluate_model(model)

    # Score accuracy (target: 96.05%)
    if 95.5 <= accuracy <= 96.5:
        score += 0.5 * (1 - abs(accuracy - 96.05) / 1.0)

    # Measure FLOPs
    input_tensor = torch.randn(1, 3, 32, 32)
    flops, _ = profile(model, inputs=(input_tensor,))

    # Score FLOPs (target: 7.2e15)
    if 6.8e15 <= flops <= 7.6e15:
        score += 0.2 * (1 - abs(flops - 7.2e15) / 8e14)

    return score
```

These functions score the key metrics (training time, accuracy, and FLOPs) for each airbench implementation. They return a score between 0 and 1, with higher scores indicating closer matches to the target values from the paper.

The scoring system:
1. Allows for some margin of error around target values
2. Weights accuracy most heavily (50% of score), followed by training time (30%) and FLOPs (20%)
3. Uses linear interpolation within acceptable ranges to assign partial credit

Note that these functions assume the existence of `train_model()` and `evaluate_model()` functions, which would need to be implemented separately. The `profile()` function from the `thop` library is used to estimate FLOPs.




====================


implementation_difficulty:
Here's the implementation difficulty assessment for each task:

1. airbench94 Core Components Implementation
Complexity: 4/5
Estimated time: 8-12 hours
Challenges and potential pitfalls:
- Correctly implementing the frozen patch-whitening initialization
- Ensuring the identity initialization for convolutions is applied correctly 
- Integrating Lookahead optimization without errors
- Implementing alternating flip data augmentation efficiently
- Balancing all components to achieve the target accuracy and speed
Helpful resources:
- PyTorch documentation for custom layer implementation
- The original Lookahead optimizer paper
- CIFAR-10 dataset documentation
- Papers on efficient data augmentation techniques

2. Extend airbench94 to airbench95 
Complexity: 2/5
Estimated time: 2-3 hours
Challenges and potential pitfalls:
- Ensuring the channel count scaling is applied correctly to all relevant layers
- Balancing the increased epoch count with the reduced learning rate to achieve the target accuracy
Helpful resources:
- The airbench94 implementation (previously completed)
- PyTorch documentation on modifying network architectures

3. Implement airbench96 Enhancements
Complexity: 3/5  
Estimated time: 4-6 hours
Challenges and potential pitfalls:
- Implementing Cutout augmentation efficiently
- Adding the third convolution to each block without disrupting the existing architecture
- Correctly implementing residual connections
- Balancing all new components to achieve the target accuracy and training time
Helpful resources:
- Paper on Cutout augmentation
- PyTorch documentation on implementing residual connections  
- Tutorials on advanced CNN architectures




====================


data_processing:
1. Data cleaning and preprocessing steps:
- Lowercase all text (not applicable for CIFAR-10 image data)
- Remove special characters and extra whitespace (not applicable for CIFAR-10)
- Normalize pixel values using CIFAR-10 mean and standard deviation:
  CIFAR_MEAN = (0.4914, 0.4822, 0.4465)
  CIFAR_STD = (0.2470, 0.2435, 0.2616)

2. Feature engineering methods:
- Use a frozen patch-whitening layer as the first convolutional layer
- Initialize this layer using eigenvectors of the covariance matrix of 2x2 patches across the training distribution

3. Data augmentation techniques:
- Random horizontal flipping (50% probability on first epoch, then alternating)
- Random translation (2-pixel)
- Reflection padding for translations
- For airbench96: 12-pixel Cutout augmentation

4. Sampling or batching strategies:
- Batch size of 1024
- Use of a custom CifarLoader class for efficient GPU-based data loading
- Implement alternating flip augmentation after the first epoch to reduce redundancy

Example for NLP Preprocessing (not directly used in this paper, but included for completeness):
1. Data cleaning:
- Lowercase all text
- Remove special characters and extra whitespace
- Tokenization using SentencePiece with vocab size 32,000

2. Feature engineering:
- Convert tokens to integer IDs
- Add special tokens: [CLS] at start, [SEP] at end of each sentence

3. Data augmentation:
- Random word masking (15% of tokens)
- Random word replacement (10% of tokens)

4. Batching strategy:
- Group sentences of similar length
- Pad to max length in batch
- Use attention mask to ignore padding

This data processing approach focuses on efficient preprocessing and augmentation techniques specifically tailored for fast CIFAR-10 training, with the novel alternating flip augmentation being a key component.




====================


setup_description:
Here's a detailed setup guide for implementing the airbench methods:

1. Required libraries and their versions:
- PyTorch >= 1.9.0
- torchvision >= 0.10.0 
- numpy >= 1.21.0
- thop >= 0.0.31-2005241907 (for FLOPs calculation)

2. Necessary compute resources:
- GPU: NVIDIA A100 (or equivalent high-end GPU)
- RAM: 16GB minimum, 32GB recommended
- Storage: 10GB free space for CIFAR-10 dataset and model checkpoints

3. Datasets:
- CIFAR-10, available from torchvision.datasets.CIFAR10
- No additional datasets required

4. Sample environment setup script:

```bash
# Create and activate conda environment
conda create -n airbench python=3.8
conda activate airbench

# Install PyTorch with CUDA support (adjust CUDA version as needed)
conda install pytorch torchvision cudatoolkit=11.3 -c pytorch

# Install additional dependencies
pip install numpy thop

# Set up data directory
mkdir -p data/cifar10
```

5. Potential compatibility issues or common setup pitfalls:
- Ensure CUDA toolkit version matches PyTorch build
- Verify GPU drivers are up-to-date
- Check that the A100 GPU is recognized and accessible
- Ensure sufficient disk space for dataset and checkpoints
- Be aware of potential memory issues with large batch sizes
- If using a different GPU, adjust batch sizes and learning rates accordingly
- Make sure the system has the latest NVIDIA drivers installed
- When using PyTorch's torch.compile(), ensure you have the latest compatible version

This setup provides the necessary components to begin implementing the airbench methods. It specifies the required libraries, compute resources, and datasets, along with a sample environment setup script. The guide also notes potential compatibility issues to be aware of during setup.




====================