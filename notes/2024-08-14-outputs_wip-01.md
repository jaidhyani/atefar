paper_content = SigData(
        "paper_content", 
        """
        The full text content of an AI research paper. Our eventual goal is to produce tasks the correspond to replicating parts of this paper 
        which can be used to evaluate the capabilities of researchers and developers. In other words, we want to identify one or more methods
        that the paper explores, as well as how the efficacy of those methods was measured (metrics). Then we want to develop a task wherein we will
        ask the subject to replicate some method explored in the paper in the form of a python function. For each task, we will eventually produce: 
        a description of the function to implement, the inputs to that function, the expected outputs from that function, (if applicable) a baseline
        implementation of the function to be given to the subject so that they can focus on implementing just the change we're interested in,
        and a scoring function for evaluating the correctness of a given function implementation.
        """
)

title = SigData("title", "The title of the paper")

abstract_plus = SigData(
    "abstract_plus", 
    """
    The abstract of the paper, plus optional additional high-level summaries to capture 
    interesting aspects of the paper not otherwise covered in the abstract
    """
)
quantitative_results_json = SigData(
    "quantitative_results_json",
    """
    List quantitative results from the paper. For each result, include unit type, value, and a description of what the result is measuring. Response should be formatted as a list
    of JSON dicts, like so:
    [
        {
            "units": "test_set_accuracy",
            "value": 0.89,
            "description": "Accuracy on ImageNet validation set",
            "method": "8 layer convnet with skip connections",
            "notes": "Trained for 5 epochs",
            "comparison": {"baseline": 0.82, "improvement": "+7%"} 
        }
    ]

    Not every result will have every field, nulls are allowed and occasionally expected.
    """
)
core_ideas_json = SigData(
    "core_ideas_json", 
    """
    Describe ideas or approaches that the paper then goes on to demonstrate enable objective improvements according to certain metrics.
    This should be a JSON dict of {"idea_name": "longer idea description"}
    
    Examples:
    The core idea in "ImageNet Classification with Deep Convolutional Neural Networks" (Krizhevsky et al., 2012) might be:
    {"deep_cnn": "Deep Convolutional neural networks can be used to classify images with high accuracy"}

    Example for ""Attention Is All You Need" (Vaswani et al., 2017):
    {
        "attention_is_all_you_need": "Attention mechanisms can be used to model long-range dependencies in sequences",
        "attn_seq_to_seq": "Attention can be highly effective for sequence-to-sequence tasks like machine translation and other NLP tasks",
        "self_attn": "Self-attention mechanisms can be used to model dependencies between different parts of the same sequence",
        "multiheaded": "Multi-head attention mechanisms can be used to model different types of dependencies in parallel",
        "positional_encodings": "Positional encodings can be used to provide information about the position of tokens in a sequence",
        "transformer": "The Transformer architecture can be used to combine these ideas into a highly effective model for sequence-to-sequence tasks"
    }
    """
)

metrics_json = SigData(
    "metrics_json", 
    """
    An enumerated list of the key metrics used to measure results in the paper. Response should be a JSON list of objects, where each object represents a metric. For example:
    [
        {
            "name": "FLOPS",
            "description": "Number of floating point operations required to train a model to a certain level of accuracy",
            "unit": "GigaFLOPS",
            "measurement_details": "Measured using NVIDIA's nvprof tool during training on a V100 GPU"
        },
        {
            "name": "Test Set Accuracy",
            "description": "Percentage of correct predictions on a held-out test set",
            "unit": "Percentage",
            "measurement_details": "Evaluated on the ImageNet validation set after training for 90 epochs"
        }
    ]
    """
)

baseline_methods_json = SigData(
    "baseline_methods_json", 
    """
    Describe one or more baseline approaches that the paper's methods are compared against and/or iterating upon. Response should be a JSON object where keys are method names and values are detailed descriptions. For example:
    {
        "standard_transformer": {
            "description": "A sequence-to-sequence model based on self-attention mechanisms",
            "key_components": [
                "Multi-head self-attention layers",
                "Feed-forward neural networks",
                "Layer normalization",
                "Positional encodings"
            ],
            "architecture_details": {
                "encoder_layers": 6,
                "decoder_layers": 6,
                "attention_heads": 8,
                "embedding_dim": 512
            },
            "training_details": {
                "optimizer": "Adam",
                "learning_rate": "Warmup over 4000 steps, then decay",
                "batch_size": 128
            }
        }
    }
    """
)

experimental_methods_json = SigData(
    "experimental_methods_json", 
    """
    An enumerated list of the key methods or strategies proposed in the paper to optimize the target metrics. Response should be a JSON list of objects, where each object represents an experimental method. For example:
    [
        {
            "name": "Sparse Attention Transformer",
            "baseline": "standard_transformer",
            "modifications": [
                "Replace full attention with sparse attention patterns",
                "Implement fixed or learned attention patterns"
            ],
            "expected_improvements": [
                "Reduced computational complexity from O(nÂ²) to O(n log n)",
                "Better handling of long sequences"
            ],
            "target_metrics": ["Inference speed", "Memory usage", "Performance on long-sequence tasks"]
        }
    ]
    """
)

method_metric_results = SigData(
    "method_metric_results", 
    """
    For each baseline and experimental method, list the metric results that were reported in the paper. Response should be a JSON object where keys are method names and values are objects containing metric results. For example:
    {
        "standard_transformer": {
            "BLEU_score": 28.4,
            "inference_time": 100,
            "model_size": 65
        },
        "sparse_attention_transformer": {
            "BLEU_score": 28.2,
            "inference_time": 80,
            "model_size": 66
        }
    }
    """
)

hw_agnostic_metrics_json = SigData(
    "hw_agnostic_metrics_json", 
    """
    List metrics that correspond to the paper's metrics, but are not hardware-specific. Response should be a JSON list of objects, where each object represents a hardware-agnostic metric. For example:
    [
        {
            "name": "FLOPs per training step",
            "description": "Number of floating point operations per training step",
            "corresponding_hw_metric": "Training time per step",
            "unit": "GigaFLOPs"
        },
        {
            "name": "Number of parameters",
            "description": "Total number of trainable parameters in the model",
            "corresponding_hw_metric": "Model size in MB",
            "unit": "Millions of parameters"
        }
    ]
    """
)

tasks_json = SigData(
    "tasks_json", 
    """
    Describe each enumerated method as a function to be implemented. Response should be a JSON list of objects, where each object represents a task. For example:
    [
        {
            "name": "implement_sparse_attention",
            "inputs": [
                {
                    "name": "input_tensor",
                    "type": "torch.Tensor",
                    "shape": "(batch_size, seq_length, d_model)"
                }
            ],
            "outputs": [
                {
                    "name": "output_tensor",
                    "type": "torch.Tensor",
                    "shape": "(batch_size, seq_length, d_model)"
                }
            ],
            "description": "Implement a sparse attention mechanism in the MultiHeadAttention class",
            "constraints": [
                "Implement a fixed sparse attention pattern (e.g., local + global attention)",
                "Ensure the sparse attention matrix is properly masked and normalized",
                "Maintain compatibility with the rest of the Transformer architecture"
            ]
        }
    ]
    """
)

task_eval_planning_json = SigData(
    "task_eval_planning_json",
    """
    Plan how to convert each enumerated task into an evaluation. Response should be a JSON list of objects, where each object represents an evaluation plan. For example:
    [
        {
            "task_name": "implement_sparse_attention",
            "provided_inputs": {
                "baseline_implementation": "transformer.py with standard MultiHeadAttention class",
                "input_data": "Sample input tensors of shape (32, 1000, 512)"
            },
            "expected_outputs": {
                "modified_attention_function": "Function implementing sparse attention",
                "output_tensors": "Tensors of shape (32, 1000, 512)"
            },
            "evaluation_criteria": [
                "Correctness: Compare attention patterns with expected sparsity",
                "Performance: Measure speed and memory usage improvements",
                "Quality: Compare perplexity on a held-out test set"
            ]
        }
    ]
    """
)

task_eval_baseline_implementation_json = SigData(
    "task_eval_baseline_implementation_json", 
    """
    For each task eval, if applicable, provide a baseline implementation. Response should be a JSON object where keys are task names and values are the baseline implementations or null. For example:
    {
        "implement_sparse_attention": "```python\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        \n        assert d_model % self.num_heads == 0\n        \n        self.depth = d_model // self.num_heads\n        \n        self.wq = nn.Linear(d_model, d_model)\n        self.wk = nn.Linear(d_model, d_model)\n        self.wv = nn.Linear(d_model, d_model)\n        \n        self.dense = nn.Linear(d_model, d_model)\n        \n    def split_heads(self, x, batch_size):\n        x = x.view(batch_size, -1, self.num_heads, self.depth)\n        return x.permute(0, 2, 1, 3)\n    \n    def forward(self, q, k, v, mask=None):\n        batch_size = q.size(0)\n        \n        q = self.wq(q)\n        k = self.wk(k)\n        v = self.wv(v)\n        \n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n        \n        scaled_attention_logits = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.depth)\n        \n        if mask is not None:\n            scaled_attention_logits += (mask * -1e9)\n        \n        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n        \n        output = torch.matmul(attention_weights, v)\n        \n        output = output.permute(0, 2, 1, 3).contiguous()\n        output = output.view(batch_size, -1, self.d_model)\n        output = self.dense(output)\n        \n        return output\n```",
        "another_task": null
    }
    """
)

task_eval_instructions_json = SigData(
    "task_eval_instructions_json", 
    """
    For each enumerated task, describe the instructions that will be given to the engineer to complete the task. Response should be a JSON object where keys are task names and values are the instructions. For example:
    {
        "implement_sparse_attention": "Modify the provided MultiHeadAttention class to implement a sparse attention mechanism. Your implementation should:\n1. Replace the full attention matrix with a sparse attention pattern (e.g., local + global attention)\n2. Ensure the sparse attention matrix is properly masked and normalized\n3. Maintain compatibility with the rest of the Transformer architecture\n\nInputs and outputs should remain the same as in the original implementation. Focus on modifying the 'forward' method to incorporate sparse attention."
    }
    """
)

task_eval_correctness_scoring_json = SigData(
    "task_eval_correctness_scoring_json", 
    """
    For each task eval, write a python function that will be used to score correctness of the implementation. Response should be a JSON object where keys are task names and values are the scoring functions as strings. For example:
    {
        "implement_sparse_attention": "```python\ndef score_sparse_attention_implementation(modified_transformer, baseline_transformer):\n    score = 0.0\n    \n    # Test preserved functionality\n    short_input = torch.randn(32, 50, 512)\n    if torch.allclose(modified_transformer(short_input), baseline_transformer(short_input), atol=1e-5):\n        score += 0.3\n    \n    # Test improvement\n    long_input = torch.randn(32, 1000, 512)\n    modified_output = modified_transformer(long_input)\n    \n    # Check for sparse attention pattern\n    attention_weights = modified_transformer.encoder.layers[0].self_attn.attn_weights\n    if attention_weights.float().to_dense().count_nonzero() / attention_weights.numel() < 0.2:\n        score += 0.4\n    \n    # Check for improved efficiency\n    start_time = time.time()\n    modified_transformer(long_input)\n    modified_time = time.time() - start_time\n    \n    start_time = time.time()\n    baseline_transformer(long_input)\n    baseline_time = time.time() - start_time\n    \n    if modified_time < 0.8 * baseline_time:\n        score += 0.3\n    \n    return score\n```"
    }
    """
)

task_eval_metric_scoring_json = SigData(
    "task_eval_metric_scoring_json", 
    """
    For each task eval, write a python function that will be used to score the metric results of the implementation. Response should be a JSON object where keys are task names and values are the metric scoring functions as strings. For example:
    {
        "implement_sparse_attention": "```python\ndef score_sparse_attention_metrics(modified_transformer, baseline_transformer, test_data):\n    baseline_perplexity = evaluate_perplexity(baseline_transformer, test_data)\n    modified_perplexity = evaluate_perplexity(modified_transformer, test_data)\n    \n    perplexity_improvement = (baseline_perplexity - modified_perplexity) / baseline_perplexity\n    \n    baseline_speed = measure_inference_speed(baseline_transformer, test_data)\n    modified_speed = measure_inference_speed(modified_transformer, test_data)\n    \n    speed_improvement = (modified_speed - baseline_speed) / baseline_speed\n    \n    # We expect a small perplexity improvement and a significant speed improvement\n    score = min(1.0, max(0, perplexity_improvement * 10)) * 0.3 + min(1.0, max(0, speed_improvement)) * 0.7\n    \n    return score\n\ndef evaluate_perplexity(model, data):\n    # Implementation of perplexity calculation\n    pass\n\ndef measure_inference_speed(model, data):\n    # Implementation of speed measurement\n    pass\n```"
    }
    """
)

setup_description = SigData(
    "setup_description", 
    """
    A description of the setup required before implementing the task functions. This should include a list of assets that need to be available to start.
    These may include datasets, trained models, training functions, etc. Assume that a python torch environment with GPUs is available. Any common libraries
    or public assets may be used (e.g. huggingface transformers, torchvision, etc.). If a dataset is required, it should be available in a public location.
    """
)







title:
94% on CIFAR-10 in 3.29 Seconds on a Single GPU


abstract_plus:
Title: 94% on CIFAR-10 in 3.29 Seconds on a Single GPU

Abstract Plus: This paper introduces fast training methods for the CIFAR-10 dataset, achieving 94% accuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds on a single NVIDIA A100 GPU. The authors propose a derandomized variant of horizontal flipping augmentation called "alternating flip," which improves performance over standard random flipping in all cases where flipping is beneficial. The methods build upon prior work, incorporating techniques such as a modified network architecture, initialization, optimizer, and frozen patch-whitening layer. The paper presents a detailed analysis of the components contributing to the speedup and demonstrates the effectiveness of the alternating flip method across various scenarios.

Additional high-level summaries:
1. The paper provides a comprehensive breakdown of the techniques used to achieve fast training, including network architecture modifications, optimization tricks, and novel data augmentation strategies.

2. The authors investigate the generalization of their methods to other datasets (CIFAR-100, SVHN, CINIC-10), showing strong performance without additional tuning.

3. The research includes an analysis of the statistical properties of the training process, including variance and class-wise calibration, providing insights into the stability and reliability of the proposed methods.

4. The paper discusses the potential impact of these fast training methods on accelerating research and reducing the cost of experiments involving large numbers of trained networks.




====================


quantitative_results_json:
[
  {
    "units": "seconds",
    "value": 3.29,
    "description": "Time to reach 94% accuracy on CIFAR-10",
    "method": "airbench94_compiled",
    "notes": "Run on a single NVIDIA A100 GPU",
    "comparison": {"baseline": 6.3, "improvement": "1.9x faster"}
  },
  {
    "units": "seconds",
    "value": 10.4,
    "description": "Time to reach 95% accuracy on CIFAR-10",
    "method": "airbench95",
    "notes": "Run on a single NVIDIA A100 GPU"
  },
  {
    "units": "seconds",
    "value": 46.3,
    "description": "Time to reach 96% accuracy on CIFAR-10",
    "method": "airbench96",
    "notes": "Run on a single NVIDIA A100 GPU"
  },
  {
    "units": "FLOPs",
    "value": 3.6e14,
    "description": "Computational cost for 94% accuracy",
    "method": "airbench94_compiled"
  },
  {
    "units": "FLOPs",
    "value": 1.4e15,
    "description": "Computational cost for 95% accuracy",
    "method": "airbench95"
  },
  {
    "units": "FLOPs",
    "value": 7.2e15,
    "description": "Computational cost for 96% accuracy",
    "method": "airbench96"
  },
  {
    "units": "accuracy",
    "value": 0.9401,
    "description": "Mean accuracy on CIFAR-10",
    "method": "airbench94 with alternating flip",
    "comparison": {"baseline": 0.9392, "improvement": "+0.09%", "baseline_method": "airbench94 with random flip"}
  },
  {
    "units": "accuracy",
    "value": 0.7976,
    "description": "Accuracy on CIFAR-100",
    "method": "airbench96",
    "comparison": {"baseline": 0.7804, "improvement": "+1.72%", "baseline_method": "ResNet-18"}
  },
  {
    "units": "accuracy",
    "value": 0.8822,
    "description": "Accuracy on CINIC-10",
    "method": "airbench96",
    "comparison": {"baseline": 0.8758, "improvement": "+0.64%", "baseline_method": "ResNet-18"}
  },
  {
    "units": "accuracy",
    "value": 0.9764,
    "description": "Accuracy on SVHN",
    "method": "airbench96",
    "comparison": {"baseline": 0.9735, "improvement": "+0.29%", "baseline_method": "ResNet-18"}
  },
  {
    "units": "standard_deviation",
    "value": 0.00029,
    "description": "Distribution-wise standard deviation of accuracy",
    "method": "airbench94 with TTA",
    "notes": "Measured over 10,000 runs"
  },
  {
    "units": "CACE",
    "value": 0.0533,
    "description": "Class-aggregated calibration error",
    "method": "airbench94 with TTA",
    "notes": "Higher CACE indicates lower class-wise calibration"
  }
]




====================


core_ideas_json:
{
  "fast_cifar10_training": "Develop methods to train CIFAR-10 models to high accuracy in seconds on a single GPU",
  "alternating_flip": "A derandomized variant of horizontal flipping augmentation that improves over standard random flipping",
  "combined_optimization_techniques": "Integrate various optimization tricks, network architecture modifications, and initialization methods to achieve fast training",
  "generalization_to_other_datasets": "The fast training methods generalize well to other datasets without additional tuning",
  "statistical_analysis": "Analyze the variance and calibration properties of the fast training process to understand its stability and reliability"
}




====================


metrics_json:
[
  {
    "name": "Training Time",
    "description": "Time required to train a model to reach a specific accuracy level on CIFAR-10",
    "unit": "seconds",
    "measurement_details": "Measured on a single NVIDIA A100 GPU"
  },
  {
    "name": "Accuracy",
    "description": "Classification accuracy on CIFAR-10 or other datasets",
    "unit": "percentage",
    "measurement_details": "Evaluated on the test set after training"
  },
  {
    "name": "Computational Cost",
    "description": "Number of floating point operations required for training",
    "unit": "FLOPs",
    "measurement_details": "Calculated for the entire training process"
  },
  {
    "name": "Distribution-wise Standard Deviation",
    "description": "Measure of variance in accuracy across multiple training runs",
    "unit": "percentage",
    "measurement_details": "Calculated over 10,000 runs of training"
  },
  {
    "name": "Class-aggregated Calibration Error (CACE)",
    "description": "Measure of class-wise calibration of the model",
    "unit": "scalar value",
    "measurement_details": "Lower values indicate better calibration"
  }
]




====================


baseline_methods_json:
{
  "tysam-code_2023": {
    "description": "Prior state-of-the-art method for fast CIFAR-10 training",
    "key_components": [
      "Modified network architecture",
      "Specialized initialization", 
      "Optimized optimizer",
      "Frozen patch-whitening layer"
    ],
    "performance": {
      "accuracy": "94%",
      "training_time": "6.3 seconds on A100 GPU"
    }
  },
  "serrano_et_al_2019": {
    "description": "DAWNBench winning submission for CIFAR-10 training",
    "key_components": [
      "Modified version of Page (2019) method"
    ],
    "performance": {
      "accuracy": "94%", 
      "training_time": "10 seconds on 8 V100 GPUs"
    }
  },
  "page_2019": {
    "description": "Fast CIFAR-10 training method",
    "key_components": [
      "Optimization tricks",
      "Frozen patch-whitening layer"
    ],
    "performance": {
      "accuracy": "94%",
      "training_time": "26 seconds on V100 GPU"
    }
  },
  "standard_resnet18": {
    "description": "Standard ResNet-18 architecture used as a baseline for comparison on various datasets",
    "key_components": [
      "18-layer residual network architecture",
      "Standard data augmentation (random crop and flip)",
      "Standard optimization techniques"  
    ],
    "performance": {
      "cifar100_accuracy": "78.04%",
      "cinic10_accuracy": "87.58%",
      "svhn_accuracy": "97.35%"
    },
    "training_details": {
      "optimizer": "SGD with momentum",
      "learning_rate": "Typically uses step decay or cosine annealing",
      "epochs": "Typically 200 or more for full convergence"
    }
  }
}




====================


experimental_methods_json:
[
  {
    "name": "airbench94",
    "baseline": "tysam-code_2023",
    "modifications": [
      "Derandomized horizontal flipping (alternating flip)",
      "Modified network architecture with 1.97 million parameters",
      "Frozen patch-whitening initialization",
      "Identity initialization for convolutions",
      "Lookahead optimization",
      "Multi-crop evaluation"
    ],
    "expected_improvements": [
      "1.9x faster training time to reach 94% accuracy",
      "Reduced computational cost (3.6e14 FLOPs)"
    ],
    "target_metrics": ["Training Time", "Accuracy", "Computational Cost"]
  },
  {
    "name": "airbench95",
    "baseline": "airbench94",
    "modifications": [
      "Increased training epochs from 9.9 to 15",
      "Scaled output channel count in convolutional blocks",
      "Reduced learning rate by a factor of 0.87"
    ],
    "expected_improvements": [
      "Achieve 95% accuracy in 10.4 seconds",
      "Maintain fast training while increasing accuracy"
    ],
    "target_metrics": ["Training Time", "Accuracy", "Computational Cost"]
  },
  {
    "name": "airbench96",
    "baseline": "airbench95",
    "modifications": [
      "Added 12-pixel Cutout augmentation",
      "Increased training epochs to 40",
      "Added third convolution to each block",
      "Scaled channel counts in convolutional blocks",
      "Added residual connections",
      "Reduced learning rate by a factor of 0.78"
    ],
    "expected_improvements": [
      "Achieve 96% accuracy in 46.3 seconds",
      "Generalize well to other datasets without additional tuning"
    ],
    "target_metrics": ["Training Time", "Accuracy", "Computational Cost", "Generalization to Other Datasets"]
  },
  {
    "name": "Alternating Flip",
    "baseline": "Standard random horizontal flipping",
    "modifications": [
      "Deterministic alternating flipping after the first epoch",
      "Eliminate redundant flipping of images across consecutive epochs"
    ],
    "expected_improvements": [
      "Consistent improvement in accuracy across various training configurations",
      "Up to 25% effective speedup in training"
    ],
    "target_metrics": ["Accuracy", "Training Time"]
  }
]




====================


method_metric_results:
{
  "tysam-code_2023": {
    "accuracy": 94.0,
    "training_time": 6.3
  },
  "serrano_et_al_2019": {
    "accuracy": 94.0,
    "training_time": 80.0
  },
  "page_2019": {
    "accuracy": 94.0,
    "training_time": 26.0
  },
  "standard_resnet18": {
    "cifar100_accuracy": 78.04,
    "cinic10_accuracy": 87.58,
    "svhn_accuracy": 97.35
  },
  "airbench94": {
    "accuracy": 94.01,
    "training_time": 3.29,
    "flops": 3.6e14
  },
  "airbench95": {
    "accuracy": 95.01,
    "training_time": 10.4,
    "flops": 1.4e15
  },
  "airbench96": {
    "accuracy": 96.05,
    "training_time": 46.3,
    "flops": 7.2e15,
    "cifar100_accuracy": 79.76,
    "cinic10_accuracy": 88.22,
    "svhn_accuracy": 97.64
  },
  "alternating_flip": {
    "accuracy": 94.01,
    "random_flip_accuracy": 93.92
  }
}




====================


hw_agnostic_metrics_json:
[
  {
    "name": "Training Epochs",
    "description": "Number of complete passes through the training dataset",
    "corresponding_hw_metric": "Training time in seconds",
    "unit": "epochs"
  },
  {
    "name": "FLOPs",
    "description": "Number of floating point operations required for training",
    "corresponding_hw_metric": "Training time in seconds",
    "unit": "FLOPs"
  },
  {
    "name": "Accuracy",
    "description": "Classification accuracy on the test set",
    "corresponding_hw_metric": "Accuracy",
    "unit": "percentage"
  },
  {
    "name": "Distribution-wise Standard Deviation",
    "description": "Measure of variance in accuracy across multiple training runs",
    "corresponding_hw_metric": "Distribution-wise Standard Deviation",
    "unit": "percentage"
  },
  {
    "name": "Class-aggregated Calibration Error (CACE)",
    "description": "Measure of class-wise calibration of the model",
    "corresponding_hw_metric": "Class-aggregated Calibration Error (CACE)",
    "unit": "scalar value"
  }
]

This list of hardware-agnostic metrics allows for a more standardized comparison of the methods presented in the paper, independent of the specific hardware used for training.




====================


tasks_json:
[
  {
    "name": "implement_alternating_flip",
    "inputs": [
      {
        "name": "inputs",
        "type": "torch.Tensor",
        "shape": "(batch_size, channels, height, width)"
      },
      {
        "name": "indices",
        "type": "torch.Tensor",
        "shape": "(batch_size,)"
      },
      {
        "name": "epoch",
        "type": "int"
      }
    ],
    "outputs": [
      {
        "name": "augmented_inputs",
        "type": "torch.Tensor",
        "shape": "(batch_size, channels, height, width)"
      }
    ],
    "description": "Implement the alternating flip data augmentation technique as described in the paper. This function should deterministically flip images based on their index and the current epoch number.",
    "constraints": [
      "Use a pseudorandom function to decide flips based on indices",
      "Flip 50% of inputs randomly in the first epoch",
      "In subsequent epochs, flip only those inputs not flipped in the first epoch for even epochs, and only those flipped in the first epoch for odd epochs"
    ]
  },
  {
    "name": "init_whitening_conv",
    "inputs": [
      {
        "name": "layer",
        "type": "torch.nn.Conv2d"
      },
      {
        "name": "train_set",
        "type": "torch.Tensor",
        "shape": "(num_samples, channels, height, width)"
      },
      {
        "name": "eps",
        "type": "float",
        "default": "5e-4"
      }
    ],
    "outputs": [
      {
        "name": "initialized_layer",
        "type": "torch.nn.Conv2d"
      }
    ],
    "description": "Initialize the first convolutional layer as a patch-whitening transformation, as described in the paper.",
    "constraints": [
      "Compute the covariance matrix of patches from the training set",
      "Initialize filters using eigenvectors of the covariance matrix",
      "Ensure the output preserves input information through subsequent activation"
    ]
  },
  {
    "name": "create_airbench_model",
    "inputs": [
      {
        "name": "widths",
        "type": "dict",
        "default": "{'block1': 64, 'block2': 256, 'block3': 256}"
      },
      {
        "name": "batchnorm_momentum",
        "type": "float",
        "default": "0.6"
      }
    ],
    "outputs": [
      {
        "name": "model",
        "type": "torch.nn.Module"
      }
    ],
    "description": "Create the neural network architecture used in the airbench methods, including the modifications described in the paper.",
    "constraints": [
      "Use the specified widths for each block",
      "Implement identity initialization for convolutions",
      "Use GELU activations",
      "Disable biases in convolutional and linear layers",
      "Disable affine scale parameters in BatchNorm layers"
    ]
  }
]

These tasks cover the main novel contributions of the paper and allow for implementation and evaluation of the key components that contribute to the fast training method.




====================


task_eval_planning_json:
[
  {
    "task_name": "implement_alternating_flip",
    "provided_inputs": {
      "sample_inputs": "Random tensor of shape (128, 3, 32, 32)",
      "sample_indices": "Tensor of integers from 0 to 127",
      "epochs": "List of integers [0, 1, 2, 3, 4]"
    },
    "expected_outputs": {
      "augmented_inputs": "Tensor of same shape as input with some images flipped"
    },
    "evaluation_criteria": [
      "Correctness: Compare output with reference implementation",
      "Consistency: Ensure same indices are flipped/not flipped in appropriate epochs",
      "Performance: Measure execution time and compare to baseline"
    ],
    "baseline_implementation": "Standard random flip function for comparison",
    "additional_context": "Pseudocode for the alternating flip algorithm"
  },
  {
    "task_name": "init_whitening_conv",
    "provided_inputs": {
      "layer": "torch.nn.Conv2d instance with kernel_size=2, in_channels=3, out_channels=24",
      "train_set": "Random tensor of shape (1000, 3, 32, 32) simulating CIFAR-10 images",
      "eps": 5e-4
    },
    "expected_outputs": {
      "initialized_layer": "Conv2d layer with weights initialized as described in the paper"
    },
    "evaluation_criteria": [
      "Correctness: Check if initialized weights form an approximate whitening transform",
      "Eigenvalue properties: Verify eigenvalues of the weight matrix",
      "Performance: Measure initialization time"
    ],
    "baseline_implementation": "Standard initialization method for comparison",
    "additional_context": "Mathematical description of the whitening process"
  },
  {
    "task_name": "create_airbench_model",
    "provided_inputs": {
      "widths": {"block1": 64, "block2": 256, "block3": 256},
      "batchnorm_momentum": 0.6
    },
    "expected_outputs": {
      "model": "PyTorch nn.Module implementing the airbench architecture"
    },
    "evaluation_criteria": [
      "Architecture: Verify model structure matches the paper description",
      "Parameter count: Check total number of parameters",
      "Initialization: Verify identity initialization for convolutions",
      "BatchNorm settings: Check momentum and affine parameters",
      "Forward pass: Run sample input through the model and check output shape"
    ],
    "baseline_implementation": "Code snippet of the expected model structure",
    "additional_context": "Diagram of the airbench model architecture"
  }
]

This task_eval_planning_json provides a structured plan for evaluating each of the three main tasks derived from the paper. It includes the necessary inputs, expected outputs, evaluation criteria, and additional context needed to thoroughly assess the correctness and performance of the implementations.




====================


task_eval_baseline_implementation_json:
{
  "implement_alternating_flip": "```python\nimport torch\nimport hashlib\n\ndef hash_fn(n, seed=42):\n    k = n * seed\n    return int(hashlib.md5(bytes(str(k), 'utf-8')).hexdigest()[-8:], 16)\n\ndef alternating_flip(inputs, indices, epoch):\n    # Applies alternating flipping to a batch of images\n    hashed_indices = torch.tensor([hash_fn(i) for i in indices.tolist()])\n    flip_mask = ((hashed_indices + epoch) % 2 == 0).view(-1, 1, 1, 1)\n    return torch.where(flip_mask, inputs.flip(-1), inputs)\n```",

  "init_whitening_conv": "```python\nimport torch\nimport torch.nn.functional as F\n\ndef get_patches(x, patch_shape):\n    c, (h, w) = x.shape[1], patch_shape\n    return x.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1,c,h,w).float()\n\ndef get_whitening_parameters(patches):\n    n,c,h,w = patches.shape\n    patches_flat = patches.view(n, -1)\n    est_patch_covariance = (patches_flat.T @ patches_flat) / n\n    eigenvalues, eigenvectors = torch.linalg.eigh(est_patch_covariance, UPLO='U')\n    return eigenvalues.flip(0).view(-1, 1, 1, 1), eigenvectors.T.reshape(c*h*w,c,h,w).flip(0)\n\ndef init_whitening_conv(layer, train_set, eps=5e-4):\n    patches = get_patches(train_set, patch_shape=layer.weight.data.shape[2:])\n    eigenvalues, eigenvectors = get_whitening_parameters(patches)\n    eigenvectors_scaled = eigenvectors / torch.sqrt(eigenvalues + eps)\n    layer.weight.data[:] = torch.cat((eigenvectors_scaled, -eigenvectors_scaled))\n```",

  "create_airbench_model": "```python\nimport torch\nfrom torch import nn\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\nclass Mul(nn.Module):\n    def __init__(self, scale):\n        super().__init__()\n        self.scale = scale\n    def forward(self, x):\n        return x * self.scale\n\nclass BatchNorm(nn.BatchNorm2d):\n    def __init__(self, num_features, momentum, eps=1e-12, weight=False, bias=True):\n        super().__init__(num_features, eps=eps, momentum=1-momentum)\n        self.weight.requires_grad = weight\n        self.bias.requires_grad = bias\n\nclass Conv(nn.Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size=3, padding='same', bias=False):\n        super().__init__(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias)\n    \n    def reset_parameters(self):\n        super().reset_parameters()\n        if self.bias is not None:\n            self.bias.data.zero_()\n        w = self.weight.data\n        torch.nn.init.dirac_(w[:w.size(1)])\n\nclass ConvGroup(nn.Module):\n    def __init__(self, channels_in, channels_out, batchnorm_momentum):\n        super().__init__()\n        self.conv1 = Conv(channels_in, channels_out)\n        self.pool = nn.MaxPool2d(2)\n        self.norm1 = BatchNorm(channels_out, batchnorm_momentum)\n        self.conv2 = Conv(channels_out, channels_out)\n        self.norm2 = BatchNorm(channels_out, batchnorm_momentum)\n        self.activ = nn.GELU()\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pool(x)\n        x = self.norm1(x)\n        x = self.activ(x)\n        x = self.conv2(x)\n        x = self.norm2(x)\n        x = self.activ(x)\n        return x\n\ndef create_airbench_model(widths={'block1': 64, 'block2': 256, 'block3': 256}, batchnorm_momentum=0.6):\n    whiten_kernel_size = 2\n    whiten_width = 2 * 3 * whiten_kernel_size**2\n    model = nn.Sequential(\n        Conv(3, whiten_width, whiten_kernel_size, padding=0, bias=True),\n        nn.GELU(),\n        ConvGroup(whiten_width, widths['block1'], batchnorm_momentum),\n        ConvGroup(widths['block1'], widths['block2'], batchnorm_momentum),\n        ConvGroup(widths['block2'], widths['block3'], batchnorm_momentum),\n        nn.MaxPool2d(3),\n        Flatten(),\n        nn.Linear(widths['block3'], 10, bias=False),\n        Mul(1/9),\n    )\n    model[0].weight.requires_grad = False\n    return model\n```"
}

This JSON object contains baseline implementations for all three tasks:

1. "implement_alternating_flip": Provides the implementation of the alternating flip augmentation technique as described in the paper.

2. "init_whitening_conv": Includes the implementation for initializing the first convolutional layer as a patch-whitening transformation.

3. "create_airbench_model": Contains the complete implementation of the airbench model architecture, including all the custom modules and the main model creation function.

These baseline implementations can be used to compare and evaluate the correctness and performance of the submissions for each task.




====================


task_eval_instructions_json:
{
  "implement_alternating_flip": "Implement the alternating flip data augmentation technique as described in the paper. Your function should take three inputs: a batch of images (torch.Tensor of shape (batch_size, channels, height, width)), a tensor of indices, and the current epoch number. It should return an augmented batch of images with some flipped horizontally based on the following rules:
  1. In the first epoch (epoch 0), randomly flip 50% of the images.
  2. In subsequent epochs, flip only those images that were not flipped in the first epoch for even-numbered epochs, and only those that were flipped in the first epoch for odd-numbered epochs.
  3. Use a pseudorandom function based on the image indices to ensure consistent flipping across epochs.
  Your implementation should be deterministic and efficient. Focus on modifying the 'forward' method of the provided baseline implementation.",

  "init_whitening_conv": "Implement the initialization of the first convolutional layer as a patch-whitening transformation, as described in the paper. Your function should take three inputs: the convolutional layer to be initialized (torch.nn.Conv2d), a tensor of training images, and an epsilon value (default 5e-4). It should modify the weights of the input layer in-place. Follow these steps:
  1. Extract patches from the training images based on the kernel size of the conv layer.
  2. Compute the covariance matrix of these patches.
  3. Perform eigendecomposition on the covariance matrix.
  4. Initialize the conv weights using the eigenvectors, scaled by the square root of their corresponding eigenvalues plus epsilon.
  5. Set half of the filters to the negation of the other half to preserve input information.
  Ensure your implementation is numerically stable and efficient.",

  "create_airbench_model": "Create the neural network architecture used in the airbench methods, as described in the paper. Your function should take two optional inputs: a dictionary of block widths and a batchnorm momentum value. It should return a PyTorch nn.Module implementing the complete model. Follow these guidelines:
  1. Use the provided custom modules (Conv, BatchNorm, ConvGroup, etc.) to construct the model.
  2. Implement the model structure: initial whitening conv, three ConvGroup blocks, global pooling, and a final linear layer.
  3. Apply identity initialization for convolutions (already implemented in the Conv class).
  4. Use GELU activations throughout the model.
  5. Disable biases in convolutional and linear layers, except for the initial whitening conv.
  6. Disable affine scale parameters in BatchNorm layers.
  7. Apply the final scaling factor of 1/9 to the model output.
  8. Set the initial whitening conv layer's weights to non-trainable.
  Ensure your implementation matches the architecture and parameter counts described in the paper."
}

This task_eval_instructions_json provides clear, detailed instructions for each of the three main tasks, guiding engineers on how to implement the key components of the paper's methods.




====================


task_eval_correctness_scoring_json:
{
  "implement_alternating_flip": """
def score_alternating_flip(implementation, baseline):
    score = 0.0
    # Test correctness
    for epoch in range(5):
        inputs = torch.randn(128, 3, 32, 32)
        indices = torch.arange(128)
        impl_output = implementation(inputs, indices, epoch)
        base_output = baseline(inputs, indices, epoch)
        if torch.allclose(impl_output, base_output):
            score += 0.15
    
    # Test determinism
    inputs = torch.randn(128, 3, 32, 32)
    indices = torch.arange(128)
    output1 = implementation(inputs, indices, 1)
    output2 = implementation(inputs, indices, 1)
    if torch.allclose(output1, output2):
        score += 0.1
    
    # Test performance
    import time
    start_time = time.time()
    for _ in range(100):
        implementation(inputs, indices, 1)
    impl_time = time.time() - start_time
    
    start_time = time.time()
    for _ in range(100):
        baseline(inputs, indices, 1)
    base_time = time.time() - start_time
    
    if impl_time <= base_time * 1.1:  # Allow 10% slower
        score += 0.15
    
    return score
  """,
  
  "init_whitening_conv": """
def score_init_whitening_conv(implementation, baseline):
    score = 0.0
    
    # Prepare inputs
    layer = nn.Conv2d(3, 24, kernel_size=2, padding=0, bias=True)
    train_set = torch.randn(1000, 3, 32, 32)
    
    # Test correctness
    implementation(layer, train_set)
    impl_weights = layer.weight.data.clone()
    
    layer.reset_parameters()
    baseline(layer, train_set)
    base_weights = layer.weight.data.clone()
    
    if torch.allclose(impl_weights, base_weights, atol=1e-5):
        score += 0.4
    
    # Test whitening properties
    patches = get_patches(train_set, (2, 2))
    whitened = F.conv2d(patches, impl_weights)
    cov = torch.cov(whitened.flatten(1).T)
    if torch.allclose(cov, torch.eye(cov.shape[0]), atol=1e-2):
        score += 0.4
    
    # Test performance
    import time
    start_time = time.time()
    for _ in range(10):
        layer.reset_parameters()
        implementation(layer, train_set)
    impl_time = time.time() - start_time
    
    start_time = time.time()
    for _ in range(10):
        layer.reset_parameters()
        baseline(layer, train_set)
    base_time = time.time() - start_time
    
    if impl_time <= base_time * 1.1:  # Allow 10% slower
        score += 0.2
    
    return score
  """,
  
  "create_airbench_model": """
def score_create_airbench_model(implementation, baseline):
    score = 0.0
    
    # Create models
    impl_model = implementation()
    base_model = baseline()
    
    # Check structure
    if type(impl_model) == type(base_model) and len(list(impl_model.modules())) == len(list(base_model.modules())):
        score += 0.2
    
    # Check parameter count
    if sum(p.numel() for p in impl_model.parameters()) == sum(p.numel() for p in base_model.parameters()):
        score += 0.1
    
    # Check initialization
    for m1, m2 in zip(impl_model.modules(), base_model.modules()):
        if isinstance(m1, nn.Conv2d) and isinstance(m2, nn.Conv2d):
            if torch.allclose(m1.weight, m2.weight, atol=1e-5):
                score += 0.05
    
    # Check BatchNorm settings
    for m in impl_model.modules():
        if isinstance(m, nn.BatchNorm2d):
            if m.momentum == 0.4 and not m.affine:
                score += 0.1
    
    # Test forward pass
    x = torch.randn(32, 3, 32, 32)
    impl_out = impl_model(x)
    base_out = base_model(x)
    if impl_out.shape == base_out.shape:
        score += 0.1
    
    # Compare performance on small dataset
    impl_model.eval()
    base_model.eval()
    test_data = torch.randn(1000, 3, 32, 32)
    test_labels = torch.randint(0, 10, (1000,))
    
    criterion = nn.CrossEntropyLoss()
    impl_loss = criterion(impl_model(test_data), test_labels)
    base_loss = criterion(base_model(test_data), test_labels)
    
    if abs(impl_loss - base_loss) < 0.1:
        score += 0.2
    
    return score
  """
}

This task_eval_correctness_scoring_json provides scoring functions for each of the three tasks. These functions compare the implemented solutions against baseline implementations, checking for correctness, performance, and adherence to the specifications described in the paper.




====================


task_eval_metric_scoring_json:
{
  "implement_alternating_flip": """
def score_alternating_flip_metrics(implementation, baseline):
    score = 0.0
    
    # Test accuracy improvement
    model = create_airbench_model()
    train_data = torch.randn(10000, 3, 32, 32)
    train_labels = torch.randint(0, 10, (10000,))
    
    # Train with implementation
    impl_acc = train_and_evaluate(model, train_data, train_labels, augmentation=implementation)
    
    # Train with baseline
    base_acc = train_and_evaluate(model, train_data, train_labels, augmentation=baseline)
    
    acc_improvement = (impl_acc - base_acc) / base_acc
    score += min(1.0, max(0, acc_improvement * 10))  # Scale improvement, cap at 1.0
    
    # Test computational efficiency
    impl_time = measure_execution_time(implementation, train_data)
    base_time = measure_execution_time(baseline, train_data)
    
    time_improvement = (base_time - impl_time) / base_time
    score += min(1.0, max(0, time_improvement * 5))  # Scale improvement, cap at 1.0
    
    return score / 2  # Average of the two metrics

def train_and_evaluate(model, data, labels, augmentation):
    # Simplified training loop
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
    criterion = nn.CrossEntropyLoss()
    for epoch in range(10):
        for batch, batch_labels in zip(data.split(128), labels.split(128)):
            augmented_batch = augmentation(batch, torch.arange(len(batch)), epoch)
            outputs = model(augmented_batch)
            loss = criterion(outputs, batch_labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    
    # Evaluate
    model.eval()
    with torch.no_grad():
        outputs = model(data)
        acc = (outputs.argmax(1) == labels).float().mean().item()
    return acc

def measure_execution_time(augmentation, data):
    start_time = time.time()
    for epoch in range(10):
        for batch in data.split(128):
            augmentation(batch, torch.arange(len(batch)), epoch)
    return time.time() - start_time
""",

  "init_whitening_conv": """
def score_init_whitening_conv_metrics(implementation, baseline):
    score = 0.0
    
    # Test whitening effectiveness
    train_set = torch.randn(1000, 3, 32, 32)
    layer = nn.Conv2d(3, 24, kernel_size=2, padding=0, bias=True)
    
    implementation(layer, train_set)
    impl_whitening_score = measure_whitening_effectiveness(layer, train_set)
    
    layer.reset_parameters()
    baseline(layer, train_set)
    base_whitening_score = measure_whitening_effectiveness(layer, train_set)
    
    whitening_improvement = (impl_whitening_score - base_whitening_score) / base_whitening_score
    score += min(1.0, max(0, whitening_improvement * 5))  # Scale improvement, cap at 1.0
    
    # Test computational efficiency
    impl_time = measure_init_time(implementation, layer, train_set)
    base_time = measure_init_time(baseline, layer, train_set)
    
    time_improvement = (base_time - impl_time) / base_time
    score += min(1.0, max(0, time_improvement * 5))  # Scale improvement, cap at 1.0
    
    return score / 2  # Average of the two metrics

def measure_whitening_effectiveness(layer, train_set):
    with torch.no_grad():
        output = layer(train_set)
        cov = torch.cov(output.flatten(2).transpose(0, 1))
        return 1 / (torch.norm(cov - torch.eye(cov.shape[0])) + 1e-6)

def measure_init_time(init_function, layer, train_set):
    start_time = time.time()
    for _ in range(100):
        layer.reset_parameters()
        init_function(layer, train_set)
    return time.time() - start_time
""",

  "create_airbench_model": """
def score_create_airbench_model_metrics(implementation, baseline):
    score = 0.0
    
    # Test accuracy on CIFAR-10
    impl_model = implementation()
    base_model = baseline()
    
    train_data, train_labels, test_data, test_labels = load_cifar10()
    
    impl_acc = train_and_evaluate_cifar10(impl_model, train_data, train_labels, test_data, test_labels)
    base_acc = train_and_evaluate_cifar10(base_model, train_data, train_labels, test_data, test_labels)
    
    acc_improvement = (impl_acc - base_acc) / base_acc
    score += min(1.0, max(0, acc_improvement * 10))  # Scale improvement, cap at 1.0
    
    # Test training speed
    impl_time = measure_training_time(impl_model, train_data, train_labels)
    base_time = measure_training_time(base_model, train_data, train_labels)
    
    time_improvement = (base_time - impl_time) / base_time
    score += min(1.0, max(0, time_improvement * 5))  # Scale improvement, cap at 1.0
    
    # Test model size (number of parameters)
    impl_params = sum(p.numel() for p in impl_model.parameters())
    base_params = sum(p.numel() for p in base_model.parameters())
    
    size_improvement = (base_params - impl_params) / base_params
    score += min(1.0, max(0, size_improvement * 5))  # Scale improvement, cap at 1.0
    
    return score / 3  # Average of the three metrics

def load_cifar10():
    # Load and preprocess CIFAR-10 dataset
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    
    train_data = torch.stack([x for x, _ in trainset])
    train_labels = torch.tensor([y for _, y in trainset])
    test_data = torch.stack([x for x, _ in testset])
    test_labels = torch.tensor([y for _, y in testset])
    
    return train_data, train_labels, test_data, test_labels

def train_and_evaluate_cifar10(model, train_data, train_labels, test_data, test_labels):
    # Simplified training loop for CIFAR-10
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
    criterion = nn.CrossEntropyLoss()
    for epoch in range(10):
        for batch, batch_labels in zip(train_data.split(128), train_labels.split(128)):
            outputs = model(batch)
            loss = criterion(outputs, batch_labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    
    # Evaluate
    model.eval()
    with torch.no_grad():
        outputs = model(test_data)
        acc = (outputs.argmax(1) == test_labels).float().mean().item()
    return acc

def measure_training_time(model, train_data, train_labels):
    start_time = time.time()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
    criterion = nn.CrossEntropyLoss()
    for epoch in range(10):
        for batch, batch_labels in zip(train_data.split(128), train_labels.split(128)):
            outputs = model(batch)
            loss = criterion(outputs, batch_labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    return time.time() - start_time
"""
}

This task_eval_metric_scoring_json provides scoring functions for each task that evaluate the implementations based on relevant metrics from the paper, such as accuracy improvement, computational efficiency, and model size. These functions compare the performance of the implemented solutions against baseline implementations, providing a comprehensive assessment of their effectiveness in achieving the paper's goals.




====================


setup_description:
To implement and evaluate the tasks from the paper "94% on CIFAR-10 in 3.29 Seconds on a Single GPU", the following setup is required:

1. Python environment with PyTorch (version 2.1.2 or later) and CUDA support.
2. NVIDIA GPU (preferably an A100 for direct comparison with the paper's results).
3. CIFAR-10 dataset, which can be downloaded automatically through torchvision.
4. Baseline implementations of the following components (provided in the task evaluation files):
   - Alternating flip data augmentation
   - Whitening convolution initialization
   - AirBench model architecture
5. Additional Python libraries:
   - torchvision (for dataset and transforms)
   - numpy (for numerical operations)
   - time (for performance measurements)

Before implementing the tasks, ensure that:
1. The CIFAR-10 dataset can be successfully loaded using torchvision.
2. The provided baseline implementations can be executed without errors.
3. Basic PyTorch operations can be run on the GPU.

Assume that common PyTorch modules (nn, optim, etc.) and standard Python libraries are available. The implementation should focus on the core algorithms and techniques described in the paper, building upon the provided baseline code.

====================