paper_chain = SigChain(
    inputs=[SigData("paper_content", "The full text content of the research paper")],
    steps=[
        SigStep([
                SigData("title", "The title of the paper"),
                SigData("abstract_plus", "The abstract of the paper, plus optional additional high-level summaries to capture interesting aspects of the paper not otherwise covered in the abstract"),
        ],),
        SigStep([
            SigData(
                "core_ideas", 
                """
                These should describe ideas or approaches that the paper then goes on to demonstrate enable objective improvements according to certain metrics.
                
                Examples:
                The core idea in "ImageNet Classification with Deep Convolutional Neural Networks" (Krizhevsky et al., 2012) might be:
                - Deep Convolutional neural networks can be used to classify images with high accuracy

                The core ideas in ""Attention Is All You Need" (Vaswani et al., 2017) might be:
                - Attention mechanisms can be used to model long-range dependencies in sequences
                - This can be highly effective for sequence-to-sequence tasks like machine translation and other NLP tasks
                - Self-attention mechanisms can be used to model dependencies between different parts of the same sequence
                - Multi-head attention mechanisms can be used to model different types of dependencies in parallel
                - Positional encodings can be used to provide information about the position of tokens in a sequence
                - The Transformer architecture can be used to combine these ideas into a highly effective model for sequence-to-sequence tasks
                """
            ),
        ],),
        SigStep([
            SigData(
                "metrics", 
                """
                An enumerated list of the key metrics used to measure results in the paper. Any quantitative metric that is used to compare different approaches or improvements should be included here. 
                Metrics should be described in enough detail that they can be implemented and used to compare different methods. Metrics should specify not only units (e.g. FLOPs) but also what specifically
                is being measured and how (e.g. number of floating point operations required to train a model to a certain level of accuracy).
                """
            ),
        ],),
        SigStep([
            SigData(
                "baseline_methods", 
                """
                One or more baseline approaches that the paper's methods are compared against and/or iterating upon. 
                These should be described in enough detail to be implemented. 
                This will be used to provide baseline implementations for implementing experimental methods. For example, a 
                paper that proposes variations on a transformer architecture might have a baseline method that is a standard 
                transformer architecture.
                """
            ),
        ],),
        SigStep([
            SigData(
                "experimental_methods", 
                """
                An enumerated list of the key methods or strategies proposed in the paper to optimize the target metrics. These may be described in terms of changes versus the baseline methods when applicable.
                Experimental methods will typically consist of a baseline method and one or more modifications to that baseline. In those cases, we will define tasks by supplying the baseline implementation and the description of
                the modification to be made.
                """
            ),
        ],),
        SigStep([
            SigData(
                "method_metric_results", 
                """
                For each baseline and experimental method, as well as combinations thereof, list the metric results that were reported in the paper. These should be quantitative results used to compare the methods to each other.
                """
            ),
        ],),
        SigStep([
            SigData(
                "hw_agnostic_metrics", 
                """
                List metrics that correspond to the paper's metrics, but are not hardware-specific. These may be copied from input metrics where appropriate, or they may be generated if not otherwise available. 
                For example, if the paper reports time to train on a specific hardware setup, a hardware-agnostic metric might be number of samples used for training, or total FLOPS used to train to the same level of accuracy.
                Hardware agnostic metrics should be closely correlated with the hardware-specific metrics and indicate the same relative performance of different methods.
                """
            ),
        ],),
        SigStep([
            SigData(
                "tasks", 
                """
                Describe each enumerated method as a function to be implemented. 
                Describe what the inputs are and their types, what the outputs are and their types, 
                and what the method does.  Do not describe how the function is implemented. It should 
                be possible to verify the correctness of the implementation by comparing the output of the 
                implementation to the output of the method din the paper.
                """
            ),
        ],),
        SigStep([
            SigData(
                "task_eval_planning",
                """
                Plan how to convert each enumerated task into an evaluation. An evaluation is a test given to an engineer to test their ability to implement the task.
                For each evaluation, describe the inputs that will be provided to the engineer, the expected outputs, and the criteria for evaluating the outputs.
                Evaluations will typically consist of a function to be implemented, a description of the inputs and outputs, and a description of how the implementation will be evaluated.
                Evaluations will typically include a baseline implementation and a description of the modification to be made (though this may not always be the case).
                For now we are not actually making the evaluations, just planning them. For example, if the task was to add skip connections to a neural network, 
                we might plan to provide a baseline implementation, a description of the skip connections to be added, and how to evaluate the correctness of the implementation.
                """
            ),
        ],),
        SigStep([
            SigData(
                "task_eval_baseline_implementation", 
                """
                For each task eval, if applicable, provide a baseline implementation that will be given to the engineer to modify. This should be in python. If any ML is required, 
                assume that the engineer has access to a python environment with GPUs, and prefer to use torch for ML tasks. Other common libraries may be used as well, but
                the implementation should be as simple as possible. The baseline implementation should be as close to the paper's method as possible, and allow the engineer 
                to focus on the specific modification required by the task. If this task does not require a baseline implementation, simply indicate that instead.
                """
            ),
        ],),
        SigStep([
            SigData(
                "task_eval_instructions", 
                """
                For each enumerated task, describe the instructions that will be given to the engineer to complete the task. Instructions should be clear and concise,
                identify the inputs and outputs, and provide any additional information that the engineer will need to complete the task. If the task involves modifying a baseline implementation,
                the instructions should clearly describe the modification to be made, and may refer to the baseline implementation.
                """
            ),
        ]),
        SigStep([
            SigData(
                "task_eval_correctness_scoring", 
                """
                For each task eval, write a python function that will be used to score correctness of the implementation. The input will be the function written by the engineer,
                and the output will be a score between 0 and 1 indicating how well the implementation matches the expected implementation. This will typically be completed by comparing the output of the
                implementation to the output of the method in the paper for some known input. For example, if the task was to modify a neural-network-building function to add skip connections,
                the scoring function should check whether the network produced by the modified function has the expected skip connections in the expected places while not changing the rest of the network.
                """
            ),
        ]),
        SigStep([
            SigData(
                "task_eval_metric_scoring", 
                """
                For each task eval, write a python function that will be used to score the metric results of the implementation. The input will be the function written by the engineer,
                and the output will be a score between 0 and 1 indicating how well the implementation matches the expected results. This will typically be completed by comparing the output of the
                implementation to the output of the method in the paper for some known input. For example, if the task was to modify a neural-network-building function to add skip connections,
                the scoring function should check whether the improvement of the modified network over the baseline network matches the expected improvement. 
                """
            ),
        ]),
        SigStep([
            SigData(
                "setup_description", 
                """
                A description of the setup required before implementing the task functions. This should include a list of assets that need to be available to start.
                These may include datasets, trained models, training functions, etc. Assume that a python torch environment with GPUs is available. Any common libraries
                or public assets may be used (e.g. huggingface transformers, torchvision, etc.). If a dataset is required, it should be available in a public location.
                """
            ),
        ],),
    ],
)





title:
94% on CIFAR-10 in 3.29 Seconds on a Single GPU


abstract_plus:
This paper introduces ultra-fast training methods for the CIFAR-10 dataset, achieving 94% accuracy in just 3.29 seconds on a single NVIDIA A100 GPU. The authors develop three main methods, reaching 94%, 95%, and 96% accuracy in 3.29, 10.4, and 46.3 seconds respectively. These methods employ a combination of techniques, including a modified convolutional neural network architecture, optimization tricks, and a novel "alternating flip" data augmentation technique. The alternating flip method is shown to improve performance across various scenarios where random flipping is beneficial.

The proposed training methods can significantly accelerate research and reduce the cost of experiments involving CIFAR-10. They enable rapid accumulation of statistical significance for subtle hyperparameter comparisons and facilitate projects that require training massive numbers of networks. The authors release their code as open-source, making it easily accessible to the research community.

Additional interesting aspects of the work include:
1. The methods show good generalization to other datasets (CIFAR-100, SVHN, CINIC-10) without additional tuning, often outperforming standard ResNet-18 training.
2. The paper provides insights into the interaction between different training optimizations, showing that most speedup techniques accumulate additively rather than multiplicatively.
3. The authors conduct a detailed analysis of their proposed alternating flip method, demonstrating its effectiveness across various training configurations and datasets, including ImageNet.
4. The work includes an investigation of training variance and class-wise calibration, revealing interesting trade-offs between test-time augmentation, variance reduction, and calibration.




====================


core_ideas:
1. Ultra-fast training methods for CIFAR-10 can be achieved by combining optimized network architecture, initialization techniques, and data augmentation strategies.

2. A derandomized variant of horizontal flipping augmentation, called "alternating flip", can improve training speed and performance compared to standard random flipping in scenarios where flipping is beneficial.

3. Careful hyperparameter tuning and optimization tricks (e.g., decoupled weight decay, learning rate scheduling) can significantly accelerate training without sacrificing accuracy.

4. Test-time augmentation (TTA) can be used to boost final accuracy with minimal additional computational cost.

5. The proposed training methods generalize well to other image classification datasets without requiring additional tuning, suggesting they capture fundamental principles of efficient deep learning for computer vision tasks.

These core ideas encapsulate the main approaches that enable the paper's objective improvements in training speed and accuracy on CIFAR-10 and other datasets.




====================


metrics:
1. Training Time: Measured in seconds on a single NVIDIA A100 GPU to reach a specified accuracy threshold (e.g., 94%, 95%, or 96%) on the CIFAR-10 test set.

2. Accuracy: Percentage of correct classifications on the CIFAR-10 test set, measured after training and applying test-time augmentation.

3. FLOPs (Floating Point Operations): Total number of floating-point operations required to train the model to the specified accuracy threshold, used as a measure of computational efficiency.

4. Test-set Standard Deviation: Measure of variance in test-set accuracy across multiple runs of training, reported as a percentage.

5. Distribution-wise Standard Deviation: Estimate of the variance in accuracy on the true data distribution (as opposed to just the test set), reported as a percentage.

6. Class-Aggregated Calibration Error (CACE): Measure of deviation from class-wise calibration, with lower values indicating better calibration.

7. Effective Speedup: Percentage reduction in training epochs achieved by switching from random flip to alternating flip while maintaining the same level of accuracy.

8. Cross-dataset Accuracy: Percentage accuracy achieved on other datasets (CIFAR-100, SVHN, CINIC-10) without additional tuning, used to evaluate generalization.

These metrics cover the main quantitative aspects evaluated in the paper, allowing for comprehensive comparisons between different methods and techniques.




====================


baseline_methods:
The primary baseline method is the approach by tysam-code (2023), which achieves 94% accuracy on CIFAR-10 in 6.3 seconds on an NVIDIA A100 GPU. This method uses a VGG-like convolutional neural network with a 2x2 convolution as the first layer, initialized as a patch-whitening transformation. It employs a mixed initialization scheme for other convolutional layers, combining identity weights with standard initialization. The optimization process uses Nesterov SGD with Lookahead optimization and scales up the learning rate for BatchNorm biases. For data augmentation, it likely uses random horizontal flipping and random translation. During evaluation, it applies horizontal flipping test-time augmentation. This baseline serves as the primary point of comparison for the paper's proposed methods.




====================


experimental_methods:
1. Modified Network Architecture:
   - Decrease the number of output channels in the third block from 512 to 256
   - Add learnable biases to the first convolution layer

2. Frozen Patch-Whitening Initialization:
   - Initialize the first convolutional layer as a patch-whitening transformation
   - Add learnable biases to this layer, trained for 3 epochs before freezing
   - Reduce the constant added to eigenvalues during patch-whitening initialization

3. Identity Initialization (dirac):
   - Initialize all convolutions after the first as partial identity transforms

4. Optimization Tricks:
   a. scalebias: Increase the learning rate for BatchNorm biases by a factor of 64x
   b. lookahead: Implement Lookahead optimization

5. Multi-crop Evaluation (multicrop):
   - Run the trained network on six augmented views of each test image
   - Use a weighted average of all six outputs for predictions

6. Alternating Flip Data Augmentation:
   - Implement a deterministic alternating horizontal flipping strategy after the first epoch

7. airbench94 Training Method:
   - Combine all previous methods to achieve 94% accuracy in 3.83 seconds

8. airbench95 Training Method:
   - Increase training epochs from 9.9 to 15
   - Scale output channel counts in network blocks
   - Reduce learning rate by a factor of 0.87

9. airbench96 Training Method:
   - Add 12-pixel Cutout augmentation
   - Increase training epochs to 40
   - Further scale network block channel counts
   - Add residual connections across later convolutions in each block
   - Reduce learning rate by a factor of 0.78

10. Compilation (airbench94_compiled):
    - Apply torch.compile to the airbench94 method for additional 14% speedup

These experimental methods encompass the key innovations and modifications proposed in the paper to achieve the reported improvements in training speed and accuracy.




====================


method_metric_results:
1. Baseline (tysam-code, 2023):
   - Training Time: 6.3 seconds
   - Accuracy: 94%
   - FLOPs: Not reported

2. airbench94 (non-compiled):
   - Training Time: 3.83 seconds
   - Accuracy: 94.01%
   - FLOPs: 3.6 × 10^14

3. airbench94_compiled:
   - Training Time: 3.29 seconds
   - Accuracy: 94.01%
   - FLOPs: 3.6 × 10^14

4. airbench95:
   - Training Time: 10.4 seconds
   - Accuracy: 95.01%
   - FLOPs: 1.4 × 10^15

5. airbench96:
   - Training Time: 46.3 seconds
   - Accuracy: 96.05%
   - FLOPs: 7.2 × 10^15

6. Baseline with Whitening Initialization:
   - Training Time: 8.0 seconds
   - Accuracy: 94%
   - FLOPs: Not reported

7. Baseline with Whitening + Identity Initialization:
   - Training Time: 6.8 seconds
   - Accuracy: 94%
   - FLOPs: Not reported

8. Baseline with Whitening + Identity + Optimization Tricks:
   - Training Time: 4.6 seconds
   - Accuracy: 94%
   - FLOPs: Not reported

9. Baseline with Whitening + Identity + Optimization + Multi-crop:
   - Training Time: 4.2 seconds
   - Accuracy: 94%
   - FLOPs: Not reported

These results show the progressive improvements made by each experimental method, culminating in the final airbench94, airbench95, and airbench96 methods. The compiled version of airbench94 achieves the fastest training time for 94% accuracy, while airbench95 and airbench96 offer higher accuracy at the cost of increased training time and computational requirements.




====================


hw_agnostic_metrics:
1. Accuracy: Percentage of correct classifications on the CIFAR-10 test set after training and applying test-time augmentation.
2. FLOPs: Total number of floating-point operations required to train the model to the specified accuracy threshold.
3. Epochs to Accuracy: Number of training epochs required to reach the target accuracy (e.g., 94%, 95%, or 96%).
4. Test-set Standard Deviation: Measure of variance in test-set accuracy across multiple runs of training, reported as a percentage.
5. Distribution-wise Standard Deviation: Estimate of the variance in accuracy on the true data distribution, reported as a percentage.
6. Class-Aggregated Calibration Error (CACE): Measure of deviation from class-wise calibration.
7. Effective Speedup: Percentage reduction in training epochs achieved by switching from random flip to alternating flip while maintaining the same level of accuracy.
8. Cross-dataset Accuracy: Percentage accuracy achieved on other datasets (CIFAR-100, SVHN, CINIC-10) without additional tuning.
9. Model Size: Number of trainable parameters in the model.
10. Convergence Rate: Average increase in accuracy per epoch during the initial training phase.

These metrics provide a comprehensive, hardware-agnostic view of the model's performance, efficiency, and generalization capabilities.




====================


tasks:
1. implement_airbench_network(widths, batchnorm_momentum, scaling_factor):
   Inputs: 
   - widths: Dictionary specifying channel widths for each block
   - batchnorm_momentum: Momentum for BatchNorm layers
   - scaling_factor: Scaling factor for final layer output
   Output: PyTorch neural network model
   Description: Implement the modified VGG-like network architecture used in airbench94, including the 2x2 first convolution and learnable biases.

2. initialize_whitening_conv(layer, train_set, eps):
   Inputs:
   - layer: First convolutional layer of the network
   - train_set: Tensor of training images
   - eps: Small constant for numerical stability
   Output: None (modifies layer in-place)
   Description: Initialize the first convolutional layer as a patch-whitening transformation based on the statistics of the training set.

3. initialize_identity_convs(model):
   Input: PyTorch neural network model
   Output: None (modifies model in-place)
   Description: Initialize all convolutional layers after the first as partial identity transforms.

4. alternating_flip(inputs, indices, epoch):
   Inputs:
   - inputs: Batch of input images
   - indices: Tensor of indices for the current batch
   - epoch: Current training epoch
   Output: Tensor of flipped images
   Description: Implement the alternating flip data augmentation strategy, deterministically flipping images based on their index and the current epoch.

5. multi_crop_inference(model, inputs):
   Inputs:
   - model: Trained neural network model
   - inputs: Batch of test images
   Output: Tensor of logits
   Description: Implement the multi-crop evaluation strategy, running the model on six augmented views of each input and returning a weighted average of the outputs.

6. train_airbench94(train_loader, test_loader, hyperparameters):
   Inputs:
   - train_loader: DataLoader for training set
   - test_loader: DataLoader for test set
   - hyperparameters: Dictionary of training hyperparameters
   Outputs:
   - trained_model: Trained neural network model
   - training_time: Time taken to train in seconds
   - final_accuracy: Final test set accuracy
   Description: Implement the full airbench94 training method, including all optimization tricks, learning rate scheduling, and Lookahead optimization.

7. calculate_cace(model, dataloader):
   Inputs:
   - model: Trained neural network model
   - dataloader: DataLoader for evaluation set
   Output: Float representing the Class-Aggregated Calibration Error
   Description: Compute the CACE metric to measure deviation from class-wise calibration.

These tasks cover the key components of the paper's methods and should allow for replication of the main results. The outputs can be compared against the reported metrics to verify correctness.




====================


task_eval_planning:
Here's a plan to convert each enumerated task into an evaluation:

1. implement_airbench_network:

Inputs:
- widths: dict with keys 'block1', 'block2', 'block3' and integer values
- batchnorm_momentum: float
- scaling_factor: float

Expected Output:
- PyTorch nn.Module

Evaluation Criteria:
a) Check model structure:
   - Verify first layer is 2x2 Conv2d with 24 output channels
   - Confirm 3 ConvGroup blocks with correct channel widths
   - Check final layers (MaxPool2d, Flatten, Linear, Mul)
b) Count parameters and compare to 1.97 million (±1%)
c) Run forward pass with dummy input (batch_size=1, channels=3, height=32, width=32)
   - Verify output shape is (1, 10)

2. initialize_whitening_conv:

Inputs:
- layer: nn.Conv2d with kernel_size=2, in_channels=3, out_channels=24
- train_set: torch.Tensor of shape (5000, 3, 32, 32) containing normalized CIFAR-10 images
- eps: float (default 5e-4)

Expected Output:
- None (layer modified in-place)

Evaluation Criteria:
a) Check weights are different from random initialization
b) Compute covariance matrix of layer output on train_set
   - Verify diagonal elements are close to 1 (within ±0.1)
c) Visualize filters and compare to Figure 2 in the paper
   - Subjective evaluation of similarity

3. initialize_identity_convs:

Inputs:
- model: Full airbench network (from task 1)

Expected Output:
- None (model modified in-place)

Evaluation Criteria:
a) For each Conv2d layer after the first:
   - Check first M filters (M = in_channels) approximate identity
     - Compute mean squared error between filter and ideal identity, should be < 0.01
   - Verify remaining filters have different statistics (e.g., mean, std) from identity filters

4. alternating_flip:

Inputs:
- inputs: torch.Tensor of shape (batch_size, 3, 32, 32)
- indices: torch.Tensor of shape (batch_size,) containing unique integers
- epoch: int

Expected Output:
- torch.Tensor of shape (batch_size, 3, 32, 32)

Evaluation Criteria:
a) For epochs 0, 1, 2, 3:
   - Verify flipping pattern matches description (50% random for epoch 0, alternating thereafter)
b) Run for two consecutive epochs with same inputs/indices
   - Check that all images are flipped exactly once
c) Test with multiple epoch values (e.g., 10, 100) to ensure consistent behavior

5. multi_crop_inference:

Inputs:
- model: Trained airbench network
- inputs: torch.Tensor of shape (batch_size, 3, 32, 32)

Expected Output:
- torch.Tensor of shape (batch_size, 10) containing logits

Evaluation Criteria:
a) Verify output shape is correct
b) Compare output to single forward pass
   - Compute mean squared difference, should be > 0
c) Evaluate accuracy on CIFAR-10 test set
   - Compare improvement over single-crop inference to reported values

6. train_airbench94:

Inputs:
- train_loader: PyTorch DataLoader for CIFAR-10 training set
- test_loader: PyTorch DataLoader for CIFAR-10 test set
- hyperparameters: dict containing all hyperparameters from the paper

Expected Outputs:
- trained_model: PyTorch nn.Module
- training_time: float (seconds)
- final_accuracy: float (percentage)

Evaluation Criteria:
a) Verify training time is within 20% of reported 3.83 seconds on A100 GPU
b) Check final accuracy is 94.01% ± 0.5%
c) Plot learning curve (accuracy vs. epoch)
   - Subjectively compare to expected curve based on paper's results

7. calculate_cace:

Inputs:
- model: Trained airbench network
- dataloader: PyTorch DataLoader for CIFAR-10 test set

Expected Output:
- cace: float

Evaluation Criteria:
a) Verify CACE is positive and between 0.01 and 0.1
b) Train two models, one with and one without test-time augmentation
   - Confirm CACE is higher for the model with test-time augmentation, matching trend in Table 4

For each evaluation, we would provide:
1. Detailed input specifications (data types, shapes, ranges)
2. Expected output format
3. Reference implementations of key components (e.g., ideal identity filter for task 3)
4. Automated tests for quantitative criteria
5. Guidelines for subjective evaluations (e.g., filter visualization comparison)

The evaluations would be designed to run automatically where possible, with clear pass/fail criteria for each check. This allows for objective assessment of the correctness of each implemented component while also capturing the nuances of the paper's methods.




====================


task_eval_baseline_implementation:
Since no specific task was requested for a baseline implementation, I'll provide a general response:

For most of the tasks outlined in this paper, a baseline implementation would not be appropriate or necessary. The tasks described are primarily about implementing novel methods or components of the airbench system, rather than improving upon existing baselines.

However, if we were to consider a baseline for the overall training method (task 6: train_airbench94), a simple baseline could be a standard CNN training on CIFAR-10 without the advanced techniques introduced in the paper. Here's a basic outline of such a baseline:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms

# Define a simple CNN
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)
        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)
        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(256 * 4 * 4, 512)
        self.fc2 = nn.Linear(512, 10)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        x = x.view(-1, 256 * 4 * 4)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Set up data loaders
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())
testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)

# Initialize the network and optimizer
net = SimpleCNN().cuda()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

# Training loop
def train():
    for epoch in range(200):  # loop over the dataset multiple times
        running_loss = 0.0
        for i, data in enumerate(trainloader, 0):
            inputs, labels = data[0].cuda(), data[1].cuda()

            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            if i % 100 == 99:    # print every 100 mini-batches
                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')
                running_loss = 0.0
        
        scheduler.step()

# Evaluation function
def evaluate():
    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].cuda(), data[1].cuda()
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    print(f'Accuracy on 10000 test images: {100 * correct / total}%')

# Run training and evaluation
train()
evaluate()
```

This baseline implementation includes:
1. A simple CNN architecture
2. Standard data augmentation (random horizontal flip and crop)
3. SGD optimizer with momentum and weight decay
4. Learning rate scheduling with cosine annealing
5. Basic training and evaluation loops

This baseline would serve as a starting point for comparison, allowing the improvements from the airbench techniques to be measured against a standard approach. The airbench implementation would then build upon this, incorporating the various optimizations and novel techniques described in the paper.




====================


task_eval_instructions:
1. implement_airbench_network:
Implement the airbench94 network architecture as described in the paper. Your function should take three inputs: a dictionary of widths for each block, a batchnorm_momentum value, and a scaling_factor. The network should follow the VGG-like structure with 7 convolutions, where the first is a 2x2 convolution and the rest are 3x3. Use GELU activations and disable biases in convolutional and linear layers except for the first convolution. The final linear layer should be scaled by the scaling_factor. Return the implemented PyTorch nn.Module.

2. initialize_whitening_conv:
Implement the patch-whitening initialization for the first convolutional layer as described in the paper. Your function should take the first convolutional layer, a tensor of training images, and an epsilon value as inputs. Compute the covariance matrix of 2x2 patches from the training images, then use its eigenvectors to initialize the layer's weights. The function should modify the layer in-place and not return anything.

3. initialize_identity_convs:
Implement the identity initialization for all convolutional layers after the first one. Your function should take the full network as input. For each convolutional layer after the first, initialize its first M filters (where M is the number of input channels) as an identity transform, and leave the remaining filters with their default initialization. The function should modify the network in-place and not return anything.

4. alternating_flip:
Implement the alternating flip data augmentation strategy. Your function should take a batch of input images, a tensor of indices for the current batch, and the current epoch number. For the first epoch, randomly flip 50% of the images. For subsequent epochs, deterministically flip images based on their index and the epoch number, ensuring that every image is flipped exactly once every two epochs. Return the augmented batch of images.

5. multi_crop_inference:
Implement the multi-crop evaluation strategy. Your function should take a trained model and a batch of test images as input. Apply six different augmentations to each input image: the original image, a version translated up-and-left by one pixel, a version translated down-and-right by one pixel, and the horizontally flipped versions of all three. Run the model on all six versions and return a weighted average of the outputs, where the two untranslated versions have a weight of 0.25 each, and the four translated versions have a weight of 0.125 each.

6. train_airbench94:
Implement the full airbench94 training method. Your function should take a training data loader, a test data loader, and a dictionary of hyperparameters as inputs. Initialize the network using the previous functions, implement the optimization process using Nesterov SGD with the provided learning rate, momentum, and weight decay values. Use the triangular learning rate schedule and Lookahead optimization as described in the paper. Train for the specified number of epochs, applying the alternating flip augmentation during training. Return the trained model, the total training time in seconds, and the final test accuracy.

7. calculate_cace:
Implement the Class-Aggregated Calibration Error (CACE) metric. Your function should take a trained model and a data loader for the evaluation set. Compute the CACE as described in the paper, measuring the deviation from class-wise calibration. Return the calculated CACE value as a float.

These instructions provide a clear outline for each task while leaving room for the engineer to implement the details. They highlight the key aspects of each method as described in the paper, ensuring that the important elements are included in the implementation.




====================


task_eval_correctness_scoring:
Here are Python functions to score the correctness of implementations for each task:

```python
import torch
import torch.nn as nn
import numpy as np
from scipy.stats import pearsonr

def score_airbench_network(implementation):
    def check_structure(model):
        layers = list(model.children())
        if len(layers) != 5:  # 3 ConvGroups + MaxPool + Linear
            return 0
        if not isinstance(layers[0], nn.Conv2d) or layers[0].kernel_size != (2, 2):
            return 0
        return 1

    def check_channel_widths(model, widths):
        for i, width in enumerate(widths.values()):
            if list(model.children())[i].conv1.out_channels != width:
                return 0
        return 1

    def check_activations(model):
        for module in model.modules():
            if isinstance(module, nn.GELU):
                return 1
        return 0

    def check_biases(model):
        for name, module in model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)) and name != '0':
                if module.bias is not None:
                    return 0
        return 1

    def check_scaling(model):
        last_layer = list(model.children())[-1]
        if isinstance(last_layer, nn.Linear):
            if abs(last_layer.weight.std().item() - 1/9) > 1e-6:
                return 0
        return 1

    widths = {'block1': 64, 'block2': 256, 'block3': 256}
    batchnorm_momentum = 0.6
    scaling_factor = 1/9

    model = implementation(widths, batchnorm_momentum, scaling_factor)
    
    score = 0
    score += 0.3 * check_structure(model)
    score += 0.2 * check_channel_widths(model, widths)
    score += 0.1 * check_activations(model)
    score += 0.2 * check_biases(model)
    score += 0.1 * check_scaling(model)

    # Check output shape
    dummy_input = torch.randn(1, 3, 32, 32)
    output = model(dummy_input)
    if output.shape == (1, 10):
        score += 0.1

    return score

def score_whitening_conv(implementation):
    class DummyConv(nn.Module):
        def __init__(self):
            super().__init__()
            self.weight = nn.Parameter(torch.randn(24, 3, 2, 2))
            self.bias = nn.Parameter(torch.zeros(24))

    layer = DummyConv()
    train_set = torch.randn(5000, 3, 32, 32)
    eps = 5e-4

    initial_weights = layer.weight.clone()
    implementation(layer, train_set, eps)

    score = 0
    
    # Check weights have changed
    if not torch.allclose(layer.weight, initial_weights):
        score += 0.3

    # Check output variance
    with torch.no_grad():
        output = nn.functional.conv2d(train_set, layer.weight, layer.bias, stride=2)
        var = output.var(dim=(0, 2, 3))
    if torch.all((var - 1).abs() < 0.1):
        score += 0.4

    # Visual check would be manual, so we'll skip it for automated scoring
    score += 0.3  # Assume visual check passes

    return score

def score_identity_convs(implementation):
    class DummyNet(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Conv2d(3, 24, 2)
            self.conv2 = nn.Conv2d(24, 64, 3)
            self.conv3 = nn.Conv2d(64, 128, 3)

    model = DummyNet()
    implementation(model)

    score = 0

    for layer in [model.conv2, model.conv3]:
        in_channels = layer.weight.shape[1]
        identity_filters = layer.weight[:in_channels, :in_channels]
        identity_error = ((identity_filters - torch.eye(in_channels).view(in_channels, in_channels, 1, 1))**2).mean()
        if identity_error < 0.01:
            score += 0.4
        
        non_identity_filters = layer.weight[in_channels:]
        if non_identity_filters.std() > 0.01:
            score += 0.1

    return score

def score_alternating_flip(implementation):
    inputs = torch.randn(100, 3, 32, 32)
    indices = torch.arange(100)

    score = 0

    # Check flipping pattern
    for epoch in range(4):
        flipped = implementation(inputs, indices, epoch)
        if epoch == 0:
            flipped_ratio = (flipped != inputs).any(dim=(1,2,3)).float().mean()
            if 0.4 < flipped_ratio < 0.6:
                score += 0.25
        else:
            expected_flipped = indices % 2 == epoch % 2
            actual_flipped = (flipped != inputs).any(dim=(1,2,3))
            if torch.all(expected_flipped == actual_flipped):
                score += 0.25

    return score

def score_multi_crop_inference(implementation):
    class DummyModel(nn.Module):
        def forward(self, x):
            return x.mean(dim=(2,3))

    model = DummyModel()
    inputs = torch.randn(10, 3, 32, 32)

    outputs = implementation(model, inputs)

    score = 0

    if outputs.shape == (10, 3):
        score += 0.3

    # Check if output is different from simple forward pass
    if not torch.allclose(outputs, model(inputs)):
        score += 0.3

    # Check if output is a weighted average
    min_val = inputs.amin(dim=(2,3)).min()
    max_val = inputs.amax(dim=(2,3)).max()
    if torch.all((outputs >= min_val) & (outputs <= max_val)):
        score += 0.4

    return score

def score_train_airbench94(implementation):
    # This function would require running the full training process,
    # which might be too time-consuming for quick evaluation.
    # Instead, we'll check if key components are present in the implementation.

    def check_component(func, component):
        return 1 if component in func.__code__.co_names else 0

    score = 0
    
    score += 0.2 * check_component(implementation, 'SGD')
    score += 0.2 * check_component(implementation, 'cosine_annealing_lr')
    score += 0.2 * check_component(implementation, 'Lookahead')
    score += 0.2 * check_component(implementation, 'alternating_flip')
    score += 0.2 * check_component(implementation, 'multi_crop_inference')

    return score

def score_calculate_cace(implementation):
    class DummyModel(nn.Module):
        def forward(self, x):
            return torch.randn(len(x), 10)

    model = DummyModel()
    dataloader = [(torch.randn(100, 3, 32, 32), torch.randint(0, 10, (100,))) for _ in range(10)]

    cace = implementation(model, dataloader)

    score = 0

    if isinstance(cace, float):
        score += 0.3

    if 0 < cace < 1:
        score += 0.3

    # Run again with a "perfect" model
    class PerfectModel(nn.Module):
        def forward(self, x):
            return torch.ones(len(x), 10) / 10

    perfect_cace = implementation(PerfectModel(), dataloader)

    if perfect_cace < cace:
        score += 0.4

    return score
```

These scoring functions aim to check the key aspects of each implementation without running full training processes. They assign partial credit for different components and behaviors expected in correct implementations. Note that these are simplified versions and may not catch all edge cases or subtle errors, but they provide a reasonable approximation of correctness for quick evaluation purposes.




====================


task_eval_metric_scoring:
Here are Python functions to score the metric results of implementations for each relevant task:

```python
import torch
import time
import numpy as np

def score_whitening_conv_metric(implementation):
    class DummyConv(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.weight = torch.nn.Parameter(torch.randn(24, 3, 2, 2))
            self.bias = torch.nn.Parameter(torch.zeros(24))

    layer = DummyConv()
    train_set = torch.randn(5000, 3, 32, 32)
    eps = 5e-4

    implementation(layer, train_set, eps)

    with torch.no_grad():
        output = torch.nn.functional.conv2d(train_set, layer.weight, layer.bias, stride=2)
        cov = torch.cov(output.reshape(output.shape[0], -1).T)
        off_diagonal = cov - torch.diag(torch.diag(cov))
        whitening_score = 1 - (off_diagonal.abs().mean() / cov.abs().mean()).item()

    return whitening_score  # Higher is better, perfect whitening would score 1

def score_alternating_flip_metric(implementation, train_function):
    # Assuming train_function takes a flip_augmentation function as an argument
    def random_flip(inputs, indices, epoch):
        return torch.where(torch.rand(inputs.shape[0], 1, 1, 1) < 0.5, inputs.flip(-1), inputs)

    random_flip_time, random_flip_acc = train_function(random_flip)
    alternating_flip_time, alternating_flip_acc = train_function(implementation)

    time_improvement = (random_flip_time - alternating_flip_time) / random_flip_time
    acc_improvement = alternating_flip_acc - random_flip_acc

    # Combine improvements, giving more weight to accuracy
    score = 0.3 * time_improvement + 0.7 * (acc_improvement / 0.01)  # Assuming 1% acc improvement is excellent
    return min(max(score, 0), 1)  # Clamp between 0 and 1

def score_multi_crop_inference_metric(implementation, model, test_loader):
    def single_crop_inference(model, inputs):
        return model(inputs)

    single_crop_acc = evaluate(model, test_loader, single_crop_inference)
    multi_crop_acc = evaluate(model, test_loader, implementation)

    improvement = multi_crop_acc - single_crop_acc
    return min(improvement / 0.02, 1)  # Assuming 2% improvement is excellent

def score_train_airbench94_metric(implementation):
    start_time = time.time()
    model, accuracy = implementation()
    end_time = time.time()

    training_time = end_time - start_time
    target_time = 3.83  # seconds, as reported in the paper
    target_accuracy = 94.01  # as reported in the paper

    time_score = max(0, 1 - abs(training_time - target_time) / target_time)
    accuracy_score = max(0, 1 - abs(accuracy - target_accuracy) / 0.5)  # 0.5% tolerance

    return 0.5 * time_score + 0.5 * accuracy_score

def score_calculate_cace_metric(implementation, model_no_tta, model_with_tta, test_loader):
    cace_no_tta = implementation(model_no_tta, test_loader)
    cace_with_tta = implementation(model_with_tta, test_loader)

    if cace_with_tta > cace_no_tta:
        return 1  # Correct behavior as per the paper
    else:
        return 0  # Incorrect behavior

# Helper function for evaluation
def evaluate(model, dataloader, inference_function):
    correct = 0
    total = 0
    model.eval()
    with torch.no_grad():
        for inputs, labels in dataloader:
            outputs = inference_function(model, inputs)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    return correct / total
```

These scoring functions aim to quantify the metric improvements described in the paper for each relevant task. They compare the implementation results to the expected outcomes based on the paper's findings. The scores are normalized to be between 0 and 1, where 1 represents performance matching or exceeding the paper's reported results.




====================


setup_description:
To implement and evaluate the airbench methods, the following setup is required:

1. Environment:
   - Python 3.7+ environment
   - PyTorch 1.7+ with CUDA support
   - NVIDIA GPU (preferably an A100 for direct comparison with paper results)
   - Common libraries: numpy, scipy, torchvision

2. Datasets:
   - CIFAR-10 dataset (will be automatically downloaded by torchvision)
   - Optional: CIFAR-100, SVHN, and CINIC-10 for generalization experiments

3. Base implementations:
   - Simple CNN class for baseline comparison
   - VGG-like network structure as described in the paper
   - Custom DataLoader class with support for alternating flip augmentation

4. Initialization functions:
   - Patch-whitening initialization function for the first convolutional layer
   - Identity initialization function for subsequent convolutional layers

5. Training components:
   - Nesterov SGD optimizer with momentum
   - Lookahead optimization wrapper
   - Triangular learning rate scheduler
   - Training loop with support for all airbench optimizations

6. Evaluation components:
   - Multi-crop inference function
   - Accuracy calculation function
   - Training time measurement utilities
   - CACE (Class-Aggregated Calibration Error) calculation function

7. Utility functions:
   - Model parameter count function
   - FLOP calculation function for training process

8. Hyperparameter configurations:
   - Dictionary or configuration file with all hyperparameters for airbench94, airbench95, and airbench96

9. Visualization tools:
   - Matplotlib or similar for plotting learning curves and filter visualizations

10. Optional: Profiling tools
    - PyTorch profiler or similar for detailed performance analysis

This setup provides all the necessary components to implement the airbench methods, run experiments, and evaluate results as described in the paper. The engineer should have access to this environment and these components before starting to implement the specific tasks.




====================