{
  "title_abstract_plus": {
    "rationale": "produce the abstract_plus. We should:\n\n1. Identify the key points from the paper content, focusing on:\n   - The main objective of the research\n   - The methods used\n   - The results achieved\n   - Any novel contributions\n\n2. Summarize these key points concisely to form the abstract.\n\n3. Add any additional high-level summaries that capture interesting aspects not covered in the abstract.\n\nHere's the abstract_plus based on the paper content:\n\nAbstract:\nThis paper introduces fast training methods for the CIFAR-10 dataset, achieving 94% accuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds on a single NVIDIA A100 GPU. The authors propose a derandomized variant of horizontal flipping augmentation called \"alternating flip,\" which improves performance over standard random flipping in all cases where flipping is beneficial. The methods build upon prior work, incorporating optimizations such as a modified network architecture, initialization techniques, and frozen patch-whitening layers. The paper presents a comprehensive analysis of the training process, including ablation studies and experiments on other datasets to demonstrate generalization.\n\nAdditional high-level summaries:\n1. The paper provides a detailed breakdown of the training method, including hyperparameter choices, network architecture, and optimization techniques.\n\n2. The authors introduce a decoupled form of expressing hyperparameters, allowing for more efficient tuning.\n\n3. The research demonstrates that the proposed methods not only achieve fast training times on CIFAR-10 but also generalize well to other datasets like CIFAR-100, SVHN, and CINIC-10.\n\n4. The paper includes an analysis of the statistical properties of the trained models, including variance and class-wise calibration.\n\n5. The authors provide open-source implementations of their methods, making them easily accessible for researchers and practitioners.",
    "title": "94% on CIFAR-10 in 3.29 Seconds on a Single GPU",
    "abstract_plus": "Title: 94% on CIFAR-10 in 3.29 Seconds on a Single GPU\n\nAbstract Plus: This paper introduces fast training methods for the CIFAR-10 dataset, achieving 94% accuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds on a single NVIDIA A100 GPU. The authors propose a derandomized variant of horizontal flipping augmentation called \"alternating flip,\" which improves performance over standard random flipping in all cases where flipping is beneficial. The methods build upon prior work, incorporating optimizations such as a modified network architecture, initialization techniques, and frozen patch-whitening layers. The paper presents a comprehensive analysis of the training process, including ablation studies and experiments on other datasets to demonstrate generalization.\n\nAdditional high-level summaries:\n1. The paper provides a detailed breakdown of the training method, including hyperparameter choices, network architecture, and optimization techniques.\n2. The authors introduce a decoupled form of expressing hyperparameters, allowing for more efficient tuning.\n3. The research demonstrates that the proposed methods not only achieve fast training times on CIFAR-10 but also generalize well to other datasets like CIFAR-100, SVHN, and CINIC-10.\n4. The paper includes an analysis of the statistical properties of the trained models, including variance and class-wise calibration.\n5. The authors provide open-source implementations of their methods, making them easily accessible for researchers and practitioners."
  },
  "quantitative_results_json": {
    "quantitative_results_json": "Let's think step by step to produce the quantitative_results_json:\n\n1. Identify the main quantitative results from the paper:\n   - Training times and accuracies for CIFAR-10\n   - Comparisons with baseline methods\n   - Results on other datasets\n   - Statistical properties of the trained models\n\n2. Format these results into JSON objects with appropriate fields:\n\n[\n  {\n    \"units\": \"seconds\",\n    \"value\": 3.29,\n    \"description\": \"Time to reach 94% accuracy on CIFAR-10\",\n    \"method\": \"airbench94_compiled\",\n    \"hardware\": \"NVIDIA A100 GPU\",\n    \"comparison\": {\n      \"baseline\": 6.3,\n      \"improvement\": \"1.9x faster\"\n    }\n  },\n  {\n    \"units\": \"seconds\",\n    \"value\": 10.4,\n    \"description\": \"Time to reach 95% accuracy on CIFAR-10\",\n    \"method\": \"airbench95\",\n    \"hardware\": \"NVIDIA A100 GPU\"\n  },\n  {\n    \"units\": \"seconds\",\n    \"value\": 46.3,\n    \"description\": \"Time to reach 96% accuracy on CIFAR-10\",\n    \"method\": \"airbench96\",\n    \"hardware\": \"NVIDIA A100 GPU\"\n  },\n  {\n    \"units\": \"accuracy\",\n    \"value\": 0.9401,\n    \"description\": \"Mean accuracy on CIFAR-10\",\n    \"method\": \"airbench94\",\n    \"notes\": \"Averaged over 1000 runs\"\n  },\n  {\n    \"units\": \"accuracy\",\n    \"value\": 0.7976,\n    \"description\": \"Accuracy on CIFAR-100\",\n    \"method\": \"airbench96\",\n    \"comparison\": {\n      \"baseline\": 0.7804,\n      \"baseline_method\": \"ResNet-18\",\n      \"improvement\": \"+1.72%\"\n    }\n  },\n  {\n    \"units\": \"accuracy\",\n    \"value\": 0.8822,\n    \"description\": \"Accuracy on CINIC-10\",\n    \"method\": \"airbench96\",\n    \"comparison\": {\n      \"baseline\": 0.8758,\n      \"baseline_method\": \"ResNet-18\",\n      \"improvement\": \"+0.64%\"\n    }\n  },\n  {\n    \"units\": \"accuracy\",\n    \"value\": 0.9764,\n    \"description\": \"Accuracy on SVHN\",\n    \"method\": \"airbench96\",\n    \"comparison\": {\n      \"baseline\": 0.9735,\n      \"baseline_method\": \"ResNet-18\",\n      \"improvement\": \"+0.29%\"\n    }\n  },\n  {\n    \"units\": \"standard_deviation\",\n    \"value\": 0.0014,\n    \"description\": \"Standard deviation of accuracy on CIFAR-10\",\n    \"method\": \"airbench94 with TTA\",\n    \"notes\": \"Test-set standard deviation\"\n  },\n  {\n    \"units\": \"standard_deviation\",\n    \"value\": 0.00029,\n    \"description\": \"Distribution-wise standard deviation of accuracy on CIFAR-10\",\n    \"method\": \"airbench94 with TTA\"\n  }\n]\n\nThis JSON list captures the main quantitative results from the paper, including the training times for different accuracy levels on CIFAR-10, comparisons with baseline methods, results on other datasets, and statistical properties of the trained models."
  },
  "core_ideas_json": {
    "core_ideas_json": "Let's think step by step to produce the core_ideas_json:\n\n1. Identify the main innovative ideas and approaches presented in the paper:\n   - Fast training methods for CIFAR-10\n   - Derandomized horizontal flipping augmentation (\"alternating flip\")\n   - Decoupled hyperparameter expression\n   - Generalization to other datasets\n   - Statistical analysis of trained models\n\n2. Formulate these ideas into concise statements:\n\n{\n  \"fast_cifar10_training\": \"Develop methods to train CIFAR-10 models to high accuracy in seconds on a single GPU\",\n  \n  \"alternating_flip\": \"A derandomized variant of horizontal flipping augmentation that improves performance over standard random flipping\",\n  \n  \"decoupled_hyperparameters\": \"Express main training hyperparameters in a decoupled form for more efficient tuning\",\n  \n  \"generalization_to_other_datasets\": \"The fast training methods generalize well to other datasets like CIFAR-100, SVHN, and CINIC-10\",\n  \n  \"statistical_analysis\": \"Analyze the statistical properties of trained models, including variance and class-wise calibration\",\n  \n  \"open_source_implementation\": \"Provide accessible open-source implementations of the fast training methods\"\n}\n\nThis core_ideas_json captures the main innovative approaches and contributions of the paper, highlighting the fast training methods, the novel alternating flip augmentation, the decoupled hyperparameter expression, generalization capabilities, statistical analysis, and the provision of open-source implementations."
  },
  "metrics_json": {
    "metrics_json": "Based on the paper content, here is the metrics_json:\n\n[\n  {\n    \"name\": \"training_time\",\n    \"description\": \"Time taken to train a model to a specified accuracy level on CIFAR-10\",\n    \"unit\": \"seconds\",\n    \"measurement_details\": \"Measured on a single NVIDIA A100 GPU\",\n    \"justification\": \"Training time is the primary metric for evaluating the speed of the proposed methods\"\n  },\n  {\n    \"name\": \"accuracy\",\n    \"description\": \"Percentage of correct predictions on the CIFAR-10 test set\",\n    \"unit\": \"percentage\",\n    \"measurement_details\": \"Evaluated after training, including test-time augmentation\",\n    \"justification\": \"Accuracy is used to ensure the fast training methods achieve high performance\"\n  },\n  {\n    \"name\": \"FLOPs\",\n    \"description\": \"Number of floating-point operations required for training\",\n    \"unit\": \"FLOPs\",\n    \"measurement_details\": \"Calculated based on the network architecture and training process\",\n    \"justification\": \"FLOPs provide a hardware-independent measure of computational complexity\"\n  },\n  {\n    \"name\": \"test_set_variance\",\n    \"description\": \"Variance in accuracy across multiple runs on the test set\",\n    \"unit\": \"standard deviation\",\n    \"measurement_details\": \"Calculated over 10,000 runs of training\",\n    \"justification\": \"Measures the consistency of the training method across different runs\"\n  },\n  {\n    \"name\": \"distribution_wise_variance\",\n    \"description\": \"Variance in accuracy across the underlying data distribution\",\n    \"unit\": \"standard deviation\",\n    \"measurement_details\": \"Estimated using methods from prior work on training variance\",\n    \"justification\": \"Provides insight into the model's generalization capabilities\"\n  },\n  {\n    \"name\": \"class_aggregated_calibration_error\",\n    \"description\": \"Deviation from perfect class-wise calibration\",\n    \"unit\": \"CACE score\",\n    \"measurement_details\": \"Calculated using the method from Jiang et al. (2021)\",\n    \"justification\": \"Measures how well the model's confidence aligns with its accuracy for each class\"\n  },\n  {\n    \"name\": \"cross_dataset_accuracy\",\n    \"description\": \"Accuracy on datasets other than CIFAR-10\",\n    \"unit\": \"percentage\",\n    \"measurement_details\": \"Evaluated on CIFAR-100, SVHN, and CINIC-10 without hyperparameter tuning\",\n    \"justification\": \"Demonstrates the generalization capabilities of the proposed methods\"\n  }\n]"
  },
  "hw_agnostic_metrics_json": {
    "hw_agnostic_metrics_json": "Let's think step by step to produce the hw_agnostic_metrics_json:\n\n1. Review the metrics provided in the metrics_json.\n2. Identify which metrics are hardware-dependent and need to be converted to hardware-agnostic versions.\n3. Create hardware-agnostic equivalents for those metrics.\n4. Format the results as a JSON list.\n\nHere's the hw_agnostic_metrics_json:\n\n[\n  {\n    \"name\": \"epochs_to_accuracy\",\n    \"description\": \"Number of training epochs required to reach a specified accuracy level on CIFAR-10\",\n    \"corresponding_hw_metric\": \"training_time\",\n    \"unit\": \"epochs\",\n    \"equivalence_justification\": \"Epochs are a hardware-independent measure of training progress, directly related to training time on any given hardware\"\n  },\n  {\n    \"name\": \"flops_to_accuracy\",\n    \"description\": \"Number of floating-point operations required to reach a specified accuracy level on CIFAR-10\",\n    \"corresponding_hw_metric\": \"training_time\",\n    \"unit\": \"FLOPs\",\n    \"equivalence_justification\": \"FLOPs provide a hardware-independent measure of computational work, directly related to training time across different hardware\"\n  },\n  {\n    \"name\": \"accuracy\",\n    \"description\": \"Percentage of correct predictions on the CIFAR-10 test set\",\n    \"corresponding_hw_metric\": \"accuracy\",\n    \"unit\": \"percentage\",\n    \"equivalence_justification\": \"Accuracy is already hardware-agnostic\"\n  },\n  {\n    \"name\": \"test_set_variance\",\n    \"description\": \"Variance in accuracy across multiple runs on the test set\",\n    \"corresponding_hw_metric\": \"test_set_variance\",\n    \"unit\": \"standard deviation\",\n    \"equivalence_justification\": \"Variance is a statistical measure independent of hardware\"\n  },\n  {\n    \"name\": \"distribution_wise_variance\",\n    \"description\": \"Variance in accuracy across the underlying data distribution\",\n    \"corresponding_hw_metric\": \"distribution_wise_variance\",\n    \"unit\": \"standard deviation\",\n    \"equivalence_justification\": \"Distribution-wise variance is a statistical measure independent of hardware\"\n  },\n  {\n    \"name\": \"class_aggregated_calibration_error\",\n    \"description\": \"Deviation from perfect class-wise calibration\",\n    \"corresponding_hw_metric\": \"class_aggregated_calibration_error\",\n    \"unit\": \"CACE score\",\n    \"equivalence_justification\": \"Calibration error is a statistical measure independent of hardware\"\n  },\n  {\n    \"name\": \"cross_dataset_accuracy\",\n    \"description\": \"Accuracy on datasets other than CIFAR-10\",\n    \"corresponding_hw_metric\": \"cross_dataset_accuracy\",\n    \"unit\": \"percentage\",\n    \"equivalence_justification\": \"Cross-dataset accuracy is a performance measure independent of hardware\"\n  }\n]\n\nThis hw_agnostic_metrics_json list provides hardware-independent equivalents for the metrics used in the paper. The main change is converting the \"training_time\" metric to \"epochs_to_accuracy\" and \"flops_to_accuracy\", which are hardware-agnostic measures of training efficiency. The other metrics remain largely unchanged as they were already hardware-independent."
  },
  "baseline_methods_json": {
    "baseline_methods_json": "Let's think step by step to produce the baseline_methods_json:\n\n1. Identify the main baseline method described in the paper:\n   - The paper mentions a \"baseline training configuration\" in Section 3.1\n\n2. Extract key details about this baseline method:\n   - Network architecture: Similar to the final model, but with some differences\n   - Optimizer: Nesterov SGD\n   - Batch size: 1024\n   - Data augmentation: Random horizontal flipping and 2-pixel random translation\n   - Learning rate schedule: Triangular\n   - Label smoothing: 0.2\n   - Evaluation: Horizontal flipping test-time augmentation\n\n3. Identify the experimental methods that improve upon or are compared to this baseline:\n   - Frozen patch-whitening initialization\n   - Identity initialization (dirac)\n   - Optimization tricks (scalebias, lookahead)\n   - Multi-crop evaluation\n   - Alternating flip\n\n4. Format this information into a JSON structure:\n\n{\n  \"baseline_sgd\": {\n    \"description\": \"Standard training configuration using Nesterov SGD\",\n    \"key_components\": [\n      \"Convolutional neural network architecture\",\n      \"Nesterov SGD optimizer\",\n      \"Random horizontal flipping\",\n      \"2-pixel random translation\",\n      \"Triangular learning rate schedule\",\n      \"Label smoothing\"\n    ],\n    \"architecture_details\": {\n      \"type\": \"Convolutional neural network\",\n      \"total_parameters\": \"1.97 million\",\n      \"conv_layers\": 7,\n      \"blocks\": 3,\n      \"first_layer\": \"2x2 convolution with no padding\"\n    },\n    \"training_details\": {\n      \"optimizer\": \"Nesterov SGD\",\n      \"batch_size\": 1024,\n      \"label_smoothing_rate\": 0.2,\n      \"learning_rate_schedule\": \"Triangular\",\n      \"data_augmentation\": [\n        \"Random horizontal flipping\",\n        \"2-pixel random translation with reflection padding\"\n      ]\n    },\n    \"evaluation_details\": {\n      \"test_time_augmentation\": \"Horizontal flipping\"\n    },\n    \"performance\": {\n      \"accuracy\": \"94%\",\n      \"training_time\": \"18.3 A100-seconds\",\n      \"epochs\": 45\n    },\n    \"experimental_methods\": [\n      \"frozen_patch_whitening\",\n      \"identity_initialization\",\n      \"scalebias_optimization\",\n      \"lookahead_optimization\",\n      \"multi_crop_evaluation\",\n      \"alternating_flip\"\n    ]\n  }\n}\n\nThis baseline_methods_json captures the key details of the baseline method described in the paper, as well as listing the experimental methods that are compared to or improve upon this baseline."
  },
  "experimental_methods_json": {
    "experimental_methods_json": "Let's think step by step to produce the experimental_methods_json:\n\n1. Identify the key experimental methods described in the paper:\n- Frozen patch-whitening initialization\n- Identity initialization (dirac)\n- Optimization tricks (scalebias, lookahead)\n- Multi-crop evaluation\n- Alternating flip\n\n2. For each method, determine:\n- Its name\n- The baseline it's compared to\n- Key modifications or features\n- Expected improvements\n- Target metrics\n\n3. Format this information into a JSON list of objects:\n\n[\n  {\n    \"name\": \"frozen_patch_whitening\",\n    \"baseline\": \"baseline_sgd\",\n    \"modifications\": [\n      \"Initialize first convolutional layer as a patch-whitening transformation\",\n      \"Freeze weights of first layer during training\",\n      \"Add learnable biases to first layer\"\n    ],\n    \"expected_improvements\": [\n      \"More than doubles training speed\",\n      \"Reduces epochs to reach 94% accuracy from 45 to 21\"\n    ],\n    \"target_metrics\": [\"training_time\", \"accuracy\"]\n  },\n  {\n    \"name\": \"identity_initialization\",\n    \"baseline\": \"baseline_sgd\",\n    \"modifications\": [\n      \"Initialize all convolutions after the first as partial identity transforms\"\n    ],\n    \"expected_improvements\": [\n      \"Reduces epochs to reach 94% accuracy from 21 to 18\"\n    ],\n    \"target_metrics\": [\"training_time\", \"accuracy\"]\n  },\n  {\n    \"name\": \"optimization_tricks\",\n    \"baseline\": \"baseline_sgd\",\n    \"modifications\": [\n      \"Increase learning rate for BatchNorm biases by 64x (scalebias)\",\n      \"Use Lookahead optimization\"\n    ],\n    \"expected_improvements\": [\n      \"Reduces epochs to reach 94% accuracy from 18 to 12\"\n    ],\n    \"target_metrics\": [\"training_time\", \"accuracy\"]\n  },\n  {\n    \"name\": \"multi_crop_evaluation\",\n    \"baseline\": \"baseline_sgd\",\n    \"modifications\": [\n      \"Evaluate model on six augmented views of each test image\",\n      \"Use weighted average of outputs for predictions\"\n    ],\n    \"expected_improvements\": [\n      \"Reduces epochs to reach 94% accuracy from 12 to 10.8\"\n    ],\n    \"target_metrics\": [\"accuracy\", \"training_time\"]\n  },\n  {\n    \"name\": \"alternating_flip\",\n    \"baseline\": \"baseline_sgd\",\n    \"modifications\": [\n      \"Use deterministic alternating pattern for horizontal flipping after first epoch\",\n      \"Ensures all unique inputs are seen every two epochs\"\n    ],\n    \"expected_improvements\": [\n      \"Reduces epochs to reach 94% accuracy from 10.8 to 9.9\",\n      \"Improves performance across various training configurations\"\n    ],\n    \"target_metrics\": [\"accuracy\", \"training_time\"]\n  }\n]\n\nThis experimental_methods_json captures the key experimental methods described in the paper, along with their modifications, expected improvements, and target metrics."
  },
  "method_metric_results": {
    "method_metric_results": "To produce the method_metric_results JSON, I'll extract the key performance metrics for the baseline and experimental methods from the paper content. Here's the step-by-step process:\n\n1. Identify the baseline and experimental methods\n2. Extract the relevant metrics for each method\n3. Format the results into a JSON structure\n\nBased on the paper content, here's the method_metric_results JSON:\n\n{\n  \"baseline_sgd\": {\n    \"accuracy\": 0.94,\n    \"training_time\": 18.3,\n    \"epochs\": 45\n  },\n  \"frozen_patch_whitening\": {\n    \"accuracy\": 0.94,\n    \"training_time\": 8.0,\n    \"epochs\": 21\n  },\n  \"identity_initialization\": {\n    \"accuracy\": 0.94,\n    \"training_time\": 6.8,\n    \"epochs\": 18\n  },\n  \"optimization_tricks\": {\n    \"accuracy\": 0.94,\n    \"training_time\": 4.6,\n    \"epochs\": 12\n  },\n  \"multi_crop_evaluation\": {\n    \"accuracy\": 0.94,\n    \"training_time\": 4.2,\n    \"epochs\": 10.8\n  },\n  \"alternating_flip\": {\n    \"accuracy\": 0.94,\n    \"training_time\": 3.83,\n    \"epochs\": 9.9\n  },\n  \"airbench94_compiled\": {\n    \"accuracy\": 0.94,\n    \"training_time\": 3.29,\n    \"epochs\": 9.9\n  },\n  \"airbench95\": {\n    \"accuracy\": 0.95,\n    \"training_time\": 10.4,\n    \"epochs\": 15\n  },\n  \"airbench96\": {\n    \"accuracy\": 0.9605,\n    \"training_time\": 46.3,\n    \"epochs\": 40\n  }\n}\n\nThis JSON structure captures the key performance metrics (accuracy, training time in seconds, and number of epochs) for the baseline method and each of the experimental methods described in the paper. The final entries (airbench94_compiled, airbench95, and airbench96) represent the culmination of all the optimizations and improvements introduced in the paper."
  },
  "task_candidates_json": {
    "task_candidates_json": "Let's think step by step to produce the task_candidates_json:\n\n1. Identify key methods and techniques from the paper that could be implemented:\n   - Alternating flip augmentation\n   - Frozen patch-whitening initialization\n   - Identity initialization for convolutions\n   - Lookahead optimization\n   - Multi-crop evaluation\n   - Decoupled hyperparameter expression\n\n2. For each method, consider:\n   - What inputs it would require\n   - What outputs it should produce\n   - What skills it would test\n   - How it could be evaluated\n\n3. Format these into JSON objects for potential tasks:\n\n{\n  \"implement_alternating_flip\": {\n    \"name\": \"Implement Alternating Flip Augmentation\",\n    \"description\": \"Implement the alternating flip augmentation method described in the paper\",\n    \"corresponding_method\": \"Alternating Flip\",\n    \"inputs\": [\n      {\n        \"name\": \"images\",\n        \"type\": \"torch.Tensor\",\n        \"shape\": \"(batch_size, channels, height, width)\"\n      },\n      {\n        \"name\": \"epoch\",\n        \"type\": \"int\"\n      }\n    ],\n    \"outputs\": [\n      {\n        \"name\": \"augmented_images\",\n        \"type\": \"torch.Tensor\",\n        \"shape\": \"(batch_size, channels, height, width)\"\n      }\n    ],\n    \"skills_tested\": [\"PyTorch\", \"Data Augmentation\", \"Conditional Logic\"],\n    \"assets_provided\": [\"baseline_random_flip_implementation\"],\n    \"evaluation_criteria\": [\n      \"Correctness: Compare output patterns with expected alternating flip behavior\",\n      \"Performance: Measure speed improvement over random flip\"\n    ],\n    \"provided_baseline\": \"Standard random flip augmentation\",\n    \"instructions_short\": \"Modify the provided random flip function to implement alternating flip as described in the paper.\",\n    \"time_to_complete\": 1.5,\n    \"difficulty\": 3,\n    \"feasibility\": 5,\n    \"research_ability\": 2\n  },\n  \n  \"implement_frozen_patch_whitening\": {\n    \"name\": \"Implement Frozen Patch-Whitening Initialization\",\n    \"description\": \"Implement the frozen patch-whitening initialization for the first convolutional layer\",\n    \"corresponding_method\": \"Frozen Patch-Whitening Initialization\",\n    \"inputs\": [\n      {\n        \"name\": \"conv_layer\",\n        \"type\": \"torch.nn.Conv2d\"\n      },\n      {\n        \"name\": \"training_images\",\n        \"type\": \"torch.Tensor\",\n        \"shape\": \"(num_images, channels, height, width)\"\n      }\n    ],\n    \"outputs\": [\n      {\n        \"name\": \"initialized_conv_layer\",\n        \"type\": \"torch.nn.Conv2d\"\n      }\n    ],\n    \"skills_tested\": [\"PyTorch\", \"Convolutional Neural Networks\", \"Linear Algebra\"],\n    \"assets_provided\": [\"sample_training_images\"],\n    \"evaluation_criteria\": [\n      \"Correctness: Verify that the initialized weights whiten the input patches\",\n      \"Performance: Measure improvement in training speed\"\n    ],\n    \"provided_baseline\": \"Standard convolutional layer initialization\",\n    \"instructions_short\": \"Implement the frozen patch-whitening initialization for a convolutional layer as described in the paper.\",\n    \"time_to_complete\": 3,\n    \"difficulty\": 4,\n    \"feasibility\": 4,\n    \"research_ability\": 3\n  },\n  \n  \"implement_lookahead_optimizer\": {\n    \"name\": \"Implement Lookahead Optimizer\",\n    \"description\": \"Implement the Lookahead optimization algorithm\",\n    \"corresponding_method\": \"Lookahead Optimization\",\n    \"inputs\": [\n      {\n        \"name\": \"base_optimizer\",\n        \"type\": \"torch.optim.Optimizer\"\n      },\n      {\n        \"name\": \"model\",\n        \"type\": \"torch.nn.Module\"\n      }\n    ],\n    \"outputs\": [\n      {\n        \"name\": \"lookahead_optimizer\",\n        \"type\": \"CustomOptimizer\"\n      }\n    ],\n    \"skills_tested\": [\"PyTorch\", \"Optimization Algorithms\", \"Object-Oriented Programming\"],\n    \"assets_provided\": [\"base_optimizer_implementation\"],\n    \"evaluation_criteria\": [\n      \"Correctness: Verify that the optimizer updates weights according to the Lookahead algorithm\",\n      \"Performance: Measure improvement in convergence speed\"\n    ],\n    \"provided_baseline\": \"Standard SGD optimizer\",\n    \"instructions_short\": \"Implement the Lookahead optimizer as described in the paper, wrapping a given base optimizer.\",\n    \"time_to_complete\": 2.5,\n    \"difficulty\": 4,\n    \"feasibility\": 5,\n    \"research_ability\": 3\n  }\n}\n\nThis task_candidates_json provides three potential tasks based on key methods from the paper. Each task includes details on inputs, outputs, evaluation criteria, and other relevant information for implementation and assessment."
  },
  "task_prerequisites_json": {
    "task_prerequisites_json": "To produce the task_prerequisites_json, let's consider the prerequisites for each task candidate:\n\n{\n  \"implement_alternating_flip\": [\n    \"PyTorch\",\n    \"CIFAR-10 dataset\",\n    \"Basic understanding of data augmentation techniques\"\n  ],\n  \"implement_frozen_patch_whitening\": [\n    \"PyTorch\",\n    \"CIFAR-10 dataset\",\n    \"NumPy\",\n    \"Basic understanding of convolutional neural networks\",\n    \"Knowledge of linear algebra (eigenvalues, eigenvectors)\"\n  ],\n  \"implement_lookahead_optimizer\": [\n    \"PyTorch\",\n    \"Basic understanding of optimization algorithms\",\n    \"Familiarity with PyTorch's optimizer classes\"\n  ]\n}\n\nThis JSON outlines the key prerequisites for each task, including necessary libraries, datasets, and background knowledge."
  },
  "task_eval_instructions_json": {
    "task_eval_instructions_json": "To produce the task_eval_instructions_json, I'll create detailed instructions for each task candidate, expanding on the short instructions provided. Here's the result:\n\n{\n  \"implement_alternating_flip\": \"Implement the alternating flip augmentation method as described in the paper. Your implementation should:\n\n1. Take a batch of images and the current epoch number as inputs.\n2. For the first epoch, randomly flip 50% of the images horizontally.\n3. For subsequent epochs, deterministically flip images based on whether they were flipped in the first epoch:\n   - On even epochs, flip only those images that were not flipped in the first epoch.\n   - On odd epochs, flip only those images that were flipped in the first epoch.\n4. Use a pseudorandom function based on image indices to determine which images to flip, avoiding the need for extra memory.\n\nYour implementation should be efficient and work with PyTorch tensors. Compare your results with the provided baseline random flip implementation to ensure correctness and measure performance improvements. Pay special attention to handling edge cases and ensuring that the alternating pattern is maintained across epochs.\",\n\n  \"implement_frozen_patch_whitening\": \"Implement the frozen patch-whitening initialization for the first convolutional layer as described in the paper. Your implementation should:\n\n1. Take a convolutional layer and a set of training images as inputs.\n2. Extract 2x2 patches from the input images.\n3. Compute the covariance matrix of these patches.\n4. Calculate the eigenvalues and eigenvectors of the covariance matrix.\n5. Initialize the convolutional layer's weights using the computed eigenvectors, scaled by the square root of their corresponding eigenvalues.\n6. Set up the layer so that its weights remain frozen during training, but allow its biases to be trained for the first few epochs.\n\nEnsure your implementation is numerically stable, especially when handling the eigenvalue calculations. Compare the initialized layer's output with the input to verify that it indeed whitens the patches. Measure the impact on training speed when using this initialization compared to standard initialization methods.\",\n\n  \"implement_lookahead_optimizer\": \"Implement the Lookahead optimization algorithm as described in the paper. Your implementation should:\n\n1. Take a base optimizer (e.g., SGD) and a PyTorch model as inputs.\n2. Create a custom optimizer class that wraps the base optimizer.\n3. Implement the Lookahead algorithm:\n   - Maintain a set of 'slow weights' initialized as a copy of the model's current weights.\n   - For k steps, update the model's weights using the base optimizer.\n   - After k steps, update the slow weights by interpolating between the current slow weights and the model's weights.\n   - Copy the updated slow weights back to the model.\n4. Implement standard optimizer methods like `step()` and `zero_grad()`.\n5. Allow configuration of the Lookahead-specific hyperparameters (k and the interpolation factor).\n\nYour implementation should be compatible with PyTorch's existing optimization framework. Test the optimizer on a simple neural network training task and compare its convergence speed and final performance with the base optimizer. Ensure that the Lookahead updates are being applied correctly by logging weight changes at each step.\"\n}\n\nThis JSON provides detailed instructions for each task, expanding on the short descriptions and giving specific steps to follow. The instructions cover the key aspects of each method, potential pitfalls to avoid, and suggestions for testing and evaluation."
  },
  "task_eval_baseline_implementation_json": {
    "task_eval_baseline_implementation_json": "Let's think step by step to produce the task_eval_baseline_implementation_json:\n\n1. We need to provide baseline implementations for each task candidate.\n2. The baselines should be simple implementations that the engineer can build upon.\n3. We'll focus on the three tasks identified earlier: implement_alternating_flip, implement_frozen_patch_whitening, and implement_lookahead_optimizer.\n4. For each task, we'll provide a basic implementation that the engineer can modify to meet the requirements.\n\nHere's the task_eval_baseline_implementation_json:\n\n{\n  \"implement_alternating_flip\": \n  \"```python\nimport torch\n\ndef random_flip(images):\n    # Standard random flip implementation\n    flip_mask = (torch.rand(len(images)) < 0.5).view(-1, 1, 1, 1)\n    return torch.where(flip_mask, images.flip(-1), images)\n\ndef alternating_flip(images, epoch):\n    # TODO: Implement alternating flip logic here\n    # For now, this just calls random_flip\n    return random_flip(images)\n\n# Example usage\nbatch_size, channels, height, width = 32, 3, 32, 32\nimages = torch.randn(batch_size, channels, height, width)\nepoch = 1\n\naugmented_images = alternating_flip(images, epoch)\n  ```\",\n\n  \"implement_frozen_patch_whitening\":\n  \"```python\nimport torch\nimport torch.nn as nn\n\ndef init_whitening_conv(conv_layer, training_images):\n    # TODO: Implement frozen patch-whitening initialization here\n    # For now, this just uses default initialization\n    nn.init.kaiming_normal_(conv_layer.weight)\n    if conv_layer.bias is not None:\n        nn.init.zeros_(conv_layer.bias)\n    return conv_layer\n\n# Example usage\nconv_layer = nn.Conv2d(3, 64, kernel_size=3, padding=1)\ntraining_images = torch.randn(1000, 3, 32, 32)  # Example training images\n\ninitialized_conv_layer = init_whitening_conv(conv_layer, training_images)\n  ```\",\n\n  \"implement_lookahead_optimizer\":\n  \"```python\nimport torch\n\nclass LookaheadOptimizer(torch.optim.Optimizer):\n    def __init__(self, base_optimizer, model):\n        self.base_optimizer = base_optimizer\n        self.model = model\n        # TODO: Implement Lookahead initialization\n\n    def step(self):\n        # TODO: Implement Lookahead step logic\n        self.base_optimizer.step()\n\n    def zero_grad(self):\n        self.base_optimizer.zero_grad()\n\n# Example usage\nmodel = torch.nn.Linear(10, 1)\nbase_optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\nlookahead_optimizer = LookaheadOptimizer(base_optimizer, model)\n\n# Training loop\nfor _ in range(100):\n    lookahead_optimizer.zero_grad()\n    # ... compute loss ...\n    # loss.backward()\n    lookahead_optimizer.step()\n  ```\"\n}\n\nThis JSON provides baseline implementations for each task. These implementations are intentionally incomplete or simplified, serving as starting points for the engineers to build upon. They include the basic structure and example usage, but leave the core functionality to be implemented by the engineer."
  },
  "task_eval_correctness_scoring_json": {
    "task_eval_correctness_scoring_json": "Let's think step by step to produce the task_eval_correctness_scoring_json:\n\n1. We need to create scoring functions for each task candidate.\n2. The scoring functions should evaluate the correctness and effectiveness of the implementations.\n3. We'll focus on the three tasks: implement_alternating_flip, implement_frozen_patch_whitening, and implement_lookahead_optimizer.\n4. For each task, we'll create a function that assigns a score between 0 and 1, with higher scores indicating better performance.\n\nHere's the task_eval_correctness_scoring_json:\n\n{\n  \"implement_alternating_flip\": \"```python\ndef score_alternating_flip_implementation(implementation, baseline):\n    score = 0.0\n    \n    # Test basic functionality\n    images = torch.randn(1000, 3, 32, 32)\n    \n    # Check first epoch behavior\n    first_epoch_flips = implementation(images, epoch=0)\n    if torch.isclose(first_epoch_flips.float().mean(), torch.tensor(0.5), atol=0.05):\n        score += 0.2\n    \n    # Check alternating behavior\n    even_epoch_flips = implementation(images, epoch=2)\n    odd_epoch_flips = implementation(images, epoch=3)\n    if not torch.allclose(even_epoch_flips, odd_epoch_flips):\n        score += 0.3\n    \n    # Check consistency across epochs\n    if torch.allclose(implementation(images, epoch=2), implementation(images, epoch=4)):\n        score += 0.2\n    \n    # Check performance\n    baseline_time = timeit.timeit(lambda: baseline(images), number=100)\n    implementation_time = timeit.timeit(lambda: implementation(images, epoch=1), number=100)\n    if implementation_time < baseline_time:\n        score += 0.3\n    \n    return score\n```\",\n\n  \"implement_frozen_patch_whitening\": \"```python\ndef score_frozen_patch_whitening_implementation(implementation, baseline):\n    score = 0.0\n    \n    # Prepare test data\n    conv_layer = nn.Conv2d(3, 64, kernel_size=2, padding=0)\n    training_images = torch.randn(1000, 3, 32, 32)\n    \n    # Initialize using the implementation\n    initialized_layer = implementation(conv_layer, training_images)\n    \n    # Check if weights are frozen\n    original_weights = initialized_layer.weight.clone()\n    dummy_input = torch.randn(1, 3, 32, 32)\n    dummy_output = initialized_layer(dummy_input)\n    dummy_output.sum().backward()\n    if torch.allclose(original_weights, initialized_layer.weight):\n        score += 0.3\n    \n    # Check whitening effect\n    input_patches = training_images.unfold(2, 2, 1).unfold(3, 2, 1).reshape(-1, 3*2*2)\n    output = initialized_layer(training_images).reshape(-1, 64)\n    input_cov = torch.cov(input_patches.T)\n    output_cov = torch.cov(output.T)\n    if torch.norm(output_cov - torch.eye(64)) < torch.norm(input_cov - torch.eye(3*2*2)):\n        score += 0.4\n    \n    # Check performance improvement\n    baseline_model = nn.Sequential(baseline(conv_layer, training_images), nn.Flatten(), nn.Linear(64*31*31, 10))\n    implementation_model = nn.Sequential(initialized_layer, nn.Flatten(), nn.Linear(64*31*31, 10))\n    \n    baseline_acc = train_and_evaluate(baseline_model, training_images)\n    implementation_acc = train_and_evaluate(implementation_model, training_images)\n    \n    if implementation_acc > baseline_acc:\n        score += 0.3\n    \n    return score\n\ndef train_and_evaluate(model, data):\n    # Simple function to train and evaluate the model\n    # Implementation details omitted for brevity\n    pass\n```\",\n\n  \"implement_lookahead_optimizer\": \"```python\ndef score_lookahead_optimizer_implementation(implementation, baseline):\n    score = 0.0\n    \n    # Prepare test data\n    model = nn.Linear(10, 1)\n    base_optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n    lookahead_optimizer = implementation(base_optimizer, model)\n    \n    # Check if slow weights are maintained\n    if hasattr(lookahead_optimizer, 'slow_weights'):\n        score += 0.2\n    \n    # Check interpolation behavior\n    initial_weights = [p.clone() for p in model.parameters()]\n    for _ in range(5):  # Simulate 5 steps\n        lookahead_optimizer.step()\n    if not all(torch.allclose(p, ip) for p, ip in zip(model.parameters(), initial_weights)):\n        score += 0.3\n    \n    # Check performance improvement\n    X = torch.randn(1000, 10)\n    y = torch.randn(1000, 1)\n    \n    baseline_loss = train_model(model, X, y, baseline)\n    lookahead_loss = train_model(model, X, y, lookahead_optimizer)\n    \n    if lookahead_loss < baseline_loss:\n        score += 0.5\n    \n    return score\n\ndef train_model(model, X, y, optimizer):\n    # Simple function to train the model and return final loss\n    # Implementation details omitted for brevity\n    pass\n```\"\n}\n\nThis task_eval_correctness_scoring_json provides scoring functions for each task. These functions evaluate the correctness and effectiveness of the implementations by checking key behaviors and comparing performance against baselines. The scores are normalized between 0 and 1, with higher scores indicating better implementations."
  },
  "task_eval_metric_scoring_json": {
    "task_eval_metric_scoring_json": "To produce the task_eval_metric_scoring_json, let's think through each task and create appropriate scoring functions:\n\n{\n  \"implement_alternating_flip\": \"```python\nimport torch\nimport time\n\ndef score_alternating_flip_metric(alternating_flip_fn, baseline_fn):\n    score = 0.0\n    batch_size, channels, height, width = 1024, 3, 32, 32\n    \n    # Prepare data\n    images = torch.randn(batch_size, channels, height, width)\n    epochs = 10\n    \n    # Measure speed\n    start_time = time.time()\n    for epoch in range(epochs):\n        _ = alternating_flip_fn(images, epoch)\n    alternating_time = time.time() - start_time\n    \n    start_time = time.time()\n    for _ in range(epochs):\n        _ = baseline_fn(images)\n    baseline_time = time.time() - start_time\n    \n    # Score based on speed improvement\n    if alternating_time < baseline_time:\n        score += 50 * (baseline_time - alternating_time) / baseline_time\n    \n    # Check correctness\n    flipped_even = alternating_flip_fn(images, 2)\n    flipped_odd = alternating_flip_fn(images, 3)\n    \n    if torch.sum(flipped_even == flipped_odd) / flipped_even.numel() < 0.1:\n        score += 25\n    \n    if torch.abs(torch.sum(flipped_even == images) / images.numel() - 0.5) < 0.05:\n        score += 25\n    \n    return score\n```\",\n\n  \"implement_frozen_patch_whitening\": \"```python\nimport torch\nimport torch.nn as nn\nimport time\n\ndef score_frozen_patch_whitening_metric(init_whitening_conv_fn, baseline_init_fn):\n    score = 0.0\n    \n    # Prepare data\n    conv_layer = nn.Conv2d(3, 64, kernel_size=2, padding=0)\n    training_images = torch.randn(1000, 3, 32, 32)\n    \n    # Measure initialization time\n    start_time = time.time()\n    whitened_layer = init_whitening_conv_fn(conv_layer, training_images)\n    whitening_time = time.time() - start_time\n    \n    start_time = time.time()\n    baseline_layer = baseline_init_fn(conv_layer, training_images)\n    baseline_time = time.time() - start_time\n    \n    # Score based on initialization speed\n    if whitening_time < baseline_time:\n        score += 20 * (baseline_time - whitening_time) / baseline_time\n    \n    # Check if weights are frozen\n    original_weights = whitened_layer.weight.clone()\n    dummy_input = torch.randn(1, 3, 32, 32)\n    dummy_output = whitened_layer(dummy_input)\n    dummy_output.sum().backward()\n    if torch.allclose(original_weights, whitened_layer.weight):\n        score += 20\n    \n    # Check whitening effect\n    input_patches = training_images.unfold(2, 2, 1).unfold(3, 2, 1).reshape(-1, 3*2*2)\n    output = whitened_layer(training_images).reshape(-1, 64)\n    input_cov = torch.cov(input_patches.T)\n    output_cov = torch.cov(output.T)\n    \n    whitening_improvement = torch.norm(input_cov - torch.eye(12)) - torch.norm(output_cov - torch.eye(64))\n    score += max(0, min(30, 30 * whitening_improvement / torch.norm(input_cov - torch.eye(12))))\n    \n    # Simple training test\n    whitened_model = nn.Sequential(whitened_layer, nn.Flatten(), nn.Linear(64*31*31, 10))\n    baseline_model = nn.Sequential(baseline_layer, nn.Flatten(), nn.Linear(64*31*31, 10))\n    \n    whitened_loss = train_and_evaluate(whitened_model, training_images)\n    baseline_loss = train_and_evaluate(baseline_model, training_images)\n    \n    if whitened_loss < baseline_loss:\n        score += 30 * (baseline_loss - whitened_loss) / baseline_loss\n    \n    return score\n\ndef train_and_evaluate(model, data):\n    # Simple function to train and evaluate the model\n    # Implementation details omitted for brevity\n    return torch.rand(1).item()  # Dummy loss\n```\",\n\n  \"implement_lookahead_optimizer\": \"```python\nimport torch\nimport torch.nn as nn\nimport time\n\ndef score_lookahead_optimizer_metric(LookaheadOptimizer, baseline_optimizer):\n    score = 0.0\n    \n    # Prepare data and model\n    X = torch.randn(1000, 10)\n    y = torch.randn(1000, 1)\n    model = nn.Linear(10, 1)\n    \n    # Initialize optimizers\n    base_optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n    lookahead_optimizer = LookaheadOptimizer(base_optimizer, model)\n    \n    # Training loop\n    n_epochs = 100\n    lookahead_losses = []\n    baseline_losses = []\n    \n    start_time = time.time()\n    for _ in range(n_epochs):\n        lookahead_loss = train_epoch(model, X, y, lookahead_optimizer)\n        lookahead_losses.append(lookahead_loss)\n    lookahead_time = time.time() - start_time\n    \n    model.reset_parameters()  # Reset model for fair comparison\n    \n    start_time = time.time()\n    for _ in range(n_epochs):\n        baseline_loss = train_epoch(model, X, y, baseline_optimizer)\n        baseline_losses.append(baseline_loss)\n    baseline_time = time.time() - start_time\n    \n    # Score based on final loss\n    if lookahead_losses[-1] < baseline_losses[-1]:\n        score += 40 * (baseline_losses[-1] - lookahead_losses[-1]) / baseline_losses[-1]\n    \n    # Score based on convergence speed\n    lookahead_converge = next((i for i, loss in enumerate(lookahead_losses) if loss <= 1.1 * lookahead_losses[-1]), n_epochs)\n    baseline_converge = next((i for i, loss in enumerate(baseline_losses) if loss <= 1.1 * baseline_losses[-1]), n_epochs)\n    \n    if lookahead_converge < baseline_converge:\n        score += 30 * (baseline_converge - lookahead_converge) / baseline_converge\n    \n    # Score based on training time\n    if lookahead_time < baseline_time:\n        score += 30 * (baseline_time - lookahead_time) / baseline_time\n    \n    return score\n\ndef train_epoch(model, X, y, optimizer):\n    model.train()\n    optimizer.zero_grad()\n    output = model(X)\n    loss = nn.MSELoss()(output, y)\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n```\"\n}\n\nThis task_eval_metric_scoring_json provides metric scoring functions for each task:\n\n1. For the alternating flip implementation, it measures speed improvement and correctness of the flipping pattern.\n\n2. For the frozen patch-whitening implementation, it evaluates initialization speed, weight freezing, whitening effect, and training performance.\n\n3. For the Lookahead optimizer implementation, it compares final loss, convergence speed, and training time against a baseline optimizer.\n\nEach function returns a score between 0 and 100, with higher scores indicating better performance on the relevant metrics for that task."
  },
  "task_eval_combined_scoring_json": {
    "task_eval_combined_scoring_json": "To produce the task_eval_combined_scoring_json, we need to create a combined scoring function for each task that incorporates both correctness and metric performance. Let's think through this step-by-step:\n\n1. For each task, we'll create a function that:\n   a) Calls both the correctness and metric scoring functions\n   b) Combines their results, giving appropriate weight to each\n   c) Returns a final score between 0 and 100\n\n2. We'll use a weighted average of the correctness and metric scores, with weights that reflect the importance of each aspect for the given task.\n\n3. We'll ensure that the combined scoring function takes only one argument (the implementation to be scored) as required.\n\nHere's the task_eval_combined_scoring_json:\n\n{\n  \"implement_alternating_flip\": \"```python\nimport torch\nimport time\n\ndef score_alternating_flip_combined(alternating_flip_fn):\n    def baseline_random_flip(images):\n        flip_mask = (torch.rand(len(images)) < 0.5).view(-1, 1, 1, 1)\n        return torch.where(flip_mask, images.flip(-1), images)\n    \n    # Correctness scoring\n    def score_correctness(implementation, baseline):\n        score = 0.0\n        images = torch.randn(1000, 3, 32, 32)\n        \n        # Check first epoch behavior\n        first_epoch_flips = implementation(images, epoch=0)\n        if torch.isclose(first_epoch_flips.float().mean(), torch.tensor(0.5), atol=0.05):\n            score += 0.2\n        \n        # Check alternating behavior\n        even_epoch_flips = implementation(images, epoch=2)\n        odd_epoch_flips = implementation(images, epoch=3)\n        if not torch.allclose(even_epoch_flips, odd_epoch_flips):\n            score += 0.3\n        \n        # Check consistency across epochs\n        if torch.allclose(implementation(images, epoch=2), implementation(images, epoch=4)):\n            score += 0.2\n        \n        # Check performance\n        baseline_time = timeit.timeit(lambda: baseline(images), number=100)\n        implementation_time = timeit.timeit(lambda: implementation(images, epoch=1), number=100)\n        if implementation_time < baseline_time:\n            score += 0.3\n        \n        return score * 100  # Scale to 0-100\n    \n    # Metric scoring\n    def score_metric(implementation, baseline):\n        score = 0.0\n        batch_size, channels, height, width = 1024, 3, 32, 32\n        images = torch.randn(batch_size, channels, height, width)\n        epochs = 10\n        \n        # Measure speed\n        start_time = time.time()\n        for epoch in range(epochs):\n            _ = implementation(images, epoch)\n        implementation_time = time.time() - start_time\n        \n        start_time = time.time()\n        for _ in range(epochs):\n            _ = baseline(images)\n        baseline_time = time.time() - start_time\n        \n        if implementation_time < baseline_time:\n            score += 50 * (baseline_time - implementation_time) / baseline_time\n        \n        # Check correctness\n        flipped_even = implementation(images, 2)\n        flipped_odd = implementation(images, 3)\n        if torch.sum(flipped_even == flipped_odd) / flipped_even.numel() < 0.1:\n            score += 25\n        if torch.abs(torch.sum(flipped_even == images) / images.numel() - 0.5) < 0.05:\n            score += 25\n        \n        return score\n    \n    correctness_score = score_correctness(alternating_flip_fn, baseline_random_flip)\n    metric_score = score_metric(alternating_flip_fn, baseline_random_flip)\n    \n    # Combine scores with weights\n    combined_score = 0.4 * correctness_score + 0.6 * metric_score\n    \n    return combined_score\n```\",\n\n  \"implement_frozen_patch_whitening\": \"```python\nimport torch\nimport torch.nn as nn\nimport time\n\ndef score_frozen_patch_whitening_combined(init_whitening_conv_fn):\n    def baseline_init(conv_layer, training_images):\n        nn.init.kaiming_normal_(conv_layer.weight)\n        if conv_layer.bias is not None:\n            nn.init.zeros_(conv_layer.bias)\n        return conv_layer\n    \n    # Correctness scoring\n    def score_correctness(implementation, baseline):\n        score = 0.0\n        conv_layer = nn.Conv2d(3, 64, kernel_size=2, padding=0)\n        training_images = torch.randn(1000, 3, 32, 32)\n        \n        initialized_layer = implementation(conv_layer, training_images)\n        \n        # Check if weights are frozen\n        original_weights = initialized_layer.weight.clone()\n        dummy_input = torch.randn(1, 3, 32, 32)\n        dummy_output = initialized_layer(dummy_input)\n        dummy_output.sum().backward()\n        if torch.allclose(original_weights, initialized_layer.weight):\n            score += 0.3\n        \n        # Check whitening effect\n        input_patches = training_images.unfold(2, 2, 1).unfold(3, 2, 1).reshape(-1, 3*2*2)\n        output = initialized_layer(training_images).reshape(-1, 64)\n        input_cov = torch.cov(input_patches.T)\n        output_cov = torch.cov(output.T)\n        if torch.norm(output_cov - torch.eye(64)) < torch.norm(input_cov - torch.eye(3*2*2)):\n            score += 0.4\n        \n        # Check performance improvement\n        baseline_model = nn.Sequential(baseline(conv_layer, training_images), nn.Flatten(), nn.Linear(64*31*31, 10))\n        implementation_model = nn.Sequential(initialized_layer, nn.Flatten(), nn.Linear(64*31*31, 10))\n        baseline_acc = train_and_evaluate(baseline_model, training_images)\n        implementation_acc = train_and_evaluate(implementation_model, training_images)\n        if implementation_acc > baseline_acc:\n            score += 0.3\n        \n        return score * 100  # Scale to 0-100\n    \n    # Metric scoring\n    def score_metric(implementation, baseline):\n        score = 0.0\n        conv_layer = nn.Conv2d(3, 64, kernel_size=2, padding=0)\n        training_images = torch.randn(1000, 3, 32, 32)\n        \n        # Measure initialization time\n        start_time = time.time()\n        whitened_layer = implementation(conv_layer, training_images)\n        whitening_time = time.time() - start_time\n        \n        start_time = time.time()\n        baseline_layer = baseline(conv_layer, training_images)\n        baseline_time = time.time() - start_time\n        \n        if whitening_time < baseline_time:\n            score += 20 * (baseline_time - whitening_time) / baseline_time\n        \n        # Check if weights are frozen\n        original_weights = whitened_layer.weight.clone()\n        dummy_input = torch.randn(1, 3, 32, 32)\n        dummy_output = whitened_layer(dummy_input)\n        dummy_output.sum().backward()\n        if torch.allclose(original_weights, whitened_layer.weight):\n            score += 20\n        \n        # Check whitening effect\n        input_patches = training_images.unfold(2, 2, 1).unfold(3, 2, 1).reshape(-1, 3*2*2)\n        output = whitened_layer(training_images).reshape(-1, 64)\n        input_cov = torch.cov(input_patches.T)\n        output_cov = torch.cov(output.T)\n        whitening_improvement = torch.norm(input_cov - torch.eye(12)) - torch.norm(output_cov - torch.eye(64))\n        score += max(0, min(30, 30 * whitening_improvement / torch.norm(input_cov - torch.eye(12))))\n        \n        # Simple training test\n        whitened_model = nn.Sequential(whitened_layer, nn.Flatten(), nn.Linear(64*31*31, 10))\n        baseline_model = nn.Sequential(baseline_layer, nn.Flatten(), nn.Linear(64*31*31, 10))\n        whitened_loss = train_and_evaluate(whitened_model, training_images)\n        baseline_loss = train_and_evaluate(baseline_model, training_images)\n        if whitened_loss < baseline_loss:\n            score += 30 * (baseline_loss - whitened_loss) / baseline_loss\n        \n        return score\n    \n    def train_and_evaluate(model, data):\n        # Simple function to train and evaluate the model\n        # Implementation details omitted for brevity\n        return torch.rand(1).item()  # Dummy loss\n    \n    correctness_score = score_correctness(init_whitening_conv_fn, baseline_init)\n    metric_score = score_metric(init_whitening_conv_fn, baseline_init)\n    \n    # Combine scores with weights\n    combined_score = 0.5 * correctness_score + 0.5 * metric_score\n    \n    return combined_score\n```\",\n\n  \"implement_lookahead_optimizer\": \"```python\nimport torch\nimport torch.nn as nn\nimport time\n\ndef score_lookahead_optimizer_combined(LookaheadOptimizer):\n    # Correctness scoring\n    def score_correctness(implementation, baseline):\n        score = 0.0\n        model = nn.Linear(10, 1)\n        base_optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n        lookahead_optimizer = implementation(base_optimizer, model)\n        \n        # Check if slow weights are maintained\n        if hasattr(lookahead_optimizer, 'slow_weights'):\n            score += 0.2\n        \n        # Check interpolation behavior\n        initial_weights = [p.clone() for p in model.parameters()]\n        for _ in range(5):  # Simulate 5 steps\n            lookahead_optimizer.step()\n        if not all(torch.allclose(p, ip) for p, ip in zip(model.parameters(), initial_weights)):\n            score += 0.3\n        \n        # Check performance improvement\n        X = torch.randn(1000, 10)\n        y = torch.randn(1000, 1)\n        baseline_loss = train_model(model, X, y, baseline)\n        lookahead_loss = train_model(model, X, y, lookahead_optimizer)\n        if lookahead_loss < baseline_loss:\n            score += 0.5\n        \n        return score * 100  # Scale to 0-100\n    \n    # Metric scoring\n    def score_metric(implementation, baseline_optimizer):\n        score = 0.0\n        X = torch.randn(1000, 10)\n        y = torch.randn(1000, 1)\n        model = nn.Linear(10, 1)\n        \n        # Initialize optimizers\n        base_optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n        lookahead_optimizer = implementation(base_optimizer, model)\n        \n        # Training loop\n        n_epochs = 100\n        lookahead_losses = []\n        baseline_losses = []\n        \n        start_time = time.time()\n        for _ in range(n_epochs):\n            lookahead_loss = train_epoch(model, X, y, lookahead_optimizer)\n            lookahead_losses.append(lookahead_loss)\n        lookahead_time = time.time() - start_time\n        \n        model.reset_parameters()  # Reset model for fair comparison\n        \n        start_time = time.time()\n        for _ in range(n_epochs):\n            baseline_loss = train_epoch(model, X, y, baseline_optimizer)\n            baseline_losses.append(baseline_loss)\n        baseline_time = time.time() - start_time\n        \n        # Score based on final loss\n        if lookahead_losses[-1] < baseline_losses[-1]:\n            score += 40 * (baseline_losses[-1] - lookahead_losses[-1]) / baseline_losses[-1]\n        \n        # Score based on convergence speed\n        lookahead_converge = next((i for i, loss in enumerate(lookahead_losses) if loss <= 1.1 * lookahead_losses[-1]), n_epochs)\n        baseline_converge = next((i for i, loss in enumerate(baseline_losses) if loss <= 1.1 * baseline_losses[-1]), n_epochs)\n        if lookahead_converge < baseline_converge:\n            score += 30 * (baseline_converge - lookahead_converge) / baseline_converge\n        \n        # Score based on training time\n        if lookahead_time < baseline_time:\n            score += 30 * (baseline_time - lookahead_time) / baseline_time\n        \n        return score\n    \n    def train_model(model, X, y, optimizer):\n        # Simple function to train the model and return final loss\n        # Implementation details omitted for brevity\n        return torch.rand(1).item()  # Dummy loss\n    \n    def train_epoch(model, X, y, optimizer):\n        model.train()\n        optimizer.zero_grad()\n        output = model(X)\n        loss = nn.MSELoss()(output, y)\n        loss.backward()\n        optimizer.step()\n        return loss.item()\n    \n    correctness_score = score_correctness(LookaheadOptimizer, torch.optim.SGD)\n    metric_score = score_metric(LookaheadOptimizer, torch.optim.SGD)\n    \n    # Combine scores with weights\n    combined_score = 0.4 * correctness_score + 0.6 * metric_score\n    \n    return combined_score\n```\"\n}\n\nThis task_eval_combined_scoring_json provides combined scoring functions for each task. These functions incorporate both correctness and metric performance, weighted appropriately for each task. The scores are normalized to a 0-100 scale, with higher scores indicating better overall performance. Each function takes only the implementation to be scored as an argument, as required."
  },
  "task_setup_script": {
    "task_setup_script": "Here's the task setup script based on the provided information:\n\n```bash\n#!/bin/bash\n\n# Help message\nif [ \"$#\" -ne 1 ]; then\n    echo \"Usage: $0 <output_directory>\"\n    exit 1\nfi\n\n# Set variables\nOUTPUT_DIR=\"$1\"\n\n# Create output directory if it doesn't exist\nmkdir -p \"$OUTPUT_DIR\"\n\n# Function to create files\ncreate_files() {\n    # Create instructions.txt\n    cat > \"$OUTPUT_DIR/instructions.txt\" << EOL\nImplement the alternating flip augmentation method as described in the paper. Your implementation should:\n1. Take a batch of images and the current epoch number as inputs.\n2. For the first epoch, randomly flip 50% of the images horizontally.\n3. For subsequent epochs, deterministically flip images based on whether they were flipped in the first epoch:\n   - On even epochs, flip only those images that were not flipped in the first epoch.\n   - On odd epochs, flip only those images that were flipped in the first epoch.\n4. Use a pseudorandom function based on image indices to determine which images to flip, avoiding the need for extra memory.\nYour implementation should be efficient and work with PyTorch tensors. Compare your results with the provided baseline random flip implementation to ensure correctness and measure performance improvements. Pay special attention to handling edge cases and ensuring that the alternating pattern is maintained across epochs.\nEOL\n\n    # Create solution.py\n    cat > \"$OUTPUT_DIR/solution.py\" << EOL\nimport torch\n\ndef random_flip(images):\n    # Standard random flip implementation\n    flip_mask = (torch.rand(len(images)) < 0.5).view(-1, 1, 1, 1)\n    return torch.where(flip_mask, images.flip(-1), images)\n\ndef alternating_flip(images, epoch):\n    # TODO: Implement alternating flip logic here\n    # For now, this just calls random_flip\n    return random_flip(images)\n\n# Example usage\nbatch_size, channels, height, width = 32, 3, 32, 32\nimages = torch.randn(batch_size, channels, height, width)\nepoch = 1\naugmented_images = alternating_flip(images, epoch)\nEOL\n\n    # Create scoring.py\n    cat > \"$OUTPUT_DIR/scoring.py\" << EOL\nimport torch\nimport time\n\ndef score_alternating_flip_combined(alternating_flip_fn):\n    def baseline_random_flip(images):\n        flip_mask = (torch.rand(len(images)) < 0.5).view(-1, 1, 1, 1)\n        return torch.where(flip_mask, images.flip(-1), images)\n\n    # Correctness scoring\n    def score_correctness(implementation, baseline):\n        score = 0.0\n        images = torch.randn(1000, 3, 32, 32)\n        \n        # Check first epoch behavior\n        first_epoch_flips = implementation(images, epoch=0)\n        if torch.isclose(first_epoch_flips.float().mean(), torch.tensor(0.5), atol=0.05):\n            score += 0.2\n        \n        # Check alternating behavior\n        even_epoch_flips = implementation(images, epoch=2)\n        odd_epoch_flips = implementation(images, epoch=3)\n        if not torch.allclose(even_epoch_flips, odd_epoch_flips):\n            score += 0.3\n        \n        # Check consistency across epochs\n        if torch.allclose(implementation(images, epoch=2), implementation(images, epoch=4)):\n            score += 0.2\n        \n        # Check performance\n        baseline_time = timeit.timeit(lambda: baseline(images), number=100)\n        implementation_time = timeit.timeit(lambda: implementation(images, epoch=1), number=100)\n        if implementation_time < baseline_time:\n            score += 0.3\n        \n        return score * 100  # Scale to 0-100\n\n    # Metric scoring\n    def score_metric(implementation, baseline):\n        score = 0.0\n        batch_size, channels, height, width = 1024, 3, 32, 32\n        images = torch.randn(batch_size, channels, height, width)\n        epochs = 10\n        \n        # Measure speed\n        start_time = time.time()\n        for epoch in range(epochs):\n            _ = implementation(images, epoch)\n        implementation_time = time.time() - start_time\n        \n        start_time = time.time()\n        for _ in range(epochs):\n            _ = baseline(images)\n        baseline_time = time.time() - start_time\n        \n        if implementation_time < baseline_time:\n            score += 50 * (baseline_time - implementation_time) / baseline_time\n        \n        # Check correctness\n        flipped_even = implementation(images, 2)\n        flipped_odd = implementation(images, 3)\n        if torch.sum(flipped_even == flipped_odd) / flipped_even.numel() < 0.1:\n            score += 25\n        if torch.abs(torch.sum(flipped_even == images) / images.numel() - 0.5) < 0.05:\n            score += 25\n        \n        return score\n\n    correctness_score = score_correctness(alternating_flip_fn, baseline_random_flip)\n    metric_score = score_metric(alternating_flip_fn, baseline_random_flip)\n    \n    # Combine scores with weights\n    combined_score = 0.4 * correctness_score + 0.6 * metric_score\n    return combined_score\nEOL\n\n    # Create requirements.txt\n    cat > \"$OUTPUT_DIR/requirements.txt\" << EOL\ntorch==1.9.0\nEOL\n}\n\n# Create the files\ncreate_files\n\n# Set up Python virtual environment\npython3 -m venv \"$OUTPUT_DIR/venv\"\nsource \"$OUTPUT_DIR/venv/bin/activate\"\n\n# Install required packages\npip install -r \"$OUTPUT_DIR/requirements.txt\"\n\necho \"Task setup complete. Files created in $OUTPUT_DIR\"\necho \"Activate the virtual environment with: source $OUTPUT_DIR/venv/bin/activate\"\n```\n\nThis script does the following:\n\n1. Takes one argument: the path to the output directory.\n2. Creates the necessary files (instructions.txt, solution.py, scoring.py, and requirements.txt) in the specified directory.\n3. Sets up a Python virtual environment in the output directory.\n4. Installs the required packages (PyTorch in this case) in the virtual environment.\n\nThe script includes error handling (checking for the correct number of arguments) and provides instructions for activating the virtual environment after setup is complete.\n\nTo use this script, save it to a file (e.g., `setup_task.sh`), make it executable with `chmod +x setup_task.sh`, and then run it with the desired output directory as an argument:\n\n```\n./setup_task.sh /path/to/output/directory\n```\n\nThis will set up the task environment in the specified directory."
  }
}