{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: magentic[anthropic] in ./.venv/lib/python3.11/site-packages (0.28.1)\n",
      "Requirement already satisfied: anthropic>=0.27.0 in ./.venv/lib/python3.11/site-packages (from magentic[anthropic]) (0.32.0)\n",
      "Requirement already satisfied: filetype in ./.venv/lib/python3.11/site-packages (from magentic[anthropic]) (1.2.0)\n",
      "Requirement already satisfied: logfire-api in ./.venv/lib/python3.11/site-packages (from magentic[anthropic]) (0.48.1)\n",
      "Requirement already satisfied: openai>=1.26.0 in ./.venv/lib/python3.11/site-packages (from magentic[anthropic]) (1.38.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in ./.venv/lib/python3.11/site-packages (from magentic[anthropic]) (2.8.2)\n",
      "Requirement already satisfied: pydantic-settings>=2.0.0 in ./.venv/lib/python3.11/site-packages (from magentic[anthropic]) (2.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (0.5.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (1.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (0.19.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (4.12.2)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.11/site-packages (from openai>=1.26.0->magentic[anthropic]) (4.66.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.0.0->magentic[anthropic]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.0.0->magentic[anthropic]) (2.20.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./.venv/lib/python3.11/site-packages (from pydantic-settings>=2.0.0->magentic[anthropic]) (1.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->anthropic>=0.27.0->magentic[anthropic]) (3.7)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->anthropic>=0.27.0->magentic[anthropic]) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->anthropic>=0.27.0->magentic[anthropic]) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic>=0.27.0->magentic[anthropic]) (0.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.11/site-packages (from tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (0.24.5)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (6.0.1)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (2.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: dspy-ai in ./.venv/lib/python3.11/site-packages (2.4.13)\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: backoff in ./.venv/lib/python3.11/site-packages (from dspy-ai) (2.2.1)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.11/site-packages (from dspy-ai) (2.20.0)\n",
      "Requirement already satisfied: joblib~=1.3 in ./.venv/lib/python3.11/site-packages (from dspy-ai) (1.4.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=0.28.1 in ./.venv/lib/python3.11/site-packages (from dspy-ai) (1.38.0)\n",
      "Requirement already satisfied: optuna in ./.venv/lib/python3.11/site-packages (from dspy-ai) (3.6.1)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from dspy-ai) (2.2.2)\n",
      "Requirement already satisfied: pydantic~=2.0 in ./.venv/lib/python3.11/site-packages (from dspy-ai) (2.8.2)\n",
      "Requirement already satisfied: regex in ./.venv/lib/python3.11/site-packages (from dspy-ai) (2024.7.24)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from dspy-ai) (2.32.3)\n",
      "Requirement already satisfied: structlog in ./.venv/lib/python3.11/site-packages (from dspy-ai) (24.4.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from dspy-ai) (4.66.4)\n",
      "Requirement already satisfied: ujson in ./.venv/lib/python3.11/site-packages (from dspy-ai) (5.10.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic~=2.0->dspy-ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.venv/lib/python3.11/site-packages (from pydantic~=2.0->dspy-ai) (2.20.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (2.0.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (0.3.8)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets->dspy-ai) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (3.10.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (0.24.5)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (6.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->dspy-ai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->dspy-ai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->dspy-ai) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->dspy-ai) (2024.7.4)\n",
      "Requirement already satisfied: alembic>=1.5.0 in ./.venv/lib/python3.11/site-packages (from optuna->dspy-ai) (1.13.2)\n",
      "Requirement already satisfied: colorlog in ./.venv/lib/python3.11/site-packages (from optuna->dspy-ai) (6.8.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in ./.venv/lib/python3.11/site-packages (from optuna->dspy-ai) (2.0.32)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->dspy-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->dspy-ai) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->dspy-ai) (2024.1)\n",
      "Requirement already satisfied: Mako in ./.venv/lib/python3.11/site-packages (from alembic>=1.5.0->optuna->dspy-ai) (1.3.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets->dspy-ai) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets->dspy-ai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets->dspy-ai) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets->dspy-ai) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets->dspy-ai) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets->dspy-ai) (1.9.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=0.28.1->dspy-ai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=0.28.1->dspy-ai) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->dspy-ai) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in ./.venv/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna->dspy-ai) (2.1.5)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install \"magentic[anthropic]\"\n",
    "! pip install dspy-ai PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jaidhyani/Desktop/atefar\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: atefar 0.1.0\n",
      "Uninstalling atefar-0.1.0:\n",
      "  Successfully uninstalled atefar-0.1.0\n",
      "Obtaining file:///Users/jaidhyani/Desktop/atefar\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: atefar\n",
      "  Building editable for atefar (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for atefar: filename=atefar-0.1.0-py2.py3-none-any.whl size=895 sha256=8608e0a9242dc90429748827ec12d87c93c52c602affb7a176e1a04597151e0a\n",
      "  Stored in directory: /private/var/folders/5k/7nfpl0cs5999pzhndyybcn800000gn/T/pip-ephem-wheel-cache-v66pt9bs/wheels/ef/a2/67/4e2ac8e515272dc3b0b25ecb48e15da2a9a7bc08f6adb925d8\n",
      "Successfully built atefar\n",
      "Installing collected packages: atefar\n",
      "Successfully installed atefar-0.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall -y atefar\n",
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update requirements.txt\n",
    "! pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version     Editable project location\n",
      "------------------ ----------- -------------------------------\n",
      "aiohappyeyeballs   2.3.5\n",
      "aiohttp            3.10.2\n",
      "aiosignal          1.3.1\n",
      "alembic            1.13.2\n",
      "annotated-types    0.7.0\n",
      "anthropic          0.32.0\n",
      "anyio              4.4.0\n",
      "appnope            0.1.4\n",
      "asttokens          2.4.1\n",
      "atefar             0.1.0       /Users/jaidhyani/Desktop/atefar\n",
      "attrs              24.2.0\n",
      "backoff            2.2.1\n",
      "certifi            2024.7.4\n",
      "charset-normalizer 3.3.2\n",
      "colorlog           6.8.2\n",
      "comm               0.2.2\n",
      "datasets           2.20.0\n",
      "debugpy            1.8.5\n",
      "decorator          5.1.1\n",
      "dill               0.3.8\n",
      "distro             1.9.0\n",
      "dspy-ai            2.4.13\n",
      "executing          2.0.1\n",
      "filelock           3.15.4\n",
      "filetype           1.2.0\n",
      "frozenlist         1.4.1\n",
      "fsspec             2024.5.0\n",
      "h11                0.14.0\n",
      "httpcore           1.0.5\n",
      "httpx              0.27.0\n",
      "huggingface-hub    0.24.5\n",
      "idna               3.7\n",
      "ipykernel          6.29.5\n",
      "ipython            8.26.0\n",
      "jedi               0.19.1\n",
      "jiter              0.5.0\n",
      "joblib             1.4.2\n",
      "jupyter_client     8.6.2\n",
      "jupyter_core       5.7.2\n",
      "logfire-api        0.48.1\n",
      "magentic           0.28.1\n",
      "Mako               1.3.5\n",
      "MarkupSafe         2.1.5\n",
      "matplotlib-inline  0.1.7\n",
      "multidict          6.0.5\n",
      "multiprocess       0.70.16\n",
      "nest-asyncio       1.6.0\n",
      "numpy              2.0.1\n",
      "openai             1.38.0\n",
      "optuna             3.6.1\n",
      "packaging          24.1\n",
      "pandas             2.2.2\n",
      "parso              0.8.4\n",
      "pexpect            4.9.0\n",
      "pip                24.0\n",
      "platformdirs       4.2.2\n",
      "prompt_toolkit     3.0.47\n",
      "psutil             6.0.0\n",
      "ptyprocess         0.7.0\n",
      "pure_eval          0.2.3\n",
      "pyarrow            17.0.0\n",
      "pyarrow-hotfix     0.6\n",
      "pydantic           2.8.2\n",
      "pydantic_core      2.20.1\n",
      "pydantic-settings  2.4.0\n",
      "Pygments           2.18.0\n",
      "PyPDF2             3.0.1\n",
      "python-dateutil    2.9.0.post0\n",
      "python-dotenv      1.0.1\n",
      "pytz               2024.1\n",
      "PyYAML             6.0.1\n",
      "pyzmq              26.0.3\n",
      "regex              2024.7.24\n",
      "requests           2.32.3\n",
      "setuptools         65.5.0\n",
      "six                1.16.0\n",
      "sniffio            1.3.1\n",
      "SQLAlchemy         2.0.32\n",
      "stack-data         0.6.3\n",
      "structlog          24.4.0\n",
      "tokenizers         0.19.1\n",
      "tornado            6.4.1\n",
      "tqdm               4.66.4\n",
      "traitlets          5.14.3\n",
      "typing_extensions  4.12.2\n",
      "tzdata             2024.1\n",
      "ujson              5.10.0\n",
      "urllib3            2.2.2\n",
      "wcwidth            0.2.13\n",
      "xxhash             3.4.1\n",
      "yarl               1.9.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/jaidhyani/miniforge3/envs/mp4/lib/python311.zip', '/Users/jaidhyani/miniforge3/envs/mp4/lib/python3.11', '/Users/jaidhyani/miniforge3/envs/mp4/lib/python3.11/lib-dynload', '', '/Users/jaidhyani/Desktop/atefar/.venv/lib/python3.11/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaidhyani/Desktop/atefar/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "\n",
    "import atefar\n",
    "from atefar import paper_analysis, pdf_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from dspy.primitives.program import Module\n",
    "from typing import Any, Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SigData:\n",
    "    name: str\n",
    "    desc: str\n",
    "\n",
    "@dataclass\n",
    "class SigStep:\n",
    "    outputs: list[SigData]\n",
    "    module: Module = dspy.ChainOfThought\n",
    "    kwargs: Optional[dict[str, Any]] = None\n",
    "\n",
    "@dataclass\n",
    "class SigChain:\n",
    "    inputs: list[SigData]\n",
    "    steps: list[SigStep]\n",
    "\n",
    "paper_chain = SigChain(\n",
    "    inputs=[SigData(\"paper_content\", \"The full text content of the research paper\")],\n",
    "    steps=[\n",
    "        SigStep([\n",
    "                SigData(\"title\", \"The title of the paper\"),\n",
    "                SigData(\"abstract_plus\", \"The abstract of the paper, plus optional additional high-level summaries to capture interesting aspects of the paper not otherwise covered in the abstract\"),\n",
    "        ],),\n",
    "        SigStep([\n",
    "            SigData(\n",
    "                \"core_ideas\", \n",
    "                \"\"\"\n",
    "                Enumerate the key innovative ideas in the paper that led to specific, implementable improvements over baseline methods. For each idea:\n",
    "                1. Briefly describe the idea\n",
    "                2. Explain how it improves upon a baseline approach\n",
    "                3. Mention the specific metric(s) it aims to improve\n",
    "\n",
    "                Example:\n",
    "                1. Idea: Skip connections in neural networks\n",
    "                - Description: Direct connections between non-adjacent layers\n",
    "                - Improvement: Allows better gradient flow in deep networks\n",
    "                - Target metrics: Training speed, final accuracy\n",
    "                \"\"\"\n",
    "            ),\n",
    "        ],),\n",
    "        SigStep([\n",
    "            SigData(\n",
    "                \"metrics\",\n",
    "                \"\"\"\n",
    "                For each metric used in the paper:\n",
    "                1. Provide a clear, implementable definition\n",
    "                2. Specify the exact calculation method\n",
    "                3. Describe the typical range of values and what they indicate\n",
    "                4. Note any specific conditions or datasets used for measurement\n",
    "                5. List which experimental methods target this metric\n",
    "                6. Provide a simple Python function to calculate the metric, taking only the experimental method as an argument\n",
    "\n",
    "                Example 1: Perplexity\n",
    "                1. Definition: A measurement of how well a probability model predicts a sample, used in NLP to evaluate language models.\n",
    "                2. Calculation: Perplexity = exp(cross_entropy_loss)\n",
    "                3. Typical range: \n",
    "                    - Lower is better\n",
    "                    - For word-level language models on PTB: 50-100 is good, <60 is state-of-the-art\n",
    "                4. Conditions: Calculated on held-out test data, using the Penn Treebank (PTB) dataset\n",
    "                5. Targeted by: \n",
    "                    - Experimental Method 1: Sparse Attention\n",
    "                    - Experimental Method 3: Adaptive Layer Normalization\n",
    "                6. Python function:\n",
    "                ```python\n",
    "                import torch\n",
    "                import torch.nn.functional as F\n",
    "\n",
    "                # Preset constants\n",
    "                test_data = load_ptb_test_data()  # Function to load PTB test data\n",
    "                vocab_size = 10000  # PTB vocabulary size\n",
    "\n",
    "                def calculate_perplexity(experimental_method):\n",
    "                    model = experimental_method()  # Assuming the method returns an instantiated model\n",
    "                    model.eval()\n",
    "                    total_loss = 0\n",
    "                    total_tokens = 0\n",
    "                    with torch.no_grad():\n",
    "                        for batch in test_data:\n",
    "                            inputs, targets = batch\n",
    "                            outputs = model(inputs)\n",
    "                            loss = F.cross_entropy(outputs.view(-1, vocab_size), targets.view(-1), reduction='sum')\n",
    "                            total_loss += loss.item()\n",
    "                            total_tokens += targets.numel()\n",
    "                    return torch.exp(total_loss / total_tokens)\n",
    "                ```\n",
    "\n",
    "                Example 2: BLEU Score (for machine translation)\n",
    "                1. Definition: Bilingual Evaluation Understudy, measures the similarity between machine-generated translations and reference translations.\n",
    "                2. Calculation: Geometric mean of n-gram precisions (typically n=1 to 4), with a brevity penalty.\n",
    "                3. Typical range: \n",
    "                    - 0 to 100, higher is better\n",
    "                    - 30-50 is considered good for many language pairs\n",
    "                4. Conditions: Calculated on the WMT14 English-German test set\n",
    "                5. Targeted by:\n",
    "                    - Experimental Method 2: Multi-head Attention Pruning\n",
    "                    - Experimental Method 4: Gradient Accumulation Training\n",
    "                6. Python function:\n",
    "                ```python\n",
    "                from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "                # Preset constants\n",
    "                test_data = load_wmt14_en_de_test_data()  # Function to load WMT14 En-De test data\n",
    "                reference_translations = load_reference_translations()  # Function to load reference translations\n",
    "\n",
    "                def calculate_bleu(experimental_method):\n",
    "                    model = experimental_method()  # Assuming the method returns an instantiated model\n",
    "                    hypotheses = []\n",
    "                    for source_sentence in test_data:\n",
    "                        translation = model.translate(source_sentence)\n",
    "                        hypotheses.append(translation)\n",
    "                    return corpus_bleu([[ref] for ref in reference_translations], hypotheses) * 100\n",
    "                ```\n",
    "\n",
    "                Example 3: Inference Speed (tokens/second)\n",
    "                1. Definition: The number of tokens processed per second during model inference.\n",
    "                2. Calculation: (Number of tokens in test set) / (Total inference time)\n",
    "                3. Typical range:\n",
    "                    - Higher is better\n",
    "                    - Varies greatly depending on model size and hardware; baseline might be 1000 tokens/sec\n",
    "                4. Conditions: Measured on a single NVIDIA V100 GPU, using a fixed test set of 10,000 sentences\n",
    "                5. Targeted by:\n",
    "                    - Experimental Method 1: Sparse Attention\n",
    "                    - Experimental Method 5: Quantization-Aware Training\n",
    "                6. Python function:\n",
    "                ```python\n",
    "                import time\n",
    "\n",
    "                # Preset constants\n",
    "                test_sentences = load_test_sentences()  # Function to load 10,000 test sentences\n",
    "                num_tokens = sum(len(sentence.split()) for sentence in test_sentences)\n",
    "\n",
    "                def measure_inference_speed(experimental_method):\n",
    "                    model = experimental_method()  # Assuming the method returns an instantiated model\n",
    "                    model.eval()\n",
    "                    start_time = time.time()\n",
    "                    with torch.no_grad():\n",
    "                        for sentence in test_sentences:\n",
    "                            _ = model(sentence)\n",
    "                    total_time = time.time() - start_time\n",
    "                    return num_tokens / total_time\n",
    "                ```\n",
    "                \"\"\"\n",
    "            ),\n",
    "        ],),\n",
    "        SigStep([\n",
    "            SigData(\n",
    "                \"baseline_methods\", \n",
    "                \"\"\"\n",
    "                    Describe the key baseline methods that the paper's improvements build upon. For each method:\n",
    "                        1. Provide a brief, high-level description\n",
    "                        2. List the key components or steps of the method\n",
    "                        3. Mention any specific architectural details, hyperparameters, or algorithmic choices that are crucial for implementation\n",
    "                        4. Note which experimental methods modify this baseline\n",
    "\n",
    "                    Example:\n",
    "                        Baseline Method: Vanilla Transformer\n",
    "                            1. Description: A sequence-to-sequence model based on self-attention mechanisms\n",
    "                            2. Key components:\n",
    "                                - Multi-head self-attention layers\n",
    "                                - Feed-forward neural networks\n",
    "                                - Layer normalization\n",
    "                                - Positional encodings\n",
    "                            3. Crucial details:\n",
    "                                - 6 encoder and 6 decoder layers\n",
    "                                - 8 attention heads\n",
    "                                - 512-dimensional embeddings\n",
    "                                - Learning rate warmup over 4000 steps\n",
    "                            4. Modified by: Experimental methods 1 (sparse attention) and 2 (adaptive layer normalization)\n",
    "                \"\"\"\n",
    "            ),\n",
    "        ],),\n",
    "        SigStep([\n",
    "            SigData(\n",
    "                \"experimental_methods\", \n",
    "                \"\"\"\n",
    "                Enumerate the key improvements or modifications proposed in the paper. For each method:\n",
    "                    1. Identify the baseline method it modifies\n",
    "                    2. Describe the specific changes or additions to the baseline\n",
    "                    3. Explain how these changes are expected to improve performance\n",
    "                    4. List the metrics this modification aims to improve\n",
    "\n",
    "                Example 1:\n",
    "                    1. Baseline: Vanilla Transformer\n",
    "                    2. Modification: Sparse Attention Mechanism\n",
    "                        - Replace full attention with sparse attention patterns\n",
    "                        - Implement fixed or learned attention patterns\n",
    "                    3. Expected improvements:\n",
    "                        - Reduced computational complexity from O(n²) to O(n log n)\n",
    "                        - Better handling of long sequences\n",
    "                    4. Target metrics: Inference speed, memory usage, performance on long-sequence tasks\n",
    "\n",
    "                Example 2:\n",
    "                    1. Baseline: Vanilla Transformer\n",
    "                    2. Modification: Adaptive Layer Normalization\n",
    "                        - Replace fixed layer norm with input-dependent dynamic scaling\n",
    "                        - Implement a small network to predict normalization parameters\n",
    "                    3. Expected improvements:\n",
    "                        - Better adaptation to varying input distributions\n",
    "                        - Improved gradient flow in deep networks\n",
    "                    4. Target metrics: Training convergence speed, final model accuracy\n",
    "                \"\"\"\n",
    "            ),\n",
    "        ],),\n",
    "        SigStep([\n",
    "            SigData(\n",
    "                \"method_metric_results\", \n",
    "                \"\"\"\n",
    "                For each baseline and experimental method, as well as combinations thereof, list the metric results that were reported in the paper. These should be quantitative results used to compare the methods to each other.\n",
    "                \"\"\"\n",
    "            ),\n",
    "        ],),\n",
    "        SigStep([\n",
    "            SigData(\n",
    "                \"hw_agnostic_metrics\", \n",
    "                \"\"\"\n",
    "                List metrics that correspond to the paper's metrics, but are not hardware-specific. These may be copied from input metrics where appropriate, or they may be generated if not otherwise available. \n",
    "                For example, if the paper reports time to train on a specific hardware setup, a hardware-agnostic metric might be number of samples used for training, or total FLOPS used to train to the same level of accuracy.\n",
    "                Hardware agnostic metrics should be closely correlated with the hardware-specific metrics and indicate the same relative performance of different methods.\n",
    "                \"\"\"\n",
    "            ),\n",
    "        ],),\n",
    "        SigStep([\n",
    "            SigData(\n",
    "                \"tasks\", \n",
    "                \"\"\"\n",
    "                For each experimental method, describe a programming task that implements the improvement. Each task should:\n",
    "                    1. Identify the baseline implementation to be modified\n",
    "                    2. Clearly state the specific modification or addition to be made\n",
    "                    3. Specify the inputs and outputs of the function, including types\n",
    "                    4. Describe any key constraints or requirements for the implementation\n",
    "                    5. Mention how the improvement's effect could be measured or observed in the output\n",
    "\n",
    "                Example:\n",
    "                    Task: Implement Sparse Attention in Transformer\n",
    "                        1. Baseline: transformer.py containing a standard Transformer implementation\n",
    "                        2. Modification: Implement a sparse attention mechanism in the MultiHeadAttention class\n",
    "                        3. Inputs/Outputs:\n",
    "                            - Input: torch.Tensor of shape (batch_size, seq_length, d_model)\n",
    "                            - Output: torch.Tensor of same shape as input\n",
    "                        4. Key requirements:\n",
    "                            - Implement a fixed sparse attention pattern (e.g., local + global attention)\n",
    "                            - Ensure the sparse attention matrix is properly masked and normalized\n",
    "                            - Maintain compatibility with the rest of the Transformer architecture\n",
    "                        5. Observable effects:\n",
    "                            - Reduced memory usage for attention weights\n",
    "                            - Faster computation time for forward and backward passes\n",
    "                            - Similar or improved perplexity on language modeling tasks\n",
    "                \"\"\"\n",
    "            ),\n",
    "        ],),\n",
    "        SigStep([\n",
    "            SigData(\n",
    "                \"task_eval_planning\",\n",
    "                \"\"\"\n",
    "\n",
    "                For each task, plan an evaluation that tests the engineer's ability to implement the specific improvement. Include:\n",
    "                    1. The baseline implementation to be provided\n",
    "                    2. A clear description of the required modification\n",
    "                    3. Sample inputs to test both the original functionality and the improvement\n",
    "                    4. Expected changes in the output that demonstrate the improvement\n",
    "                    5. Metrics to measure the implementation's correctness and performance gain\n",
    "\n",
    "                Example:\n",
    "                    Eval Plan: Sparse Attention Implementation\n",
    "                        1. Baseline: Provide transformer.py with standard MultiHeadAttention class\n",
    "                        2. Required modification: Implement sparse attention with local+global pattern\n",
    "                        3. Sample inputs:\n",
    "                            - Short sequence (length 50) to test preserved functionality\n",
    "                            - Long sequence (length 1000) to test improvement\n",
    "                        4. Expected changes:\n",
    "                            - Similar output quality for short sequences\n",
    "                            - Improved quality or efficiency for long sequences\n",
    "                            - Sparse attention weight matrices in intermediate outputs\n",
    "                        5. Evaluation metrics:\n",
    "                            - Correctness: Compare attention patterns with expected sparsity\n",
    "                            - Performance: Measure speed and memory usage improvements\n",
    "                            - Quality: Compare perplexity on a held-out test set\n",
    "                \"\"\"\n",
    "            ),\n",
    "        ],),\n",
    "        SigStep([\n",
    "            SigData(\n",
    "                \"task_eval_baseline_implementation\", \n",
    "                \"\"\"\n",
    "                For each task eval, if applicable, provide a baseline implementation that will be given to the engineer to modify. This should be in python. If any ML is required, \n",
    "                assume that the engineer has access to a python environment with GPUs, and prefer to use torch for ML tasks. Other common libraries may be used as well, but\n",
    "                the implementation should be as simple as possible. The baseline implementation should be as close to the paper's method as possible, and allow the engineer \n",
    "                to focus on the specific modification required by the task. If this task does not require a baseline implementation, simply indicate that instead.\n",
    "                \"\"\"\n",
    "            ),\n",
    "        ],),\n",
    "        SigStep([\n",
    "            SigData(\n",
    "                \"task_eval_instructions\", \n",
    "                \"\"\"\n",
    "                For each enumerated task, describe the instructions that will be given to the engineer to complete the task. Instructions should be clear and concise,\n",
    "                identify the inputs and outputs, and provide any additional information that the engineer will need to complete the task. If the task involves modifying a baseline implementation,\n",
    "                the instructions should clearly describe the modification to be made, and may refer to the baseline implementation.\n",
    "                \"\"\"\n",
    "            ),\n",
    "        ]),\n",
    "        SigStep([\n",
    "            SigData(\n",
    "                \"task_eval_correctness_scoring\", \n",
    "                \"\"\"\n",
    "                For each task, write a Python function to score the correctness of the implementation. This function should:\n",
    "                    1. Take as input the engineer's modified function and the original baseline\n",
    "                    2. Test both the preserved baseline functionality and the specific improvement\n",
    "                    3. Check for the presence and correct implementation of the new feature\n",
    "                    4. Verify that unrelated aspects of the baseline remain unchanged\n",
    "                    5. Return a score between 0 and 1, with guidelines for partial credit\n",
    "\n",
    "                Example:\n",
    "                    ```python\n",
    "                    def score_sparse_attention_implementation(modified_transformer, baseline_transformer):\n",
    "                        score = 0.0\n",
    "                        \n",
    "                        # Test preserved functionality\n",
    "                        short_input = torch.randn(32, 50, 512)\n",
    "                        if torch.allclose(modified_transformer(short_input), baseline_transformer(short_input), atol=1e-5):\n",
    "                            score += 0.3\n",
    "                        \n",
    "                        # Test improvement\n",
    "                        long_input = torch.randn(32, 1000, 512)\n",
    "                        modified_output = modified_transformer(long_input)\n",
    "                        \n",
    "                        # Check for sparse attention pattern\n",
    "                        attention_weights = modified_transformer.encoder.layers[0].self_attn.attn_weights\n",
    "                        if attention_weights.float().to_dense().count_nonzero() / attention_weights.numel() < 0.2:\n",
    "                            score += 0.4\n",
    "                        \n",
    "                        # Check for improved efficiency\n",
    "                        start_time = time.time()\n",
    "                        modified_transformer(long_input)\n",
    "                        modified_time = time.time() - start_time\n",
    "                        \n",
    "                        start_time = time.time()\n",
    "                        baseline_transformer(long_input)\n",
    "                        baseline_time = time.time() - start_time\n",
    "                        \n",
    "                        if modified_time < 0.8 * baseline_time:\n",
    "                            score += 0.3\n",
    "                        \n",
    "                        return score\n",
    "                    ```\n",
    "                \"\"\"\n",
    "            ),\n",
    "        ]),\n",
    "        SigStep([\n",
    "            SigData(\n",
    "                \"task_eval_metric_scoring\", \n",
    "                \"\"\"\n",
    "                For each task eval, write a python function that will be used to score the metric results of the implementation. The input will be the function written by the engineer,\n",
    "                and the output will be a score between 0 and 1 indicating how well the implementation matches the expected results. This will typically be completed by comparing the output of the\n",
    "                implementation to the output of the method in the paper for some known input. For example, if the task was to modify a neural-network-building function to add skip connections,\n",
    "                the scoring function should check whether the improvement of the modified network over the baseline network matches the expected improvement. \n",
    "                \"\"\"\n",
    "            ),\n",
    "        ]),\n",
    "        SigStep([\n",
    "            SigData(\n",
    "                \"implementation_difficulty\", \n",
    "                \"\"\"\n",
    "                For each task, estimate the difficulty of implementing the improvement:\n",
    "                    1. Rate the complexity on a scale of 1-5\n",
    "                    2. Estimate the time required for an average ML engineer to implement\n",
    "                    3. List any specific challenges or potential pitfalls\n",
    "                    4. Suggest any helpful resources or references for implementation\n",
    "\n",
    "                Example 1: Sparse Attention Implementation\n",
    "                    1. Complexity: 4/5\n",
    "                    2. Estimated time: 4-6 hours\n",
    "                    3. Challenges and pitfalls:\n",
    "                        - Efficient implementation of sparse matrix operations\n",
    "                        - Ensuring correct backpropagation through sparse operations\n",
    "                        - Maintaining numerical stability with sparse normalization\n",
    "                    4. Helpful resources:\n",
    "                        - \"Generating Long Sequences with Sparse Transformers\" (Child et al., 2019)\n",
    "                        - PyTorch Sparse module documentation\n",
    "                        - \"Longformer: The Long-Document Transformer\" (Beltagy et al., 2020)\n",
    "\n",
    "                Example 2: Adaptive Layer Normalization\n",
    "                1. Complexity: 3/5\n",
    "                2. Estimated time: 2-3 hours\n",
    "                3. Challenges and pitfalls:\n",
    "                    - Ensuring the adaptive parameters are properly initialized\n",
    "                    - Avoiding overfitting in the normalization parameter prediction\n",
    "                    - Maintaining overall model stability during training\n",
    "                4. Helpful resources:\n",
    "                    - \"Layer Normalization\" (Ba et al., 2016)\n",
    "                    - PyTorch nn.LayerNorm implementation\n",
    "                    - \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\" (Ioffe & Szegedy, 2015)\n",
    "                \"\"\"\n",
    "            ),\n",
    "        ],),\n",
    "        SigStep([\n",
    "            SigData(\n",
    "                \"data_processing\",\n",
    "                \"\"\"\n",
    "                Describe any specific data processing techniques used in the paper:\n",
    "                1. Data cleaning or preprocessing steps\n",
    "                2. Feature engineering methods\n",
    "                3. Data augmentation techniques\n",
    "                4. Sampling or batching strategies\n",
    "\n",
    "                Example 1: NLP Preprocessing for Transformer Model\n",
    "                1. Data cleaning:\n",
    "                    - Lowercase all text\n",
    "                    - Remove special characters and extra whitespace\n",
    "                    - Tokenization using SentencePiece with vocab size 32,000\n",
    "                2. Feature engineering:\n",
    "                    - Convert tokens to integer IDs\n",
    "                    - Add special tokens: [CLS] at start, [SEP] at end of each sentence\n",
    "                3. Data augmentation:\n",
    "                    - Random word masking (15% of tokens)\n",
    "                    - Random word replacement (10% of tokens)\n",
    "                4. Batching strategy:\n",
    "                    - Group sentences of similar length\n",
    "                    - Pad to max length in batch\n",
    "                    - Use attention mask to ignore padding\n",
    "\n",
    "                Example 2: Image Processing for CNN\n",
    "                1. Data cleaning:\n",
    "                    - Remove corrupted images\n",
    "                    - Resize all images to 224x224 pixels\n",
    "                2. Feature engineering:\n",
    "                    - Normalize pixel values to range [0, 1]\n",
    "                    - Subtract mean and divide by std dev (per channel)\n",
    "                3. Data augmentation:\n",
    "                    - Random horizontal flips (50% probability)\n",
    "                    - Random crops (224x224 from 256x256 padded image)\n",
    "                    - Color jitter (brightness, contrast, saturation)\n",
    "                4. Batching strategy:\n",
    "                    - Random sampling\n",
    "                    - Batch size of 32 for training, 64 for validation\n",
    "                \"\"\"\n",
    ") \n",
    "        ]),            \n",
    "        SigStep([\n",
    "            SigData(\n",
    "                \"setup_description\", \n",
    "                \"\"\"\n",
    "    Provide a detailed setup guide:\n",
    "        1. List all required libraries and their minimum versions (and maximum only if necessary)\n",
    "        2. Describe the necessary compute resources (GPU, RAM, etc.)\n",
    "        3. Specify any datasets, including versions and how to access them\n",
    "        4. Include a sample environment setup script\n",
    "        5. Note any potential compatibility issues or common setup pitfalls\n",
    "\n",
    "    Example:\n",
    "    1. Required libraries:\n",
    "       - PyTorch>=1.9.0\n",
    "       - torchvision>=0.10.0\n",
    "       - transformers>=4.9.2\n",
    "       - numpy>=1.21.2\n",
    "       - pandas>=1.3.3\n",
    "       - matplotlib>=3.4.3\n",
    "    2. Compute resources:\n",
    "       - GPU: NVIDIA V100 or A100 (at least 16GB VRAM)\n",
    "       - RAM: 32GB minimum, 64GB recommended\n",
    "       - Storage: 100GB free space for datasets and model checkpoints\n",
    "    3. Datasets:\n",
    "       - ImageNet (ILSVRC2012), available from image-net.org\n",
    "       - COCO 2017, available from cocodataset.org\n",
    "    4. Environment setup:\n",
    "    ```bash\n",
    "    # Create and activate conda environment\n",
    "    conda create -n paper_env python=3.8\n",
    "    conda activate paper_env\n",
    "\n",
    "    # Install PyTorch with CUDA support\n",
    "    conda install pytorch torchvision cudatoolkit=11.1 -c pytorch -c nvidia\n",
    "\n",
    "    # Install other dependencies\n",
    "    pip install transformers numpy pandas matplotlib\n",
    "\n",
    "    # Set up data directories\n",
    "    mkdir -p data/{imagenet,coco}\n",
    "    # Download datasets to respective directories\n",
    "    ```\n",
    "    5. Potential issues:\n",
    "       - CUDA version mismatch: Ensure CUDA toolkit version matches PyTorch build\n",
    "       - OOM errors: Reduce batch size or use gradient accumulation for large models\n",
    "       - Dataset download issues: Use academic credentials for ImageNet access\n",
    "                \n",
    "                \n",
    "                A description of the setup required before implementing the task functions. This should include a list of assets that need to be available to start.\n",
    "                These may include datasets, trained models, training functions, etc. Assume that a python torch environment with GPUs is available. Any common libraries\n",
    "                or public assets may be used (e.g. huggingface transformers, torchvision, etc.). If a dataset is required, it should be available in a public location.\n",
    "                \"\"\"\n",
    "            ),\n",
    "        ],),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "def solve_sigchain(chain: SigChain, inputs=dict[str, str]) -> tuple[dict[str, Any], dict[str, Any]]:\n",
    "    input_sigs = chain.inputs.copy()\n",
    "    input_vals = inputs.copy()\n",
    "    solvers = dict()\n",
    "    results = dict()\n",
    "    for step in chain.steps:\n",
    "        step_name = \"_\".join([o.name for o in step.outputs])\n",
    "        step_inputs_str = \", \".join([i.name for i in input_sigs])\n",
    "        step_outputs_str = \", \".join([o.name for o in step.outputs])\n",
    "        print(f\"Running step {step_name}\")\n",
    "        print(f\"  Inputs: {step_inputs_str}\")\n",
    "        print(f\"  Outputs: {step_outputs_str}\")\n",
    "        class NextSig(dspy.Signature):\n",
    "            pass\n",
    "        for i in input_sigs:\n",
    "            NextSig = NextSig.append(i.name, dspy.InputField(desc=i.desc))\n",
    "        for o in step.outputs:\n",
    "            NextSig = NextSig.append(o.name, dspy.OutputField(desc=o.desc))\n",
    "        step_solver = step.module(NextSig, **(step.kwargs or {}))\n",
    "        result = step_solver(**input_vals)\n",
    "        for o in step.outputs:\n",
    "            input_vals[o.name] = result[o.name]\n",
    "        input_sigs.extend(step.outputs)\n",
    "        solvers[step_name] = step_solver\n",
    "        results[step_name] = result\n",
    "    return solvers, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = pdf_utils.extract_text_from_pdf(\"papers/94cifar.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step title_abstract_plus\n",
      "  Inputs: paper_content\n",
      "  Outputs: title, abstract_plus\n",
      "Running step core_ideas\n",
      "  Inputs: paper_content, title, abstract_plus\n",
      "  Outputs: core_ideas\n",
      "Running step metrics\n",
      "  Inputs: paper_content, title, abstract_plus, core_ideas\n",
      "  Outputs: metrics\n",
      "Running step baseline_methods\n",
      "  Inputs: paper_content, title, abstract_plus, core_ideas, metrics\n",
      "  Outputs: baseline_methods\n",
      "Running step experimental_methods\n",
      "  Inputs: paper_content, title, abstract_plus, core_ideas, metrics, baseline_methods\n",
      "  Outputs: experimental_methods\n",
      "Running step method_metric_results\n",
      "  Inputs: paper_content, title, abstract_plus, core_ideas, metrics, baseline_methods, experimental_methods\n",
      "  Outputs: method_metric_results\n",
      "Running step hw_agnostic_metrics\n",
      "  Inputs: paper_content, title, abstract_plus, core_ideas, metrics, baseline_methods, experimental_methods, method_metric_results\n",
      "  Outputs: hw_agnostic_metrics\n",
      "Running step tasks\n",
      "  Inputs: paper_content, title, abstract_plus, core_ideas, metrics, baseline_methods, experimental_methods, method_metric_results, hw_agnostic_metrics\n",
      "  Outputs: tasks\n",
      "Running step task_eval_planning\n",
      "  Inputs: paper_content, title, abstract_plus, core_ideas, metrics, baseline_methods, experimental_methods, method_metric_results, hw_agnostic_metrics, tasks\n",
      "  Outputs: task_eval_planning\n",
      "Running step task_eval_baseline_implementation\n",
      "  Inputs: paper_content, title, abstract_plus, core_ideas, metrics, baseline_methods, experimental_methods, method_metric_results, hw_agnostic_metrics, tasks, task_eval_planning\n",
      "  Outputs: task_eval_baseline_implementation\n",
      "Running step task_eval_instructions\n",
      "  Inputs: paper_content, title, abstract_plus, core_ideas, metrics, baseline_methods, experimental_methods, method_metric_results, hw_agnostic_metrics, tasks, task_eval_planning, task_eval_baseline_implementation\n",
      "  Outputs: task_eval_instructions\n",
      "Running step task_eval_correctness_scoring\n",
      "  Inputs: paper_content, title, abstract_plus, core_ideas, metrics, baseline_methods, experimental_methods, method_metric_results, hw_agnostic_metrics, tasks, task_eval_planning, task_eval_baseline_implementation, task_eval_instructions\n",
      "  Outputs: task_eval_correctness_scoring\n",
      "Running step task_eval_metric_scoring\n",
      "  Inputs: paper_content, title, abstract_plus, core_ideas, metrics, baseline_methods, experimental_methods, method_metric_results, hw_agnostic_metrics, tasks, task_eval_planning, task_eval_baseline_implementation, task_eval_instructions, task_eval_correctness_scoring\n",
      "  Outputs: task_eval_metric_scoring\n",
      "Running step implementation_difficulty\n",
      "  Inputs: paper_content, title, abstract_plus, core_ideas, metrics, baseline_methods, experimental_methods, method_metric_results, hw_agnostic_metrics, tasks, task_eval_planning, task_eval_baseline_implementation, task_eval_instructions, task_eval_correctness_scoring, task_eval_metric_scoring\n",
      "  Outputs: implementation_difficulty\n",
      "Running step data_processing\n",
      "  Inputs: paper_content, title, abstract_plus, core_ideas, metrics, baseline_methods, experimental_methods, method_metric_results, hw_agnostic_metrics, tasks, task_eval_planning, task_eval_baseline_implementation, task_eval_instructions, task_eval_correctness_scoring, task_eval_metric_scoring, implementation_difficulty\n",
      "  Outputs: data_processing\n",
      "Running step setup_description\n",
      "  Inputs: paper_content, title, abstract_plus, core_ideas, metrics, baseline_methods, experimental_methods, method_metric_results, hw_agnostic_metrics, tasks, task_eval_planning, task_eval_baseline_implementation, task_eval_instructions, task_eval_correctness_scoring, task_eval_metric_scoring, implementation_difficulty, data_processing\n",
      "  Outputs: setup_description\n"
     ]
    }
   ],
   "source": [
    "solvers, results = solve_sigchain(paper_chain, {\"paper_content\": pdf_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = {}\n",
    "records[\"v4\"] = (solvers, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since no specific task was requested for a baseline implementation, I'll provide a general response:\n",
      "\n",
      "For most of the tasks outlined in this paper, a baseline implementation would not be appropriate or necessary. The tasks described are primarily about implementing novel methods or components of the airbench system, rather than improving upon existing baselines.\n",
      "\n",
      "However, if we were to consider a baseline for the overall training method (task 6: train_airbench94), a simple baseline could be a standard CNN training on CIFAR-10 without the advanced techniques introduced in the paper. Here's a basic outline of such a baseline:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "\n",
      "# Define a simple CNN\n",
      "class SimpleCNN(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(SimpleCNN, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
      "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
      "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
      "        self.fc2 = nn.Linear(512, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(torch.relu(self.conv1(x)))\n",
      "        x = self.pool(torch.relu(self.conv2(x)))\n",
      "        x = self.pool(torch.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 256 * 4 * 4)\n",
      "        x = torch.relu(self.fc1(x))\n",
      "        x = self.fc2(x)\n",
      "        return x\n",
      "\n",
      "# Set up data loaders\n",
      "transform = transforms.Compose([\n",
      "    transforms.RandomHorizontalFlip(),\n",
      "    transforms.RandomCrop(32, padding=4),\n",
      "    transforms.ToTensor(),\n",
      "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
      "])\n",
      "\n",
      "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
      "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
      "\n",
      "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
      "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
      "\n",
      "# Initialize the network and optimizer\n",
      "net = SimpleCNN().cuda()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
      "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
      "\n",
      "# Training loop\n",
      "def train():\n",
      "    for epoch in range(200):  # loop over the dataset multiple times\n",
      "        running_loss = 0.0\n",
      "        for i, data in enumerate(trainloader, 0):\n",
      "            inputs, labels = data[0].cuda(), data[1].cuda()\n",
      "\n",
      "            optimizer.zero_grad()\n",
      "            outputs = net(inputs)\n",
      "            loss = criterion(outputs, labels)\n",
      "            loss.backward()\n",
      "            optimizer.step()\n",
      "\n",
      "            running_loss += loss.item()\n",
      "            if i % 100 == 99:    # print every 100 mini-batches\n",
      "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
      "                running_loss = 0.0\n",
      "        \n",
      "        scheduler.step()\n",
      "\n",
      "# Evaluation function\n",
      "def evaluate():\n",
      "    correct = 0\n",
      "    total = 0\n",
      "    with torch.no_grad():\n",
      "        for data in testloader:\n",
      "            images, labels = data[0].cuda(), data[1].cuda()\n",
      "            outputs = net(images)\n",
      "            _, predicted = torch.max(outputs.data, 1)\n",
      "            total += labels.size(0)\n",
      "            correct += (predicted == labels).sum().item()\n",
      "    \n",
      "    print(f'Accuracy on 10000 test images: {100 * correct / total}%')\n",
      "\n",
      "# Run training and evaluation\n",
      "train()\n",
      "evaluate()\n",
      "```\n",
      "\n",
      "This baseline implementation includes:\n",
      "1. A simple CNN architecture\n",
      "2. Standard data augmentation (random horizontal flip and crop)\n",
      "3. SGD optimizer with momentum and weight decay\n",
      "4. Learning rate scheduling with cosine annealing\n",
      "5. Basic training and evaluation loops\n",
      "\n",
      "This baseline would serve as a starting point for comparison, allowing the improvements from the airbench techniques to be measured against a standard approach. The airbench implementation would then build upon this, incorporating the various optimizations and novel techniques described in the paper.\n"
     ]
    }
   ],
   "source": [
    "print(results['task_eval_baseline_implementation']['task_eval_baseline_implementation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Python environment with PyTorch (version 2.1.2 or newer) and CUDA support\n",
      "2. CUDA-capable GPU\n",
      "3. Python libraries: torch, torchvision\n",
      "4. CIFAR-10 dataset (will be downloaded automatically by torchvision)\n",
      "5. Configuration dictionary for hyperparameters\n",
      "6. Modules for:\n",
      "   - Network architecture (VGG-like with modifications)\n",
      "   - Data loading with custom augmentations (including alternating flip)\n",
      "   - Training loop with optimization techniques (Nesterov SGD, Lookahead, etc.)\n",
      "   - Evaluation function with test-time augmentation options\n",
      "7. Logging functionality for tracking training progress and results\n",
      "\n",
      "With this setup, we should be able to implement and run the airbench training methods described in the paper. The CIFAR-10 dataset will be automatically downloaded when first used, and the custom network and training procedures can be implemented using the provided PyTorch environment.\n"
     ]
    }
   ],
   "source": [
    "print(results['setup']['setup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:\n",
      "94% on CIFAR-10 in 3.29 Seconds on a Single GPU\n",
      "\n",
      "\n",
      "abstract_plus:\n",
      "This paper introduces ultra-fast training methods for the CIFAR-10 dataset, achieving 94% accuracy in just 3.29 seconds on a single NVIDIA A100 GPU. This represents a 1.9x speedup over the previous state-of-the-art. The authors also develop methods targeting 95% accuracy in 10.4 seconds and 96% accuracy in 46.3 seconds. A key innovation is the introduction of \"alternating flip\" data augmentation, which improves upon standard random flipping in most scenarios. The training methods are released as open-source code.\n",
      "\n",
      "The paper provides a detailed analysis of how different components contribute to the overall speedup, including network architecture choices, optimization techniques, and the novel data augmentation. Experiments demonstrate that the developed methods generalize well to other datasets like CIFAR-100 and SVHN without additional tuning. The authors also investigate statistical properties of their training approach, showing low variance between runs and discussing trade-offs in calibration.\n",
      "\n",
      "Interestingly, the paper notes that the progression from 80.5% accuracy in 2011 to the current 94% can be attributed entirely to algorithmic progress rather than increased computational power, as their method actually uses fewer FLOPs than earlier approaches. This work has potential implications for accelerating research and reducing the cost of experiments involving CIFAR-10 and similar datasets.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "core_ideas:\n",
      "1. Alternating flip data augmentation\n",
      "- Description: Instead of random flipping, images are flipped in a deterministic alternating pattern after the first epoch\n",
      "- Improvement: Reduces redundancy in data augmentation, ensuring all unique inputs are seen every two epochs\n",
      "- Target metrics: Training speed, final accuracy\n",
      "\n",
      "2. Frozen patch-whitening initialization\n",
      "- Description: Initialize the first convolutional layer as a patch-whitening transformation and freeze its weights during training\n",
      "- Improvement: Provides a better starting point for the network, allowing faster convergence\n",
      "- Target metrics: Training speed, final accuracy\n",
      "\n",
      "3. Identity initialization for convolutions\n",
      "- Description: Initialize convolutions after the first layer as partial identity transforms\n",
      "- Improvement: Eases gradient flow in deep networks, allowing for faster training\n",
      "- Target metrics: Training speed\n",
      "\n",
      "4. Increased learning rate for BatchNorm biases\n",
      "- Description: Scale up the learning rate for BatchNorm biases by a factor of 64\n",
      "- Improvement: Allows for faster adaptation of normalization parameters\n",
      "- Target metrics: Training speed\n",
      "\n",
      "5. Lookahead optimization\n",
      "- Description: Use the Lookahead optimizer, which maintains a slow-moving average of the weights\n",
      "- Improvement: Provides more stable optimization, potentially allowing for higher learning rates\n",
      "- Target metrics: Training stability, final accuracy\n",
      "\n",
      "6. Multi-crop evaluation\n",
      "- Description: Use multiple augmented views of test images during inference\n",
      "- Improvement: Provides more robust predictions by averaging over different views\n",
      "- Target metrics: Final accuracy\n",
      "\n",
      "7. Decoupled hyperparameters\n",
      "- Description: Express main training hyperparameters in a decoupled form for independent tuning\n",
      "- Improvement: Allows for faster and more efficient hyperparameter optimization\n",
      "- Target metrics: Ease of tuning, potentially leading to better final accuracy\n",
      "\n",
      "These core ideas collectively contribute to the significant speedup and high accuracy achieved by the proposed training method.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "metrics:\n",
      "1. Training Time\n",
      "   1. Definition: The time taken to train the model and produce test-set predictions\n",
      "   2. Calculation: Measured from first access of training data to final test-set prediction\n",
      "   3. Typical range: 3-50 seconds for CIFAR-10 (lower is better)\n",
      "   4. Conditions: Measured on a single NVIDIA A100 GPU\n",
      "   5. Targeted by: All experimental methods (airbench94, airbench95, airbench96)\n",
      "   6. Python function:\n",
      "   ```python\n",
      "   import time\n",
      "   import torch\n",
      "\n",
      "   def measure_training_time(experimental_method):\n",
      "       torch.cuda.synchronize()  # Ensure GPU is ready\n",
      "       start_time = time.time()\n",
      "       model = experimental_method()  # Train and evaluate\n",
      "       torch.cuda.synchronize()  # Ensure all GPU operations are complete\n",
      "       end_time = time.time()\n",
      "       return end_time - start_time\n",
      "   ```\n",
      "\n",
      "2. Accuracy\n",
      "   1. Definition: Percentage of correct classifications on the CIFAR-10 test set\n",
      "   2. Calculation: (Number of correct predictions / Total number of test samples) * 100\n",
      "   3. Typical range: 93-96% (higher is better)\n",
      "   4. Conditions: Uses test-time augmentation (TTA)\n",
      "   5. Targeted by: All experimental methods (airbench94, airbench95, airbench96)\n",
      "   6. Python function:\n",
      "   ```python\n",
      "   import torch\n",
      "   from torchvision import datasets, transforms\n",
      "\n",
      "   def calculate_accuracy(experimental_method):\n",
      "       model = experimental_method()\n",
      "       test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
      "       test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False)\n",
      "       \n",
      "       correct = 0\n",
      "       total = 0\n",
      "       with torch.no_grad():\n",
      "           for images, labels in test_loader:\n",
      "               outputs = model(images)\n",
      "               _, predicted = torch.max(outputs.data, 1)\n",
      "               total += labels.size(0)\n",
      "               correct += (predicted == labels).sum().item()\n",
      "       \n",
      "       return (correct / total) * 100\n",
      "   ```\n",
      "\n",
      "3. FLOPs (Floating Point Operations)\n",
      "   1. Definition: Number of floating-point operations required for training\n",
      "   2. Calculation: Sum of all floating-point operations in forward and backward passes\n",
      "   3. Typical range: 10^14 to 10^16 FLOPs for CIFAR-10 (lower is better)\n",
      "   4. Conditions: Calculated for the entire training process\n",
      "   5. Targeted by: All experimental methods (airbench94, airbench95, airbench96)\n",
      "   6. Python function:\n",
      "   ```python\n",
      "   import torch\n",
      "   from thop import profile\n",
      "\n",
      "   def calculate_flops(experimental_method):\n",
      "       model = experimental_method()\n",
      "       input = torch.randn(1, 3, 32, 32)  # CIFAR-10 input shape\n",
      "       flops, _ = profile(model, inputs=(input,))\n",
      "       return flops * num_training_iterations  # Multiply by number of training iterations\n",
      "   ```\n",
      "\n",
      "4. Test-set Variance\n",
      "   1. Definition: Variability in accuracy across multiple runs on the test set\n",
      "   2. Calculation: Standard deviation of accuracy over multiple runs\n",
      "   3. Typical range: 0.11-0.16% for airbench methods\n",
      "   4. Conditions: Calculated over 10,000 runs\n",
      "   5. Targeted by: Analysis of training stability\n",
      "   6. Python function:\n",
      "   ```python\n",
      "   import numpy as np\n",
      "\n",
      "   def calculate_test_set_variance(experimental_method, num_runs=10000):\n",
      "       accuracies = [calculate_accuracy(experimental_method) for _ in range(num_runs)]\n",
      "       return np.std(accuracies)\n",
      "   ```\n",
      "\n",
      "5. Distribution-wise Variance\n",
      "   1. Definition: Estimated variance in performance on the underlying data distribution\n",
      "   2. Calculation: Complex estimation procedure described in Jordan (2023)\n",
      "   3. Typical range: 0.018-0.037% for airbench methods (lower than test-set variance)\n",
      "   4. Conditions: Estimated from multiple runs\n",
      "   5. Targeted by: Analysis of training stability and generalization\n",
      "   6. Python function:\n",
      "   ```python\n",
      "   # This is a simplified placeholder. The actual calculation is complex and requires\n",
      "   # implementation of the method described in Jordan (2023)\n",
      "   def estimate_distribution_wise_variance(experimental_method, num_runs=10000):\n",
      "       # Placeholder for the complex estimation procedure\n",
      "       return complex_estimation_procedure(experimental_method, num_runs)\n",
      "   ```\n",
      "\n",
      "6. Class-aggregated Calibration Error (CACE)\n",
      "   1. Definition: Measure of deviation from class-wise calibration\n",
      "   2. Calculation: As defined in Jiang et al. (2021)\n",
      "   3. Typical range: 0.02-0.05 for airbench methods (lower is better)\n",
      "   4. Conditions: Calculated on the test set\n",
      "   5. Targeted by: Analysis of model calibration\n",
      "   6. Python function:\n",
      "   ```python\n",
      "   # This is a simplified placeholder. The actual calculation requires\n",
      "   # implementation of the method described in Jiang et al. (2021)\n",
      "   def calculate_cace(experimental_method):\n",
      "       model = experimental_method()\n",
      "       # Placeholder for the CACE calculation procedure\n",
      "       return cace_calculation_procedure(model, test_data)\n",
      "   ```\n",
      "\n",
      "These metrics provide a comprehensive evaluation of the training methods introduced in the paper, covering speed, accuracy, computational efficiency, stability, and calibration.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "baseline_methods:\n",
      "1. Standard CNN Training on CIFAR-10\n",
      "1. Description: A basic convolutional neural network trained on CIFAR-10 using standard techniques\n",
      "2. Key components:\n",
      "   - Convolutional layers\n",
      "   - Max pooling layers\n",
      "   - Fully connected layers\n",
      "   - SGD optimizer\n",
      "   - Cross-entropy loss\n",
      "3. Crucial details:\n",
      "   - Data augmentation: Random horizontal flips and crops\n",
      "   - Learning rate schedule: Step decay\n",
      "   - Batch size: 128\n",
      "4. Modified by: All experimental methods build upon this baseline\n",
      "\n",
      "2. tysam-code's CIFAR-10 Training Method (2023)\n",
      "1. Description: A fast training method for CIFAR-10 that achieves 94% accuracy in 6.3 A100-seconds\n",
      "2. Key components:\n",
      "   - Custom CNN architecture\n",
      "   - Patch-whitening initialization\n",
      "   - Nesterov SGD optimizer\n",
      "   - Data augmentation\n",
      "3. Crucial details:\n",
      "   - First layer: 2x2 convolution with no padding\n",
      "   - Frozen patch-whitening layer\n",
      "   - Identity initialization for convolutions\n",
      "   - Increased learning rate for BatchNorm biases\n",
      "4. Modified by: Experimental methods 1 (airbench94), 2 (airbench95), and 3 (airbench96)\n",
      "\n",
      "3. Page's CIFAR-10 Training Method (2019)\n",
      "1. Description: A training method focused on optimizing ResNet architecture for CIFAR-10\n",
      "2. Key components:\n",
      "   - Modified ResNet architecture\n",
      "   - Optimization tricks\n",
      "   - Frozen patch-whitening layer\n",
      "3. Crucial details:\n",
      "   - Increased learning rate for BatchNorm biases\n",
      "   - Custom learning rate schedule\n",
      "   - Careful weight initialization\n",
      "4. Modified by: Experimental methods 1 (airbench94), 2 (airbench95), and 3 (airbench96)\n",
      "\n",
      "4. DAWNBench Competition Winner (based on Page 2019)\n",
      "1. Description: The winning submission for the CIFAR-10 track of the 2017-2020 Stanford DAWNBench training speed competition\n",
      "2. Key components:\n",
      "   - Modified version of Page's (2019) method\n",
      "   - Optimized for multi-GPU training\n",
      "3. Crucial details:\n",
      "   - Reaches 94% accuracy in 10 seconds on 8 V100 GPUs\n",
      "   - Specific modifications to Page's method not detailed in the current paper\n",
      "4. Modified by: Not directly modified, but serves as a performance benchmark\n",
      "\n",
      "5. Standard ResNet-18 Training\n",
      "1. Description: A widely used baseline for image classification tasks\n",
      "2. Key components:\n",
      "   - ResNet-18 architecture\n",
      "   - SGD optimizer with momentum\n",
      "   - Cross-entropy loss\n",
      "3. Crucial details:\n",
      "   - 18 layers with residual connections\n",
      "   - Batch normalization after each convolutional layer\n",
      "   - ReLU activation functions\n",
      "   - Standard data augmentation (random crops and flips)\n",
      "4. Modified by: Not directly modified for CIFAR-10 in this paper, but used as a comparison point for other datasets\n",
      "\n",
      "These baseline methods provide the foundation upon which the paper's improvements are built, allowing for a clear understanding of the contributions made by the new techniques introduced in the paper.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "experimental_methods:\n",
      "Here are the key experimental methods proposed in the paper:\n",
      "\n",
      "1. airbench94\n",
      "1. Baseline: Combines elements from tysam-code (2023) and Page (2019)\n",
      "2. Modifications:\n",
      "   - Custom CNN architecture with 1.97 million parameters\n",
      "   - Frozen patch-whitening initialization for the first layer\n",
      "   - Identity initialization for convolutions after the first layer \n",
      "   - Increased learning rate for BatchNorm biases\n",
      "   - Lookahead optimization\n",
      "   - Multi-crop evaluation (test-time augmentation)\n",
      "   - Alternating flip data augmentation\n",
      "3. Expected improvements:\n",
      "   - Significantly faster training time\n",
      "   - Maintain high accuracy (94%)\n",
      "4. Target metrics: Training speed, accuracy\n",
      "\n",
      "2. airbench95 \n",
      "1. Baseline: airbench94\n",
      "2. Modifications:\n",
      "   - Increased training epochs from 9.9 to 15\n",
      "   - Scaled output channel count (first block: 64 to 128, second and third blocks: 256 to 384)\n",
      "   - Reduced learning rate by a factor of 0.87\n",
      "3. Expected improvements:\n",
      "   - Higher accuracy (95%) with moderate increase in training time\n",
      "4. Target metrics: Accuracy, while maintaining relatively fast training speed\n",
      "\n",
      "3. airbench96\n",
      "1. Baseline: airbench95 \n",
      "2. Modifications:\n",
      "   - Added 12-pixel Cutout augmentation\n",
      "   - Increased training epochs to 40\n",
      "   - Added a third convolution to each block\n",
      "   - Scaled channel counts (first block: 128, second and third blocks: 512)\n",
      "   - Added residual connections across later two convolutions of each block\n",
      "   - Reduced learning rate by a factor of 0.78\n",
      "3. Expected improvements:\n",
      "   - Higher accuracy (96%) with longer but still competitive training time\n",
      "4. Target metrics: Accuracy, while maintaining reasonable training speed\n",
      "\n",
      "These methods progressively build upon each other, offering trade-offs between speed and accuracy while incorporating the novel alternating flip data augmentation technique.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "method_metric_results:\n",
      "Baseline (tysam-code 2023):\n",
      "- Training Time: 6.3 seconds\n",
      "- Accuracy: 94%\n",
      "- FLOPs: Not reported\n",
      "\n",
      "airbench94:\n",
      "- Training Time: 3.29 seconds (compiled) / 3.83 seconds (non-compiled)\n",
      "- Accuracy: 94.01%\n",
      "- FLOPs: 3.6 × 10^14\n",
      "- Test-set Variance: 0.128%\n",
      "- Distribution-wise Variance: 0.029%\n",
      "- CACE: 0.0533\n",
      "\n",
      "airbench95:\n",
      "- Training Time: 10.4 seconds\n",
      "- Accuracy: 95.01%\n",
      "- FLOPs: 1.4 × 10^15\n",
      "\n",
      "airbench96:\n",
      "- Training Time: 46.3 seconds\n",
      "- Accuracy: 96.05%\n",
      "- FLOPs: 7.2 × 10^15\n",
      "\n",
      "These results showcase the performance improvements achieved by the airbench methods compared to the baseline, with airbench94 offering the fastest training time while maintaining 94% accuracy, and airbench95 and airbench96 providing higher accuracy at the cost of increased training time and computational complexity.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "hw_agnostic_metrics:\n",
      "1. FLOPs (Floating Point Operations): Measures computational complexity\n",
      "   - airbench94: 3.6 × 10^14 FLOPs\n",
      "   - airbench95: 1.4 × 10^15 FLOPs\n",
      "   - airbench96: 7.2 × 10^15 FLOPs\n",
      "\n",
      "2. Accuracy:\n",
      "   - airbench94: 94.01%\n",
      "   - airbench95: 95.01%\n",
      "   - airbench96: 96.05%\n",
      "\n",
      "3. Test-set Variance:\n",
      "   - airbench94: 0.128%\n",
      "\n",
      "4. Distribution-wise Variance:\n",
      "   - airbench94: 0.029%\n",
      "\n",
      "5. CACE (Class-aggregated Calibration Error):\n",
      "   - airbench94: 0.0533\n",
      "\n",
      "6. Number of Model Parameters:\n",
      "   - airbench94: 1.97 million\n",
      "\n",
      "7. Number of Training Epochs:\n",
      "   - airbench94: 9.9 epochs\n",
      "   - airbench95: 15 epochs\n",
      "   - airbench96: 40 epochs\n",
      "\n",
      "These metrics provide a hardware-agnostic view of the performance and characteristics of the proposed methods, allowing for fair comparisons across different hardware setups.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "tasks:\n",
      "1. Task: Implement airbench94 Core Components\n",
      "\n",
      "1. Baseline: Standard CNN implementation for CIFAR-10\n",
      "2. Modifications:\n",
      "   - Implement custom CNN architecture with 1.97 million parameters\n",
      "   - Add frozen patch-whitening initialization for the first layer\n",
      "   - Implement identity initialization for convolutions after the first layer\n",
      "   - Increase learning rate for BatchNorm biases\n",
      "   - Integrate Lookahead optimization\n",
      "   - Implement multi-crop evaluation (test-time augmentation)\n",
      "   - Implement alternating flip data augmentation\n",
      "3. Inputs/Outputs:\n",
      "   - Input: CIFAR-10 dataset\n",
      "   - Output: Trained model and test set predictions\n",
      "4. Key requirements:\n",
      "   - Use PyTorch for implementation\n",
      "   - Ensure compatibility with NVIDIA GPUs for fast training\n",
      "   - Implement all components in a modular fashion for easy modification\n",
      "5. Observable effects:\n",
      "   - Training time should be around 3.83 seconds on an NVIDIA A100 GPU\n",
      "   - Test set accuracy should reach 94.01%\n",
      "   - FLOPs count should be approximately 3.6 × 10^14\n",
      "\n",
      "2. Task: Extend airbench94 to airbench95\n",
      "\n",
      "1. Baseline: airbench94 implementation\n",
      "2. Modifications:\n",
      "   - Increase training epochs from 9.9 to 15\n",
      "   - Scale output channel count (first block: 64 to 128, second and third blocks: 256 to 384)\n",
      "   - Reduce learning rate by a factor of 0.87\n",
      "3. Inputs/Outputs:\n",
      "   - Input: CIFAR-10 dataset\n",
      "   - Output: Trained model and test set predictions\n",
      "4. Key requirements:\n",
      "   - Maintain modularity to allow easy switching between airbench94 and airbench95 configurations\n",
      "   - Ensure all airbench94 components are preserved and only specified changes are made\n",
      "5. Observable effects:\n",
      "   - Training time should increase to around 10.4 seconds on an NVIDIA A100 GPU\n",
      "   - Test set accuracy should reach 95.01%\n",
      "   - FLOPs count should increase to approximately 1.4 × 10^15\n",
      "\n",
      "3. Task: Implement airbench96 Enhancements\n",
      "\n",
      "1. Baseline: airbench95 implementation\n",
      "2. Modifications:\n",
      "   - Add 12-pixel Cutout augmentation\n",
      "   - Increase training epochs to 40\n",
      "   - Add a third convolution to each block\n",
      "   - Scale channel counts (first block: 128, second and third blocks: 512)\n",
      "   - Add residual connections across later two convolutions of each block\n",
      "   - Reduce learning rate by a factor of 0.78\n",
      "3. Inputs/Outputs:\n",
      "   - Input: CIFAR-10 dataset\n",
      "   - Output: Trained model and test set predictions\n",
      "4. Key requirements:\n",
      "   - Implement Cutout augmentation efficiently to minimize impact on training speed\n",
      "   - Ensure proper integration of residual connections without disrupting existing architecture\n",
      "   - Maintain flexibility to easily switch between airbench94, airbench95, and airbench96 configurations\n",
      "5. Observable effects:\n",
      "   - Training time should increase to around 46.3 seconds on an NVIDIA A100 GPU\n",
      "   - Test set accuracy should reach 96.05%\n",
      "   - FLOPs count should increase to approximately 7.2 × 10^15\n",
      "\n",
      "These tasks cover the implementation of the three main experimental methods presented in the paper, focusing on the key improvements and modifications for each method.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "task_eval_planning:\n",
      "1. Eval Plan: airbench94 Core Components Implementation\n",
      "\n",
      "1. Baseline: Provide a standard CNN implementation for CIFAR-10 using PyTorch.\n",
      "\n",
      "2. Required modifications:\n",
      "   - Implement custom CNN architecture with 1.97 million parameters\n",
      "   - Add frozen patch-whitening initialization for the first layer\n",
      "   - Implement identity initialization for convolutions after the first layer\n",
      "   - Increase learning rate for BatchNorm biases\n",
      "   - Integrate Lookahead optimization\n",
      "   - Implement multi-crop evaluation (test-time augmentation)\n",
      "   - Implement alternating flip data augmentation\n",
      "\n",
      "3. Sample inputs:\n",
      "   - A batch of CIFAR-10 images (32x32x3)\n",
      "   - Training dataset for full training run\n",
      "   - Test dataset for evaluation\n",
      "\n",
      "4. Expected changes in output:\n",
      "   - Faster convergence during training\n",
      "   - Higher accuracy on test set\n",
      "   - Consistent performance across multiple runs\n",
      "\n",
      "5. Evaluation metrics:\n",
      "   - Training time (target: ~3.83 seconds on A100 GPU)\n",
      "   - Test set accuracy (target: 94.01%)\n",
      "   - FLOPs count (target: ~3.6 × 10^14)\n",
      "   - Test-set variance (target: ~0.128%)\n",
      "   - Distribution-wise variance (target: ~0.029%)\n",
      "   - CACE (target: ~0.0533)\n",
      "\n",
      "2. Eval Plan: Extend airbench94 to airbench95\n",
      "\n",
      "1. Baseline: Provide the implemented airbench94 code.\n",
      "\n",
      "2. Required modifications:\n",
      "   - Increase training epochs from 9.9 to 15\n",
      "   - Scale output channel count (first block: 64 to 128, second and third blocks: 256 to 384)\n",
      "   - Reduce learning rate by a factor of 0.87\n",
      "\n",
      "3. Sample inputs:\n",
      "   - Same as airbench94 evaluation\n",
      "\n",
      "4. Expected changes in output:\n",
      "   - Longer training time\n",
      "   - Higher accuracy on test set\n",
      "   - Increased FLOPs count\n",
      "\n",
      "5. Evaluation metrics:\n",
      "   - Training time (target: ~10.4 seconds on A100 GPU)\n",
      "   - Test set accuracy (target: 95.01%)\n",
      "   - FLOPs count (target: ~1.4 × 10^15)\n",
      "\n",
      "3. Eval Plan: Implement airbench96 Enhancements\n",
      "\n",
      "1. Baseline: Provide the implemented airbench95 code.\n",
      "\n",
      "2. Required modifications:\n",
      "   - Add 12-pixel Cutout augmentation\n",
      "   - Increase training epochs to 40\n",
      "   - Add a third convolution to each block\n",
      "   - Scale channel counts (first block: 128, second and third blocks: 512)\n",
      "   - Add residual connections across later two convolutions of each block\n",
      "   - Reduce learning rate by a factor of 0.78\n",
      "\n",
      "3. Sample inputs:\n",
      "   - Same as airbench94 and airbench95 evaluations\n",
      "   - Additional test cases for Cutout augmentation\n",
      "\n",
      "4. Expected changes in output:\n",
      "   - Significantly longer training time\n",
      "   - Higher accuracy on test set\n",
      "   - Further increased FLOPs count\n",
      "   - Improved robustness due to Cutout augmentation\n",
      "\n",
      "5. Evaluation metrics:\n",
      "   - Training time (target: ~46.3 seconds on A100 GPU)\n",
      "   - Test set accuracy (target: 96.05%)\n",
      "   - FLOPs count (target: ~7.2 × 10^15)\n",
      "   - Robustness to occlusions (qualitative assessment using partially occluded test images)\n",
      "\n",
      "For all evaluations, also check:\n",
      "- Correct implementation of all specified components\n",
      "- Modularity and ease of switching between configurations\n",
      "- Reproducibility of results across multiple runs\n",
      "\n",
      "These evaluation plans will help assess whether the implementations correctly achieve the performance improvements and characteristics described in the paper for each airbench variant.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "task_eval_baseline_implementation:\n",
      "Here's a baseline implementation for the standard CNN training on CIFAR-10 using PyTorch:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "import time\n",
      "\n",
      "# Device configuration\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "# Hyperparameters\n",
      "num_epochs = 200\n",
      "batch_size = 128\n",
      "learning_rate = 0.1\n",
      "\n",
      "# Data loading and preprocessing\n",
      "transform = transforms.Compose([\n",
      "    transforms.RandomHorizontalFlip(),\n",
      "    transforms.RandomCrop(32, padding=4),\n",
      "    transforms.ToTensor(),\n",
      "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
      "])\n",
      "\n",
      "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
      "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
      "\n",
      "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
      "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
      "\n",
      "# CNN Model (to be replaced with airbench94 architecture)\n",
      "class BasicCNN(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(BasicCNN, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
      "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
      "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.fc1 = nn.Linear(64 * 4 * 4, 64)\n",
      "        self.fc2 = nn.Linear(64, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(torch.relu(self.conv1(x)))\n",
      "        x = self.pool(torch.relu(self.conv2(x)))\n",
      "        x = self.pool(torch.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 64 * 4 * 4)\n",
      "        x = torch.relu(self.fc1(x))\n",
      "        x = self.fc2(x)\n",
      "        return x\n",
      "\n",
      "model = BasicCNN().to(device)\n",
      "\n",
      "# Loss and optimizer\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
      "\n",
      "# Training loop\n",
      "def train():\n",
      "    model.train()\n",
      "    for epoch in range(num_epochs):\n",
      "        for i, (images, labels) in enumerate(train_loader):\n",
      "            images = images.to(device)\n",
      "            labels = labels.to(device)\n",
      "\n",
      "            # Forward pass\n",
      "            outputs = model(images)\n",
      "            loss = criterion(outputs, labels)\n",
      "\n",
      "            # Backward and optimize\n",
      "            optimizer.zero_grad()\n",
      "            loss.backward()\n",
      "            optimizer.step()\n",
      "\n",
      "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
      "\n",
      "# Evaluation function\n",
      "def evaluate():\n",
      "    model.eval()\n",
      "    with torch.no_grad():\n",
      "        correct = 0\n",
      "        total = 0\n",
      "        for images, labels in test_loader:\n",
      "            images = images.to(device)\n",
      "            labels = labels.to(device)\n",
      "            outputs = model(images)\n",
      "            _, predicted = torch.max(outputs.data, 1)\n",
      "            total += labels.size(0)\n",
      "            correct += (predicted == labels).sum().item()\n",
      "\n",
      "        print(f'Accuracy on the test images: {100 * correct / total}%')\n",
      "\n",
      "# Run training and evaluation\n",
      "start_time = time.time()\n",
      "train()\n",
      "end_time = time.time()\n",
      "print(f'Training time: {end_time - start_time:.2f} seconds')\n",
      "evaluate()\n",
      "\n",
      "# TODO: Implement airbench94 modifications:\n",
      "# 1. Replace BasicCNN with custom architecture (1.97 million parameters)\n",
      "# 2. Add frozen patch-whitening initialization for the first layer\n",
      "# 3. Implement identity initialization for convolutions after the first layer\n",
      "# 4. Increase learning rate for BatchNorm biases\n",
      "# 5. Integrate Lookahead optimization\n",
      "# 6. Implement multi-crop evaluation (test-time augmentation)\n",
      "# 7. Implement alternating flip data augmentation\n",
      "```\n",
      "\n",
      "This baseline implementation provides a starting point for the airbench94 task. It includes a simple CNN model, data loading and preprocessing for CIFAR-10, a basic training loop, and evaluation on the test set. The TODO comments at the end indicate where the airbench94 modifications should be implemented.\n",
      "\n",
      "The engineer can use this as a foundation to implement the airbench94 improvements, replacing components as needed and adding the required optimizations. This baseline allows for easy comparison between the standard approach and the airbench94 method in terms of training time, accuracy, and other metrics specified in the evaluation plan.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "task_eval_instructions:\n",
      "1. Implement airbench94:\n",
      "\n",
      "Your task is to implement the airbench94 method for fast CIFAR-10 training. Start with the provided baseline implementation and make the following modifications:\n",
      "\n",
      "a) Replace the BasicCNN with the custom architecture described in the paper, ensuring it has 1.97 million parameters.\n",
      "b) Implement frozen patch-whitening initialization for the first layer.\n",
      "c) Add identity initialization for convolutions after the first layer.\n",
      "d) Increase the learning rate for BatchNorm biases by a factor of 64.\n",
      "e) Integrate Lookahead optimization.\n",
      "f) Implement multi-crop evaluation (test-time augmentation).\n",
      "g) Implement alternating flip data augmentation.\n",
      "\n",
      "Use the CIFAR-10 dataset provided in the baseline. Your implementation should output a trained model and test set predictions.\n",
      "\n",
      "Evaluate your implementation by measuring:\n",
      "- Training time (target: ~3.83 seconds on an A100 GPU)\n",
      "- Test set accuracy (target: 94.01%)\n",
      "- FLOPs count (target: ~3.6 × 10^14)\n",
      "\n",
      "Run your implementation for at least 10 trials and report the average results. Document any implementation choices or deviations from the paper's description.\n",
      "\n",
      "2. Extend airbench94 to airbench95:\n",
      "\n",
      "Building upon your airbench94 implementation, make the following modifications to create airbench95:\n",
      "\n",
      "a) Increase the number of training epochs from 9.9 to 15.\n",
      "b) Scale the output channel count:\n",
      "   - First block: from 64 to 128\n",
      "   - Second and third blocks: from 256 to 384\n",
      "c) Reduce the learning rate by a factor of 0.87.\n",
      "\n",
      "Use the same CIFAR-10 dataset and evaluation process as in airbench94. Your implementation should output a trained model and test set predictions.\n",
      "\n",
      "Evaluate your implementation by measuring:\n",
      "- Training time (target: ~10.4 seconds on an A100 GPU)\n",
      "- Test set accuracy (target: 95.01%)\n",
      "- FLOPs count (target: ~1.4 × 10^15)\n",
      "\n",
      "Run your implementation for at least 10 trials and report the average results. Document any implementation choices or deviations from the paper's description.\n",
      "\n",
      "3. Implement airbench96:\n",
      "\n",
      "Building upon your airbench95 implementation, make the following modifications to create airbench96:\n",
      "\n",
      "a) Add 12-pixel Cutout augmentation.\n",
      "b) Increase the number of training epochs to 40.\n",
      "c) Add a third convolution to each block.\n",
      "d) Scale the channel counts:\n",
      "   - First block: 128\n",
      "   - Second and third blocks: 512\n",
      "e) Add residual connections across the later two convolutions of each block.\n",
      "f) Reduce the learning rate by a factor of 0.78.\n",
      "\n",
      "Use the same CIFAR-10 dataset and evaluation process as in previous implementations. Your implementation should output a trained model and test set predictions.\n",
      "\n",
      "Evaluate your implementation by measuring:\n",
      "- Training time (target: ~46.3 seconds on an A100 GPU)\n",
      "- Test set accuracy (target: 96.05%)\n",
      "- FLOPs count (target: ~7.2 × 10^15)\n",
      "\n",
      "Additionally, assess the robustness to occlusions by testing on partially occluded images from the test set.\n",
      "\n",
      "Run your implementation for at least 10 trials and report the average results. Document any implementation choices or deviations from the paper's description.\n",
      "\n",
      "For all implementations:\n",
      "- Ensure your code is modular and allows easy switching between airbench94, airbench95, and airbench96 configurations.\n",
      "- Use PyTorch for implementation and ensure compatibility with NVIDIA GPUs.\n",
      "- Provide clear documentation for your code, especially for the custom components like alternating flip and multi-crop evaluation.\n",
      "- Report any challenges faced during implementation and how you addressed them.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "task_eval_correctness_scoring:\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import time\n",
      "\n",
      "def score_airbench94_implementation(modified_model, baseline_model):\n",
      "    score = 0.0\n",
      "    \n",
      "    # Check architecture\n",
      "    if isinstance(modified_model, nn.Module) and sum(p.numel() for p in modified_model.parameters()) == 1.97e6:\n",
      "        score += 0.1\n",
      "    \n",
      "    # Check frozen patch-whitening initialization\n",
      "    if isinstance(modified_model[0], nn.Conv2d) and not modified_model[0].weight.requires_grad:\n",
      "        score += 0.1\n",
      "    \n",
      "    # Check identity initialization for convolutions\n",
      "    for m in modified_model.modules():\n",
      "        if isinstance(m, nn.Conv2d) and m != modified_model[0]:\n",
      "            if torch.allclose(m.weight[:m.in_channels], torch.eye(m.in_channels).unsqueeze(-1).unsqueeze(-1), atol=1e-6):\n",
      "                score += 0.05\n",
      "    \n",
      "    # Check increased learning rate for BatchNorm biases\n",
      "    optimizer = torch.optim.SGD(modified_model.parameters(), lr=0.1)\n",
      "    for group in optimizer.param_groups:\n",
      "        if 'bias' in group['name'] and group['lr'] == 0.1 * 64:\n",
      "            score += 0.1\n",
      "    \n",
      "    # Check Lookahead optimization\n",
      "    if any(isinstance(opt, torch.optim.Optimizer) and 'lookahead' in opt.__class__.__name__.lower() for opt in optimizer.state.values()):\n",
      "        score += 0.1\n",
      "    \n",
      "    # Check multi-crop evaluation\n",
      "    def check_multicrop(model, input_tensor):\n",
      "        original_output = model(input_tensor)\n",
      "        flipped_output = model(torch.flip(input_tensor, [-1]))\n",
      "        translated_output = model(torch.roll(input_tensor, shifts=(1, 1), dims=(-2, -1)))\n",
      "        return not torch.allclose(original_output, flipped_output) and not torch.allclose(original_output, translated_output)\n",
      "    \n",
      "    if check_multicrop(modified_model, torch.randn(1, 3, 32, 32)):\n",
      "        score += 0.1\n",
      "    \n",
      "    # Check alternating flip data augmentation\n",
      "    # This is a simplified check and may need to be adjusted based on the specific implementation\n",
      "    def check_alternating_flip(model, dataloader):\n",
      "        flipped = []\n",
      "        for i, (inputs, _) in enumerate(dataloader):\n",
      "            if i % 2 == 1:\n",
      "                inputs = torch.flip(inputs, [-1])\n",
      "            flipped.append((inputs == dataloader.dataset[i][0]).all())\n",
      "            if i > 10:\n",
      "                break\n",
      "        return all(flipped)\n",
      "    \n",
      "    if check_alternating_flip(modified_model, torch.utils.data.DataLoader(torch.randn(100, 3, 32, 32))):\n",
      "        score += 0.1\n",
      "    \n",
      "    # Evaluate performance\n",
      "    start_time = time.time()\n",
      "    accuracy = evaluate_model(modified_model, test_loader)\n",
      "    end_time = time.time()\n",
      "    training_time = end_time - start_time\n",
      "    \n",
      "    if 3.5 < training_time < 4.2:  # Allow some margin around the target 3.83 seconds\n",
      "        score += 0.1\n",
      "    if 93.5 < accuracy < 94.5:  # Allow some margin around the target 94.01%\n",
      "        score += 0.1\n",
      "    \n",
      "    # Check FLOPs (this is a simplified check and may need a more accurate FLOP counting method)\n",
      "    flops = count_flops(modified_model)\n",
      "    if 3.5e14 < flops < 3.7e14:\n",
      "        score += 0.05\n",
      "    \n",
      "    return score\n",
      "\n",
      "def score_airbench95_implementation(modified_model, airbench94_model):\n",
      "    score = 0.0\n",
      "    \n",
      "    # Check increased epoch count\n",
      "    if modified_model.num_epochs == 15:\n",
      "        score += 0.2\n",
      "    \n",
      "    # Check scaled channel counts\n",
      "    if (modified_model[1].out_channels == 128 and\n",
      "        modified_model[2].out_channels == 384 and\n",
      "        modified_model[3].out_channels == 384):\n",
      "        score += 0.2\n",
      "    \n",
      "    # Check adjusted learning rate\n",
      "    if abs(modified_model.lr - airbench94_model.lr * 0.87) < 1e-6:\n",
      "        score += 0.2\n",
      "    \n",
      "    # Evaluate performance\n",
      "    start_time = time.time()\n",
      "    accuracy = evaluate_model(modified_model, test_loader)\n",
      "    end_time = time.time()\n",
      "    training_time = end_time - start_time\n",
      "    \n",
      "    if 10 < training_time < 11:  # Allow some margin around the target 10.4 seconds\n",
      "        score += 0.2\n",
      "    if 94.5 < accuracy < 95.5:  # Allow some margin around the target 95.01%\n",
      "        score += 0.1\n",
      "    \n",
      "    # Check FLOPs\n",
      "    flops = count_flops(modified_model)\n",
      "    if 1.3e15 < flops < 1.5e15:\n",
      "        score += 0.1\n",
      "    \n",
      "    return score\n",
      "\n",
      "def score_airbench96_implementation(modified_model, airbench95_model):\n",
      "    score = 0.0\n",
      "    \n",
      "    # Check Cutout augmentation\n",
      "    if hasattr(modified_model, 'cutout') and modified_model.cutout.length == 12:\n",
      "        score += 0.1\n",
      "    \n",
      "    # Check increased epoch count\n",
      "    if modified_model.num_epochs == 40:\n",
      "        score += 0.1\n",
      "    \n",
      "    # Check additional convolutions and channel counts\n",
      "    if (len(modified_model[1]) == 3 and\n",
      "        len(modified_model[2]) == 3 and\n",
      "        len(modified_model[3]) == 3 and\n",
      "        modified_model[1].out_channels == 128 and\n",
      "        modified_model[2].out_channels == 512 and\n",
      "        modified_model[3].out_channels == 512):\n",
      "        score += 0.2\n",
      "    \n",
      "    # Check residual connections\n",
      "    if hasattr(modified_model[1], 'residual') and hasattr(modified_model[2], 'residual') and hasattr(modified_model[3], 'residual'):\n",
      "        score += 0.1\n",
      "    \n",
      "    # Check adjusted learning rate\n",
      "    if abs(modified_model.lr - airbench95_model.lr * 0.78) < 1e-6:\n",
      "        score += 0.1\n",
      "    \n",
      "    # Evaluate performance\n",
      "    start_time = time.time()\n",
      "    accuracy = evaluate_model(modified_model, test_loader)\n",
      "    end_time = time.time()\n",
      "    training_time = end_time - start_time\n",
      "    \n",
      "    if 45 < training_time < 48:  # Allow some margin around the target 46.3 seconds\n",
      "        score += 0.1\n",
      "    if 95.5 < accuracy < 96.5:  # Allow some margin around the target 96.05%\n",
      "        score += 0.1\n",
      "    \n",
      "    # Check FLOPs\n",
      "    flops = count_flops(modified_model)\n",
      "    if 7e15 < flops < 7.4e15:\n",
      "        score += 0.1\n",
      "    \n",
      "    # Check robustness to occlusions\n",
      "    occluded_accuracy = evaluate_model_with_occlusions(modified_model, test_loader)\n",
      "    if occluded_accuracy > 0.9 * accuracy:  # Assume less than 10% drop in accuracy for occluded images\n",
      "        score += 0.1\n",
      "    \n",
      "    return score\n",
      "\n",
      "# Helper functions (to be implemented)\n",
      "def evaluate_model(model, dataloader):\n",
      "    # Implement model evaluation on the given dataloader\n",
      "    pass\n",
      "\n",
      "def count_flops(model):\n",
      "    # Implement FLOP counting for the given model\n",
      "    pass\n",
      "\n",
      "def evaluate_model_with_occlusions(model, dataloader):\n",
      "    # Implement model evaluation on occluded images\n",
      "    pass\n",
      "```\n",
      "\n",
      "These scoring functions check for the correct implementation of each component and assign partial credit based on the presence and correctness of each feature. They also evaluate the performance metrics and compare them to the target values from the paper. \n",
      "\n",
      "Note that some aspects, like the exact implementation of alternating flip or multi-crop evaluation, may need more sophisticated checks depending on how they are implemented. The provided checks are simplified versions and may need to be adjusted based on the specific implementation details.\n",
      "\n",
      "Also, helper functions like `evaluate_model`, `count_flops`, and `evaluate_model_with_occlusions` need to be implemented separately, as their exact implementation would depend on the specific setup and available libraries.\n",
      "\n",
      "These scoring functions provide a comprehensive evaluation of each airbench implementation, covering all the key aspects described in the paper and allowing for partial credit on individual components.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "task_eval_metric_scoring:\n",
      "Here are Python functions to score the metric results for each airbench implementation:\n",
      "\n",
      "```python\n",
      "import time\n",
      "import torch\n",
      "from thop import profile\n",
      "\n",
      "def score_airbench94_metrics(model):\n",
      "    score = 0.0\n",
      "\n",
      "    # Measure training time\n",
      "    start_time = time.time()\n",
      "    train_model(model)  # Assume this function exists to train the model\n",
      "    end_time = time.time()\n",
      "    training_time = end_time - start_time\n",
      "\n",
      "    # Score training time (target: 3.83 seconds)\n",
      "    if 3.5 <= training_time <= 4.2:\n",
      "        score += 0.3 * (1 - abs(training_time - 3.83) / 0.7)\n",
      "\n",
      "    # Measure accuracy\n",
      "    accuracy = evaluate_model(model)  # Assume this function exists to evaluate the model\n",
      "\n",
      "    # Score accuracy (target: 94.01%)\n",
      "    if 93.5 <= accuracy <= 94.5:\n",
      "        score += 0.5 * (1 - abs(accuracy - 94.01) / 1.0)\n",
      "\n",
      "    # Measure FLOPs\n",
      "    input_tensor = torch.randn(1, 3, 32, 32)\n",
      "    flops, _ = profile(model, inputs=(input_tensor,))\n",
      "\n",
      "    # Score FLOPs (target: 3.6e14)\n",
      "    if 3.4e14 <= flops <= 3.8e14:\n",
      "        score += 0.2 * (1 - abs(flops - 3.6e14) / 4e13)\n",
      "\n",
      "    return score\n",
      "\n",
      "def score_airbench95_metrics(model):\n",
      "    score = 0.0\n",
      "\n",
      "    # Measure training time\n",
      "    start_time = time.time()\n",
      "    train_model(model)\n",
      "    end_time = time.time()\n",
      "    training_time = end_time - start_time\n",
      "\n",
      "    # Score training time (target: 10.4 seconds)\n",
      "    if 9.5 <= training_time <= 11.3:\n",
      "        score += 0.3 * (1 - abs(training_time - 10.4) / 1.8)\n",
      "\n",
      "    # Measure accuracy\n",
      "    accuracy = evaluate_model(model)\n",
      "\n",
      "    # Score accuracy (target: 95.01%)\n",
      "    if 94.5 <= accuracy <= 95.5:\n",
      "        score += 0.5 * (1 - abs(accuracy - 95.01) / 1.0)\n",
      "\n",
      "    # Measure FLOPs\n",
      "    input_tensor = torch.randn(1, 3, 32, 32)\n",
      "    flops, _ = profile(model, inputs=(input_tensor,))\n",
      "\n",
      "    # Score FLOPs (target: 1.4e15)\n",
      "    if 1.3e15 <= flops <= 1.5e15:\n",
      "        score += 0.2 * (1 - abs(flops - 1.4e15) / 2e14)\n",
      "\n",
      "    return score\n",
      "\n",
      "def score_airbench96_metrics(model):\n",
      "    score = 0.0\n",
      "\n",
      "    # Measure training time\n",
      "    start_time = time.time()\n",
      "    train_model(model)\n",
      "    end_time = time.time()\n",
      "    training_time = end_time - start_time\n",
      "\n",
      "    # Score training time (target: 46.3 seconds)\n",
      "    if 42 <= training_time <= 51:\n",
      "        score += 0.3 * (1 - abs(training_time - 46.3) / 9)\n",
      "\n",
      "    # Measure accuracy\n",
      "    accuracy = evaluate_model(model)\n",
      "\n",
      "    # Score accuracy (target: 96.05%)\n",
      "    if 95.5 <= accuracy <= 96.5:\n",
      "        score += 0.5 * (1 - abs(accuracy - 96.05) / 1.0)\n",
      "\n",
      "    # Measure FLOPs\n",
      "    input_tensor = torch.randn(1, 3, 32, 32)\n",
      "    flops, _ = profile(model, inputs=(input_tensor,))\n",
      "\n",
      "    # Score FLOPs (target: 7.2e15)\n",
      "    if 6.8e15 <= flops <= 7.6e15:\n",
      "        score += 0.2 * (1 - abs(flops - 7.2e15) / 8e14)\n",
      "\n",
      "    return score\n",
      "```\n",
      "\n",
      "These functions score the key metrics (training time, accuracy, and FLOPs) for each airbench implementation. They return a score between 0 and 1, with higher scores indicating closer matches to the target values from the paper.\n",
      "\n",
      "The scoring system:\n",
      "1. Allows for some margin of error around target values\n",
      "2. Weights accuracy most heavily (50% of score), followed by training time (30%) and FLOPs (20%)\n",
      "3. Uses linear interpolation within acceptable ranges to assign partial credit\n",
      "\n",
      "Note that these functions assume the existence of `train_model()` and `evaluate_model()` functions, which would need to be implemented separately. The `profile()` function from the `thop` library is used to estimate FLOPs.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "implementation_difficulty:\n",
      "Here's the implementation difficulty assessment for each task:\n",
      "\n",
      "1. airbench94 Core Components Implementation\n",
      "Complexity: 4/5\n",
      "Estimated time: 8-12 hours\n",
      "Challenges and potential pitfalls:\n",
      "- Correctly implementing the frozen patch-whitening initialization\n",
      "- Ensuring the identity initialization for convolutions is applied correctly \n",
      "- Integrating Lookahead optimization without errors\n",
      "- Implementing alternating flip data augmentation efficiently\n",
      "- Balancing all components to achieve the target accuracy and speed\n",
      "Helpful resources:\n",
      "- PyTorch documentation for custom layer implementation\n",
      "- The original Lookahead optimizer paper\n",
      "- CIFAR-10 dataset documentation\n",
      "- Papers on efficient data augmentation techniques\n",
      "\n",
      "2. Extend airbench94 to airbench95 \n",
      "Complexity: 2/5\n",
      "Estimated time: 2-3 hours\n",
      "Challenges and potential pitfalls:\n",
      "- Ensuring the channel count scaling is applied correctly to all relevant layers\n",
      "- Balancing the increased epoch count with the reduced learning rate to achieve the target accuracy\n",
      "Helpful resources:\n",
      "- The airbench94 implementation (previously completed)\n",
      "- PyTorch documentation on modifying network architectures\n",
      "\n",
      "3. Implement airbench96 Enhancements\n",
      "Complexity: 3/5  \n",
      "Estimated time: 4-6 hours\n",
      "Challenges and potential pitfalls:\n",
      "- Implementing Cutout augmentation efficiently\n",
      "- Adding the third convolution to each block without disrupting the existing architecture\n",
      "- Correctly implementing residual connections\n",
      "- Balancing all new components to achieve the target accuracy and training time\n",
      "Helpful resources:\n",
      "- Paper on Cutout augmentation\n",
      "- PyTorch documentation on implementing residual connections  \n",
      "- Tutorials on advanced CNN architectures\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "data_processing:\n",
      "1. Data cleaning and preprocessing steps:\n",
      "- Lowercase all text (not applicable for CIFAR-10 image data)\n",
      "- Remove special characters and extra whitespace (not applicable for CIFAR-10)\n",
      "- Normalize pixel values using CIFAR-10 mean and standard deviation:\n",
      "  CIFAR_MEAN = (0.4914, 0.4822, 0.4465)\n",
      "  CIFAR_STD = (0.2470, 0.2435, 0.2616)\n",
      "\n",
      "2. Feature engineering methods:\n",
      "- Use a frozen patch-whitening layer as the first convolutional layer\n",
      "- Initialize this layer using eigenvectors of the covariance matrix of 2x2 patches across the training distribution\n",
      "\n",
      "3. Data augmentation techniques:\n",
      "- Random horizontal flipping (50% probability on first epoch, then alternating)\n",
      "- Random translation (2-pixel)\n",
      "- Reflection padding for translations\n",
      "- For airbench96: 12-pixel Cutout augmentation\n",
      "\n",
      "4. Sampling or batching strategies:\n",
      "- Batch size of 1024\n",
      "- Use of a custom CifarLoader class for efficient GPU-based data loading\n",
      "- Implement alternating flip augmentation after the first epoch to reduce redundancy\n",
      "\n",
      "Example for NLP Preprocessing (not directly used in this paper, but included for completeness):\n",
      "1. Data cleaning:\n",
      "- Lowercase all text\n",
      "- Remove special characters and extra whitespace\n",
      "- Tokenization using SentencePiece with vocab size 32,000\n",
      "\n",
      "2. Feature engineering:\n",
      "- Convert tokens to integer IDs\n",
      "- Add special tokens: [CLS] at start, [SEP] at end of each sentence\n",
      "\n",
      "3. Data augmentation:\n",
      "- Random word masking (15% of tokens)\n",
      "- Random word replacement (10% of tokens)\n",
      "\n",
      "4. Batching strategy:\n",
      "- Group sentences of similar length\n",
      "- Pad to max length in batch\n",
      "- Use attention mask to ignore padding\n",
      "\n",
      "This data processing approach focuses on efficient preprocessing and augmentation techniques specifically tailored for fast CIFAR-10 training, with the novel alternating flip augmentation being a key component.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "setup_description:\n",
      "Here's a detailed setup guide for implementing the airbench methods:\n",
      "\n",
      "1. Required libraries and their versions:\n",
      "- PyTorch >= 1.9.0\n",
      "- torchvision >= 0.10.0 \n",
      "- numpy >= 1.21.0\n",
      "- thop >= 0.0.31-2005241907 (for FLOPs calculation)\n",
      "\n",
      "2. Necessary compute resources:\n",
      "- GPU: NVIDIA A100 (or equivalent high-end GPU)\n",
      "- RAM: 16GB minimum, 32GB recommended\n",
      "- Storage: 10GB free space for CIFAR-10 dataset and model checkpoints\n",
      "\n",
      "3. Datasets:\n",
      "- CIFAR-10, available from torchvision.datasets.CIFAR10\n",
      "- No additional datasets required\n",
      "\n",
      "4. Sample environment setup script:\n",
      "\n",
      "```bash\n",
      "# Create and activate conda environment\n",
      "conda create -n airbench python=3.8\n",
      "conda activate airbench\n",
      "\n",
      "# Install PyTorch with CUDA support (adjust CUDA version as needed)\n",
      "conda install pytorch torchvision cudatoolkit=11.3 -c pytorch\n",
      "\n",
      "# Install additional dependencies\n",
      "pip install numpy thop\n",
      "\n",
      "# Set up data directory\n",
      "mkdir -p data/cifar10\n",
      "```\n",
      "\n",
      "5. Potential compatibility issues or common setup pitfalls:\n",
      "- Ensure CUDA toolkit version matches PyTorch build\n",
      "- Verify GPU drivers are up-to-date\n",
      "- Check that the A100 GPU is recognized and accessible\n",
      "- Ensure sufficient disk space for dataset and checkpoints\n",
      "- Be aware of potential memory issues with large batch sizes\n",
      "- If using a different GPU, adjust batch sizes and learning rates accordingly\n",
      "- Make sure the system has the latest NVIDIA drivers installed\n",
      "- When using PyTorch's torch.compile(), ensure you have the latest compatible version\n",
      "\n",
      "This setup provides the necessary components to begin implementing the airbench methods. It specifies the required libraries, compute resources, and datasets, along with a sample environment setup script. The guide also notes potential compatibility issues to be aware of during setup.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in results.keys():\n",
    "    for kk in results[k].keys():\n",
    "        if kk == \"rationale\":\n",
    "            continue\n",
    "        print(f\"{kk}:\\n{results[k][kk]}\\n\\n\")\n",
    "    print (\"\\n\\n====================\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperAnalysis(dspy.Signature):\n",
    "    \"\"\"Analyze a research paper and provide core information.\"\"\"\n",
    "\n",
    "    paper_content = dspy.InputField(desc=\"The full text content of the research paper\")\n",
    "\n",
    "    title = dspy.OutputField(desc=\"The title of the paper\")\n",
    "    summary = dspy.OutputField(desc=\"A concise summary of the paper's main contributions\")\n",
    "    core_ideas = dspy.OutputField(desc=\"The core idea(s) of the paper\")\n",
    "    methods = dspy.OutputField(desc=\"Key methods or strategies proposed in the paper\")\n",
    "    metrics = dspy.OutputField(desc=\"Primary metrics or evaluation criteria used in the paper\")\n",
    "    requirements = dspy.OutputField(desc=\"Key requirements for implementing and evaluating the paper's methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_lm():\n",
    "    \"\"\"Configure and return a language model.\"\"\"\n",
    "    lm = dspy.Claude(\"claude-3-5-sonnet-20240620\", api_key=\"***REMOVED***\")\n",
    "    dspy.configure(lm=lm)\n",
    "    return lm\n",
    "\n",
    "def analyze_paper(pdf_path, model_name=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Analyze a research paper given its PDF path.\"\"\"\n",
    "\n",
    "    # Configure LM\n",
    "    lm = configure_lm()\n",
    "\n",
    "    # Extract text from PDF\n",
    "    paper_content = pdf_utils.extract_text_from_pdf(pdf_path)\n",
    " \n",
    "    # Initialize and compile the analyzer\n",
    "    analyzer = dspy.ChainOfThought(paper_analysis.PaperAnalysis)\n",
    "    # Analyze the paper\n",
    "    result = analyzer(paper_content=paper_content)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=analyze_paper(\"papers/94cifar.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rationale', 'summary', 'core_ideas', 'methods', 'metrics', 'requirements']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Modified network architecture based on prior work, with some adjustments to layer sizes and initialization.\\n2. Frozen patch-whitening initialization for the first convolutional layer.\\n3. Identity initialization for subsequent convolutional layers.\\n4. Optimization tricks including increased learning rate for BatchNorm biases and Lookahead optimization.\\n5. Multi-crop evaluation using six augmented views of each test image.\\n6. Alternating flip augmentation technique.\\n7. Compilation using torch.compile for improved GPU utilization.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Training time to reach target accuracy (94%, 95%, or 96%)\\n2. Number of FLOPs (Floating Point Operations) required\\n3. Test set accuracy\\n4. Class-aggregated calibration error (CACE)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
