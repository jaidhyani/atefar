{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: magentic[anthropic] in ./.venv/lib/python3.11/site-packages (0.28.1)\n",
      "Requirement already satisfied: anthropic>=0.27.0 in ./.venv/lib/python3.11/site-packages (from magentic[anthropic]) (0.32.0)\n",
      "Requirement already satisfied: filetype in ./.venv/lib/python3.11/site-packages (from magentic[anthropic]) (1.2.0)\n",
      "Requirement already satisfied: logfire-api in ./.venv/lib/python3.11/site-packages (from magentic[anthropic]) (0.48.1)\n",
      "Requirement already satisfied: openai>=1.26.0 in ./.venv/lib/python3.11/site-packages (from magentic[anthropic]) (1.38.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in ./.venv/lib/python3.11/site-packages (from magentic[anthropic]) (2.8.2)\n",
      "Requirement already satisfied: pydantic-settings>=2.0.0 in ./.venv/lib/python3.11/site-packages (from magentic[anthropic]) (2.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (0.5.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (1.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (0.19.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (4.12.2)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.11/site-packages (from openai>=1.26.0->magentic[anthropic]) (4.66.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.0.0->magentic[anthropic]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.0.0->magentic[anthropic]) (2.20.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./.venv/lib/python3.11/site-packages (from pydantic-settings>=2.0.0->magentic[anthropic]) (1.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->anthropic>=0.27.0->magentic[anthropic]) (3.7)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->anthropic>=0.27.0->magentic[anthropic]) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->anthropic>=0.27.0->magentic[anthropic]) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic>=0.27.0->magentic[anthropic]) (0.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.11/site-packages (from tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (0.24.5)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (6.0.1)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (2.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: dspy-ai in ./.venv/lib/python3.11/site-packages (2.4.13)\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: backoff in ./.venv/lib/python3.11/site-packages (from dspy-ai) (2.2.1)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.11/site-packages (from dspy-ai) (2.20.0)\n",
      "Requirement already satisfied: joblib~=1.3 in ./.venv/lib/python3.11/site-packages (from dspy-ai) (1.4.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=0.28.1 in ./.venv/lib/python3.11/site-packages (from dspy-ai) (1.38.0)\n",
      "Requirement already satisfied: optuna in ./.venv/lib/python3.11/site-packages (from dspy-ai) (3.6.1)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from dspy-ai) (2.2.2)\n",
      "Requirement already satisfied: pydantic~=2.0 in ./.venv/lib/python3.11/site-packages (from dspy-ai) (2.8.2)\n",
      "Requirement already satisfied: regex in ./.venv/lib/python3.11/site-packages (from dspy-ai) (2024.7.24)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from dspy-ai) (2.32.3)\n",
      "Requirement already satisfied: structlog in ./.venv/lib/python3.11/site-packages (from dspy-ai) (24.4.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from dspy-ai) (4.66.4)\n",
      "Requirement already satisfied: ujson in ./.venv/lib/python3.11/site-packages (from dspy-ai) (5.10.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic~=2.0->dspy-ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.venv/lib/python3.11/site-packages (from pydantic~=2.0->dspy-ai) (2.20.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (2.0.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (0.3.8)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets->dspy-ai) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (3.10.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (0.24.5)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (6.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->dspy-ai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->dspy-ai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->dspy-ai) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->dspy-ai) (2024.7.4)\n",
      "Requirement already satisfied: alembic>=1.5.0 in ./.venv/lib/python3.11/site-packages (from optuna->dspy-ai) (1.13.2)\n",
      "Requirement already satisfied: colorlog in ./.venv/lib/python3.11/site-packages (from optuna->dspy-ai) (6.8.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in ./.venv/lib/python3.11/site-packages (from optuna->dspy-ai) (2.0.32)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->dspy-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->dspy-ai) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->dspy-ai) (2024.1)\n",
      "Requirement already satisfied: Mako in ./.venv/lib/python3.11/site-packages (from alembic>=1.5.0->optuna->dspy-ai) (1.3.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets->dspy-ai) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets->dspy-ai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets->dspy-ai) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets->dspy-ai) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets->dspy-ai) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets->dspy-ai) (1.9.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=0.28.1->dspy-ai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=0.28.1->dspy-ai) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->dspy-ai) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in ./.venv/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna->dspy-ai) (2.1.5)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install \"magentic[anthropic]\"\n",
    "! pip install dspy-ai PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jaidhyani/Desktop/atefar\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: atefar 0.1.0\n",
      "Uninstalling atefar-0.1.0:\n",
      "  Successfully uninstalled atefar-0.1.0\n",
      "Obtaining file:///Users/jaidhyani/Desktop/atefar\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: atefar\n",
      "  Building editable for atefar (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for atefar: filename=atefar-0.1.0-py2.py3-none-any.whl size=895 sha256=8608e0a9242dc90429748827ec12d87c93c52c602affb7a176e1a04597151e0a\n",
      "  Stored in directory: /private/var/folders/5k/7nfpl0cs5999pzhndyybcn800000gn/T/pip-ephem-wheel-cache-v66pt9bs/wheels/ef/a2/67/4e2ac8e515272dc3b0b25ecb48e15da2a9a7bc08f6adb925d8\n",
      "Successfully built atefar\n",
      "Installing collected packages: atefar\n",
      "Successfully installed atefar-0.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall -y atefar\n",
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update requirements.txt\n",
    "! pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping src as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip uninstall src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version     Editable project location\n",
      "------------------ ----------- -------------------------------\n",
      "aiohappyeyeballs   2.3.5\n",
      "aiohttp            3.10.2\n",
      "aiosignal          1.3.1\n",
      "alembic            1.13.2\n",
      "annotated-types    0.7.0\n",
      "anthropic          0.32.0\n",
      "anyio              4.4.0\n",
      "appnope            0.1.4\n",
      "asttokens          2.4.1\n",
      "atefar             0.1.0       /Users/jaidhyani/Desktop/atefar\n",
      "attrs              24.2.0\n",
      "backoff            2.2.1\n",
      "certifi            2024.7.4\n",
      "charset-normalizer 3.3.2\n",
      "colorlog           6.8.2\n",
      "comm               0.2.2\n",
      "datasets           2.20.0\n",
      "debugpy            1.8.5\n",
      "decorator          5.1.1\n",
      "dill               0.3.8\n",
      "distro             1.9.0\n",
      "dspy-ai            2.4.13\n",
      "executing          2.0.1\n",
      "filelock           3.15.4\n",
      "filetype           1.2.0\n",
      "frozenlist         1.4.1\n",
      "fsspec             2024.5.0\n",
      "h11                0.14.0\n",
      "httpcore           1.0.5\n",
      "httpx              0.27.0\n",
      "huggingface-hub    0.24.5\n",
      "idna               3.7\n",
      "ipykernel          6.29.5\n",
      "ipython            8.26.0\n",
      "jedi               0.19.1\n",
      "jiter              0.5.0\n",
      "joblib             1.4.2\n",
      "jupyter_client     8.6.2\n",
      "jupyter_core       5.7.2\n",
      "logfire-api        0.48.1\n",
      "magentic           0.28.1\n",
      "Mako               1.3.5\n",
      "MarkupSafe         2.1.5\n",
      "matplotlib-inline  0.1.7\n",
      "multidict          6.0.5\n",
      "multiprocess       0.70.16\n",
      "nest-asyncio       1.6.0\n",
      "numpy              2.0.1\n",
      "openai             1.38.0\n",
      "optuna             3.6.1\n",
      "packaging          24.1\n",
      "pandas             2.2.2\n",
      "parso              0.8.4\n",
      "pexpect            4.9.0\n",
      "pip                24.0\n",
      "platformdirs       4.2.2\n",
      "prompt_toolkit     3.0.47\n",
      "psutil             6.0.0\n",
      "ptyprocess         0.7.0\n",
      "pure_eval          0.2.3\n",
      "pyarrow            17.0.0\n",
      "pyarrow-hotfix     0.6\n",
      "pydantic           2.8.2\n",
      "pydantic_core      2.20.1\n",
      "pydantic-settings  2.4.0\n",
      "Pygments           2.18.0\n",
      "PyPDF2             3.0.1\n",
      "python-dateutil    2.9.0.post0\n",
      "python-dotenv      1.0.1\n",
      "pytz               2024.1\n",
      "PyYAML             6.0.1\n",
      "pyzmq              26.0.3\n",
      "regex              2024.7.24\n",
      "requests           2.32.3\n",
      "setuptools         65.5.0\n",
      "six                1.16.0\n",
      "sniffio            1.3.1\n",
      "SQLAlchemy         2.0.32\n",
      "stack-data         0.6.3\n",
      "structlog          24.4.0\n",
      "tokenizers         0.19.1\n",
      "tornado            6.4.1\n",
      "tqdm               4.66.4\n",
      "traitlets          5.14.3\n",
      "typing_extensions  4.12.2\n",
      "tzdata             2024.1\n",
      "ujson              5.10.0\n",
      "urllib3            2.2.2\n",
      "wcwidth            0.2.13\n",
      "xxhash             3.4.1\n",
      "yarl               1.9.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/jaidhyani/miniforge3/envs/mp4/lib/python311.zip', '/Users/jaidhyani/miniforge3/envs/mp4/lib/python3.11', '/Users/jaidhyani/miniforge3/envs/mp4/lib/python3.11/lib-dynload', '', '/Users/jaidhyani/Desktop/atefar/.venv/lib/python3.11/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaidhyani/Desktop/atefar/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "\n",
    "import atefar\n",
    "from atefar import paper_analysis, pdf_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from unittest import result\n",
    "from dspy.primitives.program import Module\n",
    "from typing import Any, Optional\n",
    "\n",
    "from regex import F\n",
    "\n",
    "@dataclass\n",
    "class SigData:\n",
    "    name: str\n",
    "    desc: str\n",
    "\n",
    "@dataclass\n",
    "class SigStep:\n",
    "    outputs: list[SigData]\n",
    "    module: Module\n",
    "    kwargs: Optional[dict[str, Any]] = None\n",
    "\n",
    "@dataclass\n",
    "class SigChain:\n",
    "    inputs: list[SigData]\n",
    "    steps: list[SigStep]\n",
    "\n",
    "paper_chain = SigChain(\n",
    "    inputs=[SigData(\"paper_content\", \"The full text content of the research paper\")],\n",
    "    steps=[\n",
    "        SigStep(\n",
    "            outputs=[\n",
    "                SigData(\"title\", \"The title of the paper\"),\n",
    "                SigData(\"summary\", \"A concise summary of the paper's key ideas, target metrics, and strategies employed to target those metrics, with an eye towards reproducing the results\"),\n",
    "            ],\n",
    "            module=dspy.ChainOfThought,\n",
    "        ),\n",
    "        SigStep(\n",
    "            outputs=[\n",
    "                SigData(\"core_ideas\", \"An enumerated list of the specific ideas or concepts that are central to the paper's main contributions\"),\n",
    "            ],\n",
    "            module=dspy.ChainOfThought,\n",
    "        ),\n",
    "        SigStep(\n",
    "            outputs=[\n",
    "                SigData(\"metrics\", \"An enumerated list of the key metrics used in the paper. In particular, these should be objectively measurably quantities that measure the performance of the proposed methods.\"),\n",
    "            ],\n",
    "            module=dspy.ChainOfThought,\n",
    "        ),\n",
    "        SigStep(\n",
    "            outputs=[\n",
    "                SigData(\"methods\", \"An enumerated list of the key methods or strategies proposed in the paper to optimize the target metrics\"),\n",
    "            ],\n",
    "            module=dspy.ChainOfThought,\n",
    "        ),\n",
    "        SigStep(\n",
    "            outputs=[\n",
    "                SigData(\"method_metric_results\", \"For each method, list the metric results that were reported in the paper. These should be the results that were used to compare the methods to each other.\"),\n",
    "            ],\n",
    "            module=dspy.ChainOfThought,\n",
    "        ),\n",
    "        SigStep(\n",
    "            outputs=[\n",
    "                SigData(\"hw_agnostic_metrics\", \"Metrics that correspond to the paper's metrics, but are not hardware-specific. These may be copied from input metrics where appropriate, or they may be generated if not otherwise available.\"),\n",
    "            ],\n",
    "            module=dspy.ChainOfThought,\n",
    "        ),\n",
    "        SigStep(\n",
    "            outputs=[\n",
    "                SigData(\n",
    "                    \"tasks\", \n",
    "                    \"\"\"\n",
    "                    Describe each enumerated method as a function to be implemented. \n",
    "                    Describe what the inputs are and their types, what the outputs are and their types, \n",
    "                    and what the method does.  Do not describe how the function is implemented. It should \n",
    "                    be possible to verify the correctness of the implementation by comparing the output of the \n",
    "                    implementation to the output of the method din the paper.\n",
    "                    \"\"\"\n",
    "                ),\n",
    "            ],\n",
    "            module=dspy.ChainOfThought,\n",
    "        ),\n",
    "        SigStep(\n",
    "            outputs=[\n",
    "                SigData(\n",
    "                    \"setup\", \n",
    "                    \"\"\"\n",
    "                    A description of the setup required before implementing the task functions. This should include a list of assets that need to be available to start.\n",
    "                    These may include datasets, trained models, training functions, etc. Assume that a python torch environment with GPUs is available. Any common libraries\n",
    "                    or public assets may be used (e.g. huggingface transformers, torchvision, etc.). If a dataset is required, it should be available in a public location.\n",
    "                    \"\"\"\n",
    "                ),\n",
    "            ],\n",
    "            module=dspy.ChainOfThought,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "    paper_content = dspy.InputField(desc=\"The full text content of the research paper\")\n",
    "\n",
    "    title = dspy.OutputField(desc=\"The title of the paper\")\n",
    "    summary = dspy.OutputField(desc=\"A concise summary of the paper's main contributions\")\n",
    "    core_ideas = dspy.OutputField(desc=\"The core idea(s) of the paper\")\n",
    "    methods = dspy.OutputField(desc=\"Key methods or strategies proposed in the paper\")\n",
    "    metrics = dspy.OutputField(desc=\"Primary metrics or evaluation criteria used in the paper\")\n",
    "    requirements = dspy.OutputField(desc=\"Key requirements for implementing and evaluating the paper's methods\")\n",
    "\"\"\"\n",
    "\n",
    "def solve_sigchain(chain: SigChain, inputs=dict[str, str]) -> tuple[dict[str, Any], dict[str, Any]]:\n",
    "    input_sigs = chain.inputs.copy()\n",
    "    input_vals = inputs.copy()\n",
    "    solvers = dict()\n",
    "    results = dict()\n",
    "    for step in chain.steps:\n",
    "        step_name = \"_\".join([o.name for o in step.outputs])\n",
    "        step_inputs_str = \", \".join([i.name for i in input_sigs])\n",
    "        step_outputs_str = \", \".join([o.name for o in step.outputs])\n",
    "        print(f\"Running step {step_name}\")\n",
    "        print(f\"  Inputs: {step_inputs_str}\")\n",
    "        print(f\"  Outputs: {step_outputs_str}\")\n",
    "        class NextSig(dspy.Signature):\n",
    "            pass\n",
    "        for i in input_sigs:\n",
    "            NextSig = NextSig.append(i.name, dspy.InputField(desc=i.desc))\n",
    "        for o in step.outputs:\n",
    "            NextSig = NextSig.append(o.name, dspy.OutputField(desc=o.desc))\n",
    "        step_solver = step.module(NextSig)\n",
    "        result = step_solver(**input_vals)\n",
    "        for o in step.outputs:\n",
    "            input_vals[o.name] = result[o.name]\n",
    "        input_sigs.extend(step.outputs)\n",
    "        solvers[step_name] = step_solver\n",
    "        results[step_name] = result\n",
    "    return solvers, results\n",
    "\n",
    "\n",
    "def cascading_signatures(sig_elements: list[list[SigData]]) -> list[dspy.Signature]:\n",
    "    inputs = sig_elements.pop(0)\n",
    "    names = [i.name for i in inputs]\n",
    "    assert len(set(names)) == len(names), \"Input names must be unique\"\n",
    "    sigs = []\n",
    "    while len(sig_elements) > 0:\n",
    "        outputs = sig_elements.pop(0)\n",
    "        class NextSig(dspy.Signature):\n",
    "            pass\n",
    "        for i in inputs:\n",
    "            setattr(NextSig, i.name, dspy.Input(i.desc))\n",
    "        for o in outputs:\n",
    "            setattr(NextSig, o.name, dspy.Output(o.desc))\n",
    "        sigs.append(NextSig)\n",
    "        inputs.extend(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = pdf_utils.extract_text_from_pdf(\"papers/94cifar.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step title_summary\n",
      "  Inputs: paper_content\n",
      "  Outputs: title, summary\n",
      "Running step core_ideas\n",
      "  Inputs: paper_content, title, summary\n",
      "  Outputs: core_ideas\n",
      "Running step metrics\n",
      "  Inputs: paper_content, title, summary, core_ideas\n",
      "  Outputs: metrics\n",
      "Running step methods\n",
      "  Inputs: paper_content, title, summary, core_ideas, metrics\n",
      "  Outputs: methods\n",
      "Running step method_metric_results\n",
      "  Inputs: paper_content, title, summary, core_ideas, metrics, methods\n",
      "  Outputs: method_metric_results\n",
      "Running step hw_agnostic_metrics\n",
      "  Inputs: paper_content, title, summary, core_ideas, metrics, methods, method_metric_results\n",
      "  Outputs: hw_agnostic_metrics\n",
      "Running step tasks\n",
      "  Inputs: paper_content, title, summary, core_ideas, metrics, methods, method_metric_results, hw_agnostic_metrics\n",
      "  Outputs: tasks\n",
      "Running step setup\n",
      "  Inputs: paper_content, title, summary, core_ideas, metrics, methods, method_metric_results, hw_agnostic_metrics, tasks\n",
      "  Outputs: setup\n"
     ]
    }
   ],
   "source": [
    "solvers, results = solve_sigchain(paper_chain, {\"paper_content\": pdf_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. implement_vgg_like_network(widths, batchnorm_momentum)\n",
      "   Inputs: \n",
      "   - widths: dictionary specifying the number of channels for each block\n",
      "   - batchnorm_momentum: float for BatchNorm momentum\n",
      "   Outputs: \n",
      "   - model: PyTorch nn.Module representing the network\n",
      "   Description: Implements the modified VGG-like network architecture described in the paper.\n",
      "\n",
      "2. initialize_whitening_conv(layer, train_set, eps)\n",
      "   Inputs:\n",
      "   - layer: first convolutional layer of the network\n",
      "   - train_set: tensor of training images\n",
      "   - eps: small constant for numerical stability\n",
      "   Outputs:\n",
      "   - None (modifies layer in-place)\n",
      "   Description: Initializes the first convolutional layer as a patch-whitening transformation.\n",
      "\n",
      "3. initialize_identity_conv(layer)\n",
      "   Inputs:\n",
      "   - layer: convolutional layer to initialize\n",
      "   Outputs:\n",
      "   - None (modifies layer in-place)\n",
      "   Description: Initializes convolutional layers after the first as partial identity transforms.\n",
      "\n",
      "4. alternating_flip(inputs, indices, epoch)\n",
      "   Inputs:\n",
      "   - inputs: batch of images\n",
      "   - indices: tensor of indices for the batch\n",
      "   - epoch: current training epoch\n",
      "   Outputs:\n",
      "   - flipped_inputs: batch of images with alternating flip applied\n",
      "   Description: Implements the novel alternating flip data augmentation technique.\n",
      "\n",
      "5. create_cifar10_loader(path, train, batch_size, aug)\n",
      "   Inputs:\n",
      "   - path: string path to CIFAR-10 dataset\n",
      "   - train: boolean indicating whether to load training or test set\n",
      "   - batch_size: int for batch size\n",
      "   - aug: dictionary specifying augmentation parameters\n",
      "   Outputs:\n",
      "   - loader: DataLoader object for CIFAR-10\n",
      "   Description: Creates a GPU-accelerated dataloader for CIFAR-10 with alternating flip augmentation.\n",
      "\n",
      "6. train_epoch(model, loader, optimizer, scheduler, lookahead_state)\n",
      "   Inputs:\n",
      "   - model: neural network model\n",
      "   - loader: DataLoader for training data\n",
      "   - optimizer: SGD optimizer\n",
      "   - scheduler: learning rate scheduler\n",
      "   - lookahead_state: object for Lookahead optimization\n",
      "   Outputs:\n",
      "   - train_loss: average training loss for the epoch\n",
      "   - train_acc: training accuracy for the epoch\n",
      "   Description: Performs one epoch of training using the methods described in the paper.\n",
      "\n",
      "7. evaluate_model(model, loader, tta_level)\n",
      "   Inputs:\n",
      "   - model: trained neural network model\n",
      "   - loader: DataLoader for test data\n",
      "   - tta_level: int specifying level of test-time augmentation\n",
      "   Outputs:\n",
      "   - accuracy: test set accuracy\n",
      "   Description: Evaluates the model on the test set, optionally using test-time augmentation.\n",
      "\n",
      "8. train_airbench(hyp)\n",
      "   Inputs:\n",
      "   - hyp: dictionary of hyperparameters\n",
      "   Outputs:\n",
      "   - model: trained model\n",
      "   - final_accuracy: final test set accuracy\n",
      "   - training_time: total training time in seconds\n",
      "   Description: Implements the full training pipeline for airbench94, airbench95, or airbench96 based on the provided hyperparameters.\n",
      "\n",
      "These tasks cover the key components needed to implement the methods described in the paper and reproduce its main results.\n"
     ]
    }
   ],
   "source": [
    "print(results['tasks']['tasks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperAnalysis(dspy.Signature):\n",
    "    \"\"\"Analyze a research paper and provide core information.\"\"\"\n",
    "\n",
    "    paper_content = dspy.InputField(desc=\"The full text content of the research paper\")\n",
    "\n",
    "    title = dspy.OutputField(desc=\"The title of the paper\")\n",
    "    summary = dspy.OutputField(desc=\"A concise summary of the paper's main contributions\")\n",
    "    core_ideas = dspy.OutputField(desc=\"The core idea(s) of the paper\")\n",
    "    methods = dspy.OutputField(desc=\"Key methods or strategies proposed in the paper\")\n",
    "    metrics = dspy.OutputField(desc=\"Primary metrics or evaluation criteria used in the paper\")\n",
    "    requirements = dspy.OutputField(desc=\"Key requirements for implementing and evaluating the paper's methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_lm():\n",
    "    \"\"\"Configure and return a language model.\"\"\"\n",
    "    lm = dspy.Claude(\"claude-3-5-sonnet-20240620\", api_key=\"***REMOVED***\")\n",
    "    dspy.configure(lm=lm)\n",
    "    return lm\n",
    "\n",
    "def analyze_paper(pdf_path, model_name=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Analyze a research paper given its PDF path.\"\"\"\n",
    "\n",
    "    # Configure LM\n",
    "    lm = configure_lm()\n",
    "\n",
    "    # Extract text from PDF\n",
    "    paper_content = pdf_utils.extract_text_from_pdf(pdf_path)\n",
    " \n",
    "    # Initialize and compile the analyzer\n",
    "    analyzer = dspy.ChainOfThought(paper_analysis.PaperAnalysis)\n",
    "    # Analyze the paper\n",
    "    result = analyzer(paper_content=paper_content)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=analyze_paper(\"papers/94cifar.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rationale', 'summary', 'core_ideas', 'methods', 'metrics', 'requirements']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Modified network architecture based on prior work, with some adjustments to layer sizes and initialization.\\n2. Frozen patch-whitening initialization for the first convolutional layer.\\n3. Identity initialization for subsequent convolutional layers.\\n4. Optimization tricks including increased learning rate for BatchNorm biases and Lookahead optimization.\\n5. Multi-crop evaluation using six augmented views of each test image.\\n6. Alternating flip augmentation technique.\\n7. Compilation using torch.compile for improved GPU utilization.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Training time to reach target accuracy (94%, 95%, or 96%)\\n2. Number of FLOPs (Floating Point Operations) required\\n3. Test set accuracy\\n4. Class-aggregated calibration error (CACE)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
