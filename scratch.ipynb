{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: magentic[anthropic] in ./.venv/lib/python3.11/site-packages (0.28.1)\n",
      "Requirement already satisfied: anthropic>=0.27.0 in ./.venv/lib/python3.11/site-packages (from magentic[anthropic]) (0.32.0)\n",
      "Requirement already satisfied: filetype in ./.venv/lib/python3.11/site-packages (from magentic[anthropic]) (1.2.0)\n",
      "Requirement already satisfied: logfire-api in ./.venv/lib/python3.11/site-packages (from magentic[anthropic]) (0.48.1)\n",
      "Requirement already satisfied: openai>=1.26.0 in ./.venv/lib/python3.11/site-packages (from magentic[anthropic]) (1.38.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in ./.venv/lib/python3.11/site-packages (from magentic[anthropic]) (2.8.2)\n",
      "Requirement already satisfied: pydantic-settings>=2.0.0 in ./.venv/lib/python3.11/site-packages (from magentic[anthropic]) (2.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (0.5.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (1.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (0.19.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.11/site-packages (from anthropic>=0.27.0->magentic[anthropic]) (4.12.2)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.11/site-packages (from openai>=1.26.0->magentic[anthropic]) (4.66.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.0.0->magentic[anthropic]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.0.0->magentic[anthropic]) (2.20.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./.venv/lib/python3.11/site-packages (from pydantic-settings>=2.0.0->magentic[anthropic]) (1.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->anthropic>=0.27.0->magentic[anthropic]) (3.7)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->anthropic>=0.27.0->magentic[anthropic]) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->anthropic>=0.27.0->magentic[anthropic]) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic>=0.27.0->magentic[anthropic]) (0.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.11/site-packages (from tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (0.24.5)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (6.0.1)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.27.0->magentic[anthropic]) (2.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: dspy-ai in ./.venv/lib/python3.11/site-packages (2.4.13)\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: backoff in ./.venv/lib/python3.11/site-packages (from dspy-ai) (2.2.1)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.11/site-packages (from dspy-ai) (2.20.0)\n",
      "Requirement already satisfied: joblib~=1.3 in ./.venv/lib/python3.11/site-packages (from dspy-ai) (1.4.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=0.28.1 in ./.venv/lib/python3.11/site-packages (from dspy-ai) (1.38.0)\n",
      "Requirement already satisfied: optuna in ./.venv/lib/python3.11/site-packages (from dspy-ai) (3.6.1)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from dspy-ai) (2.2.2)\n",
      "Requirement already satisfied: pydantic~=2.0 in ./.venv/lib/python3.11/site-packages (from dspy-ai) (2.8.2)\n",
      "Requirement already satisfied: regex in ./.venv/lib/python3.11/site-packages (from dspy-ai) (2024.7.24)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from dspy-ai) (2.32.3)\n",
      "Requirement already satisfied: structlog in ./.venv/lib/python3.11/site-packages (from dspy-ai) (24.4.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from dspy-ai) (4.66.4)\n",
      "Requirement already satisfied: ujson in ./.venv/lib/python3.11/site-packages (from dspy-ai) (5.10.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic~=2.0->dspy-ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.venv/lib/python3.11/site-packages (from pydantic~=2.0->dspy-ai) (2.20.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (2.0.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (0.3.8)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets->dspy-ai) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (3.10.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (0.24.5)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from datasets->dspy-ai) (6.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->dspy-ai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->dspy-ai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->dspy-ai) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->dspy-ai) (2024.7.4)\n",
      "Requirement already satisfied: alembic>=1.5.0 in ./.venv/lib/python3.11/site-packages (from optuna->dspy-ai) (1.13.2)\n",
      "Requirement already satisfied: colorlog in ./.venv/lib/python3.11/site-packages (from optuna->dspy-ai) (6.8.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in ./.venv/lib/python3.11/site-packages (from optuna->dspy-ai) (2.0.32)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->dspy-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->dspy-ai) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->dspy-ai) (2024.1)\n",
      "Requirement already satisfied: Mako in ./.venv/lib/python3.11/site-packages (from alembic>=1.5.0->optuna->dspy-ai) (1.3.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets->dspy-ai) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets->dspy-ai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets->dspy-ai) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets->dspy-ai) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets->dspy-ai) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets->dspy-ai) (1.9.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=0.28.1->dspy-ai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=0.28.1->dspy-ai) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->dspy-ai) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in ./.venv/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna->dspy-ai) (2.1.5)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install dspy-ai PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update requirements.txt\n",
    "! pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaidhyani/Desktop/atefar/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "import atefar\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "from atefar.sig_types import SigChain, SigStep\n",
    "\n",
    "\n",
    "def solve_sigchain(chain: SigChain, inputs=dict[str, str]) -> tuple[dict[str, Any], dict[str, Any]]:\n",
    "    input_sigs = chain.inputs.copy()\n",
    "    input_vals = inputs.copy()\n",
    "    solvers = dict()\n",
    "    results = dict()\n",
    "    for step in chain.steps:\n",
    "        try:\n",
    "            assert isinstance(step, SigStep), f\"Expected SigStep, got {type(step)}: {step}\"\n",
    "            step_name = \"_\".join([o.name for o in step.outputs])\n",
    "            step_inputs_str = \", \".join([i.name for i in input_sigs])\n",
    "            step_outputs_str = \", \".join([o.name for o in step.outputs])\n",
    "            print(f\"Running step {step_name}\")\n",
    "            print(f\"  Inputs: {step_inputs_str}\")\n",
    "            print(f\"  Outputs: {step_outputs_str}\")\n",
    "            class NextSig(dspy.Signature):\n",
    "                pass\n",
    "            for i in input_sigs:\n",
    "                NextSig = NextSig.append(i.name, dspy.InputField(desc=i.desc))\n",
    "            for o in step.outputs:\n",
    "                NextSig = NextSig.append(o.name, dspy.OutputField(desc=o.desc))\n",
    "            step_solver = step.module(NextSig, **(step.kwargs or {}))\n",
    "            result = step_solver(**input_vals)\n",
    "            for o in step.outputs:\n",
    "                input_vals[o.name] = result[o.name]\n",
    "            input_sigs.extend(step.outputs)\n",
    "            solvers[step_name] = step_solver\n",
    "            results[step_name] = result\n",
    "        except Exception as e:\n",
    "            print(f\"Error running step {step_name}: {e}; returning early\")\n",
    "    return solvers, results\n",
    "\n",
    "def construct_solvers(chain: SigChain) -> dict[str, Any]:\n",
    "    solvers = dict()\n",
    "    input_sigs = chain.inputs.copy()\n",
    "\n",
    "    for step in chain.steps:\n",
    "        assert isinstance(step, SigStep), f\"Expected SigStep, got {type(step)}: {step}\"\n",
    "        step_name = \"_\".join([o.name for o in step.outputs])\n",
    "\n",
    "        class NextSig(dspy.Signature):\n",
    "            pass\n",
    "\n",
    "        for i in input_sigs:\n",
    "            NextSig = NextSig.append(i.name, dspy.InputField(desc=i.desc))\n",
    "        for o in step.outputs:\n",
    "            NextSig = NextSig.append(o.name, dspy.OutputField(desc=o.desc))\n",
    "\n",
    "        step_solver = step.module(NextSig, **(step.kwargs or {}))\n",
    "        solvers[step_name] = step_solver\n",
    "        input_sigs.extend(step.outputs)\n",
    "\n",
    "    return solvers\n",
    "\n",
    "def execute_solvers(chain: SigChain, solvers: dict[str, Any], inputs: dict[str, str]) -> dict[str, Any]:\n",
    "    input_vals = inputs.copy()\n",
    "    results = dict()\n",
    "\n",
    "    for step in chain.steps:\n",
    "        step_name = \"_\".join([o.name for o in step.outputs])\n",
    "        step_inputs_str = \", \".join([i.name for i in chain.inputs])\n",
    "        step_outputs_str = \", \".join([o.name for o in step.outputs])\n",
    "\n",
    "        print(f\"Running step {step_name}\")\n",
    "        print(f\"  Inputs: {step_inputs_str}\")\n",
    "        print(f\"  Outputs: {step_outputs_str}\")\n",
    "\n",
    "        step_solver = solvers[step_name]\n",
    "        result = step_solver(**input_vals)\n",
    "\n",
    "        for o in step.outputs:\n",
    "            input_vals[o.name] = result[o.name]\n",
    "\n",
    "        results[step_name] = result\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dsp.modules.anthropic.Claude at 0x106283ac0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def configure_lm():\n",
    "    \"\"\"Configure and return a language model.\"\"\"\n",
    "    lm = dspy.Claude(\"claude-3-5-sonnet-20240620\", api_key=\"***REMOVED***\")\n",
    "    dspy.configure(lm=lm)\n",
    "    return lm\n",
    "\n",
    "configure_lm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atefar import pdf_utils\n",
    "pdf_text = atefar.pdf_utils.extract_text_from_pdf(\"papers/94cifar.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step title_abstract_plus\n",
      "  Inputs: paper_content\n",
      "  Outputs: title, abstract_plus\n",
      "Running step quantitative_results_json\n",
      "  Inputs: paper_content, title, abstract_plus\n",
      "  Outputs: quantitative_results_json\n",
      "Running step core_ideas_json\n",
      "  Inputs: paper_content, title, abstract_plus, quantitative_results_json\n",
      "  Outputs: core_ideas_json\n",
      "Running step metrics_json\n",
      "  Inputs: paper_content, title, abstract_plus, quantitative_results_json, core_ideas_json\n",
      "  Outputs: metrics_json\n",
      "Running step hw_agnostic_metrics_json\n",
      "  Inputs: paper_content, title, abstract_plus, quantitative_results_json, core_ideas_json, metrics_json\n",
      "  Outputs: hw_agnostic_metrics_json\n",
      "Running step baseline_methods_json\n",
      "  Inputs: paper_content, title, abstract_plus, quantitative_results_json, core_ideas_json, metrics_json, hw_agnostic_metrics_json\n",
      "  Outputs: baseline_methods_json\n",
      "Running step experimental_methods_json\n",
      "  Inputs: paper_content, title, abstract_plus, quantitative_results_json, core_ideas_json, metrics_json, hw_agnostic_metrics_json, baseline_methods_json\n",
      "  Outputs: experimental_methods_json\n",
      "Running step method_metric_results\n",
      "  Inputs: paper_content, title, abstract_plus, quantitative_results_json, core_ideas_json, metrics_json, hw_agnostic_metrics_json, baseline_methods_json, experimental_methods_json\n",
      "  Outputs: method_metric_results\n",
      "Running step task_candidates_json\n",
      "  Inputs: paper_content, title, abstract_plus, quantitative_results_json, core_ideas_json, metrics_json, hw_agnostic_metrics_json, baseline_methods_json, experimental_methods_json, method_metric_results\n",
      "  Outputs: task_candidates_json\n",
      "Running step task_prerequisites_json\n",
      "  Inputs: paper_content, title, abstract_plus, quantitative_results_json, core_ideas_json, metrics_json, hw_agnostic_metrics_json, baseline_methods_json, experimental_methods_json, method_metric_results, task_candidates_json\n",
      "  Outputs: task_prerequisites_json\n",
      "Running step task_eval_instructions_json\n",
      "  Inputs: paper_content, title, abstract_plus, quantitative_results_json, core_ideas_json, metrics_json, hw_agnostic_metrics_json, baseline_methods_json, experimental_methods_json, method_metric_results, task_candidates_json, task_prerequisites_json\n",
      "  Outputs: task_eval_instructions_json\n",
      "Running step task_eval_baseline_implementation_json\n",
      "  Inputs: paper_content, title, abstract_plus, quantitative_results_json, core_ideas_json, metrics_json, hw_agnostic_metrics_json, baseline_methods_json, experimental_methods_json, method_metric_results, task_candidates_json, task_prerequisites_json, task_eval_instructions_json\n",
      "  Outputs: task_eval_baseline_implementation_json\n",
      "Error in code execution\n",
      "Running step task_eval_correctness_scoring_json\n",
      "  Inputs: paper_content, title, abstract_plus, quantitative_results_json, core_ideas_json, metrics_json, hw_agnostic_metrics_json, baseline_methods_json, experimental_methods_json, method_metric_results, task_candidates_json, task_prerequisites_json, task_eval_instructions_json, task_eval_baseline_implementation_json\n",
      "  Outputs: task_eval_correctness_scoring_json\n",
      "Error in code execution\n",
      "Error in code execution\n",
      "Running step task_eval_metric_scoring_json\n",
      "  Inputs: paper_content, title, abstract_plus, quantitative_results_json, core_ideas_json, metrics_json, hw_agnostic_metrics_json, baseline_methods_json, experimental_methods_json, method_metric_results, task_candidates_json, task_prerequisites_json, task_eval_instructions_json, task_eval_baseline_implementation_json, task_eval_correctness_scoring_json\n",
      "  Outputs: task_eval_metric_scoring_json\n",
      "Error in code execution\n",
      "Error in code execution\n",
      "Running step task_eval_combined_scoring_json\n",
      "  Inputs: paper_content, title, abstract_plus, quantitative_results_json, core_ideas_json, metrics_json, hw_agnostic_metrics_json, baseline_methods_json, experimental_methods_json, method_metric_results, task_candidates_json, task_prerequisites_json, task_eval_instructions_json, task_eval_baseline_implementation_json, task_eval_correctness_scoring_json, task_eval_metric_scoring_json\n",
      "  Outputs: task_eval_combined_scoring_json\n",
      "Error in code execution\n",
      "Error in code execution\n",
      "Running step task_setup_script\n",
      "  Inputs: paper_content, title, abstract_plus, quantitative_results_json, core_ideas_json, metrics_json, hw_agnostic_metrics_json, baseline_methods_json, experimental_methods_json, method_metric_results, task_candidates_json, task_prerequisites_json, task_eval_instructions_json, task_eval_baseline_implementation_json, task_eval_correctness_scoring_json, task_eval_metric_scoring_json, task_eval_combined_scoring_json\n",
      "  Outputs: task_setup_script\n"
     ]
    }
   ],
   "source": [
    "from atefar.sig_chains import paper_chain_v5\n",
    "solvers, results = solve_sigchain(paper_chain_v5, {\"paper_content\": pdf_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = solvers['task_setup_script']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    A bash script that sets up the environment for running the task evaluation functions in a Linux environment. The script should:\\n    1. Take one positional argument: the path to the directory where the task evaluation code will be located\\n    2. Create the following files in the specified directory:\\n       - instructions.txt: A text file containing the detailed instructions for each task\\n       - solution.py: A python file containing the baseline implementations\\n       - scoring.py: A python file containing the scoring functions\\n       - requirements.txt: A text file listing all Python package dependencies\\n    3. Set up a Python virtual environment and install required packages\\n    4. Include error handling and logging\\n    5. Add a help message when run without arguments\\n\\n    The script should be self-contained.\\n    It should not contain any placeholders. Do not assume that any environment variables are set. \\n    Include comments for clarity. \\n    Do not include any text which would cause the output to not be a valid bash script. \\n\\n    Example output (note that there is no commentary outside of the script):\\n    ```bash\\n    #!/bin/bash\\n\\n    # Help message\\n    if [ \"$#\" -ne 1 ]; then\\n        echo \"Usage: $0 <output_directory>\"\\n        exit 1\\n    fi\\n\\n    # Set variables\\n    OUTPUT_DIR=\"$1\"\\n\\n    # Create output directory if it doesn\\'t exist\\n    mkdir -p \"$OUTPUT_DIR\"\\n\\n    # Function to create files\\n    create_files() {\\n        # Create instructions.txt\\n        cat > \"$OUTPUT_DIR/instructions.txt\" << EOL\\n        Modify the provided MultiHeadAttention class to implement a sparse attention mechanism. \\n        Your implementation should:\\n            1. Replace the full attention matrix with a sparse attention pattern (e.g., local + global attention)\\n            2. Ensure the sparse attention matrix is properly masked and normalized\\n            3. Maintain compatibility with the rest of the Transformer architecture\\n            \\n        Inputs and outputs should remain the same as in the original implementation. \\n        Focus on modifying the \\'forward\\' method to incorporate sparse attention.\\n    EOL\\n\\n        # Create solution.py\\n        cat > \"$OUTPUT_DIR/solution.py\" << EOL\\nclass MultiHeadAttention(nn.Module):\\n    def __init__(self, d_model, num_heads):\\n    super().__init__()\\n    self.num_heads = num_heads\\n    self.d_model = d_model\\n    \\n    assert d_model % self.num_heads == 0\\n\\n    self.depth = d_model // self.num_heads\\n    self.wq = nn.Linear(d_model, d_model)\\n    self.wk = nn.Linear(d_model, d_model)\\n    self.wv = nn.Linear(d_model, d_model)\\n    self.dense = nn.Linear(d_model, d_model)\\n\\n    def split_heads(self, x, batch_size):\\n        x = x.view(batch_size, -1, self.num_heads, self.depth)\\n        return x.permute(0, 2, 1, 3)\\n    \\n    def forward(self, q, k, v, mask=None):\\n        batch_size = q.size(0)\\n        \\n        q = self.wq(q)\\n        k = self.wk(k)\\n        v = self.wv(v)\\n        \\n        q = self.split_heads(q, batch_size)\\n        k = self.split_heads(k, batch_size)\\n        v = self.split_heads(v, batch_size)\\n        \\n        scaled_attention_logits = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.depth)\\n        \\n        if mask is not None:\\n            scaled_attention_logits += (mask * -1e9)\\n        \\n        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\\n        \\n        output = torch.matmul(attention_weights, v)\\n        \\n        output = output.permute(0, 2, 1, 3).contiguous()\\n        output = output.view(batch_size, -1, self.d_model)\\n        output = self.dense(output)\\n        \\n        return output\\n\\n    EOL\\n\\n        cat > \"$OUTPUT_DIR/scoring.py\" << EOL\\n\\ndef score_sparse_attention_implementation(modified_transformer, baseline_transformer):\\n    score = 0.0\\n    \\n    # Test preserved functionality\\n    short_input = torch.randn(32, 50, 512)\\n    if torch.allclose(modified_transformer(short_input), baseline_transformer(short_input), atol=1e-5):\\n        score += 0.3\\n    \\n    # Test improvement\\n    long_input = torch.randn(32, 1000, 512)\\n    modified_output = modified_transformer(long_input)\\n    \\n    # Check for sparse attention pattern\\n    attention_weights = modified_transformer.encoder.layers[0].self_attn.attn_weights\\n    if attention_weights.float().to_dense().count_nonzero() / attention_weights.numel() < 0.2:\\n        score += 0.4\\n    \\n    # Check for improved efficiency\\n    start_time = time.time()\\n    modified_transformer(long_input)\\n    modified_time = time.time() - start_time\\n    \\n    start_time = time.time()\\n    baseline_transformer(long_input)\\n    baseline_time = time.time() - start_time\\n    \\n    if modified_time < 0.8 * baseline_time:\\n        score += 0.3\\n    \\n    return score\\n\\n\\ndef score_sparse_attention_metrics(modified_transformer, baseline_transformer, test_data):\\n    baseline_perplexity = evaluate_perplexity(baseline_transformer, test_data)\\n    modified_perplexity = evaluate_perplexity(modified_transformer, test_data)\\n    \\n    perplexity_improvement = (baseline_perplexity - modified_perplexity) / baseline_perplexity\\n    \\n    baseline_speed = measure_inference_speed(baseline_transformer, test_data)\\n    modified_speed = measure_inference_speed(modified_transformer, test_data)\\n    \\n    speed_improvement = (modified_speed - baseline_speed) / baseline_speed\\n    \\n    # We expect a small perplexity improvement and a significant speed improvement\\n    score = min(1.0, max(0, perplexity_improvement * 10)) * 0.3 + min(1.0, max(0, speed_improvement)) * 0.7\\n    \\n    return score\\n\\ndef evaluate_perplexity(model, data):\\n    # In the actual script, this should be implemented; it\\'s omitted here to keep documentation concise\\n\\ndef measure_inference_speed(model, data):\\n    # In the actual script, this should be implemented; it\\'s omitted here to keep documentation concise\\n```\"\\n\\n\\ndef score_sparse_attention(modified_transformer, baseline_transformer, test_data):\\n    correctness_score = score_sparse_attention_implementation(modified_transformer, baseline_transformer)\\n    metric_score = score_sparse_attention_metrics(modified_transformer, baseline_transformer, test_data)\\n    \\n    return 0.6 * correctness_score + 0.4 * metric_score\\n    EOL\\n\\n        # Create requirements.txt\\n        cat > \"$OUTPUT_DIR/requirements.txt\" << EOL\\n    torch\\n    numpy\\n    # Add other dependencies as needed\\n    EOL\\n    }\\n\\n    # Function to set up Python environment\\n    setup_python_env() {\\n        python3 -m venv \"$OUTPUT_DIR/venv\"\\n        source \"$OUTPUT_DIR/venv/bin/activate\"\\n        pip install -r \"$OUTPUT_DIR/requirements.txt\"\\n    }\\n\\n    # Main execution\\n    main() {\\n        create_files\\n        setup_python_env\\n        echo \"Setup complete. Evaluation environment is ready in $OUTPUT_DIR\"\\n    }\\n\\n    # Run the script\\n    main\\n    ```\\n    '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_chain_v5.steps[-1].outputs[0].desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the bash script to set up the task evaluation environment:\\n\\n```bash\\n#!/bin/bash\\n\\n# Help message\\nif [ \"$#\" -ne 1 ]; then\\n    echo \"Usage: $0 <output_directory>\"\\n    exit 1\\nfi\\n\\n# Set variables\\nOUTPUT_DIR=\"$1\"\\n\\n# Create output directory if it doesn\\'t exist\\nmkdir -p \"$OUTPUT_DIR\"\\n\\n# Function to create files\\ncreate_files() {\\n    # Create instructions.txt\\n    cat > \"$OUTPUT_DIR/instructions.txt\" << EOL\\nImplement the alternating flip augmentation method as described in Section 3.6 of the paper. Your implementation should:\\n1. Take a batch of images and the current epoch number as input.\\n2. For the first epoch, randomly flip 50% of the images horizontally.\\n3. For subsequent epochs, deterministically flip images based on whether they were flipped in the first epoch:\\n   - On even epochs (2, 4, 6, ...), flip only those images that were not flipped in the first epoch.\\n   - On odd epochs (3, 5, 7, ...), flip only those images that were flipped in the first epoch.\\n4. Use a pseudorandom function based on image indices to determine which images to flip, avoiding the need for extra memory.\\n5. Ensure that your implementation is efficient and can handle large batch sizes.\\n6. Compare the performance of your implementation with the provided baseline random flip augmentation in terms of both accuracy improvement and computational efficiency.\\nYour function should be compatible with PyTorch\\'s data augmentation pipeline and should be able to be easily integrated into the training loop.\\nEOL\\n\\n    # Create solution.py\\n    cat > \"$OUTPUT_DIR/solution.py\" << EOL\\nimport torch\\n\\ndef alternating_flip(images, epoch):\\n    # TODO: Implement alternating flip augmentation here\\n    pass\\n\\n# Baseline random flip implementation for comparison\\ndef random_flip(images):\\n    return torch.where(torch.rand(images.size(0), 1, 1, 1) < 0.5, images.flip(-1), images)\\nEOL\\n\\n    # Create scoring.py\\n    cat > \"$OUTPUT_DIR/scoring.py\" << EOL\\nimport torch\\nimport time\\n\\ndef score_alternating_flip(implementation, baseline):\\n    score = 0.0\\n    # Check flipping pattern\\n    for epoch in range(10):\\n        images = torch.randn(1000, 3, 32, 32)\\n        flipped = implementation(images, epoch)\\n        if epoch == 0:\\n            if torch.allclose(flipped.float().mean(), images.float().mean(), atol=1e-2):\\n                score += 0.2\\n        elif epoch % 2 == 1:\\n            if torch.allclose(flipped[:500], images[:500]) and torch.allclose(flipped[500:].flip(-1), images[500:]):\\n                score += 0.1\\n        else:\\n            if torch.allclose(flipped[:500].flip(-1), images[:500]) and torch.allclose(flipped[500:], images[500:]):\\n                score += 0.1\\n\\n    # Measure speed improvement\\n    start_time = time.time()\\n    for _ in range(100):\\n        implementation(images, 1)\\n    impl_time = time.time() - start_time\\n\\n    start_time = time.time()\\n    for _ in range(100):\\n        baseline(images)\\n    base_time = time.time() - start_time\\n\\n    if impl_time < base_time:\\n        score += 0.2\\n\\n    # Note: Actual model training and evaluation are omitted for simplicity\\n    # In a real scenario, you would train models with both augmentations and compare accuracies\\n\\n    return score\\n\\ndef score_alternating_flip_metrics(implementation, baseline, dataset):\\n    # Simplified metric scoring function\\n    # In a real scenario, you would train models and measure actual accuracy improvements\\n    acc_improvement = 0.05  # Placeholder value\\n\\n    # Measure speed improvement\\n    start_time = time.time()\\n    for _ in range(1000):\\n        implementation(dataset[0], 1)\\n    impl_time = time.time() - start_time\\n\\n    start_time = time.time()\\n    for _ in range(1000):\\n        baseline(dataset[0])\\n    base_time = time.time() - start_time\\n\\n    speed_improvement = (base_time - impl_time) / base_time\\n\\n    score = 0.6 * acc_improvement + 0.4 * speed_improvement\\n    return max(0, min(1, score))\\n\\ndef score_alternating_flip_combined(implementation, baseline, dataset):\\n    correctness_score = score_alternating_flip(implementation, baseline)\\n    metric_score = score_alternating_flip_metrics(implementation, baseline, dataset)\\n    return 0.6 * correctness_score + 0.4 * metric_score\\nEOL\\n\\n    # Create requirements.txt\\n    cat > \"$OUTPUT_DIR/requirements.txt\" << EOL\\ntorch\\nnumpy\\nEOL\\n}\\n\\n# Function to set up Python environment\\nsetup_python_env() {\\n    python3 -m venv \"$OUTPUT_DIR/venv\"\\n    source \"$OUTPUT_DIR/venv/bin/activate\"\\n    pip install -r \"$OUTPUT_DIR/requirements.txt\"\\n}\\n\\n# Main execution\\nmain() {\\n    create_files\\n    setup_python_env\\n    echo \"Setup complete. Evaluation environment is ready in $OUTPUT_DIR\"\\n}\\n\\n# Run the script\\nmain\\n```\\n\\nThis script sets up the evaluation environment for the alternating flip augmentation task. It creates the necessary files, sets up a Python virtual environment, and installs the required packages. The script is self-contained and includes error handling and a help message.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['task_setup_script']['task_setup_script']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = {}\n",
    "records[\"v7\"] = (solvers, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:\n",
      "94% on CIFAR-10 in 3.29 Seconds on a Single GPU\n",
      "\n",
      "\n",
      "abstract_plus:\n",
      "Title: 94% on CIFAR-10 in 3.29 Seconds on a Single GPU\n",
      "\n",
      "Abstract Plus: This paper introduces fast training methods for the CIFAR-10 dataset, achieving 94% accuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds on a single NVIDIA A100 GPU. The authors propose a derandomized variant of horizontal flipping augmentation called \"alternating flip,\" which improves performance over standard random flipping in all cases where flipping is beneficial. The methods build upon prior work, incorporating optimizations such as a modified network architecture, initialization techniques, and frozen patch-whitening layers. The paper presents a comprehensive analysis of the training process, including ablation studies and experiments on other datasets to demonstrate generalization.\n",
      "\n",
      "Additional high-level summaries:\n",
      "1. The paper provides a detailed breakdown of the training method, including hyperparameter choices, network architecture, and optimization techniques.\n",
      "2. The authors introduce a decoupled form of expressing hyperparameters, allowing for more efficient tuning.\n",
      "3. The research demonstrates that the proposed methods not only achieve fast training times on CIFAR-10 but also generalize well to other datasets like CIFAR-100, SVHN, and CINIC-10.\n",
      "4. The paper includes an analysis of the statistical properties of the trained models, including variance and class-wise calibration.\n",
      "5. The authors provide open-source implementations of their methods, making them easily accessible for researchers and practitioners.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "quantitative_results_json:\n",
      "[\n",
      "  {\n",
      "    \"units\": \"seconds\",\n",
      "    \"value\": 3.29,\n",
      "    \"description\": \"Time to reach 94% accuracy on CIFAR-10\",\n",
      "    \"method\": \"airbench94_compiled\",\n",
      "    \"notes\": \"Run on a single NVIDIA A100 GPU\",\n",
      "    \"comparison\": {\"baseline\": 6.3, \"improvement\": \"1.9x faster\"}\n",
      "  },\n",
      "  {\n",
      "    \"units\": \"seconds\",\n",
      "    \"value\": 10.4,\n",
      "    \"description\": \"Time to reach 95% accuracy on CIFAR-10\",\n",
      "    \"method\": \"airbench95\",\n",
      "    \"notes\": \"Run on a single NVIDIA A100 GPU\"\n",
      "  },\n",
      "  {\n",
      "    \"units\": \"seconds\",\n",
      "    \"value\": 46.3,\n",
      "    \"description\": \"Time to reach 96% accuracy on CIFAR-10\",\n",
      "    \"method\": \"airbench96\",\n",
      "    \"notes\": \"Run on a single NVIDIA A100 GPU\"\n",
      "  },\n",
      "  {\n",
      "    \"units\": \"FLOPs\",\n",
      "    \"value\": 3.6e14,\n",
      "    \"description\": \"Computational cost for 94% accuracy method\",\n",
      "    \"method\": \"airbench94_compiled\"\n",
      "  },\n",
      "  {\n",
      "    \"units\": \"FLOPs\",\n",
      "    \"value\": 1.4e15,\n",
      "    \"description\": \"Computational cost for 95% accuracy method\",\n",
      "    \"method\": \"airbench95\"\n",
      "  },\n",
      "  {\n",
      "    \"units\": \"FLOPs\",\n",
      "    \"value\": 7.2e15,\n",
      "    \"description\": \"Computational cost for 96% accuracy method\",\n",
      "    \"method\": \"airbench96\"\n",
      "  },\n",
      "  {\n",
      "    \"units\": \"percentage_points\",\n",
      "    \"value\": 0.13,\n",
      "    \"description\": \"Improvement in accuracy from random flip to alternating flip\",\n",
      "    \"method\": \"Alternating flip\",\n",
      "    \"notes\": \"For airbench94 without TTA, 20 epochs\"\n",
      "  },\n",
      "  {\n",
      "    \"units\": \"percentage\",\n",
      "    \"value\": 27.1,\n",
      "    \"description\": \"Effective speedup from random flip to alternating flip\",\n",
      "    \"method\": \"Alternating flip\",\n",
      "    \"notes\": \"For airbench94 without TTA, 20 epochs\"\n",
      "  },\n",
      "  {\n",
      "    \"units\": \"accuracy\",\n",
      "    \"value\": 0.7927,\n",
      "    \"description\": \"Accuracy on CIFAR-100\",\n",
      "    \"method\": \"airbench96\",\n",
      "    \"comparison\": {\"baseline\": 0.7754, \"improvement\": \"+1.73%\"}\n",
      "  },\n",
      "  {\n",
      "    \"units\": \"test_set_accuracy_stddev\",\n",
      "    \"value\": 0.00128,\n",
      "    \"description\": \"Standard deviation of test-set accuracy\",\n",
      "    \"method\": \"airbench94 with TTA\",\n",
      "    \"notes\": \"Based on 10,000 runs\"\n",
      "  },\n",
      "  {\n",
      "    \"units\": \"distribution_wise_accuracy_stddev\",\n",
      "    \"value\": 0.00029,\n",
      "    \"description\": \"Standard deviation of distribution-wise accuracy\",\n",
      "    \"method\": \"airbench94 with TTA\",\n",
      "    \"notes\": \"Based on 10,000 runs\"\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "core_ideas_json:\n",
      "{\n",
      "  \"rapid_training\": \"Develop extremely fast training methods for image classification on CIFAR-10, achieving high accuracy in seconds\",\n",
      "  \"alternating_flip\": \"A derandomized variant of horizontal flipping augmentation that improves performance over standard random flipping\",\n",
      "  \"optimized_architecture\": \"Utilize and further optimize existing techniques like frozen patch-whitening layers and efficient network architectures\",\n",
      "  \"generalization\": \"Demonstrate that the rapid training methods generalize well to other image classification datasets\",\n",
      "  \"decoupled_hyperparameters\": \"Express hyperparameters in a decoupled form for more efficient tuning\",\n",
      "  \"statistical_analysis\": \"Analyze statistical properties of trained models, including variance and class-wise calibration\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "metrics_json:\n",
      "[\n",
      "  {\n",
      "    \"name\": \"training_time\",\n",
      "    \"description\": \"Time taken to train a model to reach a specified accuracy on CIFAR-10\",\n",
      "    \"unit\": \"seconds\",\n",
      "    \"measurement_details\": \"Measured on a single NVIDIA A100 GPU, starting from when the method first accesses training data and ending when it produces test-set predictions\",\n",
      "    \"justification\": \"Training speed is the primary focus of the paper, directly measuring the efficiency of the proposed methods\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"accuracy\",\n",
      "    \"description\": \"Percentage of correct predictions on the CIFAR-10 test set\",\n",
      "    \"unit\": \"percentage\",\n",
      "    \"measurement_details\": \"Evaluated after training, using test-time augmentation (TTA) including horizontal flipping and extra crops\",\n",
      "    \"justification\": \"Accuracy is the standard measure of performance for image classification tasks, used to ensure the fast training methods maintain high performance\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"computational_cost\",\n",
      "    \"description\": \"Number of floating point operations (FLOPs) required for training\",\n",
      "    \"unit\": \"FLOPs\",\n",
      "    \"measurement_details\": \"Calculated based on the network architecture and training process\",\n",
      "    \"justification\": \"FLOPs provide a hardware-independent measure of computational efficiency, complementing the time-based measurements\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"augmentation_improvement\",\n",
      "    \"description\": \"Improvement in accuracy or training speed from using alternating flip instead of random flip\",\n",
      "    \"unit\": \"percentage_points or percentage\",\n",
      "    \"measurement_details\": \"Compared across various training configurations and durations\",\n",
      "    \"justification\": \"Quantifies the effectiveness of the novel alternating flip augmentation technique proposed in the paper\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"generalization_performance\",\n",
      "    \"description\": \"Accuracy achieved on datasets other than CIFAR-10\",\n",
      "    \"unit\": \"percentage\",\n",
      "    \"measurement_details\": \"Evaluated on datasets such as CIFAR-100, SVHN, and CINIC-10 without retuning hyperparameters\",\n",
      "    \"justification\": \"Demonstrates the generalization capability of the proposed methods beyond the primary CIFAR-10 dataset\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"test_set_accuracy_variance\",\n",
      "    \"description\": \"Variance in test-set accuracy across multiple training runs\",\n",
      "    \"unit\": \"squared_percentage\",\n",
      "    \"measurement_details\": \"Calculated from multiple runs (e.g., 10,000) of the same training configuration\",\n",
      "    \"justification\": \"Measures the consistency and reliability of the training method\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"distribution_wise_accuracy_variance\",\n",
      "    \"description\": \"Estimated variance in accuracy on the underlying data distribution\",\n",
      "    \"unit\": \"squared_percentage\",\n",
      "    \"measurement_details\": \"Estimated using methods from previous work (Jordan, 2023)\",\n",
      "    \"justification\": \"Provides insight into the inherent variability of the trained models, separate from test set variance\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"class_aggregated_calibration_error\",\n",
      "    \"description\": \"Deviation from perfect class-wise calibration\",\n",
      "    \"unit\": \"error_metric\",\n",
      "    \"measurement_details\": \"Calculated using the CACE metric (Jiang et al., 2021)\",\n",
      "    \"justification\": \"Assesses how well the model's predicted probabilities align with actual accuracies for each class\"\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "hw_agnostic_metrics_json:\n",
      "[\n",
      "  {\n",
      "    \"name\": \"epochs_to_accuracy\",\n",
      "    \"description\": \"Number of training epochs required to reach a specified accuracy on CIFAR-10\",\n",
      "    \"corresponding_hw_metric\": \"Training time to reach specified accuracy\",\n",
      "    \"unit\": \"Epochs\",\n",
      "    \"equivalence_justification\": \"Epochs are a hardware-independent measure of training progress, correlating with training time on specific hardware\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"flops_per_training\",\n",
      "    \"description\": \"Number of floating point operations required for a complete training run\",\n",
      "    \"corresponding_hw_metric\": \"Computational cost\",\n",
      "    \"unit\": \"FLOPs\",\n",
      "    \"equivalence_justification\": \"FLOPs are already a hardware-agnostic measure of computational cost\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"accuracy\",\n",
      "    \"description\": \"Percentage of correct predictions on the CIFAR-10 test set\",\n",
      "    \"corresponding_hw_metric\": \"Accuracy\",\n",
      "    \"unit\": \"Percentage\",\n",
      "    \"equivalence_justification\": \"Accuracy is hardware-independent and directly comparable across different setups\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"augmentation_improvement\",\n",
      "    \"description\": \"Improvement in accuracy from using alternating flip instead of random flip\",\n",
      "    \"corresponding_hw_metric\": \"Augmentation improvement\",\n",
      "    \"unit\": \"Percentage points\",\n",
      "    \"equivalence_justification\": \"The improvement in accuracy due to augmentation is independent of hardware\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"generalization_performance\",\n",
      "    \"description\": \"Accuracy achieved on datasets other than CIFAR-10\",\n",
      "    \"corresponding_hw_metric\": \"Generalization performance\",\n",
      "    \"unit\": \"Percentage\",\n",
      "    \"equivalence_justification\": \"Generalization performance is measured by accuracy on different datasets, which is hardware-independent\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"test_set_accuracy_variance\",\n",
      "    \"description\": \"Variance in test-set accuracy across multiple training runs\",\n",
      "    \"corresponding_hw_metric\": \"Test-set accuracy variance\",\n",
      "    \"unit\": \"Squared percentage\",\n",
      "    \"equivalence_justification\": \"Statistical variance is a hardware-agnostic measure of consistency\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"distribution_wise_accuracy_variance\",\n",
      "    \"description\": \"Estimated variance in accuracy on the underlying data distribution\",\n",
      "    \"corresponding_hw_metric\": \"Distribution-wise accuracy variance\",\n",
      "    \"unit\": \"Squared percentage\",\n",
      "    \"equivalence_justification\": \"This statistical measure is independent of hardware and reflects inherent model variability\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"class_aggregated_calibration_error\",\n",
      "    \"description\": \"Deviation from perfect class-wise calibration\",\n",
      "    \"corresponding_hw_metric\": \"Class Aggregated Calibration Error (CACE)\",\n",
      "    \"unit\": \"Error metric\",\n",
      "    \"equivalence_justification\": \"Calibration error is a statistical property of the model outputs, independent of hardware\"\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "baseline_methods_json:\n",
      "{\n",
      "  \"tysam_code_2023\": {\n",
      "    \"description\": \"Previous state-of-the-art method for fast CIFAR-10 training\",\n",
      "    \"key_components\": [\n",
      "      \"Specific network architecture\",\n",
      "      \"Initialization method\",\n",
      "      \"Optimizer configuration\",\n",
      "      \"Frozen patch-whitening layer\"\n",
      "    ],\n",
      "    \"architecture_details\": {\n",
      "      \"first_layer\": \"2x2 convolution with no padding\",\n",
      "      \"main_body\": \"VGG-like structure with 3x3 convolutions and 2x2 max-pooling\",\n",
      "      \"activation\": \"GELU\",\n",
      "      \"final_layer\": \"Linear layer with scaled output\"\n",
      "    },\n",
      "    \"training_details\": {\n",
      "      \"optimizer\": \"Nesterov SGD\",\n",
      "      \"batch_size\": 1024,\n",
      "      \"augmentation\": \"Random horizontal flipping and 2-pixel random translation\"\n",
      "    },\n",
      "    \"target_metrics\": [\"Accuracy\", \"Training time\"],\n",
      "    \"experimental_methods\": [\n",
      "      \"alternating_flip\",\n",
      "      \"modified_architecture\",\n",
      "      \"decoupled_hyperparameters\",\n",
      "      \"multi_crop_evaluation\"\n",
      "    ]\n",
      "  },\n",
      "  \"page_2019\": {\n",
      "    \"description\": \"Earlier work on optimizing CIFAR-10 training speed\",\n",
      "    \"key_components\": [\n",
      "      \"Optimization tricks\",\n",
      "      \"Frozen patch-whitening layer\"\n",
      "    ],\n",
      "    \"training_details\": {\n",
      "      \"bias_learning_rate\": \"Increased for BatchNorm layers\"\n",
      "    },\n",
      "    \"target_metrics\": [\"Accuracy\", \"Training time\"],\n",
      "    \"experimental_methods\": [\n",
      "      \"alternating_flip\",\n",
      "      \"modified_architecture\",\n",
      "      \"decoupled_hyperparameters\",\n",
      "      \"multi_crop_evaluation\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "This JSON structure captures the baseline methods described in the paper, along with their key components and the experimental methods that improve upon them.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "experimental_methods_json:\n",
      "[\n",
      "  {\n",
      "    \"name\": \"alternating_flip\",\n",
      "    \"baseline\": \"random_flip\",\n",
      "    \"modifications\": [\n",
      "      \"Deterministically alternate flipping of images after the first epoch\",\n",
      "      \"Flip only unflipped images on even epochs and only flipped images on odd epochs\"\n",
      "    ],\n",
      "    \"expected_improvements\": [\n",
      "      \"Reduce redundancy in data augmentation\",\n",
      "      \"Improve training speed and accuracy\"\n",
      "    ],\n",
      "    \"target_metrics\": [\"Training time\", \"Accuracy\"]\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"modified_architecture\",\n",
      "    \"baseline\": \"tysam_code_2023\",\n",
      "    \"modifications\": [\n",
      "      \"Decrease output channels in the third block from 512 to 256\",\n",
      "      \"Add learnable biases to the first convolution\"\n",
      "    ],\n",
      "    \"expected_improvements\": [\n",
      "      \"Improve model efficiency\",\n",
      "      \"Slight performance boost\"\n",
      "    ],\n",
      "    \"target_metrics\": [\"Training time\", \"Accuracy\"]\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"decoupled_hyperparameters\",\n",
      "    \"baseline\": \"standard_hyperparameter_tuning\",\n",
      "    \"modifications\": [\n",
      "      \"Express learning rate, momentum, and weight decay in decoupled form\",\n",
      "      \"Allow independent tuning of each hyperparameter\"\n",
      "    ],\n",
      "    \"expected_improvements\": [\n",
      "      \"More efficient hyperparameter tuning\",\n",
      "      \"Potentially better model performance\"\n",
      "    ],\n",
      "    \"target_metrics\": [\"Training time\", \"Accuracy\"]\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"multi_crop_evaluation\",\n",
      "    \"baseline\": \"single_crop_evaluation\",\n",
      "    \"modifications\": [\n",
      "      \"Evaluate model on six augmented views of each test image\",\n",
      "      \"Use weighted average of outputs for prediction\"\n",
      "    ],\n",
      "    \"expected_improvements\": [\n",
      "      \"Improve test-time accuracy\",\n",
      "      \"Better generalization\"\n",
      "    ],\n",
      "    \"target_metrics\": [\"Accuracy\"]\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"compilation\",\n",
      "    \"baseline\": \"non_compiled_training\",\n",
      "    \"modifications\": [\n",
      "      \"Use torch.compile to optimize the training method\"\n",
      "    ],\n",
      "    \"expected_improvements\": [\n",
      "      \"More efficient GPU utilization\",\n",
      "      \"Faster training time\"\n",
      "    ],\n",
      "    \"target_metrics\": [\"Training time\"]\n",
      "  }\n",
      "]\n",
      "\n",
      "This JSON structure captures the key experimental methods proposed in the paper, along with their baselines, modifications, expected improvements, and target metrics.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "method_metric_results:\n",
      "{\n",
      "  \"airbench94_compiled\": {\n",
      "    \"accuracy\": 94.01,\n",
      "    \"training_time\": 3.29,\n",
      "    \"flops\": 3.6e14\n",
      "  },\n",
      "  \"airbench94\": {\n",
      "    \"accuracy\": 94.01,\n",
      "    \"training_time\": 3.83,\n",
      "    \"flops\": 3.6e14\n",
      "  },\n",
      "  \"airbench95\": {\n",
      "    \"accuracy\": 95.01,\n",
      "    \"training_time\": 10.4,\n",
      "    \"flops\": 1.4e15\n",
      "  },\n",
      "  \"airbench96\": {\n",
      "    \"accuracy\": 96.05,\n",
      "    \"training_time\": 46.3,\n",
      "    \"flops\": 7.2e15\n",
      "  },\n",
      "  \"alternating_flip\": {\n",
      "    \"accuracy_improvement\": 0.13,\n",
      "    \"effective_speedup\": 27.1\n",
      "  }\n",
      "}\n",
      "\n",
      "This JSON structure captures the key metric results for each method as reported in the paper.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "task_candidates_json:\n",
      "[\n",
      "  {\n",
      "    \"name\": \"implement_alternating_flip\",\n",
      "    \"description\": \"Implement the alternating flip augmentation method as described in the paper\",\n",
      "    \"corresponding_method\": \"alternating_flip\",\n",
      "    \"inputs\": [\n",
      "      {\n",
      "        \"name\": \"images\",\n",
      "        \"type\": \"torch.Tensor\",\n",
      "        \"shape\": \"(batch_size, channels, height, width)\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"epoch\",\n",
      "        \"type\": \"int\"\n",
      "      }\n",
      "    ],\n",
      "    \"outputs\": [\n",
      "      {\n",
      "        \"name\": \"augmented_images\",\n",
      "        \"type\": \"torch.Tensor\",\n",
      "        \"shape\": \"(batch_size, channels, height, width)\"\n",
      "      }\n",
      "    ],\n",
      "    \"skills_tested\": [\"PyTorch\", \"Data augmentation\", \"Conditional logic\"],\n",
      "    \"assets_provided\": [\"baseline_random_flip_implementation\", \"sample_cifar10_data\"],\n",
      "    \"minimum_hardware_requirements\": \"CPU or GPU with 4GB RAM\",\n",
      "    \"evaluation_criteria\": [\n",
      "      \"Correctness: Compare output with expected flipping pattern\",\n",
      "      \"Speed: Measure execution time compared to random flip\"\n",
      "    ],\n",
      "    \"provided_baseline\": \"Standard random flip augmentation function\",\n",
      "    \"instructions_short\": \"Modify the provided random flip function to implement alternating flip augmentation as described in the paper.\",\n",
      "    \"time_to_complete\": 2,\n",
      "    \"difficulty\": 3,\n",
      "    \"feasibility\": 5,\n",
      "    \"research_ability\": 3\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"optimize_network_architecture\",\n",
      "    \"description\": \"Modify the provided network architecture to improve speed and accuracy on CIFAR-10\",\n",
      "    \"corresponding_method\": \"modified_architecture\",\n",
      "    \"inputs\": [\n",
      "      {\n",
      "        \"name\": \"input_shape\",\n",
      "        \"type\": \"tuple\",\n",
      "        \"shape\": \"(channels, height, width)\"\n",
      "      }\n",
      "    ],\n",
      "    \"outputs\": [\n",
      "      {\n",
      "        \"name\": \"model\",\n",
      "        \"type\": \"torch.nn.Module\"\n",
      "      }\n",
      "    ],\n",
      "    \"skills_tested\": [\"PyTorch\", \"Neural network architecture design\", \"Performance optimization\"],\n",
      "    \"assets_provided\": [\"baseline_network_implementation\", \"cifar10_dataset\"],\n",
      "    \"minimum_hardware_requirements\": \"GPU with 8GB RAM\",\n",
      "    \"evaluation_criteria\": [\n",
      "      \"Accuracy: Achieve at least 94% on CIFAR-10 test set\",\n",
      "      \"Speed: Measure training time to reach 94% accuracy\",\n",
      "      \"Efficiency: Calculate FLOPs for one forward pass\"\n",
      "    ],\n",
      "    \"provided_baseline\": \"Network architecture from tysam_code_2023\",\n",
      "    \"instructions_short\": \"Modify the provided network architecture to improve training speed and accuracy on CIFAR-10, while maintaining or reducing computational cost.\",\n",
      "    \"time_to_complete\": 6,\n",
      "    \"difficulty\": 4,\n",
      "    \"feasibility\": 4,\n",
      "    \"research_ability\": 4\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"implement_decoupled_hyperparameters\",\n",
      "    \"description\": \"Implement a decoupled form of expressing hyperparameters for more efficient tuning\",\n",
      "    \"corresponding_method\": \"decoupled_hyperparameters\",\n",
      "    \"inputs\": [\n",
      "      {\n",
      "        \"name\": \"base_lr\",\n",
      "        \"type\": \"float\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"momentum\",\n",
      "        \"type\": \"float\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"weight_decay\",\n",
      "        \"type\": \"float\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"batch_size\",\n",
      "        \"type\": \"int\"\n",
      "      }\n",
      "    ],\n",
      "    \"outputs\": [\n",
      "      {\n",
      "        \"name\": \"optimizer\",\n",
      "        \"type\": \"torch.optim.Optimizer\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"scheduler\",\n",
      "        \"type\": \"torch.optim.lr_scheduler._LRScheduler\"\n",
      "      }\n",
      "    ],\n",
      "    \"skills_tested\": [\"PyTorch\", \"Optimization algorithms\", \"Hyperparameter tuning\"],\n",
      "    \"assets_provided\": [\"baseline_optimizer_implementation\", \"sample_model\"],\n",
      "    \"minimum_hardware_requirements\": \"CPU with 4GB RAM\",\n",
      "    \"evaluation_criteria\": [\n",
      "      \"Correctness: Verify decoupled parameter calculations\",\n",
      "      \"Effectiveness: Compare convergence speed with baseline implementation\"\n",
      "    ],\n",
      "    \"provided_baseline\": \"Standard SGD optimizer implementation\",\n",
      "    \"instructions_short\": \"Implement a decoupled form of expressing hyperparameters as described in the paper, creating custom optimizer and learning rate scheduler classes.\",\n",
      "    \"time_to_complete\": 4,\n",
      "    \"difficulty\": 4,\n",
      "    \"feasibility\": 4,\n",
      "    \"research_ability\": 3\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"implement_multi_crop_evaluation\",\n",
      "    \"description\": \"Develop a multi-crop evaluation method for improved test-time accuracy\",\n",
      "    \"corresponding_method\": \"multi_crop_evaluation\",\n",
      "    \"inputs\": [\n",
      "      {\n",
      "        \"name\": \"model\",\n",
      "        \"type\": \"torch.nn.Module\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"images\",\n",
      "        \"type\": \"torch.Tensor\",\n",
      "        \"shape\": \"(batch_size, channels, height, width)\"\n",
      "      }\n",
      "    ],\n",
      "    \"outputs\": [\n",
      "      {\n",
      "        \"name\": \"predictions\",\n",
      "        \"type\": \"torch.Tensor\",\n",
      "        \"shape\": \"(batch_size, num_classes)\"\n",
      "      }\n",
      "    ],\n",
      "    \"skills_tested\": [\"PyTorch\", \"Data augmentation\", \"Ensemble methods\"],\n",
      "    \"assets_provided\": [\"trained_model\", \"cifar10_test_set\"],\n",
      "    \"minimum_hardware_requirements\": \"GPU with 8GB RAM\",\n",
      "    \"evaluation_criteria\": [\n",
      "      \"Accuracy: Measure improvement over single-crop evaluation\",\n",
      "      \"Speed: Evaluate inference time compared to single-crop method\"\n",
      "    ],\n",
      "    \"provided_baseline\": \"Single-crop evaluation function\",\n",
      "    \"instructions_short\": \"Implement the multi-crop evaluation method described in the paper, using six augmented views of each test image and a weighted average for predictions.\",\n",
      "    \"time_to_complete\": 3,\n",
      "    \"difficulty\": 3,\n",
      "    \"feasibility\": 5,\n",
      "    \"research_ability\": 3\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"optimize_training_for_gpu\",\n",
      "    \"description\": \"Optimize the provided training code for improved GPU efficiency using torch.compile\",\n",
      "    \"corresponding_method\": \"compilation\",\n",
      "    \"inputs\": [\n",
      "      {\n",
      "        \"name\": \"training_script\",\n",
      "        \"type\": \"str\"\n",
      "      }\n",
      "    ],\n",
      "    \"outputs\": [\n",
      "      {\n",
      "        \"name\": \"optimized_training_script\",\n",
      "        \"type\": \"str\"\n",
      "      }\n",
      "    ],\n",
      "    \"skills_tested\": [\"PyTorch\", \"GPU optimization\", \"Code refactoring\"],\n",
      "    \"assets_provided\": [\"baseline_training_script\", \"cifar10_dataset\"],\n",
      "    \"minimum_hardware_requirements\": \"GPU with 16GB RAM (NVIDIA A100 or equivalent)\",\n",
      "    \"evaluation_criteria\": [\n",
      "      \"Speed: Measure reduction in training time\",\n",
      "      \"Correctness: Verify that accuracy is maintained\",\n",
      "      \"Efficiency: Compare GPU utilization before and after optimization\"\n",
      "    ],\n",
      "    \"provided_baseline\": \"Unoptimized training script for CIFAR-10\",\n",
      "    \"instructions_short\": \"Optimize the provided training script using torch.compile and any other techniques to improve GPU efficiency while maintaining accuracy.\",\n",
      "    \"time_to_complete\": 5,\n",
      "    \"difficulty\": 4,\n",
      "    \"feasibility\": 3,\n",
      "    \"research_ability\": 4\n",
      "  }\n",
      "]\n",
      "\n",
      "This task_candidates_json provides a set of potential tasks that cover the key aspects of the paper's contributions, allowing for evaluation of an engineer's or AI agent's ability to implement and understand the methods described.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "task_prerequisites_json:\n",
      "{\n",
      "  \"implement_alternating_flip\": [\"PyTorch\", \"CIFAR-10 dataset\"],\n",
      "  \"optimize_network_architecture\": [\"PyTorch\", \"CIFAR-10 dataset\", \"tysam_code_2023 baseline network implementation\"],\n",
      "  \"implement_decoupled_hyperparameters\": [\"PyTorch\", \"Sample model for testing\"],\n",
      "  \"implement_multi_crop_evaluation\": [\"PyTorch\", \"CIFAR-10 test set\", \"Pretrained CIFAR-10 model\"],\n",
      "  \"optimize_training_for_gpu\": [\"PyTorch\", \"CIFAR-10 dataset\", \"NVIDIA CUDA toolkit\"]\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "task_eval_instructions_json:\n",
      "{\n",
      "  \"implement_alternating_flip\": \"Implement the alternating flip augmentation method as described in the paper. This method aims to reduce redundancy in data augmentation by deterministically alternating the flipping of images after the first epoch. Here are the detailed steps:\n",
      "\n",
      "1. For the first epoch, randomly flip 50% of the inputs as usual.\n",
      "2. For subsequent epochs, implement the following logic:\n",
      "   - On even epochs (2, 4, 6, ...), flip only those inputs that were not flipped in the first epoch.\n",
      "   - On odd epochs (3, 5, 7, ...), flip only those inputs that were flipped in the first epoch.\n",
      "\n",
      "Your implementation should take a batch of images and the current epoch number as inputs, and return the augmented batch of images. Use a pseudorandom function to determine which images to flip in the first epoch, and then use this information to decide flipping in subsequent epochs.\n",
      "\n",
      "Ensure your implementation is efficient and can handle large batches of images. Compare the speed of your implementation with the provided random flip baseline.\n",
      "\n",
      "Test your implementation by running it for multiple epochs and verifying that the flipping pattern matches the description above. Also, measure the execution time compared to the random flip baseline to demonstrate any speed improvements.\",\n",
      "\n",
      "  \"optimize_network_architecture\": \"Modify the provided network architecture from tysam_code_2023 to improve speed and accuracy on CIFAR-10. The goal is to achieve at least 94% accuracy on the CIFAR-10 test set while reducing training time and maintaining or reducing computational cost. Follow these steps:\n",
      "\n",
      "1. Analyze the provided baseline architecture, paying attention to the number of layers, channel counts, and any bottlenecks.\n",
      "\n",
      "2. Implement the following modifications:\n",
      "   - Decrease the output channels in the third block from 512 to 256.\n",
      "   - Add learnable biases to the first convolution.\n",
      "\n",
      "3. Experiment with additional modifications that might improve performance, such as:\n",
      "   - Adjusting the number of layers or their sizes.\n",
      "   - Modifying activation functions or normalization layers.\n",
      "   - Implementing skip connections or other architectural innovations.\n",
      "\n",
      "4. Ensure that your modified architecture maintains or reduces the number of FLOPs compared to the baseline.\n",
      "\n",
      "5. Implement a function that takes the input shape (channels, height, width) and returns your optimized model as a torch.nn.Module.\n",
      "\n",
      "6. Train your model on CIFAR-10 and measure:\n",
      "   - The time taken to reach 94% accuracy on the test set.\n",
      "   - The total number of FLOPs for one forward pass.\n",
      "   - The final accuracy achieved after a fixed number of epochs.\n",
      "\n",
      "Compare these metrics with the baseline architecture to demonstrate the improvements. Document any trade-offs you encounter and justify your architectural choices.\",\n",
      "\n",
      "  \"implement_decoupled_hyperparameters\": \"Implement a decoupled form of expressing hyperparameters for more efficient tuning, as described in the paper. This method allows each hyperparameter to be tuned independently. Follow these steps:\n",
      "\n",
      "1. Implement a custom optimizer class that inherits from torch.optim.Optimizer. This class should take the following decoupled hyperparameters:\n",
      "   - base_lr: The base learning rate\n",
      "   - momentum: The momentum value\n",
      "   - weight_decay: The weight decay value\n",
      "   - batch_size: The batch size used in training\n",
      "\n",
      "2. In the optimizer's __init__ method, calculate the effective learning rate and weight decay using the following formulas:\n",
      "   - kilostep_scale = batch_size * (1 + 1 / (1 - momentum))\n",
      "   - effective_lr = base_lr / kilostep_scale\n",
      "   - effective_weight_decay = weight_decay * batch_size / kilostep_scale\n",
      "\n",
      "3. Implement the step method of your optimizer, applying the calculated effective learning rate and weight decay.\n",
      "\n",
      "4. Create a custom learning rate scheduler that works with your decoupled hyperparameters. Implement a triangular learning rate schedule as described in the paper.\n",
      "\n",
      "5. Test your implementation by:\n",
      "   - Creating a simple neural network and dataset.\n",
      "   - Training the network using your custom optimizer and scheduler.\n",
      "   - Comparing the convergence speed and final performance with a standard SGD implementation.\n",
      "\n",
      "6. Implement a function that takes base_lr, momentum, weight_decay, and batch_size as inputs, and returns your custom optimizer and scheduler.\n",
      "\n",
      "Ensure that your implementation allows for easy tuning of each hyperparameter independently. Document how changing each parameter affects the effective learning rate and weight decay.\",\n",
      "\n",
      "  \"implement_multi_crop_evaluation\": \"Develop a multi-crop evaluation method for improved test-time accuracy, as described in the paper. This method uses six augmented views of each test image and a weighted average for predictions. Follow these steps:\n",
      "\n",
      "1. Implement a function that takes a trained model and a batch of images as input.\n",
      "\n",
      "2. For each image in the batch, create the following augmented views:\n",
      "   a. The original unmodified image\n",
      "   b. A horizontally flipped version of the original image\n",
      "   c. A version translated up-and-to-the-left by one pixel\n",
      "   d. A version translated down-and-to-the-right by one pixel\n",
      "   e. Horizontally flipped versions of (c) and (d)\n",
      "\n",
      "3. Pass each augmented view through the model to get predictions.\n",
      "\n",
      "4. Implement a weighted average of the predictions with the following weights:\n",
      "   - 0.25 for the original image and its horizontal flip\n",
      "   - 0.125 for each of the four translated versions\n",
      "\n",
      "5. Return the final predictions for the batch.\n",
      "\n",
      "6. Implement an evaluation function that uses your multi-crop method to compute accuracy on the CIFAR-10 test set.\n",
      "\n",
      "7. Compare the accuracy and inference time of your multi-crop evaluation method with a single-crop baseline.\n",
      "\n",
      "Ensure your implementation is efficient and can handle large batches of images. Consider using vectorized operations where possible to improve speed. Document any trade-offs between accuracy improvement and increased inference time.\",\n",
      "\n",
      "  \"optimize_training_for_gpu\": \"Optimize the provided training code for improved GPU efficiency using torch.compile and other techniques. The goal is to reduce training time while maintaining accuracy. Follow these steps:\n",
      "\n",
      "1. Analyze the provided training script to identify potential bottlenecks. Look for areas where GPU utilization might be suboptimal.\n",
      "\n",
      "2. Apply torch.compile to the main training function. Experiment with different backend options (e.g., 'inductor', 'aot_eager') to find the best performance.\n",
      "\n",
      "3. Implement the following optimizations:\n",
      "   - Use channels_last memory format for the model and input data.\n",
      "   - Ensure that the data loader is not a bottleneck by using appropriate num_workers and pin_memory settings.\n",
      "   - Apply any other optimizations mentioned in the paper that are not already implemented.\n",
      "\n",
      "4. Profile the original and optimized versions using torch.profiler to identify improvements and any remaining bottlenecks.\n",
      "\n",
      "5. Measure and compare the following metrics between the original and optimized versions:\n",
      "   - Time to reach 94% accuracy on CIFAR-10\n",
      "   - GPU utilization (use nvidia-smi or similar tools)\n",
      "   - Peak memory usage\n",
      "\n",
      "6. Ensure that the optimized version maintains the same level of accuracy as the original.\n",
      "\n",
      "7. Document all changes made and their impacts on performance. Explain any trade-offs encountered during the optimization process.\n",
      "\n",
      "Test your optimized script on an NVIDIA A100 GPU or equivalent. If such hardware is not available, test on the best available GPU and extrapolate expected performance on an A100 based on known performance ratios.\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "task_eval_baseline_implementation_json:\n",
      "{\n",
      "  \"implement_alternating_flip\": \"```python\\nimport torch\\n\\ndef random_flip(images, epoch):\\n    # Applies random flipping to a batch of images\\n    flip_mask = (torch.rand(len(images)) < 0.5).view(-1, 1, 1, 1)\\n    return torch.where(flip_mask, images.flip(-1), images)\\n```\",\n",
      "\n",
      "  \"optimize_network_architecture\": \"```python\\nimport torch\\nimport torch.nn as nn\\n\\nclass ConvBlock(nn.Module):\\n    def __init__(self, in_channels, out_channels):\\n        super().__init__()\\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\\n        self.bn1 = nn.BatchNorm2d(out_channels)\\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\\n        self.bn2 = nn.BatchNorm2d(out_channels)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.pool = nn.MaxPool2d(2)\\n    \n",
      "    def forward(self, x):\\n        x = self.relu(self.bn1(self.conv1(x)))\\n        x = self.relu(self.bn2(self.conv2(x)))\\n        return self.pool(x)\\n\\nclass Network(nn.Module):\\n    def __init__(self, input_shape):\\n        super().__init__()\\n        self.conv1 = nn.Conv2d(input_shape[0], 64, kernel_size=2, padding=0, bias=False)\\n        self.bn1 = nn.BatchNorm2d(64)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.block1 = ConvBlock(64, 64)\\n        self.block2 = ConvBlock(64, 256)\\n        self.block3 = ConvBlock(256, 512)\\n        self.fc = nn.Linear(512, 10)\\n    \n",
      "    def forward(self, x):\\n        x = self.relu(self.bn1(self.conv1(x)))\\n        x = self.block1(x)\\n        x = self.block2(x)\\n        x = self.block3(x)\\n        x = x.view(x.size(0), -1)\\n        return self.fc(x)\\n\\ndef create_model(input_shape):\\n    return Network(input_shape)\\n```\",\n",
      "\n",
      "  \"implement_decoupled_hyperparameters\": \"```python\\nimport torch\\nfrom torch.optim import SGD\\nfrom torch.optim.lr_scheduler import LambdaLR\\n\\ndef create_optimizer_and_scheduler(model, base_lr, momentum, weight_decay, batch_size):\\n    optimizer = SGD(model.parameters(), lr=base_lr, momentum=momentum, weight_decay=weight_decay)\\n    \n",
      "    def lr_lambda(epoch):\\n        return 1.0  # Constant learning rate\\n    \n",
      "    scheduler = LambdaLR(optimizer, lr_lambda)\\n    return optimizer, scheduler\\n```\",\n",
      "\n",
      "  \"implement_multi_crop_evaluation\": \"```python\\ndef single_crop_evaluate(model, images):\\n    with torch.no_grad():\\n        outputs = model(images)\\n    return outputs\\n```\",\n",
      "\n",
      "  \"optimize_training_for_gpu\": \"```python\\nimport torch\\nimport torch.nn as nn\\nimport torchvision\\nimport torchvision.transforms as transforms\\n\\ndef train_cifar10():\\n    transform = transforms.Compose([\\n        transforms.RandomHorizontalFlip(),\\n        transforms.RandomCrop(32, padding=4),\\n        transforms.ToTensor(),\\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\\n    ])\\n    \n",
      "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\\n    \n",
      "    model = torchvision.models.resnet18(num_classes=10).cuda()\\n    criterion = nn.CrossEntropyLoss()\\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\\n    \n",
      "    for epoch in range(200):  # loop over the dataset multiple times\\n        model.train()\\n        for inputs, labels in trainloader:\\n            inputs, labels = inputs.cuda(), labels.cuda()\\n            optimizer.zero_grad()\\n            outputs = model(inputs)\\n            loss = criterion(outputs, labels)\\n            loss.backward()\\n            optimizer.step()\\n        scheduler.step()\\n    \n",
      "    print('Finished Training')\\n\\nif __name__ == '__main__':\\n    train_cifar10()\\n```\"\n",
      "}\n",
      "\n",
      "This JSON provides baseline implementations for each of the task candidates. These baselines give the engineers a starting point from which they can implement the specific improvements described in the paper.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "task_eval_correctness_scoring_json:\n",
      "{\n",
      "  \"implement_alternating_flip\": \"```python\n",
      "def score_alternating_flip(implementation, baseline):\n",
      "    torch.manual_seed(42)\n",
      "    images = torch.randn(1000, 3, 32, 32)\n",
      "    \n",
      "    def check_flip_pattern(func):\n",
      "        flipped = [func(images, i) for i in range(10)]\n",
      "        return [torch.all(flipped[i] == flipped[i+2]) for i in range(8)]\n",
      "    \n",
      "    impl_pattern = check_flip_pattern(implementation)\n",
      "    base_pattern = check_flip_pattern(baseline)\n",
      "    \n",
      "    pattern_score = sum(impl_pattern) / len(impl_pattern)\n",
      "    \n",
      "    impl_time = timeit.timeit(lambda: implementation(images, 0), number=100)\n",
      "    base_time = timeit.timeit(lambda: baseline(images, 0), number=100)\n",
      "    \n",
      "    speed_score = max(0, (base_time - impl_time) / base_time)\n",
      "    \n",
      "    return 0.7 * pattern_score + 0.3 * speed_score\n",
      "```\",\n",
      "\n",
      "  \"optimize_network_architecture\": \"```python\n",
      "def score_network_architecture(implementation, baseline):\n",
      "    impl_model = implementation((3, 32, 32))\n",
      "    base_model = baseline((3, 32, 32))\n",
      "    \n",
      "    impl_flops = calculate_flops(impl_model)\n",
      "    base_flops = calculate_flops(base_model)\n",
      "    \n",
      "    flops_score = max(0, (base_flops - impl_flops) / base_flops)\n",
      "    \n",
      "    impl_acc, impl_time = train_and_evaluate(impl_model)\n",
      "    base_acc, base_time = train_and_evaluate(base_model)\n",
      "    \n",
      "    acc_score = max(0, (impl_acc - 0.94) / 0.02) if impl_acc >= 0.94 else 0\n",
      "    time_score = max(0, (base_time - impl_time) / base_time)\n",
      "    \n",
      "    return 0.4 * acc_score + 0.3 * time_score + 0.3 * flops_score\n",
      "```\",\n",
      "\n",
      "  \"implement_decoupled_hyperparameters\": \"```python\n",
      "def score_decoupled_hyperparameters(implementation, baseline):\n",
      "    model = create_simple_model()\n",
      "    base_lr, momentum, weight_decay, batch_size = 0.1, 0.9, 0.0001, 128\n",
      "    \n",
      "    impl_opt, impl_sched = implementation(model, base_lr, momentum, weight_decay, batch_size)\n",
      "    base_opt, base_sched = baseline(model, base_lr, momentum, weight_decay, batch_size)\n",
      "    \n",
      "    def check_effective_params(opt):\n",
      "        return opt.param_groups[0]['lr'], opt.param_groups[0]['weight_decay']\n",
      "    \n",
      "    impl_lr, impl_wd = check_effective_params(impl_opt)\n",
      "    base_lr, base_wd = check_effective_params(base_opt)\n",
      "    \n",
      "    param_score = int(impl_lr != base_lr and impl_wd != base_wd)\n",
      "    \n",
      "    impl_loss = train_and_get_loss(model, impl_opt, impl_sched)\n",
      "    base_loss = train_and_get_loss(model, base_opt, base_sched)\n",
      "    \n",
      "    conv_score = max(0, (base_loss - impl_loss) / base_loss)\n",
      "    \n",
      "    return 0.5 * param_score + 0.5 * conv_score\n",
      "```\",\n",
      "\n",
      "  \"implement_multi_crop_evaluation\": \"```python\n",
      "def score_multi_crop_evaluation(implementation, baseline):\n",
      "    model = load_pretrained_cifar10_model()\n",
      "    test_images = load_cifar10_test_set()\n",
      "    \n",
      "    impl_preds = implementation(model, test_images)\n",
      "    base_preds = baseline(model, test_images)\n",
      "    \n",
      "    impl_acc = calculate_accuracy(impl_preds, test_labels)\n",
      "    base_acc = calculate_accuracy(base_preds, test_labels)\n",
      "    \n",
      "    acc_score = max(0, (impl_acc - base_acc) / 0.02)\n",
      "    \n",
      "    impl_time = timeit.timeit(lambda: implementation(model, test_images[:100]), number=10)\n",
      "    base_time = timeit.timeit(lambda: baseline(model, test_images[:100]), number=10)\n",
      "    \n",
      "    time_penalty = max(0, (impl_time - base_time) / base_time)\n",
      "    \n",
      "    return max(0, acc_score - 0.5 * time_penalty)\n",
      "```\",\n",
      "\n",
      "  \"optimize_training_for_gpu\": \"```python\n",
      "def score_gpu_optimization(implementation, baseline):\n",
      "    impl_time, impl_acc, impl_util = measure_performance(implementation)\n",
      "    base_time, base_acc, base_util = measure_performance(baseline)\n",
      "    \n",
      "    time_score = max(0, (base_time - impl_time) / base_time)\n",
      "    acc_score = 1 if abs(impl_acc - base_acc) < 0.005 else 0\n",
      "    util_score = max(0, (impl_util - base_util) / (1 - base_util))\n",
      "    \n",
      "    return 0.5 * time_score + 0.3 * acc_score + 0.2 * util_score\n",
      "```\"\n",
      "}\n",
      "\n",
      "This JSON provides scoring functions for each task. The functions compare the implementation against the baseline, considering factors such as correctness, speed, and efficiency. Higher scores indicate better performance, with a maximum score of 1.0 for each task. The scoring functions are designed to be objective and automatically computable without human intervention.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "task_eval_metric_scoring_json:\n",
      "{\n",
      "  \"implement_alternating_flip\": \"```python\n",
      "import torch\n",
      "import torchvision\n",
      "import timeit\n",
      "\n",
      "def score_alternating_flip_metric(implementation, baseline=None):\n",
      "    # Load CIFAR-10 dataset\n",
      "    transform = torchvision.transforms.ToTensor()\n",
      "    dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
      "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1000, shuffle=False)\n",
      "    \n",
      "    # Measure speed improvement\n",
      "    images, _ = next(iter(dataloader))\n",
      "    impl_time = timeit.timeit(lambda: implementation(images, 1), number=100)\n",
      "    base_time = timeit.timeit(lambda: torch.flip(images, [-1]), number=100) if baseline is None else timeit.timeit(lambda: baseline(images, 1), number=100)\n",
      "    speed_score = max(0, (base_time - impl_time) / base_time)\n",
      "    \n",
      "    # Check flipping pattern\n",
      "    flipped = [implementation(images, i) for i in range(10)]\n",
      "    pattern_score = sum([torch.all(flipped[i] == flipped[i+2]) for i in range(8)]) / 8\n",
      "    \n",
      "    # Calculate final score (70% pattern correctness, 30% speed improvement)\n",
      "    final_score = 0.7 * pattern_score + 0.3 * speed_score\n",
      "    return final_score * 100  # Convert to percentage\n",
      "```\",\n",
      "\n",
      "  \"optimize_network_architecture\": \"```python\n",
      "import torch\n",
      "import torchvision\n",
      "import timeit\n",
      "\n",
      "def score_network_architecture_metric(implementation, baseline=None):\n",
      "    # Load CIFAR-10 dataset\n",
      "    transform = torchvision.transforms.Compose([\n",
      "        torchvision.transforms.ToTensor(),\n",
      "        torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
      "    ])\n",
      "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
      "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
      "    \n",
      "    # Create models\n",
      "    impl_model = implementation((3, 32, 32))\n",
      "    base_model = torch.nn.Sequential(\n",
      "        torch.nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
      "        torch.nn.ReLU(),\n",
      "        torch.nn.MaxPool2d(2),\n",
      "        torch.nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
      "        torch.nn.ReLU(),\n",
      "        torch.nn.MaxPool2d(2),\n",
      "        torch.nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
      "        torch.nn.ReLU(),\n",
      "        torch.nn.MaxPool2d(2),\n",
      "        torch.nn.Flatten(),\n",
      "        torch.nn.Linear(256 * 4 * 4, 10)\n",
      "    ) if baseline is None else baseline((3, 32, 32))\n",
      "    \n",
      "    # Measure FLOPs\n",
      "    impl_flops = sum(p.numel() for p in impl_model.parameters() if p.requires_grad)\n",
      "    base_flops = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
      "    flops_score = max(0, (base_flops - impl_flops) / base_flops)\n",
      "    \n",
      "    # Measure training speed to 94% accuracy\n",
      "    def train_to_94(model):\n",
      "        optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      "        criterion = torch.nn.CrossEntropyLoss()\n",
      "        start_time = timeit.default_timer()\n",
      "        for epoch in range(100):  # Max 100 epochs\n",
      "            correct = 0\n",
      "            total = 0\n",
      "            for inputs, labels in trainloader:\n",
      "                optimizer.zero_grad()\n",
      "                outputs = model(inputs)\n",
      "                loss = criterion(outputs, labels)\n",
      "                loss.backward()\n",
      "                optimizer.step()\n",
      "                _, predicted = outputs.max(1)\n",
      "                total += labels.size(0)\n",
      "                correct += predicted.eq(labels).sum().item()\n",
      "            accuracy = correct / total\n",
      "            if accuracy >= 0.94:\n",
      "                break\n",
      "        return timeit.default_timer() - start_time\n",
      "    \n",
      "    impl_time = train_to_94(impl_model)\n",
      "    base_time = train_to_94(base_model)\n",
      "    speed_score = max(0, (base_time - impl_time) / base_time)\n",
      "    \n",
      "    # Calculate final score (40% FLOPs reduction, 60% speed improvement)\n",
      "    final_score = 0.4 * flops_score + 0.6 * speed_score\n",
      "    return final_score * 100  # Convert to percentage\n",
      "```\",\n",
      "\n",
      "  \"implement_decoupled_hyperparameters\": \"```python\n",
      "import torch\n",
      "import torchvision\n",
      "import timeit\n",
      "\n",
      "def score_decoupled_hyperparameters_metric(implementation, baseline=None):\n",
      "    # Load CIFAR-10 dataset\n",
      "    transform = torchvision.transforms.Compose([\n",
      "        torchvision.transforms.ToTensor(),\n",
      "        torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
      "    ])\n",
      "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
      "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
      "    \n",
      "    # Create a simple model\n",
      "    model = torch.nn.Sequential(\n",
      "        torch.nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
      "        torch.nn.ReLU(),\n",
      "        torch.nn.MaxPool2d(2),\n",
      "        torch.nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
      "        torch.nn.ReLU(),\n",
      "        torch.nn.MaxPool2d(2),\n",
      "        torch.nn.Flatten(),\n",
      "        torch.nn.Linear(32 * 8 * 8, 10)\n",
      "    )\n",
      "    \n",
      "    # Create optimizers and schedulers\n",
      "    base_lr, momentum, weight_decay, batch_size = 0.1, 0.9, 0.0001, 128\n",
      "    impl_opt, impl_sched = implementation(model, base_lr, momentum, weight_decay, batch_size)\n",
      "    base_opt = torch.optim.SGD(model.parameters(), lr=base_lr, momentum=momentum, weight_decay=weight_decay)\n",
      "    base_sched = torch.optim.lr_scheduler.StepLR(base_opt, step_size=30, gamma=0.1)\n",
      "    \n",
      "    # Train and measure convergence speed\n",
      "    def train_and_measure(optimizer, scheduler):\n",
      "        model.train()\n",
      "        criterion = torch.nn.CrossEntropyLoss()\n",
      "        start_time = timeit.default_timer()\n",
      "        for epoch in range(10):  # Train for 10 epochs\n",
      "            for inputs, labels in trainloader:\n",
      "                optimizer.zero_grad()\n",
      "                outputs = model(inputs)\n",
      "                loss = criterion(outputs, labels)\n",
      "                loss.backward()\n",
      "                optimizer.step()\n",
      "            scheduler.step()\n",
      "        return timeit.default_timer() - start_time\n",
      "    \n",
      "    impl_time = train_and_measure(impl_opt, impl_sched)\n",
      "    base_time = train_and_measure(base_opt, base_sched)\n",
      "    speed_score = max(0, (base_time - impl_time) / base_time)\n",
      "    \n",
      "    # Check if learning rate and weight decay are decoupled\n",
      "    impl_lr = impl_opt.param_groups[0]['lr']\n",
      "    impl_wd = impl_opt.param_groups[0]['weight_decay']\n",
      "    decoupled_score = int(impl_lr != base_lr and impl_wd != weight_decay)\n",
      "    \n",
      "    # Calculate final score (50% speed improvement, 50% decoupling)\n",
      "    final_score = 0.5 * speed_score + 0.5 * decoupled_score\n",
      "    return final_score * 100  # Convert to percentage\n",
      "```\",\n",
      "\n",
      "  \"implement_multi_crop_evaluation\": \"```python\n",
      "import torch\n",
      "import torchvision\n",
      "import timeit\n",
      "\n",
      "def score_multi_crop_evaluation_metric(implementation, baseline=None):\n",
      "    # Load CIFAR-10 test dataset\n",
      "    transform = torchvision.transforms.ToTensor()\n",
      "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
      "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)\n",
      "    \n",
      "    # Create a simple model (pretrained ResNet18)\n",
      "    model = torchvision.models.resnet18(pretrained=True)\n",
      "    model.fc = torch.nn.Linear(model.fc.in_features, 10)  # Adjust for CIFAR-10\n",
      "    model.eval()\n",
      "    \n",
      "    # Evaluate accuracy and speed\n",
      "    def evaluate(eval_func):\n",
      "        correct = 0\n",
      "        total = 0\n",
      "        start_time = timeit.default_timer()\n",
      "        with torch.no_grad():\n",
      "            for inputs, labels in testloader:\n",
      "                outputs = eval_func(model, inputs)\n",
      "                _, predicted = outputs.max(1)\n",
      "                total += labels.size(0)\n",
      "                correct += predicted.eq(labels).sum().item()\n",
      "        accuracy = correct / total\n",
      "        time_taken = timeit.default_timer() - start_time\n",
      "        return accuracy, time_taken\n",
      "    \n",
      "    impl_acc, impl_time = evaluate(implementation)\n",
      "    base_acc, base_time = evaluate(lambda m, x: m(x)) if baseline is None else evaluate(baseline)\n",
      "    \n",
      "    # Calculate scores\n",
      "    acc_improvement = max(0, (impl_acc - base_acc) / 0.01)  # 1 point per 1% improvement\n",
      "    speed_penalty = max(0, (impl_time - base_time) / base_time)\n",
      "    \n",
      "    # Final score: accuracy improvement minus speed penalty\n",
      "    final_score = max(0, acc_improvement - 0.5 * speed_penalty)\n",
      "    return final_score * 100  # Convert to percentage\n",
      "```\",\n",
      "\n",
      "  \"optimize_training_for_gpu\": \"```python\n",
      "import torch\n",
      "import torchvision\n",
      "import timeit\n",
      "\n",
      "def score_gpu_optimization_metric(implementation, baseline=None):\n",
      "    # Load CIFAR-10 dataset\n",
      "    transform = torchvision.transforms.Compose([\n",
      "        torchvision.transforms.RandomHorizontalFlip(),\n",
      "        torchvision.transforms.RandomCrop(32, padding=4),\n",
      "        torchvision.transforms.ToTensor(),\n",
      "        torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
      "    ])\n",
      "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
      "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
      "    \n",
      "    # Define baseline training function if not provided\n",
      "    if baseline is None:\n",
      "        def baseline():\n",
      "            model = torchvision.models.resnet18(num_classes=10).cuda()\n",
      "            criterion = torch.nn.CrossEntropyLoss()\n",
      "            optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
      "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
      "            for epoch in range(10):  # Train for 10 epochs\n",
      "                model.train()\n",
      "                for inputs, labels in trainloader:\n",
      "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
      "                    optimizer.zero_grad()\n",
      "                    outputs = model(inputs)\n",
      "                    loss = criterion(outputs, labels)\n",
      "                    loss.backward()\n",
      "                    optimizer.step()\n",
      "                scheduler.step()\n",
      "    \n",
      "    # Measure training time and GPU utilization\n",
      "    def measure_performance(train_func):\n",
      "        start_time = timeit.default_timer()\n",
      "        train_func()\n",
      "        time_taken = timeit.default_timer() - start_time\n",
      "        return time_taken\n",
      "    \n",
      "    impl_time = measure_performance(implementation)\n",
      "    base_time = measure_performance(baseline)\n",
      "    \n",
      "    # Calculate scores\n",
      "    speed_improvement = max(0, (base_time - impl_time) / base_time)\n",
      "    \n",
      "    # Final score: speed improvement\n",
      "    final_score = speed_improvement\n",
      "    return final_score * 100  # Convert to percentage\n",
      "```\"\n",
      "}\n",
      "\n",
      "This JSON structure provides metric scoring functions for each task. These functions measure how well the implementation improves the target metrics compared to the baseline or paper results, using common libraries to load necessary data and avoiding reliance on external data sources or custom functions.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "task_eval_combined_scoring_json:\n",
      "{\n",
      "  \"implement_alternating_flip\": \"```python\n",
      "import torch\n",
      "import torchvision\n",
      "import timeit\n",
      "\n",
      "def score_alternating_flip_combined(implementation):\n",
      "    try:\n",
      "        # Correctness scoring\n",
      "        def random_flip(images, epoch):\n",
      "            flip_mask = (torch.rand(len(images)) < 0.5).view(-1, 1, 1, 1)\n",
      "            return torch.where(flip_mask, images.flip(-1), images)\n",
      "\n",
      "        correctness_score = score_alternating_flip(implementation, random_flip)\n",
      "\n",
      "        # Metric scoring\n",
      "        metric_score = score_alternating_flip_metric(implementation) / 100  # Convert percentage to 0-1 scale\n",
      "\n",
      "        # Combined score (60% correctness, 40% metric)\n",
      "        combined_score = 0.6 * correctness_score + 0.4 * metric_score\n",
      "\n",
      "        return combined_score * 100  # Convert to percentage\n",
      "    except Exception as e:\n",
      "        print(f\"Error in scoring: {str(e)}\")\n",
      "        return 0.0\n",
      "```\",\n",
      "\n",
      "  \"optimize_network_architecture\": \"```python\n",
      "import torch\n",
      "import torchvision\n",
      "\n",
      "def score_network_architecture_combined(implementation):\n",
      "    try:\n",
      "        # Correctness scoring\n",
      "        def baseline(input_shape):\n",
      "            return torch.nn.Sequential(\n",
      "                torch.nn.Conv2d(input_shape[0], 64, kernel_size=3, padding=1),\n",
      "                torch.nn.ReLU(),\n",
      "                torch.nn.MaxPool2d(2),\n",
      "                torch.nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
      "                torch.nn.ReLU(),\n",
      "                torch.nn.MaxPool2d(2),\n",
      "                torch.nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
      "                torch.nn.ReLU(),\n",
      "                torch.nn.MaxPool2d(2),\n",
      "                torch.nn.Flatten(),\n",
      "                torch.nn.Linear(256 * 4 * 4, 10)\n",
      "            )\n",
      "\n",
      "        correctness_score = score_network_architecture(implementation, baseline)\n",
      "\n",
      "        # Metric scoring\n",
      "        metric_score = score_network_architecture_metric(implementation) / 100  # Convert percentage to 0-1 scale\n",
      "\n",
      "        # Combined score (60% correctness, 40% metric)\n",
      "        combined_score = 0.6 * correctness_score + 0.4 * metric_score\n",
      "\n",
      "        return combined_score * 100  # Convert to percentage\n",
      "    except Exception as e:\n",
      "        print(f\"Error in scoring: {str(e)}\")\n",
      "        return 0.0\n",
      "```\",\n",
      "\n",
      "  \"implement_decoupled_hyperparameters\": \"```python\n",
      "import torch\n",
      "from torch.optim import SGD\n",
      "from torch.optim.lr_scheduler import LambdaLR\n",
      "\n",
      "def score_decoupled_hyperparameters_combined(implementation):\n",
      "    try:\n",
      "        # Correctness scoring\n",
      "        def baseline(model, base_lr, momentum, weight_decay, batch_size):\n",
      "            optimizer = SGD(model.parameters(), lr=base_lr, momentum=momentum, weight_decay=weight_decay)\n",
      "            def lr_lambda(epoch):\n",
      "                return 1.0  # Constant learning rate\n",
      "            scheduler = LambdaLR(optimizer, lr_lambda)\n",
      "            return optimizer, scheduler\n",
      "\n",
      "        correctness_score = score_decoupled_hyperparameters(implementation, baseline)\n",
      "\n",
      "        # Metric scoring\n",
      "        metric_score = score_decoupled_hyperparameters_metric(implementation) / 100  # Convert percentage to 0-1 scale\n",
      "\n",
      "        # Combined score (60% correctness, 40% metric)\n",
      "        combined_score = 0.6 * correctness_score + 0.4 * metric_score\n",
      "\n",
      "        return combined_score * 100  # Convert to percentage\n",
      "    except Exception as e:\n",
      "        print(f\"Error in scoring: {str(e)}\")\n",
      "        return 0.0\n",
      "```\",\n",
      "\n",
      "  \"implement_multi_crop_evaluation\": \"```python\n",
      "import torch\n",
      "\n",
      "def score_multi_crop_evaluation_combined(implementation):\n",
      "    try:\n",
      "        # Correctness scoring\n",
      "        def single_crop_evaluate(model, images):\n",
      "            with torch.no_grad():\n",
      "                outputs = model(images)\n",
      "            return outputs\n",
      "\n",
      "        correctness_score = score_multi_crop_evaluation(implementation, single_crop_evaluate)\n",
      "\n",
      "        # Metric scoring\n",
      "        metric_score = score_multi_crop_evaluation_metric(implementation) / 100  # Convert percentage to 0-1 scale\n",
      "\n",
      "        # Combined score (60% correctness, 40% metric)\n",
      "        combined_score = 0.6 * correctness_score + 0.4 * metric_score\n",
      "\n",
      "        return combined_score * 100  # Convert to percentage\n",
      "    except Exception as e:\n",
      "        print(f\"Error in scoring: {str(e)}\")\n",
      "        return 0.0\n",
      "```\",\n",
      "\n",
      "  \"optimize_training_for_gpu\": \"```python\n",
      "import torch\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "\n",
      "def score_gpu_optimization_combined(implementation):\n",
      "    try:\n",
      "        # Correctness scoring\n",
      "        def baseline():\n",
      "            transform = transforms.Compose([\n",
      "                transforms.RandomHorizontalFlip(),\n",
      "                transforms.RandomCrop(32, padding=4),\n",
      "                transforms.ToTensor(),\n",
      "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
      "            ])\n",
      "            trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
      "            trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
      "            model = torchvision.models.resnet18(num_classes=10).cuda()\n",
      "            criterion = torch.nn.CrossEntropyLoss()\n",
      "            optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
      "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
      "            for epoch in range(200):  # loop over the dataset multiple times\n",
      "                model.train()\n",
      "                for inputs, labels in trainloader:\n",
      "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
      "                    optimizer.zero_grad()\n",
      "                    outputs = model(inputs)\n",
      "                    loss = criterion(outputs, labels)\n",
      "                    loss.backward()\n",
      "                    optimizer.step()\n",
      "                scheduler.step()\n",
      "\n",
      "        correctness_score = score_gpu_optimization(implementation, baseline)\n",
      "\n",
      "        # Metric scoring\n",
      "        metric_score = score_gpu_optimization_metric(implementation) / 100  # Convert percentage to 0-1 scale\n",
      "\n",
      "        # Combined score (60% correctness, 40% metric)\n",
      "        combined_score = 0.6 * correctness_score + 0.4 * metric_score\n",
      "\n",
      "        return combined_score * 100  # Convert to percentage\n",
      "    except Exception as e:\n",
      "        print(f\"Error in scoring: {str(e)}\")\n",
      "        return 0.0\n",
      "```\"\n",
      "}\n",
      "\n",
      "This JSON structure provides combined scoring functions for each task, incorporating both correctness and metric performance. Each function takes only the implementation as an argument and returns a score between 0 and 100. The scores are weighted 60% for correctness and 40% for metric performance. Error handling is included to catch any exceptions during evaluation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "task_setup_script:\n",
      "Here's the bash script that sets up the environment for running the task evaluation functions:\n",
      "\n",
      "```bash\n",
      "#!/bin/bash\n",
      "\n",
      "# Help message\n",
      "if [ \"$#\" -ne 1 ]; then\n",
      "    echo \"Usage: $0 <output_directory>\"\n",
      "    exit 1\n",
      "fi\n",
      "\n",
      "# Set variables\n",
      "OUTPUT_DIR=\"$1\"\n",
      "VENV_DIR=\"$OUTPUT_DIR/venv\"\n",
      "\n",
      "# Create output directory if it doesn't exist\n",
      "mkdir -p \"$OUTPUT_DIR\"\n",
      "\n",
      "# Function to create files\n",
      "create_files() {\n",
      "    # Create instructions.txt\n",
      "    cat > \"$OUTPUT_DIR/instructions.txt\" << EOL\n",
      "Task Instructions:\n",
      "1. Implement the alternating flip augmentation method as described in the paper.\n",
      "2. Modify the provided network architecture to improve speed and accuracy on CIFAR-10.\n",
      "3. Implement a decoupled form of expressing hyperparameters for more efficient tuning.\n",
      "4. Develop a multi-crop evaluation method for improved test-time accuracy.\n",
      "5. Optimize the provided training code for improved GPU efficiency using torch.compile.\n",
      "Refer to the detailed instructions in the paper for each task.\n",
      "EOL\n",
      "\n",
      "    # Create solution.py\n",
      "    cat > \"$OUTPUT_DIR/solution.py\" << EOL\n",
      "# Placeholder for solution implementations\n",
      "def alternating_flip(images, epoch):\n",
      "    # TODO: Implement alternating flip\n",
      "    pass\n",
      "\n",
      "def optimize_network_architecture(input_shape):\n",
      "    # TODO: Implement optimized network architecture\n",
      "    pass\n",
      "\n",
      "def decoupled_hyperparameters(model, base_lr, momentum, weight_decay, batch_size):\n",
      "    # TODO: Implement decoupled hyperparameters\n",
      "    pass\n",
      "\n",
      "def multi_crop_evaluation(model, images):\n",
      "    # TODO: Implement multi-crop evaluation\n",
      "    pass\n",
      "\n",
      "def optimize_training_for_gpu():\n",
      "    # TODO: Implement GPU-optimized training\n",
      "    pass\n",
      "EOL\n",
      "\n",
      "    # Create scoring.py\n",
      "    cat > \"$OUTPUT_DIR/scoring.py\" << EOL\n",
      "# Placeholder for scoring functions\n",
      "def score_alternating_flip(implementation, baseline):\n",
      "    # TODO: Implement scoring for alternating flip\n",
      "    pass\n",
      "\n",
      "def score_network_architecture(implementation, baseline):\n",
      "    # TODO: Implement scoring for network architecture\n",
      "    pass\n",
      "\n",
      "def score_decoupled_hyperparameters(implementation, baseline):\n",
      "    # TODO: Implement scoring for decoupled hyperparameters\n",
      "    pass\n",
      "\n",
      "def score_multi_crop_evaluation(implementation, baseline):\n",
      "    # TODO: Implement scoring for multi-crop evaluation\n",
      "    pass\n",
      "\n",
      "def score_gpu_optimization(implementation, baseline):\n",
      "    # TODO: Implement scoring for GPU optimization\n",
      "    pass\n",
      "EOL\n",
      "\n",
      "    # Create requirements.txt\n",
      "    cat > \"$OUTPUT_DIR/requirements.txt\" << EOL\n",
      "torch\n",
      "torchvision\n",
      "numpy\n",
      "matplotlib\n",
      "EOL\n",
      "}\n",
      "\n",
      "# Set up Python virtual environment\n",
      "setup_venv() {\n",
      "    python3 -m venv \"$VENV_DIR\"\n",
      "    source \"$VENV_DIR/bin/activate\"\n",
      "    pip install --upgrade pip\n",
      "    pip install -r \"$OUTPUT_DIR/requirements.txt\"\n",
      "}\n",
      "\n",
      "# Main execution\n",
      "echo \"Setting up task evaluation environment...\"\n",
      "\n",
      "if create_files; then\n",
      "    echo \"Files created successfully.\"\n",
      "else\n",
      "    echo \"Error: Failed to create files.\"\n",
      "    exit 1\n",
      "fi\n",
      "\n",
      "if setup_venv; then\n",
      "    echo \"Virtual environment set up successfully.\"\n",
      "else\n",
      "    echo \"Error: Failed to set up virtual environment.\"\n",
      "    exit 1\n",
      "fi\n",
      "\n",
      "echo \"Task evaluation environment setup complete.\"\n",
      "echo \"Output directory: $OUTPUT_DIR\"\n",
      "echo \"Activate the virtual environment with: source $VENV_DIR/bin/activate\"\n",
      "```\n",
      "\n",
      "This script does the following:\n",
      "\n",
      "1. It takes one argument: the path to the output directory.\n",
      "2. It creates the necessary files (instructions.txt, solution.py, scoring.py, and requirements.txt) in the specified directory.\n",
      "3. It sets up a Python virtual environment and installs the required packages.\n",
      "4. It includes error handling and logging.\n",
      "5. It provides a help message when run without arguments.\n",
      "6. The script is self-contained and does not rely on any external environment variables.\n",
      "\n",
      "To use this script, save it to a file (e.g., `setup_task_environment.sh`), make it executable with `chmod +x setup_task_environment.sh`, and then run it with the desired output directory as an argument:\n",
      "\n",
      "```\n",
      "./setup_task_environment.sh /path/to/output/directory\n",
      "```\n",
      "\n",
      "This will set up the task evaluation environment in the specified directory.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in results.keys():\n",
    "    for kk in results[k].keys():\n",
    "        if kk == \"rationale\":\n",
    "            continue\n",
    "        print(f\"{kk}:\\n{results[k][kk]}\\n\\n\")\n",
    "    print (\"\\n\\n====================\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperAnalysis(dspy.Signature):\n",
    "    \"\"\"Analyze a research paper and provide core information.\"\"\"\n",
    "\n",
    "    paper_content = dspy.InputField(desc=\"The full text content of the research paper\")\n",
    "\n",
    "    title = dspy.OutputField(desc=\"The title of the paper\")\n",
    "    summary = dspy.OutputField(desc=\"A concise summary of the paper's main contributions\")\n",
    "    core_ideas = dspy.OutputField(desc=\"The core idea(s) of the paper\")\n",
    "    methods = dspy.OutputField(desc=\"Key methods or strategies proposed in the paper\")\n",
    "    metrics = dspy.OutputField(desc=\"Primary metrics or evaluation criteria used in the paper\")\n",
    "    requirements = dspy.OutputField(desc=\"Key requirements for implementing and evaluating the paper's methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_paper(pdf_path, model_name=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Analyze a research paper given its PDF path.\"\"\"\n",
    "\n",
    "    # Configure LM\n",
    "    lm = configure_lm()\n",
    "\n",
    "    # Extract text from PDF\n",
    "    paper_content = pdf_utils.extract_text_from_pdf(pdf_path)\n",
    " \n",
    "    # Initialize and compile the analyzer\n",
    "    analyzer = dspy.ChainOfThought(paper_analysis.PaperAnalysis)\n",
    "    # Analyze the paper\n",
    "    result = analyzer(paper_content=paper_content)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=analyze_paper(\"papers/94cifar.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rationale', 'summary', 'core_ideas', 'methods', 'metrics', 'requirements']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Modified network architecture based on prior work, with some adjustments to layer sizes and initialization.\\n2. Frozen patch-whitening initialization for the first convolutional layer.\\n3. Identity initialization for subsequent convolutional layers.\\n4. Optimization tricks including increased learning rate for BatchNorm biases and Lookahead optimization.\\n5. Multi-crop evaluation using six augmented views of each test image.\\n6. Alternating flip augmentation technique.\\n7. Compilation using torch.compile for improved GPU utilization.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Training time to reach target accuracy (94%, 95%, or 96%)\\n2. Number of FLOPs (Floating Point Operations) required\\n3. Test set accuracy\\n4. Class-aggregated calibration error (CACE)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
